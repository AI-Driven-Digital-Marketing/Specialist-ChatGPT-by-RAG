{"HC-ML": [{"text": "  [CLICK] DAVID SONTAG: So welcome\nto spring 2019 Machine Learning for Healthcare. My name is David Sontag. I'm a professor in\ncomputer science. Also I'm in the Institute\nfor Medical Engineering and Science. My co-instructor today\nwill be Pete Szolovits, who I'll introduce more towards\nthe end of today's lecture, along with the rest\nof the course staff. So the problem. The problem is that healthcare\nin the United States costs too much. Currently, we're spending\n$3 trillion a year, and we're not even necessarily\ndoing a very good job. Patients who have\nchronic disease often find that these chronic\ndiseases are diagnosed late. They're often not managed well.", "id": "vof7x8r_ZUA_0", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  of the world's best clinicians. Moreover, medical\nerrors are happening all of the time,\nerrors that if caught, would have prevented\nneedless deaths, needless worsening\nof disease, and more. And healthcare\nimpacts all of us. So I imagine that almost\neveryone here in this room have had a family member,\na loved one, a dear friend, or even themselves suffer\nfrom a health condition which impacts your quality of\nlife, which has affected your work, your\nstudies, and possibly has led to a needless death. And so the question that we're\nasking in this course today is how can we use\nmachine learning, artificial intelligence, as\none piece of a bigger puzzle to try to transform healthcare.", "id": "vof7x8r_ZUA_1", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  I myself have\npersonal stories that have led me to be\ninterested in this area. My grandfather, who had\nAlzheimer's disease, was diagnosed quite late\nin his Alzheimer's disease. There aren't good treatments\ntoday for Alzheimer's, and so it's not that I would\nhave expected the outcome to be different. But had he been\ndiagnosed earlier, our family would have\nrecognized that many of the erratic things\nthat he was doing towards the later years of his\nlife were due to this disease and not due to\nsome other reason. My mother, who had multiple\nmyeloma, a blood cancer, who was diagnosed five\nyears ago now, never started treatment\nfor her cancer before she died one year ago. Now, why did she die? Well, it was believed\nthat her cancer was still in its very early stages. Her blood markers that\nwere used to track the progress of the cancer put\nher in a low risk category.", "id": "vof7x8r_ZUA_2", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  of the disease that\nwould, according to today's standard\nguidelines, require treatment to be initiated. And as a result, the belief\nwas the best strategy was to wait and see. But unbeknownst to her and\nto my family, her blood cancer, which was caused\nby light chains which were accumulating, ended\nup leading to organ damage. In this case, the light chains\nwere accumulating in her heart, and she died of heart failure. Had we recognized that her\ndisease was further along, she might have\ninitiated treatment. And there are now over 20\ntreatments for multiple myeloma which are believed to have\nlife-lengthening effect. And I can give you four\nor five other stories from my own personal\nfamily and my friends, where similar things\nhave happened. And I have no doubt that\nall of you have as well. So what can we do about\nit is the question", "id": "vof7x8r_ZUA_3", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  And don't get me wrong. Machine learning,\nartificial intelligence, will only be one\npiece of the puzzle. There's so many other\nsystematic changes that we're going to have to\nmake into our healthcare system. But let's try to understand\nwhat those AI elements might be. So let's start in\ntoday's lecture by giving a bit of a background\non artificial intelligence and machine learning\nin healthcare. And I'll tell you why I think\nthe time is right now, in 2019, to really start to make a\nbig dent at this problem. And then I'll tell you about-- I'll give you a\nfew examples of how machine learning is likely\nto transform healthcare over the next decade. And of course we're\njust guessing, but this is really\nguided by the latest and greatest in research, a lot\nof it happening here at MIT. And then we'll close\ntoday's lecture with an overview of\nwhat's different, what's unique about machine\nlearning healthcare. All of you have taken some\nmachine learning course in the past, and so\nyou know the basics of supervised prediction. Many of you have studied\nthings like clustering.", "id": "vof7x8r_ZUA_4", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  to the news, where you\nsee news every single day about Google,\nFacebook, Microsoft's latest advances in speech\nrecognition, computer vision, and so on. So what's really\ndifferent about trying to apply these techniques\nin the healthcare domain? The answer is that there's\na huge amount of difference, and there are a lot\nof subtleties to doing machine learning right here. And we'll talk about\nthat throughout the whole entire semester. So to begin, this\nisn't a new field. Artificial intelligence in\nmedicine goes back to the 1970s or sometime even in the '60s. One of the earliest\nexamples of trying to use artificial\nintelligence for diagnosis was this MYCIN system developed\nat Stanford, where the goal was try to identify bacteria\nthat might cause infection and then to try to\nguide what would be the appropriate\ntherapy for that bacteria. Now, it was found that this\nalgorithm, this machine", "id": "vof7x8r_ZUA_5", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  was able to propose a good\ntherapy in 69% of cases, which at the time was better\nthan the best or very good infectious disease experts. Now, it also had some\nother elements of it which were quite interesting. So those of you\nwho've been following the latest in chat bots might\nfind this a little prescient. So there's a dialog interface\nwhere the computer says, I am ready. The computer clinician\nresponds, this is a 26-year-old male patient. The computer tests\nits understanding of what's going on. It says, the age of\nthe patient is 26. The sex of the patient is male. The clinician writes,\nfive days ago, he had respiratory\ntract infections. The computer asks\nwhat is his name. Joe. The computer responds,\nmy understanding is the name of the\npatient is Joe. Respiratory tract is one of\nthe symptoms the patient had.", "id": "vof7x8r_ZUA_6", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  before the admission,\nhe had malaise, which is general tiredness. And the computer\nresponds, please give me a date of admission. The clinician responds, March\n12, 1979, and the computer again confirms that it's\nunderstood appropriately. And this is the preface to\nthe later diagnostic stages. So the ideas of how AI\ncan really impact medicine have been around a long time. Yet these algorithms\nwhich have been shown to be very effective,\neven going back to the 1970s, didn't translate\ninto clinical care. A second example, oh so equally\nimpressive in its nature, was work from the\n1980s in Pittsburgh, developing what is known as the\nINTERNIST-1 or Quick Medical Reference system. This was now used not\nfor infectious diseases, but for primary care. Here one might ask, how\ncan we try to do diagnosis at a much larger scale, where\npatients might come in with one", "id": "vof7x8r_ZUA_7", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  and could report thousands of\ndifferent symptoms, each one giving you some\nview, noisy view, into what may be going on\nwith a patient's health. And at a high level, they\nmodeled this as something like a Bayesian network. It wasn't strictly\na Bayesian network. It was a bit more\nheuristic at the time. It was later developed to be so. But at a high level, there were\na number of latent variables or hidden variables\ncorresponding to different diseases\nthe patient might have, like flu or pneumonia\nor diabetes. And then there were\na number of variables on the very bottom, which were\nsymptoms, which are all binary, so the diseases are\neither on or off. And here the symptoms are\neither present or not. And these symptoms can include\nthings like fatigue or cough. They could also be things that\nresult from laboratory test results, like a high\nvalue of hemoglobin A1C. And this algorithm would\nthen take this model, take the symptoms that were\nreported for the patient, and try to do reasoning over\nwhat action might be going on", "id": "vof7x8r_ZUA_8", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  diagnosis is. There are over 40,000 edges\nconnecting diseases to symptoms that those diseases were\nbelieved to have caused. And this knowledge base, which\nwas probabilistic in nature, because it captured the idea\nthat some symptoms would only occur with some\nprobability for a disease, took over 15 person\nyears to elicit from a large medical team. And so it was a lot of effort. And even in going\nforward to today's time, there have been few similar\nefforts at a scale as impressive as this one. But again, what happened? These algorithms are not\nbeing used anywhere today in our clinical workflows. And the challenges that\nhave prevented them from being used\ntoday are numerous. But I used a word\nin my explanation", "id": "vof7x8r_ZUA_9", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  I used the word\nclinical workflow. And this, I think, is one\nof the biggest challenges. Which is that the\nalgorithms were designed to solve narrow problems. They weren't necessarily even\nthe most important problems, because clinicians generally do\na very good job at diagnosis. And there was a big gap between\nthe input that they expected and the current\nclinical workflow. So imagine that you have\nnow a mainframe computer. I mean, this was the '80s. And you have a clinician who\nhas to talk to the patient and get some information. Go back to the computer. Type in a structured\ndata, the symptoms that the patient's reporting. Get information back from\nthe computer and iterate. As you can imagine, that takes a\nlot of time, and time is money. And unfortunately, it\nprevents it from being used. Moreover, despite the fact\nthat it took a lot of effort", "id": "vof7x8r_ZUA_10", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  these systems were also\nreally difficult to maintain. So I talked about\nhow this was elicited from 15 person years of work. There was no machine\nlearning here. It was called\nartificial intelligence because one tries to reason\nin an artificial way, like humans might. But there was no learning\nfrom data in this. And so what that means is if\nyou then go to a new place, let's say this was\ndeveloped in Pittsburgh, and now you go to Los Angeles\nor to Beijing or to London, and you want to apply\nthe same algorithms, you suddenly have to\nre-derive parts of this model from scratch. For example, the prior\nprobability of the diseases are going to be very\ndifferent, depending on where you are in the world. Now, you might want to\ngo to a different domain outside of primary care. And again, one has to spend\na huge amount of effort to derive such models. As new medicine\ndiscoveries are made, one has to, again,\nupdate these models. And this has been a huge\nblocker to deployment.", "id": "vof7x8r_ZUA_11", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  also from the 1980s. And this is now for a\ndifferent type of question. Not one of how do you do\ndiagnosis, but how do you actually do discovery. So this is an example\nfrom Stanford. And it was a really\ninteresting case where one took a\ndata-driven approach to try to make medical discoveries. There was a database of what's\ncalled a disease registry from patients with\nrheumatoid arthritis, which is a chronic disease. It's an autoimmune\ncondition, where for each patient, over a\nseries of different visits, one would record,\nfor example, here it shows this is\nvisit number one. The date was January 17, 1979. The knee pain, patient's knee\npain, was reported as severe. Their fatigue was moderate. Temperatures was 38.5 Celsius.", "id": "vof7x8r_ZUA_12", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  a different autoimmune\ncondition called systemic lupus. We have some laboratory test\nvalues for their creatinine and blood nitrogen,\nand we know something about their medication. In this case, they were\non prednisone, a steroid. And one has this data\nat every point in time. This almost certainly\nwas recorded on paper and then later,\nthese were collected into a computer format. But then it provides\nthe possibility to ask questions and\nmake new discoveries. So for example, in\nthis work, there was a discovery\nmodule which would make causal hypotheses\nabout what aspects might cause other aspects. It would then do\nsome basic statistics to check about the\nstatistical validity of those causal hypotheses. It would then present\nthose to a domain expert to try to check off does\nthis make sense or not. For those that are accepted,\nit then uses that knowledge", "id": "vof7x8r_ZUA_13", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  to try to make new discoveries. And one of the main\nfindings from this paper was that prednisone\nelevates cholesterol. That was published in the Annals\nof Internal Medicine in 1986. So these are all\nvery early examples of data-driven approaches\nto improve both medicine and healthcare. Now flip forward to the 1990s. Neural networks started\nto become popular. Not quite the neural\nnetworks that we're familiar with in\ntoday's day and age, but nonetheless, they shared\nvery much of the same elements. So just in 1990, there\nwere 88 published studies using neural networks for\nvarious different medical problems. One of the things that really\ndifferentiated those approaches to what we see in\ntoday's landscape is that the number of\nfeatures were very small. So usually features which were\nsimilar to what I showed you in the previous slide. So structured data\nthat was manually curated for the purpose of\nusing in machine learning.", "id": "vof7x8r_ZUA_14", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  So one would have to have\nassistants gather the data. And because of that,\ntypically, there were very small number of\nsamples for each study that were used in machine learning. Now, these models,\nalthough very effective, and I'll show you some examples\nin the next slide, also suffered from the same\nchallenges I mentioned earlier. They didn't fit well\ninto clinical workflows. It was hard to get enough\ntraining data because of the manual efforts involved. And what the community found,\neven in the early 1990s, is that these algorithms\ndid not generalize well. If you went through this huge\neffort of collecting training data, learning your model, and\nvalidating your model at one institution, and you then\ntake it to a different one, it just works much worse. OK? And that really\nprevented translation of these technologies\ninto clinical practice. So what were these different\ndomains that were studied? Well, here are a few examples.", "id": "vof7x8r_ZUA_15", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  It was studied in breast cancer,\nmyocardial infarction, which is heart attack,\nlower back pain, used to predict\npsychiatric length of stay for inpatient, skin tumors,\nhead injuries, prediction of dementia, understanding\nprogression of diabetes, and a variety of other problems,\nwhich again are of the nature that we see about, we\nread about in the news today in modern\nattempts to apply machine learning in healthcare. The number of training\nexamples, as mentioned, were very few, ranging from\n39 to, in some cases, 3,000. Those are individuals, humans. And the networks,\nthe neural networks, they weren't completely\nshallow, but they weren't very deep either. So these were the\narchitectures they might be 60 neurons, then\n7, and then 6, for example, in terms of each of the\nlayers of the neural network.", "id": "vof7x8r_ZUA_16", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  given the type of data\nthat was fed into it. So none of this is new,\nin terms of the goals. So what's changed? Why do I think that\ndespite the fact that we've had\nwhat could arguably be called a failure for\nthe last 30 or 40 years, that we might actually have\nsome chance of succeeding now. And the big differentiator, what\nI'll call now the opportunity, is data. So whereas in the\npast, much of the work in artificial intelligence in\nmedicine was not data driven. It was based on trying to elicit\nas much domain knowledge as one can from clinical\ndomain experts. In some cases, gathering\na little bit of data. Today, we have an\namazing opportunity because of the prevalence of\nelectronic medical records, both in the United\nStates and elsewhere. Now, here the United States,\nfor example, the story wasn't that way,\neven back in 2008, when the adoption of\nelectronic medical records", "id": "vof7x8r_ZUA_17", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  But then there wasn't an\neconomic disaster in the US. And as part of the economic\nstimulus package, which President Obama initiated, there\nwas something like $30 billion allocated to\nhospitals purchasing electronic medical records. And this is already\na first example that we see of\npolicy being really influential to create the-- to open the stage\nto the types of work that we're going to be able\nto do in this course today. So money was then made available\nas incentives for hospitals to purchase electronic\nmedical records. And as a result, the adoption\nincreased dramatically. This is a really old number\nfrom 2015 of 84% of hospitals, and now today, it's\nactually much larger. So data is being collected\nin an electronic form, and that presents an opportunity\nto try to do research on it. It presents an opportunity\nto do machine learning on it, and it presents an opportunity\nto start to deploy machine", "id": "vof7x8r_ZUA_18", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  than having to manually\ninput data for a patient, we can just draw\nit automatically from data that's already\navailable in electronic form. And so there are a\nnumber of data sets that have been made available\nfor research and development in this space. Here at MIT, there has\nbeen a major effort pioneered by Professor Roger\nMark, in the ECS and Institute for Medical\nEngineering department, to create what's known as the\nPhysioNet or Mimic databases. Mimic contains data from\nover 40,000 patients and intensive care units. And it's very rich data. It contains basically\neverything that's being collected in the\nintensive care unit. Everything from notes that\nare written by both nurses and by attendings, to vital\nsigns that are being collected by monitors that are\nattached to patients, collecting their blood pressure,\noxygen saturation, heart", "id": "vof7x8r_ZUA_19", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  to blood test results as they're\nmade available, and outcomes. And of course also\nmedications that are being prescribed as it goes. And so this is a wealth\nof data that now one could use to try\nto study, at least study in a very narrow\nsetting of an intensive care unit, how machine learning\ncould be used in that location. And I don't want to\nunder-emphasize the importance of this database, both\nthrough this course and through the broader field. This is really the\nonly publicly available electronic medical\nrecord data set of any reasonable size\nin the whole world, and it was created here at MIT. And we'll be using\nit extensively in our homework\nassignments as a result. There are other data sets that\naren't publicly available, but which have been\ngathered by industry. And one prime example is the\nTruven Market Scan database,", "id": "vof7x8r_ZUA_20", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  was later acquired by\nIBM, as I'll tell you about more in a few minutes. Now, this data-- and there are\nmany competing companies that have similar data sets-- is created not from\nelectronic medical records, but rather from-- typically, it's created\nfrom insurance claims. So every time you\ngo to see a doctor, there's usually\nsome record of that that is associated to the\nbilling of that visit. So your provider will send a\nbill to your health insurance saying basically what\nhappened, so what procedures were performed,\nproviding diagnoses that are used to justify\nthe cost of those procedures and tests. And from that data, you\nnow get a holistic view, a longitudinal view,\nof what's happened to that patient's health. And then there is\na lot of money that passes behind the scenes\nbetween insurers and hospitals", "id": "vof7x8r_ZUA_21", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  which collect that\ndata and then resell it for research purposes. And one of the biggest\npurchasers of data like this is the pharmaceutical industry. So this data, unfortunately, is\nnot usually publicly available, and that's actually a big\nproblem, both in the US and elsewhere. It's a big obstacle to\nresearch in this field, that only people who\nhave millions of dollars to pay for it really\nget access to it, and it's something that\nI'm going to return to throughout the semester. It's something\nwhere I think policy can make a big difference. But luckily, here\nat MIT, the story's going to be a bit different. So thanks to the MIT\nIBM Watson AI Lab, MIT has a close\nrelationship with IBM. And fingers crossed, it\nlooks like we'll get access to this database for our\nhomework and projects for this semester. Now, there are a lot\nof other initiatives that are creating\nlarge data sets. A really important\nexample here in the US is President Obama's\nPrecision Medicine Initiative,", "id": "vof7x8r_ZUA_22", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  And this initiative\nis creating a data set of one million patients, drawn\nin a representative manner, from across the United\nStates, to capture patients both poor and rich,\npatients who are healthy and have chronic disease,\nwith the goal of trying to create a research\ndatabase where all of us and other people,\nboth inside and outside the US, could do research to\nmake medical discoveries. And this will include\ndata such as data from a baseline health exam,\nwhere the typical vitals are taken, blood is drawn. It'll combine data of\nthe previous two types I've mentioned,\nincluding both data from electronic medical records\nand health insurance claims. And a lot of this work is\nalso happening here in Boston. So right across the street\nat the Broad Institute, there is a team\nwhich is creating all of the software\ninfrastructure to accommodate this data. And there are a large\nnumber of recruitment sites here in the broader\nBoston area where", "id": "vof7x8r_ZUA_23", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  to be part of this study. I just got a letter in the mail\nlast week inviting me to go, and I was really\nexcited to see that. So all sorts of\ndifferent data is being created as a\nresult of these trends that I've been mentioning. And it ranges from unstructured\ndata, like clinical notes, to imaging, lab\ntests, vital signs. Nowadays, what we used to think\nabout just as clinical data now has started to\nreally come to have a very tight tie to what we\nthink about as biological data. So data from genomics\nand proteomics is starting to play a major\nrole in both clinical research and clinical practice. Of course, not everything\nthat we traditionally think about healthcare data-- there are also some\nnon-traditional views on health. So for example, social\nmedia is an interesting way of thinking through both\npsychiatric disorders, where many of us will post things\non Facebook and other places", "id": "vof7x8r_ZUA_24", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  a lens on our mental health. Your phone, which is\ntracking your activity, will give us a view\non how active we are. It might help us diagnose\nearly the variety of conditions as well that I'll mention later. So we have-- to this\nwhole theme right now is about what's changed\nsince the previous approaches at AI medicine. I've just talked about\ndata, but data alone is not nearly enough. The other major\nchange is that there has been decades' worth of work\non standardizing health data. So for example, when I\nmentioned to you that when you go to a doctor's office,\nand they send a bill, that bill is associated\nwith a diagnosis. And that diagnosis is coded in\na system called ICD-9 or ICD-10, which is a standardized\nsystem where, for many, not all,\nbut many diseases, there is a corresponding\ncode associated with it. ICD-10, which was\nrecently rolled", "id": "vof7x8r_ZUA_25", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  than the previous\ncoding system, includes some interesting categories. For example, bitten by a\nturtle has a code for it. Bitten by sea lion,\nstruck by [INAUDIBLE].. So it's starting to get\nreally detailed here, which has its benefits and its\ndisadvantages when it comes to research using that data. But certainly, we can do\nmore with detailed data than we could with\nless detailed data. Laboratory test results are\nstandardized using a system called LOINC, here\nin the United States. Every lab test order has\nan associated code for it. I just want to point out briefly\nthat the values associated with those lab tests\nare less standardized. Pharmacy, national drug codes\nshould be very familiar to you. If you take any medication\nthat you've been prescribed, and you look carefully,\nyou'll see a number on it,", "id": "vof7x8r_ZUA_26", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  medication. In fact, it's even unique to\nthe brand of that medication. And there's an associated\ntaxonomy with it. And so one can really understand\nin a very structured way what medications a patient is on and\nhow those medications relate to one another. A lot of medical data is found\nnot in the structured form, but in free text, in\nnotes written by doctors. And these notes have,\noften, lots of mentions of symptoms and\nconditions in them. And one can try to\nstandardize those by mapping them to what's called\na unified medical language system, which is an\nontology with millions of different medical\nconcepts in them. So I'm not going to go\ntoo much more into these. They'll be the subject of much\ndiscussion in this semester, but particularly in the\nnext two lectures by Pete.", "id": "vof7x8r_ZUA_27", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  have a standardized vocabulary. So one thing you\ncan do is you could build APIs, or Application\nProgramming Interfaces, for now sending that\ndata from place to place. And FHIR, F-H-I-R,\nis a new standard, which has widespread adoption\nnow here in the United States for hospitals to provide data\nboth for downstream clinical purposes but also\ndirectly to patients. And in this\nstandard, it will use many of the\nvocabularies I mentioned to you in the previous slides to\nencode diagnoses, medications, allergies, problems, and\neven financial aspects that are relevant to the\ncare of this patient. And for those of you who have\nan Apple phone, for example, and if you open up a\nApple Health Records,", "id": "vof7x8r_ZUA_28", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  from over 50\ndifferent hospitals. And you should expect\nto see many competitors to them in the future,\nbecause of the fact that it's now an open standard. Now other types of data,\nlike the health insurance claims I mentioned\nearlier, is often encoded in a slightly\ndifferent data model. One which my lab works quite\na bit with is called OMOP, and it's being maintained by a\nnonprofit organization called the Observational Health Data\nSciences Initiative Odyssey. And this common data\nmodel gives a standard way of taking data from\nan institution which might have its own intricacies\nand really mapping it to this common language, so that\nif you write a machine learning algorithm once, then\nthat machine learning algorithm reads in\ndata in this format, you can then apply it\nsomewhere else very easily. And the portions\nof these standards really can't be\nunderstated, the importance for translating what\nwe're doing in this class into clinical practice. And so we'll be\nreturning to these things", "id": "vof7x8r_ZUA_29", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  So we've talked about data. We've talked about standards. And the third wheel\nis breakthroughs in machine learning. And this should be no surprise\nto anyone in this room. All right, we've been\nseeing time and time again, over the last five years,\nbenchmark after benchmark being improved upon and human\nperformance beaten by state-of-the-art machine\nlearning algorithms. Here I'm just\nshowing you a figure that I imagine many\nof you have seen, on the error rates on the image\nnet competition for object recognition. The error rates\nin 2011 were 25%. And even just a few\nyears ago, it already surpassed human\nlevel to under 5%. Now, the changes that have led\nto those advances in object recognition are going to have\nsome parallels in healthcare, but only up to some point. For example, there was big\ndata, large training sets that were critical for this.", "id": "vof7x8r_ZUA_30", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  particular convolutional\nneural networks, that played a huge role. And there was open source\nsoftware that was created, things like TensorFlow\nand PyTorch, which allow a researcher or\nindustry worker in one place to very, very quickly\nbuild upon successes from other researchers\nin other places and then release the code,\nso that one can really accelerate the rate of\nprogress in this field. Now, in terms of those\nalgorithmic advances that have made a big\ndifference, the ones that I would really\nlike to point out because of their\nrelevance to this course are learning with high\ndimensional features. So this was really the\nadvances in the early 2000s, for example. And support vector machines and\nlearning with L1 regularization as a type of sparsity. And then more recently,\nin the last six years, on stochastic gradient\ndescent, like methods", "id": "vof7x8r_ZUA_31", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  that will play a\nhuge role in what we'll be doing in this course. In the last few\nyears, there have been a huge amount\nof progress in unsupervised and semi-supervised\nlearning algorithms. And as I'll tell you\nabout much later, one of the major\nchallenges in healthcare is that despite the fact that\nwe have a large amount of data, we have very little\nlabeled data. And so these semi-supervised\nlearning algorithms are going to play a\nmajor role in being able to really take advantage\nof the data that we do have. And then of course the modern\ndeep learning algorithms. Convolutional neural networks,\nrecurrent neural networks, and ways of trying\nto train them. So those played a major\nrole in the advances in the tech industry. And to some extent, they'll\nplay a major role in healthcare as well. And I'll point out a\nfew examples of that in the rest of today's lecture. So all of this coming together,\nthe data availability, the advances in other\nfields of machine learning, and the huge amount of potential\nfinancial gain in healthcare", "id": "vof7x8r_ZUA_32", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  has not gone unnoticed. And there's a huge\namount of industry interested in this field. These are just some examples\nfrom names I think many of you are familiar with, like\nDeepMind Health and IBM Watson to startup companies\nlike Bay Labs and PathAI, which is here\nin Boston, all of which are really trying to build\nthe next generation of tools for healthcare, now based on\nmachine learning algorithms. There's been billions\nof dollars of funding in the recent quarters towards\ndigital health efforts, with hundreds of\ndifferent startups that are focused specifically\non using artificial intelligence and healthcare. And there's the\nrecognition that data is so essential to\nthis process has led to an all-out purchasing\neffort to try to get as much of that data as you can. So for example, IBM\npurchased a company", "id": "vof7x8r_ZUA_33", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  and thus had accumulated a large\namount of medical imaging data for $1 billion in 2015. They purchased Truven\nfor $2.6 billion in 2016. Flatiron Health, which\nis a company in New York City focused on oncology, was\npurchased for almost $2 billion by Roche, a pharmaceutical\ncompany, just last year. And there's several more\nof these industry moves. Again, I'm just tying to get you\nthinking about what it really takes in this field, and\ngetting access to data is actually a really\nimportant one, obviously. So let's now move\non to some examples of how machine learning\nwill transform healthcare. To begin with, I want to really\nlay out the landscape here and define some language. There are a number\nof different players when it comes to the\nhealthcare space. They're us, patients, consumers. They are the doctors\nthat we go to, which you could think\nabout as providers.", "id": "vof7x8r_ZUA_34", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  they're also nurses and\ncommunity health workers and so on. There are payers, which\nprovide the-- where there is-- these edges are really\nshowing relationships between the different\nplayers, so our consumers, we often, either from our\njob or directly from us, we will pay premiums for a\nhealth insurance company, to a health insurance\ncompany, and then that health insurance company\nis responsible for payments to the providers to provide\nservices to us patients. Now, here in the US, the\npayers are both commercial and governmental. So many of you will know\ncompanies like Cigna or Aetna or Blue Cross, which\nare commercial providers of healthcare, of\nhealth insurance, but there are also\ngovernmental ones. For example, the Veterans\nHealth Administration runs one of the biggest health\norganizations in the United States, servicing our\nveterans from the department,", "id": "vof7x8r_ZUA_35", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  has the one of\nthe second biggest health systems, the\nDefense Health Agency. And that is an\norganization where-- both of those\norganizations, where both the payer and the\nprovider are really one. The Center for Medicare\nand Medicaid Services here in the US provides\nhealth insurance for all retirees in\nthe United States. And also Medicaid, which\nis run at a state level, provides health insurance\nto a variety of individuals who would otherwise\nhave difficulty purchasing or obtaining\ntheir own health insurance. And those are examples of\nstate-run or federally run health insurance agencies. And then internationally,\nsometimes the lines are even more blurred. So of course in places\nlike the United Kingdom, where you have a government-run\nhealth system, the National", "id": "vof7x8r_ZUA_36", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  and providing the services. Now, the reason why this\nis really important for us to think about\nalready in lecture one is because what's so\nessential about this field is figuring out where the\nknob is that you can turn to try to improve healthcare. Where can we deploy\nmachine learning algorithms within healthcare? So some algorithms are going\nto be better run by providers, others are going to be\nbetter run by payers, others are going to be\ndirectly provided to patients, and some all of the above. We also have to think\nabout industrial questions, in terms of what is it going to\ntake to develop a new product. Who will pay for this product? Which is again an\nimportant question when it comes to\ndeploying algorithms here. So I'll run through a couple of\nvery high-level examples driven from my own work, focused\non the provider space, and then I'll bump up to\ntalk a bit more broadly.", "id": "vof7x8r_ZUA_37", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  I've been doing a lot\nof work in collaboration with Beth Israel\nDeaconess Medical Center, across the river, with\ntheir emergency department. And the emergency department\nis a really interesting clinical setting, because\nyou have a very short period of time from when a patient\ncomes into the hospital to diagnose what's going on\nwith them, to initiate therapy, and then to decide\nwhat to do next. Do you keep them\nin the hospital? Do you send them home? If you-- for each\none of those things, what should the most\nimmediate actions be? And at least here in the US,\nwe're always understaffed. So we've got limited resources\nand very critical decisions to make. So this is one example\nof a setting where algorithms that are\nrunning behind the scenes could potentially really help\nwith some of the challenges I mentioned earlier. So for example, one could\nimagine an algorithm which builds on techniques\nlike what I mentioned to you for an internist one\nor quick medical reference, try to reason about what's\ngoing on with the patient based", "id": "vof7x8r_ZUA_38", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  But the modern view of\nthis shouldn't, of course, use binary indicators\nof each symptom, which have to be entered in manually,\nbut rather all of these things should be\nautomatically extracted from the electronic medical\nrecord or listed as necessary. And then if one could\nreason about what's going on with a patient,\nwe wouldn't necessarily want to use it for a diagnosis,\nalthough in some cases, you might use it for\nan earlier diagnosis. But it could also be used for\na number of other more subtle interventions, for\nexample, better triage to figure out which patients\nneed to be seen first. Early detection of adverse\nevents or recognition that there might be some unusual\nactions which might actually be medical errors that you\nwant to surface now and draw attention to. Now, you could also\nuse this understanding of what's going\non with a patient to change the way\nthat clinicians interact with patient data. So for example, one can try\nto propagate best practices", "id": "vof7x8r_ZUA_39", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  automatically triggering\nthis clinical decision support for patients that you\nthink it might be relevant for. And here's one\nexample, where it says, the ED Dashboard, the Emergency\nDepartment Dashboard decision support algorithms\nhave determined this patient may be eligible for\nthe atria cellulitis pathway. Cellulitis is often\ncaused by infections. Please choose from\none of the options. Enroll in the pathway, decline-- and if you decline,\nyou must include a comment for the reviewers. Now, if you clicked on enroll\nin the pathway, at that moment, machine learning disappears. Rather, there is a\nstandardized process. It's an algorithm, but it's\na deterministic algorithm, for how patients with cellulitis\nshould be properly managed, diagnosed, and treated. That algorithm comes\nfrom best practices,", "id": "vof7x8r_ZUA_40", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  understanding what\nwould be good ways to treat patients of this\ntype, and then formalizing that in a document. The challenge is that there\nmight be hundreds or even thousands of these\nbest practices. And in an academic\nmedical center, where you have patients coming-- where you have medical\nstudents or residents who are very quickly rotating\nthrough the system and thus may not be familiar\nwith which are the most appropriate clinical\nguidelines to use for any one patient in this institution. Or if you go to a\nrural site, where this academic nature of\nthinking through what the right clinical guidelines\nare is a little bit less of the mainstream, everyday\nactivity, the question of which one to use when is\nvery challenging. And so that's where the\nmachine learning algorithms can come in. By reasoning about what's\ngoing on with a patient, you might have a\ngood guess of what might be appropriate\nfor this patient,", "id": "vof7x8r_ZUA_41", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  the right clinical\ndecisions for a trigger. Another example\nis by just trying to anticipate clinician needs. So for example, if you think\nthat this patient might be coming in for a psychiatric\ncondition, or maybe you recognize that the\npatient came in that triage and was complaining\nof chest pain, then there might\nbe a psych order set, which includes\nlaboratory test results that are relevant for\npsychiatric patients, or a chest pain order set, which\nincludes both laboratory tests and interventions, like aspirin,\nthat might be suggested. Now, these are also examples\nwhere these order sets are not created by machine\nlearning algorithms. Although that's something\nwe could discuss later in the semester. Rather, they're standardized. But the goal of the\nmachine learning algorithm is just to figure out\nwhich ones to show when directly to the clinicians. I'm showing you\nthese examples to try to point out that diagnosis\nisn't the whole story. Thinking through what are the\nmore subtle interventions we", "id": "vof7x8r_ZUA_42", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  is going to be really\nimportant to having the impact that it could have. So other examples, now a bit\nmore on the diagnosis style, are reducing the need\nfor specialist consults. So you might have\na patient come in, and it might be really quick\nto get the patient in front of an X-ray to do a\nchest X-ray, but then finding the radiologist\nto review that X-ray could take a lot of time. And in some places,\nradiologist consults could take days, depending on\nthe urgency of the condition. So this is an area where\ndata is quite standardized. In fact, MIT just\nreleased last week a data set of\n300,000 chest x-rays with associated labels on them. And one could try to ask\nthe question of could we build machine\nlearning algorithms using the convolutional\nneural network type techniques that we've seen\nplay a big role in object recognition to try\nto understand what's going on with this patient. For example, in this\ncase, the prediction", "id": "vof7x8r_ZUA_43", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  And using those\nsystems, it could help both reduce the load\nof radiology consults, and it could allow us to really\ntranslate these algorithms to settings which\nmight be much more resource poor, for example,\nin developing nations. Now, the same\nsorts of techniques can be used for other\ndata modalities. So this is an\nexample of data that could be obtained from an EKG. And from looking\nat this EKG, one can try to predict, does the\npatient have a heart condition, such as an arrhythmia. Now, these types of\ndata used to just be obtained when you go\nto a doctor's office. But today, they're\navailable to all of us. For example, in Apple's most\nrecent watch that was released, it has a single-lead\nEKG built into it, which can attempt to predict\nif a patient has an arrhythmia or not. And there are a\nlot of subtleties, of course, around what it took\nto get regulatory approval", "id": "vof7x8r_ZUA_44", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  in the semester, and how one\nsafely deploys such algorithms directly to consumers. And there, there are a\nvariety of techniques that could be used. And in a few lectures,\nI'll talk to you about techniques from\nthe '80s and '90s which were based on trying\nto signal processing, trying to detect where are\nthe peaks of the signal, look at a distance\nbetween peaks. And more recently, because\nof the large wealth of data that is\navailable, we've been using convolutional neural\nnetwork-based approaches to try to understand this\ndata and predict from it. Yet another example\nfrom the ER really has to do with not how do we\ncare for the patient today, but how do we get\nbetter data, which will then result\nin taking better care of the patient tomorrow. And so one example\nof that, which my group deployed at\nBeth Israel Deaconess, and it's still running there\nin the emergency department, has to do with getting higher\nquality chief complaints. The chief complaint is\nusually a very short,", "id": "vof7x8r_ZUA_45", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  right upper quadrant,\nRUQ, abdominal pain. And it's just a\nvery quick summary of why did the patient\ncome into the ER today. And despite the fact\nthat it's so few words, it plays a huge role in\nthe care of a patient. If you look at the\nbig screens in the ER, which summarize who are the\npatients and on what beds, they have the chief\ncomplaint next to it. Chief complaints are used as\ncriteria for enrolling patients in clinical trials. It's used as criteria for doing\nretrospective quality research to see how do we care for\npatients in a particular type. So it plays a very big role. But unfortunately, the data\nthat we've been getting has been crap. And that's because\nit was free text, and it was sufficiently\nhigh dimensional that just attempting\nto standardize it with a big dropdown list,\nlike you see over here, would have killed the\nclinical workflow. It would've taken\nway too much time", "id": "vof7x8r_ZUA_46", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  And so it just wouldn't\nhave been used. And that's where some very\nsimple machine learning algorithms turned out\nto be really valuable. So for example, we changed\nthe workflow altogether. Rather than the chief\ncomplaint being the first thing that the triage nurse assigns\nwhen the patient comes in, it's the last thing. First, the nurse takes the vital\nsigns, patient's temperature, heart rate, blood\npressure, respiratory rate, and oxygen saturation. They talk to the patient. They write up a 10-word or\n30-word note about what's going on with the patient. Here it says, \"69-year-old\nmale patient with severe intermittent right\nupper quadrant pain. Began soon after eating. Also is a heavy drinker.\" So quite a bit of\ninformation in that. We take that. We use a machine learning\nalgorithm, a supervised machine learning algorithm in\nthis case, to predict a set of chief\ncomplaints now drawn from a standardized ontology. We show the five most likely\nones, and the clinician, in this case, a nurse, could\njust click one of them,", "id": "vof7x8r_ZUA_47", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  We also allow the nurse to type\nin part of a chief complaint. But rather than just\ndoing a text matching to find words that match\nwhat's being typed in, we do a contextual autocomplete. So we use our\npredictions to prioritize what's the most likely chief\ncomplaint that contains that sequence of characters. And that way it's\nway faster to enter in the relevant information. And what we found\nis that over time, we got much higher\nquality data out. And again, this\nis something we'll be talking about in one of\nour lectures in this course. So I just gave you an\nexample, a few examples, of how machine learning\nand artificial tolerance will transform the\nprovider space, but now I want to\njump up a level and think through not how\ndo we treat a patient today, but how do we think about the\nprogression of a patient's chronic disease over\na period of years. It could be 10 years, 20 years.", "id": "vof7x8r_ZUA_48", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  is something which affects\nall aspects of the healthcare ecosystem. It'll be used by\nproviders, payers, and also by patients themselves. So consider a patient with\nchronic kidney disease. Chronic kidney disease, it\ntypically only gets worse. So you might start with\nthe patient being healthy and then have some\nincreased risk. Eventually, they have\nsome kidney damage. Over time, they\nreach kidney failure. And once they reach\nkidney failure, typically, they need dialysis\nor a kidney transplant. But understanding when\neach of these things is going to happen for\npatients is actually really, really challenging. Right now, we have one way\nof trying to stage patients. The standard approach\nis known as the EGFR. It's derived predominantly from\nthe patient's creatinine, which", "id": "vof7x8r_ZUA_49", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  And it gives you a number out. And from that\nnumber, you can get some sense of where the\npatient is in this trajectory. But it's really coarse\ngrained, and it's not at all predictive about\nwhen the patient is going to progress to the\nnext stage of the disease. Now, other conditions,\nfor example, some cancers, like I'll\ntell you about next, don't follow that\nlinear trajectory. Rather, patients' conditions\nand the disease burden, which is what I'm\nshowing you in the y-axis here, might get worse,\nbetter, worse again, better again, worse again, and\nso on, and of course is a function of the\ntreatment for the patient and other things that\nare going on with them. And understanding\nwhat influences, how a patient's disease\nis going to progress, and when is that\nprogression going to happen, could be enormously valuable for\nmany of those different parts of the healthcare ecosystem. So one concrete example of\nhow that type of prediction could be used would be in a\ntype of precision medicine.", "id": "vof7x8r_ZUA_50", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  in the very beginning\nof today's lecture of multiple myeloma, which\nI said my mother died of, there are a large number\nof existing treatments for multiple myeloma. And we don't really know which\ntreatments work best for whom. But imagine a day where\nwe have algorithms that could take what you\nknow about a patient at one point in time. That might include, for\nexample, blood test results. It might include RNA\nseq, which gives you some sense of the\ngene expression for the patient,\nthat in this case would be derived from a sample\ntaken from the patient's bone marrow. You could take that\ndata and try to predict what would happen to a patient\nunder two different scenarios. The blue scenario that\nI'm showing you here, if you give them treatment\nA, or this red scenario here, where you give them treatment\nB. And of course, treatment A and treatment B aren't\njust one-time treatments, but they're strategies. So they're repeated\ntreatments across time, with some intervals.", "id": "vof7x8r_ZUA_51", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  this is what's going to\nhappen, then you might-- the clinician might think, OK. Treatment B is probably\nthe way to go here. It's going to long-term\ncontrol the patient's disease burden the best. And this is an example\nof a causal question. Because we want\nto know how do we cause a change in the\npatient's disease trajectory. And we can try to answer\nthis now using data. So for example, one of the data\nsets that's available for you to use in your course projects\nis from the Multiple Myeloma Research Foundation. It's an example of\na disease registry, just like the disease registry\nI talked to you about earlier for rheumatoid arthritis. And it follows\nabout 1,000 patients across time, patients who\nhave multiple myeloma. What treatments they're\ngetting, what their symptoms are, and at a couple\nof different stages, very detailed biological\ndata about their cancer, in this case, RNA seq. And one could attempt to use\nthat data to learn models", "id": "vof7x8r_ZUA_52", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  But such predictions\nare fraught with errors. And one of the things\nthat Pete and I will be teaching in this\ncourse is that there's a very big difference between\nprediction and prediction for the purpose of\nmaking causal statements. And the way that you interpret\nthe data that you have, when your goal is to\ndo treatment suggestion or optimization, is going to\nbe very different from what you were taught in your\nintroductory machine learning algorithms class. So other ways that we could try\nto treat and manage patients with chronic disease\ninclude early diagnosis. For example, patients\nwith Alzheimer's disease, there's been some really\ninteresting results just in the last few years, here. Or new modalities altogether. For example, liquid\nbiopsies that are able to do early\ndiagnosis of cancer, even without having to do a\nbiopsy of the cancer tumor itself. We can also think about how\ndo we better track and measure chronic disease.", "id": "vof7x8r_ZUA_53", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  is from Dina Katabi's lab\nhere at MIT and CSAIL, where they've developed a\nsystem called Emerald, which is using wireless signals,\nthe same wireless signals that we have in this room\ntoday, to try to track patients. And they can actually\nsee behind walls, which is quite impressive. So using this for\nthe signal, you could install what looks\nlike just a regular wireless router in an elderly\nperson's home, and you could detect if\nthat elderly patient falls. And of course if the patient\nhas fallen, and they're elderly, it might be very hard\nfor them to get back up. They might have broken\na hip, for example. And one could then alert the\ncaregivers, maybe if necessary, bring in emergency support. And that could have a long-term\noutcome for this patient which would really help them. So this is an example of what\nI mean by better tracking patients with chronic disease. Another example\ncomes from patients who have type 1 diabetes.", "id": "vof7x8r_ZUA_54", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  generally develops in\npatients at a very early age. Usually as children\nit's diagnosed. And one is typically managed by\nhaving an insulin pump, which is attached to a patient and\ncan give injections of insulin on the fly, as necessary. But there's a really challenging\ncontrol problem there. If you give a patient too much\ninsulin, you could kill them. If you give them\ntoo little insulin, you could really hurt them. And how much insulin\nyou give them is going to be a function\nof their activity. It's going to be a function\nof what food they're eating and various other factors. So this is a question which the\ncontrol theory community has been thinking through\nfor a number of years, and there are a number of\nsophisticated algorithms that are present in\ntoday's products, and I wouldn't be surprised if\none or two people in the room today have one of these. But it also presents a really\ninteresting opportunity for machine learning.", "id": "vof7x8r_ZUA_55", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  at predicting future\nglucose levels, which is essential to figure out\nhow to regulate insulin. And if we had algorithms\nthat could, for example, take a patient's phone, take\na picture of the food that a patient is eating,\nhave that automatically feed into an algorithm that\npredicts its caloric content and how quickly that'll\nbe processed by the body. And then as a result\nof that, think about when, based on this patient's\nmetabolic system, when should you start increasing insulin\nlevels and by how much. That could have a huge\nimpact in quality of life of these types of patients. So finally, we've\ntalked a lot about how do we manage healthcare,\nbut equally important is about discovery. So the same data\nthat we could use to try to change the way that\nalgorithms are implemented could be used to think through\nwhat would be new treatments and make new discoveries\nabout disease subtypes. So at one point later\nin the semester, we'll be talking about\ndisease progression modeling, and we'll talk about how to\nuse data-driven approaches", "id": "vof7x8r_ZUA_56", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  And on the left,\nhere, I'm showing you an example of a really nice\nstudy from back in 2008 that used a k-means\nclustering algorithm to discover subtypes of asthma. One could also use\nmachine learning to try to make discoveries about\nwhat proteins, for example, are important in\nregulating disease. How can we differentiate at a\nbiological level which patients will progress quickly,\nwhich patients will respond to treatment. And that of course will\nthen suggest new ways of-- new drug targets for new\npharmaceutical efforts. Another direction also\nstudied here at MIT, by quite a few labs, actually,\nhas to do with drug creation or discovery. So one could use machine\nlearning algorithms to try to predict what\nwould a good antibody be", "id": "vof7x8r_ZUA_57", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  So that's all for my overview. And in the remaining\n20 minutes, I'm going to tell you a\nlittle bit about what's unique about machine\nlearning in healthcare, and then an overview\nof the class syllabus. And I do see that it says,\nreplace lamp in six minutes, or power will turn off\nand go into standby mode. AUDIENCE: We have\nthat one [INAUDIBLE].. DAVID SONTAG: Ah, OK. Good. You're hired. If you didn't get into the\nclass, talk to me afterwards. All right. AUDIENCE: [INAUDIBLE]. DAVID SONTAG: [LAUGHS] We hope. So what's unique about\nmachine learning healthcare? I gave you already\nsome hints at this. So first, healthcare is\nultimately, unfortunately, about life or death decisions. So we need robust algorithms\nthat don't screw up. A prime example of this, which\nI'll tell you a little bit more", "id": "vof7x8r_ZUA_58", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  is from a major software\nerror that occurred something like 20, 30 years ago in a-- in an X-ray type\nof device, where an overwhelming\namount of radiation was exposed to a patient just\nbecause of a software overflow problem, a bug. And of course that resulted\nin a number of patients dying. So that was a software\nerror from decades ago, where there was no machine\nlearning in the loop. And as a result of that and\nsimilar types of disasters, including in the space industry\nand airplanes and so on, led to a whole area of\nresearch in computer science in formal methods and how do we\ndesign computer algorithms that can check that a\npiece of software", "id": "vof7x8r_ZUA_59", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  and that there\nare no bugs in it. But now that we're going to\nstart to bring data and machine learning algorithms\ninto the picture, we are really suffering\nfor lack of good tools for doing similar formal\nchecking of our algorithms and their behavior. And so this is going\nto be really important in the future decade, as\nmachine learning gets deployed not just in settings\nlike healthcare, but also in other settings\nof life and death, such as in autonomous driving. And it's something\nthat we'll touch on throughout the semester. So for example, when one deploys\nmachine learning algorithms, we need to be thinking about\nare they safe, but also how do we check for\nsafety long-term? What are checks and\nbalances that we should put into the deployment\nof the algorithm to make sure that it's still\nworking as it was intended? We also need fair and\naccountable algorithms. Because increasingly,\nmachine learning results are being used to\ndrive resources in a healthcare setting.", "id": "vof7x8r_ZUA_60", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  when we talk about\nrisk stratification, is that algorithms are\nbeing used by payers to risk stratify patients. For example, to figure\nout which patients are likely to be readmitted\nto the hospital in the next 30 days, or are likely\nto have undiagnosed diabetes, or are likely\nto progress quickly in their diabetes. And based on those\npredictions, they're doing a number of interventions. For example, they might send\nnurses to the patient's home. They might offer\ntheir members access to a weight loss program. And each of these interventions\nhas money associated to them. They have a cost. And so you can't do\nthem for everyone. And so one uses machine\nlearning algorithms to prioritize who do you\ngive those interventions to. But because health\nis so intimately tied to socioeconomic\nstatus, one can think about what happens if these\nalgorithms are not fair.", "id": "vof7x8r_ZUA_61", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  and it's something that we're\ngoing to talk about later in the semester as well. Now, I mentioned\nearlier that many of the questions that we\nneed to study in the field don't have good label data. In cases where we know\nwe want to predict, there's a supervised\nprediction problem, often we just don't have\nlabels for that thing we want to predict. But also, in many\nsituations, we're not interested in just\npredicting something. We're interested in discovery. So for example, when I talk\nabout disease subtyping or disease progression,\nit's much harder to quantify what\nyou're looking for. And so unsupervised\nlearning algorithms are going to be really\nimportant for what we do. And finally, I already mentioned\nhow many of the questions we want to answer\nare causal in nature, particularly when\nyou want to think about treatment strategies. And so we'll have two\nlectures on causal inference, and we'll have two lectures on\nreinforcement learning, which is increasingly being\nused to learn treatment policies in healthcare.", "id": "vof7x8r_ZUA_62", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  that we've talked about result\nin our having to rethink how do we do machine\nlearning in this setting. For example, because\ndriving labels for supervised\nprediction is very hard, one has to think through how\ncould we automatically build algorithms to do what's\ncalled electronic phenotyping to discover, to figure\nout automatically, what is the relevant labels\nfor a set of patients that one could then attempt\nto predict in the future. Because we often have\nvery little data, for example, some\nrare diseases, there might only be a few\nhundred or a few thousand people in the nation\nthat have that disease. Some common diseases\npresent in very diverse ways and [INAUDIBLE] are very rare. Because of that, you have just a\nsmall number of patient samples that you could get,\neven if you had all of the data in the right place.", "id": "vof7x8r_ZUA_63", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  how can we bring together\ndomain knowledge. How can we bring together\ndata from other areas-- will everyone look\nover here now-- from other areas,\nother diseases, in order to learn\nsomething that then we could refine for\nthe foreground question of interest. Finally, there is a ton of\nmissing data in healthcare. So raise your hand\nif you've only been seeing your current\nprimary care physician for less than four years. OK. Now, this was an easy\nguess, because all of you are students, and you\nprobably don't live in Boston. But here in the US, even\nafter you graduate, you go out into the world, you\nhave a job, and that job pays your health insurance. And you know what? Most of you are going to\ngo into the tech industry, and most of you are going to\nswitch jobs every four years. And so your health\ninsurance is going", "id": "vof7x8r_ZUA_64", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  And unfortunately,\ndata doesn't tend to follow people when you\nchange providers or payers. And so what that\nmeans is for any one thing we might want\nto study, we tend to not have very good\nlongitudinal data on those individuals, at least\nnot here in the United States. That story is a little bit\ndifferent in other places, like the UK or\nIsrael, for example. Moreover, we also\nhave a very bad lens on that healthcare data. So even if you've been going\nto the same doctor for a while, we tend to only have data on you\nwhen something's been recorded. So if you went to a doctor,\nyou had a lab test performed, we know the results of it. If you've never gotten\nyour glucose tested, it's very hard,\nthough not impossible, to figure out if you\nmight be diabetic. So thinking about how\ndo we deal with the fact that there's a large\namount of missing data, where that missing data\nhas very different patterns across patients, and\nwhere there might be a big difference between\ntrain and test distributions", "id": "vof7x8r_ZUA_65", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  And finally, the last\nexample is censoring. I think I've said\nfinally a few times. So censoring, which we'll\ntalk about in two weeks, is what happens\nwhen you have data only for small windows of time. So for example, you have a\ndata set where your goal is to predict survival. You want to know how\nlong until a person dies. But a person-- you\nonly have data on them up to January 2009,\nand they haven't yet died by January 2009. Then that individual\nis censored. You don't know what\nwould have happened, you don't know when they died. So that doesn't mean you should\nthrow away that data point. In fact, we'll\ntalk about learning algorithms that can learn from\ncensored data very effectively. So there are a number of also\nlogistical challenges to doing machine learning in healthcare. I talked about how having\naccess to data is so important, but one of the reasons--\nthere are others-- for why getting large amounts\nof data in the public domain is challenging is because\nit's so sensitive.", "id": "vof7x8r_ZUA_66", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  from data which\nincludes free text notes can be very challenging. And as a result, when we\ndo research here at MIT, typically, it takes us\nanywhere from a few months-- which has never happened--\nto two years, which is the usual situation, to\nnegotiate a data sharing agreement to get the health\ndata to MIT to do research on. And of course then\nmy students write code, which we're very happy to\nopen source under MIT license, but that code is\ncompletely useless, because no one can reproduce\ntheir results on the same data because they don't\nhave access to it. So that's a major\nchallenge to this field. Another challenge is\nabout the difficulty in deploying machine learning\nalgorithms due to the challenge of integration. So you build a good algorithm. You want to deploy it at\nyour favorite hospital, but guess what? That hospital has Epic\nor Cerner or Athena or some other commercial\nelectronic medical records", "id": "vof7x8r_ZUA_67", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  is not built for your\nalgorithm to plug into. So there is a big\ngap, a large amount of difficulty to getting your\nalgorithms into production systems, which we'll talk about\nas well during the semester. So the goals that Pete and I\nhave for you are as follows. We want you to get intuition for\nworking with healthcare data. And so the next two\nlectures after today are going to focus on what\nhealthcare is really like, and what is the\nhealthcare data that's created by the practice\nof healthcare like. We want you to get\nintuition for how to formalize machine learning\nchallenges as healthcare problems. And that formalization step\nis often the most tricky and something you'll spend\na lot of time thinking through as part of\nyour problem sets. Not all machine learning\nalgorithms are equally useful.", "id": "vof7x8r_ZUA_68", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  throughout the semester\nis that despite the fact that deep learning is good\nfor many speech recognition and computer vision problems,\nit actually isn't the best match to many problems in healthcare. And you'll explore that also\nas part of your problem sets, or at least one of them. And we want you to understand\nalso the subtleties in robustly and safely deploying\nmachine learning algorithms. Now, more broadly,\nthis is a young field. So for example, just recently,\njust about three years ago, was created the first\nconference on Machine Learning in Healthcare, by that name. And new publication venues are\nbeing created every single day by Nature, Lancet, and also\nmachine learning journals, for publishing research on\nmachine learning healthcare. Because it's one of those\nissues we talked about, like access to data, not\nvery good benchmarks, reproducibility has\nbeen a major challenge.", "id": "vof7x8r_ZUA_69", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  to really grapple with. And so as part of this\ncourse, oh so many of you are currently PhD students\nor will soon be PhD students, we're going to\nthink through what are some of the challenges\nfor the research field. What are some of\nthe open problems that you might want to work\non, either during your PhD", "id": "vof7x8r_ZUA_70"}, {"text": "  PETER SZOLOVITS:\nAs David said, I've been at this for a long time. I am not a medical doctor. But I've probably\nlearned enough medicine to be able to play one on\ntelevision over the years. And actually, that's\nrelevant to today's lecture because today's\nlecture is really trying to set the scene\nfor you to say, well, what are the kinds of\nproblems that doctors are interested in by looking\nat what is it that they do. OK. So that's our goal for today. So what we're going\nto do today is to talk about, in a\nvery general way, what are the goals of health care. So how many of you are doctors? A couple.", "id": "DS97JV_o0Fs_0", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  OK. So fix me when I blow it. All right. Please feel free to interrupt. So that's going to\nbe my first task. And then the second\none is going to be what are the things that\npeople actually do in order to try to achieve these goals. What is the practice\nof medicine like? What is the process\nthat generates the data that we're going to be\nusing to learn from? And then I can't resist\ntalking a little bit about paying for health care\nat the end of the lecture because a lot of the\nproblems that come up and a lot of the\ninterest that people show in doing the kind of analysis\nwe're talking about in fact is motivated by money. They want to be able to\nsave money, or spend less money, or something like that. So it's important to know that. OK. Medicine's been around\nfor a long time. I think from probably the\nearliest of recorded history,", "id": "DS97JV_o0Fs_1", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  what the cause of disease\nis, how to cure it. They came up with some\nfairly cockamamie theories because they didn't have a\nlot of scientific, modern approaches to it. But for example, this\nis a photo on the left of a shaman I think from a\nCanadian Indian tribe who's working on the boy\nlying there who's sick. And this shaman would use\nhis knowledge of experience that he'd had with\nother patients. They did know a lot\nabout medicinal plants. They knew something about how\nto care for injuries and things like that. And so this was an effective\nform of health care. Not much record keeping. You don't see electronic medical\nrecords system in that scene.", "id": "DS97JV_o0Fs_2", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of the New York area hospitals. And so there are\ntraditional cultures in which that sort of hands\non interaction with the healer is considered a\nvery important part of the practice of medicine. And if you listen to futurist\ndoctors talking about what medicine is likely to be\nlike, they emphasize the fact that the role of a\nhealer is not just to be a good automaton who\nfigures out the right things but is to persuade\na patient to trust them to do the things that\nhe or she is suggesting to the patient. And there are a lot\nof placebo effects that we know from lots\nand lots of experiments that say that if you think\nyou're going to get better, you are going to get\nbetter, on average. No guarantees. OK.", "id": "DS97JV_o0Fs_3", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So this is an\nintensive care unit. And what you see\nis a patient who has got all kinds of\nelectrical leads, and tubes, and things going into\nthem and is surrounded by tons and tons\nof equipment which is monitoring him and\nperhaps keeping him alive. And so this is the\nhigh tech medicine that we think of as the\ncontemporary version of clinical care. Well, you might say, OK, what\ndoes it mean to be healthy? Right. If the goal of medicine\nis to make people or keep people healthy, what is health? So we turn to the World\nHealth Organization. And they have this lovely,\nvery comprehensive notion of the definition of health. A state of complete physical,\nmental, and social well-being,", "id": "DS97JV_o0Fs_4", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And then they categorize. This and they say, well,\nthere's physical health, there's mental health,\nand there's social health. Social health is\nespecially hard to measure. And I'll come back to\nthat in a little while. So what's easiest to measure\nis how long people live. And so we've had data\non survival analysis for a long time. And this is kind of shocking. If you look here, this lower\ncurve is from around 1800. And what it shows you is that if\nyou lived in India around 1800, your life expectancy\nwas about 25 years. It's not very good. And if you lived in the richest\ncountries, which in those days were typically\nEuropean, in Belgium, your life expectancy was\nway up there at 40 years. Right.", "id": "DS97JV_o0Fs_5", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  OK. Good. I didn't until I\nstarted looking at this. Now by 1950, which is\nnot that long ago, it was like 69 years ago, in\nNorway your expectation was that you'd live\ninto your early 70s, in the US that you would live\ninto your late 60s on average. There was still a\nhuge cliff where, if you lived in Bhutan,\nor Somalia, or something, you were still down around 30. Today, well in 2012,\nwe're doing a lot better. And the thing that's\nstriking is not only that the people who were\ndoing well have gotten better but that a lot of the people\nwho were doing very poorly have also gotten better. And so we're now up-- India, remember, was at 25\nyears of life expectancy", "id": "DS97JV_o0Fs_6", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Now of course,\nthese are averages. And so individuals vary a lot. But it's kind of interesting. So if you look at\nthe numbers, you see that even on a shorter\nterm, there are big changes. So for example, if\nyou're a male living in Rwanda, which is among the\nworst places in terms of life expectancy, your\nlife expectancy, if you were born today,\nis about 62 and 1/2 years. Right. If you were born in 2001,\nit was only 38 years. Now what was going\non in Rwanda in 2001? Genocide. Yeah. They were killing each other. So that's sort of an\nexceptional situation. And that's gotten much\nbetter because they've stopped killing each\nother as genocidal attacks between the Hutu and Tutsi,\nI think, if I remember right.", "id": "DS97JV_o0Fs_7", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  What was going on in\nSouth Africa in 2001. I'm not sure I heard you. AUDIENCE: Failure to\naddress the HIV crisis. PETER SZOLOVITS: Yes. The government at\nthe time was claiming that HIV was not\nthe cause of AIDS and therefore there was no point\nin controlling HIV infections because AIDS was caused\nby something else. Pretty crazy. So that was terrible. And they've gotten\nmuch better at it. So that's what you tend to see\nin a lot of African countries. And what you also see is\nthat there has really been improvement everywhere. So in the US, we went\nfrom males expected to live 74 years to almost 78\nyears, so about a four year increase in life expectancy\nover a period of just 17 years.", "id": "DS97JV_o0Fs_8", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  There's some biological thing\nthat seems to work that way. OK. So a typical way\nthat people look at the survival\nof a population is to say, well, given\na cohort of people born at some instant zero,\nwhat fraction of them are still alive after a\ncertain period of time? And what you see\nis that, of course, 2031 we haven't reached yet. And so these are\nprojections based on sort of theoretical\nextrapolations of actual data. But the older data is real. And what you see is that\nfrom 1851 to, you know, 2011 let's say, these\nnumbers have gone way up.", "id": "DS97JV_o0Fs_9", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Well, it used to be that\nchildhood mortality was enormous. And so if you look at 1851, by\nage 10 about 30% of children had died. And so we've gotten a lot\nbetter at stopping that from happening. People also look at\ncurves like this. So this is a distribution\nof death rates by age. This happens to be for\nJapan a few years ago. And again, females\ndo better than males. The gold curve in the middle\nis the average of the two. And this is very typical\nof almost any country that you look at. The shape of this curve\nis pretty universal. So what does this say? It says that when you are born,\nthere is a relatively high risk", "id": "DS97JV_o0Fs_10", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So these are kids who have\ncongenital abnormalities, have prenatal problems, have\nall kinds of difficulties. And they don't make it. So there's a fairly high\ndeath rate at birth. But once you make it to, I\nthink, about two years old, the death rate is down to\nabout one in 10,000 per year. Right. And then it stays quite low\nuntil you become a teenager. Now why might the death rate go\nup when you become a teenager? Well, suicide is the\nextreme example of that. But teenagers tend\nto be risk seeking rather than risk averse. You know, they\nstart driving cars. They go skiing,\nskydiving, whatever it is that they're doing.", "id": "DS97JV_o0Fs_11", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  But then if you\nmake it to about 20, then there is a relatively\nflat region where by then you've developed enough sense\nto know what risks are worth taking and which ones aren't. And so it's relatively flat\nuntil about age 35 or 40 at which point it starts\ninexorably rising. And of course, as you\nget older and older, the probability that you're\ngoing to die in the next year becomes higher and higher. Right. This is uncomfortable\nfor somebody with my amount of gray hair. Now there is a peculiarity\nin Japan which people puzzled over for a while. And that is that weird\ndip up at age 106. So first of all, that's a\nvery small number of people that that represents. And it turned out\nthat it was fraud.", "id": "DS97JV_o0Fs_12", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to report the death of\ntheir ancient grandmother or great-grandmother\nbecause they wanted to continue collecting\nsocial security payments from the government. So that's an artifact. OK. Now this is a\nserious problem which we're going to return to in\na more technical way later in the semester, which is\nthis problem of disparities. So if you look at, for\nexample, the difference between white and black\nfemale life expectancy in the United States, you\nsee that everybody's life expectancy, as we've shown,\nis going up gradually in this case from 1975 to 2015. But there continues to be a gap\nbetween black and white females and between black\nand white males", "id": "DS97JV_o0Fs_13", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  or less likely to survive longer\ngiven the disparities that exist socioeconomically. Maybe medically. We don't know exactly. And then if you\nlook at Hispanics, however, they do pretty well. So in 2015, you're\nactually a little bit better off to be Hispanic,\neither male or female, than you are to be\neither white or black. But it's still worse to be black\nthan to be white or Hispanic. Right. So these are the\nkinds of facts that drive some of the issues in\nwhat we do in medical care. Now what do people die of? Well, about a quarter of\nthem die of heart disease. And a little over a fifth\nof them die of cancer. This is USA data from 2014 so\nit's not completely up to date but hasn't changed that much.", "id": "DS97JV_o0Fs_14", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of deaths from\nvarious other causes. So heart disease,\ncancer, or chronic lower respiratory disease. So this is like\nCOPD that's caused by smoking, stuff like that. Accidents account for\nabout 5% of deaths. Stroke, cerebral vascular\nevents, Alzheimer's disease, diabetes, influenza, pneumonia,\nkidney disease, suicide, and then everything else\nis about another quarter. OK. Now take a look at those. What kind of diseases\nare these, the biggies? Well. They're chronic. Most of them are chronic. They're also not infectious. But except for\ninfluenza and pneumonia, nothing else there is\ninfectious as far as we know.", "id": "DS97JV_o0Fs_15", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  That asterisk you should\nput after every statement about current medicine. So that's interesting, because\nif you wrote the same table back in 1850, you would\nfind that a lot of people were dying of infections. And they weren't\ntypically living long enough to develop these\nlovely chronic diseases of the aging. So there have been\nbig changes there. Now the other thing\nthat's worth looking at is, in addition to the\nreasons that people die, they start getting sicker. And they are getting sort\nof less value out of life because they're developing\nall these other conditions. So if you look at people\nover 65, about half of them have some form of arthritis.", "id": "DS97JV_o0Fs_16", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  About 40% have hypertension. By the way, if you\nhave trouble with any of the medicalese\nwords, just interrupt. Hypertension is\nhigh blood pressure. Hearing impairment. Me, I'm wearing a\nhearing aid on one side because my ears are going bad. Heart disease, about a quarter. Orthostatic impairment--\nthat means people who wobble because their sense of\nbalance is not so good-- 16%. Cataracts, chronic\nsinusitis, visual impairment, genitourinary problems,\ndiabetes, et cetera. So these are all growing. Here's the list of the next 10. And varicose veins, hernia,\nhemorrhoids, psoriasis, hardening of the arteries,\ntinnitus, corns, calluses,", "id": "DS97JV_o0Fs_17", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  All right. So people develop these by\nthe time they're over 65. So one question we\nmight ask is, well, what is the quality of life? So for example, a\nlot of the doctors that I started working\nwith in the 1970s were great advocates\nof the application of decision analysis\ndecision theory to making medical decisions. And so the problem is how\ndo you evaluate an outcome? And they said, well, the way\nwe evaluate an outcome is we look at your longevity. Obviously, the longer you\nlive, the better typically. But we also look at your quality\nof life during that time. And we say that if you're\nconfined to a wheelchair, let's say, your quality\nof life might not", "id": "DS97JV_o0Fs_18", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  or if you're suffering\nfrom chronic pain, your quality of life\nmight not be as good as if you were pain free. And so we came up with\nthis model that says, well, the value of your life\nis essentially an integral from time zero to\nhowever long you're going to live of\na function Q that says this is a measure of\nhow good your quality of life is at that particular point\nin time and then some discount factor. Right. So what's the role of\nthe discount factor? Well, it's just\nlike in economics. If I offer you some\nhorribly painful thing today versus 10 years from now,\nwhich are you going to choose? Most of us will say later.", "id": "DS97JV_o0Fs_19", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Now who knows what the\nright discount rate is? So in some of their work,\nthey were doing crazy things like taking the financial\ndiscount factors about bank interest rates and\nthings like that and applying them to\nthese health things just because they didn't have\nany better numbers to do. That seems a little suspicious. But nevertheless,\nmethodologically, it's a way of doing it. OK. So how do you measure\nthe quality of life? Well, there is this notion of\nthe activities of daily living. So can you bathe and shower? Can you brush your teeth\nand groom your hair? Can you get dressed? Can you go to the toilet,\nclean yourself up? Are you able to walk,\nget in and out of bed, get in and out of a chair? Can you feed yourself? And then there are a bunch of\ninstrumental factors, things like are you able\nto clean your house and can you manage\nyour money and so on.", "id": "DS97JV_o0Fs_20", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  But they are ways of trying to\nquantify that quality of life by saying how many of these\nthings are you able to do. And there are a lot of\nfederal regulations, for example, that take advantage\nof quantification like this. So if you're asking to be put\non some sort of disability where the government sends\nyou a check to keep you alive, you have to demonstrate\nthat you are at a certain point\non the scale that's derived from these\ncapabilities in order to justifiably get that. So occupational therapy\nis one of the things that people try to\nteach the elderly. My parents died in their\nlate 80s and my dad was 90. And I remember when he would\nhave some medical problem, then he would be put\ninto the clutches", "id": "DS97JV_o0Fs_21", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that he was able to communicate,\nand get around, and not fall for tricks where people\nwanted to get him to send all his money to them,\nor meal preparation and stuff like that. So these are sort of the-- occupational is a\nfunny term for it, because this typically\napplies to people who are retired so it's\nnot really an occupation. But it's the sort\nof things that you need to do in order to be\nable to have a decent life. Well, now there's an\ninteresting valuation question. So if you look at\nthe top right model, we actually don't have very\ngood data on anything other than mortality. So mortality is who dies. So the blue curve\nthere is the curve that you've seen before,\nwhich is, of a cohort,", "id": "DS97JV_o0Fs_22", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of years? The red curve is\na morbidity curve which says how many of those\npeople are still alive and have no sort of problematic\nchronic diseases. So they're not in constant pain. And they're not immobilized. And they're not unable to\ndo the things that I just listed on the previous slides. And then disability\nis when you really become incapable of\ntaking care of yourself. And it typically involves\nmoving into an assisted living facility, or a nursing home,\nor something like that, which is kind of a nightmare\nfor many people and also very\nexpensive for society. So as I said, the\nblue curve there", "id": "DS97JV_o0Fs_23", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  The red curve is a\nhypothetical curve where I just assumed that the\nrate of developing a morbidity is about twice\nthe rate of dying. And the green curve I\nassumed that the rate of developing a disability is-- I can't remember-- I think\nthree times as high as the rate of dying, something like that. So that's why those\ncurves are lower. And they look\napproximately right. But we don't have\ngood data on those. Now the question\nthat you have to ask is, how do you want\nto change this? So for example, suppose that\nwe kept the same situation. We reduced mortality to\n20% of its actual rate. But we kept the disability\nand morbidity rates the same", "id": "DS97JV_o0Fs_24", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So what would that do? Well, what that would do\nis create a huge number of people who are\ndisabled because they're going to live longer\nbeyond the point where they're able\nto function fully. So this is-- yeah. AUDIENCE: Can I just ask,\nwhy does green not just mean healthy? To me, it just seems-- PETER SZOLOVITS: Green\njust means healthy. AUDIENCE: OK. It does. PETER SZOLOVITS: Yeah. So beyond green is what I\nmeant is the morbidity curve. And beyond red is\nthe disability curve. OK. I may have misspoken. So green is healthy. Red means suffering\nfrom some morbidity. And blue means disabled. So if we just extend life but\nwe don't make it any better,", "id": "DS97JV_o0Fs_25", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So other possibilities are\ncompression of morbidity. So for example, if we reduce the\nrate at which people get sick and die but we increase it-- we decrease it initially\nand then increase it so that, on average, people live\nabout the same length of time that they now do, then we're\ngoing to have fewer people who are suffering from\nmorbidity or who are disabled because you last\ndoing well and then you die. So this is the\nwonderful one horse shay where everything falls apart at\nonce, which, you know, frankly, as somebody who's a\nlittle closer to the end than you guys are, I wouldn't\nmind that kind of exit. Right. I don't want to be\ndisabled for 20 years.", "id": "DS97JV_o0Fs_26", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and then drop dead\nat some point. OK. My dad used to say\nthat he wanted to die by being hit by a meteor. Right. He wouldn't know it's coming. It's instant. No suffering, no pain. Perfect. He almost got it but not quite. OK. And then the final story\nis lifespan extension, which is where we simply lower\nthe mortality rate and all the other rates in proportion. And what happens\nis that you start having healthy 107-year-olds\nand not so healthy 120-year-olds in the\npopulation in larger numbers than we do now. OK. Social quality of life. That's a tough one.", "id": "DS97JV_o0Fs_27", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So here's a naive theory. We take the quality of life\nof every person on the planet and we sum them up. And we say, OK, that's the\nsocial quality of life. Is that a good idea? Probably not. Right. For one thing, it\nwould say that we ought to have as much population\nexplosion as possible, because then we'd have more\npeople to integrate over, which doesn't seem sensible. Right. Now obviously if we start\npacking the world so that it's super crowded, then\nthe quality of life will go down eventually\nenough that adding more people is probably not optimal. But nevertheless, that doesn't\nseem like a real satisfying solution. How about less? It was popular\nabout 10 years ago", "id": "DS97JV_o0Fs_28", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  about how would the world\nlook if half the people died. Right. Other than the\ntrauma of the half the people dying,\nyou know, they were proposing that this would\nbe some wonderful sylvan sort of ideal old\nfashioned kind of world. I didn't buy that. And of course, we don't\nknow of a good way to get there,\nalthough it is true that in at least\ndeveloped countries, birth rates keep falling to\nthe point where populations are worried about underpopulation. The Japanese, for example,\nhave very strict immigration policies so you\ncan't become Japanese if you're not born Japanese. And Japanese people\naren't having enough kids to replace themselves. And so the natural population\nof Japan is falling.", "id": "DS97JV_o0Fs_29", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  has all these immigrants\ncoming in and trying to become Italians. And of course, this leads to big\npolitical fights about who gets to be an Italian\nand who doesn't. And then, of course,\nthere's the question of money, which as I say\nwe'll return to later. Now one other important\nthing to consider is that because of the\nincrease in life expectancy, there has been a big change\nin timescale in the way people think about medical care. So it used to be long,\nlong ago in the shaman era, you wouldn't go to a shaman\nto say keep me healthy. You would go to a shaman saying,\nyou know, I broke my arm, or I have this pain in my\nleg, or fix me somehow. And so things were focused\non the notion of cure.", "id": "DS97JV_o0Fs_30", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  But as we've gotten\nbetter at treating acute illnesses--\nwhich, by the way, didn't happen all that long ago. I mean antibiotics\nwere only invented in the early 20th century. And that made a huge\ndifference in stopping people from dying of infections. So then it became more a\nmatter of managing long term chronic illnesses. And that's pretty\nmuch where we are now. The medical world at the\nmoment, most of the action is in trying to understand\nthings like diabetes, and heart disease,\nand cancer, and things like that that develop\nover a long time. And they don't\nkill you instantly like infectious diseases did. But they produce a real burden. And then, of course, the next\nstep that everybody expects is, well, how do\nwe prevent disease?", "id": "DS97JV_o0Fs_31", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  How can we change\nyour motivation? How can we change your diet? How can we change whatever\nit is that we need to change? How can we change your\ngenes to prevent you from developing these\ndiseases in the first place? And that's sort of the future. OK. So that's about what\nmedicine tries to do. But how does it do it? And so we're going\nto talk a little bit about the traditional tasks that\nare attributed to health care practice. So traditionally, people talk\nabout diagnosis, prognosis, and therapy. So diagnosis-- I\ngo to my doctor. I say, doc, I've got\nthis horrible headache. I've had it for two weeks. What's wrong with me? And his job-- in my case,\nit happens to be a \"he.\" His job is to come\nup with an answer.", "id": "DS97JV_o0Fs_32", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Prognosis is he's\nsupposed to predict what's going to happen to\nme, at least if he doesn't do anything. So is this headache\ngoing to go away or is it going to turn out\nto be a brain tumor that will kill me, or is it going\nto be some amoeba that's living in my brain and eat my neurons? All kinds of horrible\nthings are possible. So it's the prospect of\nrecovery as anticipated from the usual course of disease\nor peculiarities of the case. And then therapy, of course,\nis what do you do about it. And prognosis is definitely\ninformed by diagnosis, because if you don't know\nwhat's wrong with me, then it's much harder to predict\nwhat's going to happen to me.", "id": "DS97JV_o0Fs_33", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  then it's much\nharder to figure out what to do to prevent\nthat from happening or to encourage that\nto happen, right? So this is kind of\na serial process. And the way I look\nat it is that there's a kind of cyclic\nprocess of care. And the process starts with\nan initial presentation. So I show up at\nmy doctor's office and I complain about something. And if you ever listen\nto a doctor interacting with a patient, the first\ntime the patient comes in, the first question is always,\nwhat brought you here? Right? And that's called the\npresenting complaint. So if I say, you\nknow, my ankle hurts like hell, that's very\ndifferent than if I say, I stopped being able to\nhear in my right ear, or I have this horrible skin\nrash on my arm, or whatever.", "id": "DS97JV_o0Fs_34", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So then what happens is\nthat the doctor examines you and generates a bunch of data. So these are measurements. And of course, it used to be\nthat those measurements were based on observation. So there are very famous\ndoctors from 100 years ago who were spectacularly good at\nbeing able to look at a patient and figure out what's\nwrong with them by being very astute observers-- the Sherlock Holmes\nkind of subtle, oh, I see that you have a cut\non the inside of your shoe, which means you must have\nbeen going through brambles, and you know, whatever. I'm making up a Sherlock\nHolmes story here. So that generates data. And then we interpret\nthat data to generate some kind of information\nor interpreted data about the patient. And based on that, we\ndetermine a diagnosis.", "id": "DS97JV_o0Fs_35", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Maybe not. Maybe we guess a diagnosis. One of the things I learned\nearly on working in this field is that doctors are actually\nquite willing to make guesses, because it's so useful to\nbelieve that you understand what's going on. If you say, well, there's\nsome probability distribution over a vast number of\npossible things, that doesn't give you very good\nguidance on what to do next. Whereas if you can\nsay, oh, I think this patient is developing\ntype 2 diabetes, then you're locked\ninto a set of questions and a set of approaches\nthat you might try. Now, when we come back to\nlooking at machine learning, machines don't have the\nsame limitations as people. And so for a machine to\nintegrate over a vast number of possibilities is not difficult.\nBut for a human cognition to do", "id": "DS97JV_o0Fs_36", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And so this is actually an\nimportant characteristic of the way doctors think\nabout diagnostic reasoning. So then, having made a\ndiagnosis or made a guess, they plan some kind of therapy. They apply that therapy,\nand then they wait awhile and they see what happened. So if your diagnosis led\nyou to a choice of therapy, you gave that therapy to\nthe patient and the patient got better, then you\nsay, well, it must have been the right diagnosis. If the patient\ndidn't get better, then you say, well, how did\nwhat happened to the patient differ from what I expected\nto happen to the patient? And that drives your revision\nof this whole process. So we again examine\nwhat happened as a result of the therapy. We gather more data.", "id": "DS97JV_o0Fs_37", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  We come up with the revised\ndiagnostic hypothesis. We come up with a\nrevised therapeutic plan, and we keep going\naround the cycle. Now, that cycle\nhappens very quickly if you're a hospitalized\npatient, because you're there all the time. You're available. They're trying to do\nthings to you constantly. And so this cycle happens on\nthe order of hours or a day, whereas if you were\nan outpatient, then you're not dealing with\nsome urgent problem. It may happen over a\nmuch slower period. It may be that your\ndoctor says, well, we're going to\nadjust your drug dose and see if that helps bring\ndown your cholesterol, or manage your pain, or whatever\nit is that he's trying to do. Or worse yet, we're\ngoing to try to convince you to eat more healthy food,\nand six months later we'll see if your hemoglobin\nA1C came down, that you're less close to being diabetic.", "id": "DS97JV_o0Fs_38", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  But that process of continually\nreinterpreting things is a really critical feature, I\nthink, of all of medical care. And if you look back,\nAlan Turing actually talked, in the early\n1950s, about health care as being one of the\ninteresting application areas of artificial intelligence. Why? Well, because it was\nan important topic. And he had the vision\nthat says, as we start getting more\ndata about health care, we're going to be able to\nbuild the kinds of models that we're going to be\ntalking about in this class. But a lot of the early work took\na kind of one-shot approach. So they said, well,\nwe're going to solve the diagnostic problem. So we're going to take a\nsnapshot of a patient, all", "id": "DS97JV_o0Fs_39", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  We're going to feed\nit into an algorithm. It'll come up with a diagnosis. We're done. And that wasn't very\nuseful, because it didn't obey the cyclic nature of\nthe process of providing health care. And so this was a revolution\nthat started, really, around the 1980s when people\nrealized that you have to be in it for the\nlong-run and not for sort of single interactions. OK, well, this is\njust some definitions of these care processes. So here I've listed some ideas\nthat came from a 1976 paper by several of my\ncolleagues, who said, well, here's a cognitive\ntheory of diagnosis. From the initial complaints,\nguess a suitable hypothesis. Use the current\nactive hypotheses", "id": "DS97JV_o0Fs_40", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  so to order more tests, to\nask questions of the patient. And it's the failure\nto satisfy expectations that's the strongest clue to how\nto develop a better hypothesis. And then the hypotheses\ncould be in an activated, deactivated, confirmed,\nor rejected state. They actually built\na computer program that implemented this theory\nof diagnostic reasoning. And these rules, essentially,\nabout whether to activate, deactivate, confirm,\nor reject something could be based both\non logical criteria and on a kind of very\nbad probabilistic model. So it was very bad,\nbecause what they really needed was Bayesian networks. And those were about a decade\nin the future at that point. So they and every other\nsystem built in the 1970s", "id": "DS97JV_o0Fs_41", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  because we didn't understand\nhow to do it correctly. Now, what's interesting\nis somebody noticed that if you strip away\nthe medicine from this, this is kind of like the\nscientific method, right? If you're trying to\nunderstand something, you form a hypothesis. You perform an experiment. If the experiment is consistent\nwith your expectations, then you go on and you've gotten\na little bit more confident in your hypothesis. If your experiment\nis inconsistent with your expectations, then\nyou have to change your theory, change your hypothesis. You go back and gather more\ndata, and then keep doing this until you're satisfied\nthat you've come up with an adequate theory. So this was a\nsurprise to doctors, because they thought\nof themselves more as artists than as scientists. But in a way, they\nact like scientists,", "id": "DS97JV_o0Fs_42", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  All right, this doesn't\nstop with caring for a single patient. So we have all these meta-level\nprocesses about the acquisition and the application of\nknowledge about education, quality control and process\nimprovement, cost containment, and developing references. So this is a picture\nfrom David Margulies, who was chief information\nofficer at Children's Hospital here. And the cycle that I described\nis the one right here. This is the care team\ntaking care of a patient. But of course at some point,\nthat patient is discharged. And then they're in\ncommunity care and self-care, and then maybe they're in some\nsort of active health status management. And then if that goes\nbadly, then there's some episode where they\nreconnect to the health care system. They get authorized\nto come back.", "id": "DS97JV_o0Fs_43", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And so the process\nof care involves people going to the\nhospital, getting taken care of, they get better,\nthey get discharged, they live their\nlives for a while. Maybe they get sick again. They come back. And so there's this larger\ncycle around that issue. And then around that are all\nkinds of things about health plan design and membership\nand what coverage you have and so on. And then I would add\none more idea, which is that if you have\na system like this, you actually want to study,\nat the next meta-level, that system, make\nobservations about it, analyze it, model it,\nplan some improvements, and then intervene in the system\nand observe how it's working", "id": "DS97JV_o0Fs_44", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So in terms of tasks that are\nimportant for us in this class, this class of tasks is\ncentral, because one of the things we're\ntrying to do is to look at the way\nhealth care works and to figure out\nhow to make it better by examining its operation. And that can be done at\nany of these three levels. It can be done in the\nmore acute phase, where we're dealing with\nsomebody who's in the middle of an illness. It can be done at the larger\nphase of somebody who's going through the cycle\nof being well for a while and then being sick, and then\nbeing well again and being sick again. And it can be in terms\nof the system itself, of how do you design a health\ncare system that works better for the population? So this notion of a\nlearning health care system is now a journal.", "id": "DS97JV_o0Fs_45", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  started this new journal. And it's full of articles\nabout this third level of external cycle. So how does the health\ncare system learn? Well, I'll tell you an anecdote. In the mid-1980s, I was teaching\nan AI expert systems course. And I had just come\nback from a conference of medical informatics\npeople, where they were talking about\nthis great new idea called evidence-based medicine. And I remember mentioning this\nto a bunch of MIT engineering students. And one of them raised his hand\nand said, as opposed to what? Right? I mean, to an\nengineer, it's obvious that evidence is the\nbasis on which you analyze things and make things better.", "id": "DS97JV_o0Fs_46", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And so this was almost\na revolutionary change. And the idea that\nthey fostered was the idea of the randomized\ncontrolled clinical trial. So I'm going to sketch\nwhat that looks like. Of course, there\nare many variations. But suppose that I'm one of the\ndrug companies around Kendall Square, and I come\nup with a new drug and I want to prove that it's\nmore effective for condition X than some existing drug B. So what do I do? I find some patients\nwho are suffering from X and I try very hard to\nfind patients who are not suffering from anything else. I want the purest case possible. I then go to my\nstatisticians and I say, let's design\nan experiment where we're going to collect\na standard set of data about all these patients. And then we're going to give\nsome of Drug A and some of them", "id": "DS97JV_o0Fs_47", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And we'll pre-define what\nwe mean by do better. So like, not dying is\nconsidered doing better, or not suffering some bad thing\nthat people are suffering from is considered doing better. And then the\nstatisticians also will tell me, given that you\nexpect Drug A to be, let's say, 10%\nbetter than Drug B, how many patients do you\nhave to enroll in this trial in order to be likely to get\na statistically significant answer to that question? And then they do it. The statisticians\nanalyze the data. Hopefully you've gotten\np less than 0.05. You go to the Food and\nDrug Administration and say, give me permission to\nmarket this drug as the hottest new cure for something\nor other, and then you", "id": "DS97JV_o0Fs_48", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Right? This is the standard\nway that pharma works. Now, there are some problems. So most of the cases\nto which the results of a trial like this are\napplied wouldn't have qualified to be in the trial. For example, we talked\nabout morbidities, about the chronic problems\nthat people have. Well, if you're dealing\nwith one disease, you want to make sure that those\npopulations you're dealing with don't have any of\nthese other diseases. But in the real\nworld, people do. And so we've never\nactually measured what happens to those people\nif you give them this drug and they have these\ncomorbidities. The other problem is\nthat the drug company wants to start making\nthese billions of dollars as soon as possible. And so they want the trial\nto be as short as possible.", "id": "DS97JV_o0Fs_49", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  need to get that 0.05\nstatistical significance. So these are all problematic,\nand they lead to real problems. So I didn't bring any\nexamples, but there are plenty of stories\nwhere the FDA has approved some drug on this\nbasis, and then later they discovered that\nalthough the drug works well in the short-term,\nit has horrible side effects in the long-term,\nor that it has interactions with other diseases where\nit doesn't work effectively for people except for these pure\ncases that it was tested on. So the other idea, the\ncompeting idea is to say, let's build this learning health\ncare system in which progress in science, informatics-- whatever-- generates\nnew knowledge as an ongoing natural byproduct\nof the care experience", "id": "DS97JV_o0Fs_50", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  for continuous improvement\nin health and health care. Wonderful words from the\nInstitute of Medicine, now called the National\nAcademy of Medicine. But it's hard to do this. And the reason it's\nhard to do this is mainly for a very\nprofound underlying reason, which is that people\nare not treated by experimental protocols. So it's very important\nin that sketch that I gave you of the\nDrug A versus Drug B that there is a randomization\nstep where I flip a coin to decide which drug any\nparticular individual is going to get. If I allow that decision to\nbe biased by my expectations or by something else I\nknow about the patient, then I'm no longer\ndoing a fair trial.", "id": "DS97JV_o0Fs_51", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  about how actual patients\nare being treated, they're being treated according\nto what their doctors think is best for them, and so\nthere is no randomization. I mean, if I went to Mass\nGeneral and said, could you guys please treat\neverybody randomly so that we collect\nreally good data, they would throw\nme out properly. OK, so we also need a whole lot\nof technical infrastructure. We need to capture all kinds of\nnovel data sources, which we'll talk about in the next lecture. And then we need a\ntechnical infrastructure for truly big data. So just for example, Dana Farber\nstarted about five years ago-- it's a cancer hospital. And so for every\nsolid tumor, they would take samples of the\ntumor and genotype it-- multiple samples, because\ntumors are not uniform.", "id": "DS97JV_o0Fs_52", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and being able to\ncome up with it. You've got three gigabases, so\nabout over a gigabyte of data-- from each sample,\nfrom each tumor, times all the people who\ncome in and have this test. So you buy some big\ndisk drives or you farm it out to Google or something. But then you need to\norganize it in some way so that it's usefully\neasy to find that data. So today's technique,\ntoday's prejudice is what I call the meat\ngrinder story, which is you take medical\nrecords, genetic data, environmental data,\ndata from wearables, you put them into an\nold-fashioned meat grinder, and out come bits, which\nyou store on a disk. And then you have all\nthe data from which you can build models.", "id": "DS97JV_o0Fs_53", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  You're going to see a lot\nof that in this course. OK, the other thing that\nmedicine tries to do is not to cure people\nbut to keep them healthy. And this has been pretty much\nthe domain of the public health infrastructure. So if you go across the river\nto the Harvard Medical area, there are a couple of\nbig buildings, which is the Harvard School\nof Public Health, and this is what\nthey're all about. So they do things like\ntracking disease prevalence and tracking\ninfections, and worrying about quarantining people. They also do a lot\nof the kind of work we're going to talk about in\nthis class, which is modeling in order to try to understand\nwhat's going on in individual's health, in the health\nof a population, in the operations of\na health care system. So they're very much into this. Now historically, I looked\nback and it turns out", "id": "DS97JV_o0Fs_54", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  in the 17th century, started by\na gentleman named John Graunt. And he was interested\njust in figuring out, how long do people live? And so he came up with\nthese bills of mortality, where he went around to\ndifferent parts of London, talked to undertakers\nand hospitals and whatever health care\nproviders existed at the time, and collected data\non what people died and how many people were\nliving in that area. And so for example, he estimated\nthat the mortality before age six in the 17th century-- this\nwas a long time ago, 1600s-- was about 36%. So if you were a kid, your\nchances of making it to age six", "id": "DS97JV_o0Fs_55", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Kind of shocking. In the 18th century,\npeople you've never heard of-- and Linnaeus, who\nyou probably have heard of, because he was one\nof the early taxonomists for biological and animal\nspecies and so on-- made the first attempts at\nsystemic classification. In the mid-1850s,\nmid-1800s, there was a Congress of the First\nInternational Statistical Congress. And a gentleman named\nWilliam Farr came up with an interesting\ncategorization that said, well, if we're going to\ntaxonomize the diseases, we should separate\nepidemic diseases from constitutional diseases\nfrom local diseases, whereby \"local\"\nhe means affecting a particular part of the body\nfrom developmental diseases.", "id": "DS97JV_o0Fs_56", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  or failure of mental development\nor speech development, and then diseases that are\nthe direct result of violence. So this is things like broken\nbones and bar fight results and stuff like that. So that was the\nfirst classification of disease in about 1853. Note, by the way, that this\nis before Louis Pasteur and his theory of the\ngerm cause of disease. And so this was a\npretty early attempt, and obviously could\nhave benefited from what Pasteur later discovered. So by the 1890s,\nwhich is post-Pasteur, they came up with\na classification that was a hierarchic\nclassification, 44 top-level hierarchies broken up\ninto 99 lower-level categories", "id": "DS97JV_o0Fs_57", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And they adopted this\nas a way of getting, typically, mortality\ndata of what was it that people were dying of. And by the 1920s,\nyou've heard of ICD-- ICD-9, ICD-10. So this is currently used as\na way of classifying diseases and disorders. The International List\nof the Causes of Death was the first ICD back\nin, I think, the 1920s. And then it kept developing\nthrough multiple versions. In 1975, ICD-9 was adopted. In 2015, ICD-10. And these are under the\ncontrol of the World Health Organization, which is now a\nUN agency, although I think it predates the UN actually. And then we have ICD-9\nCM and ICD-10 CM,", "id": "DS97JV_o0Fs_58", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  are an extension of the\nICD-9 and 10 coding. And they are primarily\nused for billing. But they're also used for\nepidemiological research. And if you look at the Centers\nfor Disease Control, CDC, they collect death\ncertificates like this from all over the country. And so this is a person who died\nof a cerebral hemorrhage which was due to nephritis, which was\ndue to cirrhosis of the liver. And so you can use this\nkind of data to say, well, here's the\nimmediate cause of death, here's sort of the\nproximate cause of death, and here's the underlying\ncause of death. And so this is the sort\nof statistical data that we now have available. Now, do any of you watch\nthe PBS show Victoria? Nobody?", "id": "DS97JV_o0Fs_59", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  All right, that's pretty cool. I was stunned, because as I\nwas preparing this lecture and I had the next\nslide, it turns out this plays a role in\none of the episodes that was broadcast about\na week and a half ago. So in the 1850s, there was a big\noutbreak of cholera in London. And John Snow was\na doctor who did this amazing epidemiological\nstudy to try to figure out what was causing cholera. The accepted opinion was that\ncholera was caused by miasma. What's a miasma? STUDENT: Bad air. PETER SZOLOVITS: Bad air, OK? So it's bad air. Somehow, bad air was causing\npeople to get sick and die.", "id": "DS97JV_o0Fs_60", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Whereas Snow started plotting\non a map of the area in London where these were concentrated\nwhere everybody lived. And what he discovered,\ninterestingly enough, is that right in the\nmiddle of Broad Street, which is pretty much at the\nepicenter of all these people dying, was a water pump that\neverybody in the neighborhood used. And that water pump, its supply\nhad become infected by cholera. And so people were\npumping water, taking it home, drinking\nit, and then dying, or at least getting very sick. And he looked at this\nand he said, well, if we turn off that pump,\nthe epidemic will stop. And he actually went to\nthe queen, Queen Victoria-- hence the tie-in to\nthe television show-- and convinced her that this\nwas worth trying, because they", "id": "DS97JV_o0Fs_61", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And they took the pump\nhandle off the pump, and sure enough the\ncholera epidemic abated. Now, of course the underlying\nproblem was sanitation. And they didn't fix that. That took longer. But what's\ninteresting is here is a 2003 study of the\nspread of West Nile virus. So this is mosquitoes that are\nbiting people and infecting them with this nasty disease. And they actually used\nvery similar techniques to figure out that\nmaybe this was coming in on airplanes through JFK. So mosquitoes were hitching\na ride on an airplane and coming into the US. We need to build a border\nwall against mosquitoes.", "id": "DS97JV_o0Fs_62", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that used to be used a lot by\npublic health officials, which was to quarantine people. And so there are\nlots of examples. Anybody's relatives come\nthrough Ellis Island? Must be a few. OK, so they were\nsubject to quarantine. If you were sick when you\narrived at Ellis Island and they didn't know\nexactly how sick you were, they would put you in a\nbuilding and wait a month and see if you got better\nor if you got worse, and then decide whether to\nsend you back or to let you in. So that was a pretty\ncommon practice. There's a famous\nstory of Typhoid Mary, who was a carrier\nof typhoid fever but was not herself affected. Unfortunately, she was a cook. And wherever she was employed,\npeople got really sick.", "id": "DS97JV_o0Fs_63", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  forcibly essentially jailed\nher in some sanatorium in order to keep\nher from continuing to infect people-- it was a\nvery controversial case, as you might imagine. You don't have to\ngo that far back. Here's a 1987 article from\nUPI from The Chicago Tribune. So Jesse Helms,\nwho was a senator, was calling for everybody who\nhas AIDS to be quarantined. So fortunately,\nthat didn't happen. But the idea is still\naround, to say, well, we're going to stop this\ninfection by quarantining people. And then here's a recent\nreport about the Ebola response in Africa over the\npast few years, when Ebola was ravaging\nparts of that continent. And their conclusions\nare it's controversial,", "id": "DS97JV_o0Fs_64", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Quarantine should be\nused as a last resort. Quarantines in urban\nareas are really hard. Mobile populations make it hard. And this is the most\ntechnical conclusion, that if you're going to\nquarantine a bunch of people, you have a huge waste disposal\nproblem on your hands. Because if you have people\nwho might have Ebola, you can't just\ntake their garbage and throw it out somewhere. You have to dispose\nof it properly. OK, so the last thing I\nwanted to talk about-- hopefully mercifully short--\nwill be paying for health care. So I remember reading\nabout 20 years ago that if you bought a\nChevy from General Motors, they spent more money on\nhealth insurance and health care for their employees than\nthey did on the steel that", "id": "DS97JV_o0Fs_65", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  That's pretty amazing. So why is this? Well, essentially, there's\nan insatiable demand for health care, right? Nobody wants to die, except\nfor the suicidal people. And so if I'm sick, I want\nthe best care possible, and I want as much\nof it as possible. Because you know, what\nis more important in life than continuing to live? So we also have gotten better at\nmaking drugs and better tests, and so on. I remember one MRI\nmachines became popular about 30 years ago. The state of\nMassachusetts, for example, had a commission that\nyou had to convince them to be allowed to buy an\nMRI machine for your hospital, because MRI machines\nwere very expensive", "id": "DS97JV_o0Fs_66", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And so they wanted to contain\nthe cost of health care by limiting the number\nof such machines. Well, eventually\nthe costs came down. And so we're doing better. But if you read\nthe newspapers, you see that drug therapy\nis very expensive. We have these wonder drugs\nfor rare diseases or cancers that cost $1 million a year\nto pay for your dosage. And so there is a\nhigh human motivation to do this, and\nnot much pushback-- except from insurance companies. But they just pass the cost\non in insurance contracts. There's also waste. So there are lots of\nstories about half of health care\nexpenses are spent in the last year\nof somebody's life. Although, Ingelfinger, who was\nthe editor of The New England Journal, gave a talk\nhere about 25 years ago.", "id": "DS97JV_o0Fs_67", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  no patient ever came into\nmy office saying, doc, I'm in the last year of my life. And so that was a\ndifficult criterion to try to operationalize. There are marginally\nuseful procedures. The IOM estimated\nthat there are sort of 40,000 to 100,000 quote\nunquote unnecessary deaths per year-- in other words,\ndeaths that could be avoided by just being\nmore careful and a little bit smarter. Well, so the result of this is\nthat if you look at health care spending as a percentage\nof gross domestic product from 1970 to I think 2017,\nI believe, on this graph, what you see is\none real outlier. And that's the\nUnited States on top. So I've selected a few\nof these just to look at. There's the US on top.", "id": "DS97JV_o0Fs_68", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  are roughly at that\nhighest of the crowd level. Canada is down there. The UK is a little lower. Spain is a little lower. Israel is a little lower. Turkey is the lowest\nof the OECD countries in terms of percentage of GDP\nthat they spend on health care. So well, that's OK. But maybe we're getting\nmore for our money than these other countries. And so there are\na lot of analyses that look at stuff like this. They say, well, if\nwe spend so much money per patient\nper year, what do we get in terms of\nthe simplest thing to measure, which\nis life expectancy? How long do people live? And what we discover is\nthat in the United States, we're spending $9,000\na year on patients and we're getting a life span\nof somewhere in the high 70s.", "id": "DS97JV_o0Fs_69", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Whereas in Switzerland,\nthey're spending about $6,000-- so about 2/3-- and they're\ngetting 83 years or something of life for the\nsame price, and so on for all these\ndifferent countries. By the way, this is\nall from Gapminder, which is a wonderful\ndata visualization tool. And I don't have\ntime to show you, but you can click\non individual lines there and slide the slider about\nwhat euro you're talking about, and the data moves. And it's beautiful, and\nit's a wonderful way of understanding it. So there's an important\nthing to remember, which was taught to\nme by my friend Chris Dede at the Harvard\nEducation School. And that is that it's not even\ngood enough to stand still.", "id": "DS97JV_o0Fs_70", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  If you look at the\ngrowth of productivity over this period of\n10 years by industry, you discover that\nproductivity went up by seven point something\npercent for durable goods and went down by something\nlike 2% for mining. About 1% for construction. Information technologies grew\nby 5 and 1/2% or something. So if you ask the\nquestion, what happens if the demand for\nthese goods remains constant over a period of time? And what you discover is that\nbecause the more productive things become\ncheaper, they wind up occupying a smaller fraction\nof the total amount of money that's being spent. So your computer, your\nlaptop, is a lot cheaper today than it was 30 years ago.", "id": "DS97JV_o0Fs_71", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of spending that people do\non things like information technology, at least\nper item, is much lower than it used to be. And if in the aggregate it's\nalso lower than it used to be, that means that something\nelse must be higher, right? Because it sums to 100%. And so what this\nshows is that if you spend 30 years at the same\nrates of productivity growth, mining grows from whatever\nfraction of the economy it was to something that's about\nthree times as big a fraction. Right? And if productivity\ngrowth is better, then that sector of\nthe economy shrinks. So I think something\nlike this is happening in health\ncare, which is that there's infinite demand.", "id": "DS97JV_o0Fs_72", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  We're not getting better\nas fast as electronics is getting better, for example. So people have tried\ndoing various things. \"Managed care\" was the\nbuzzword of the 1980s and '90s. And they said, well,\nwhat we're going to do is to prevent\npeople from overusing medical services by requiring\npre-admission review, continued stay review, second\nsurgical opinion. We're going to have\npost-care management, where if you're released\nfrom the hospital and you're in that\nsecond of the cycles, people will call you at home\nand try to help you out and make sure that you're doing whatever\nis best to keep you out of the hospital. And We're going to try\nvarious experiments, like institutional\narrangements, that say, well, if I as a doctor agree to refer\nall my patients to Mass General", "id": "DS97JV_o0Fs_73", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And they'll get some kind of\nefficiencies from aggregation. And so maybe that's one\nway of controlling costs. So leakage is the idea of\nkeeping people in-system. And capitation is\nan interesting idea, which says that\ninstead of paying for what the\nhospital does for me or what the doctor\ndoes for me, we simply pay him a flat fee for the\nyear to take care of me. And that takes away\nthe incentive for him to do more and more and\nmore to get paid more. But of course, it\ngives them an incentive to do less and less and\nless so that he doesn't have to spend as much money. And so it's sort\nof a knife balance to figure out how to do this. But that's an\nimportant component. So if you look at the\nevaluation of managed care a long time ago,\nwhat they said was it helped reduce inpatient costs\nby increasing outpatient costs.", "id": "DS97JV_o0Fs_74", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  from going to the hospital into\ngoing to their doctor's office. But it's been pretty much a wash\nin terms of overall spending. Doctors also hated managed care. I was sitting with\none of my colleagues at a Boston hospital, and\nan insurance company clerk called him to dun him for\nhaving ordered a certain test on a heart patient. And so he was furious. And he turns to her and\nsays, so which medical school do you have your diploma from? And of course, she doesn't\nhave a medical degree. She's following some\nrules on a sheet about how to harass doctors\nnot to order expensive tests. And so we have Edward Annis,\nthe past president of the AMA, who says, well,\nin the glory days there was no bureaucratic\nregimentation, no forms,", "id": "DS97JV_o0Fs_75", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And all my patients were\nhappy, and I was happy, and things were ideal. If you actually look\nback at those days, it wasn't as good as\nthe cracks it up to be. And some of those\nissues of disparity that we were talking\nabout were horrible. So for his patients\nwho were rich and who could afford to see\nhim, life was pretty good. But not so much for\nunderserved populations. So you've seen ObamaCare come\nand partly go, and continues to be controversial. But it's trying to foster\nbetter information technology as a basis for getting doctors\nto make better decisions. It's trying to foster these\naccountable care organizations, which is a version\nof capitation, to put the pressure on to reduce\nthe amount of health services", "id": "DS97JV_o0Fs_76", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  There is a hospital readmission\nreduction program now that says if you are a\nMedicare patient, for example, and you're discharged\nfrom the hospital and you're readmitted within\n30 days of your discharge, then they're going to\ndun you and not pay you for that readmission,\nor not pay you for part of that readmission. But if you look at the\nstatistics, which I just did, that's the distribution of the\npayment adjustment factors. So it turns out the\nlowest number is 97%. So a 3% decrease\nin reimbursement is something that the\nCFO of your hospital would really care about. But it's not like a 25%\nreduction in reimbursements. So this has had a\nfairly minor effect. Let me just finish by saying\nthat money determines much.", "id": "DS97JV_o0Fs_77", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  is that IT traditionally\ngets about 1% to 2% of spending in medical centers,\nwhereas it gets about 6% or 7% in business overall and\nabout 10% to 12% for banking. And a lot of these systems\nare managed by accountants, although that is\nslowly changing. So in the 1990s,\nHST with Harvard started a training program\nfor medical doctors to become medical\ninformaticians-- so practicing this sort of witchcraft. And our first two\ngraduates, one of them is now CIO at the\nBeth Israel Deaconess. The second one is CIO\nat Children's Hospital. And so one of my big\nsuccesses personally has been to displace\nsome accountants by doctors who\nactually understand this technology to some extent.", "id": "DS97JV_o0Fs_78", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  There's a funny last\nslide here, which has a pointer that I would\nwant you to remember. The slides will be\nup on our website. You can follow it. MIT has a program called GEMS. It's the General Education\nin Medical Sciences, intended as a minor program for\npeople in the PhD program in some other field. And if you're serious about\nreally concentrating on health care, developing at least\nthe kind of understanding of the health care process that\nI've tried to give you today and that would allow you to\nplay a doctor on television is really important. And there is a program that\nhelps you achieve that, which I commend to people.", "id": "DS97JV_o0Fs_79"}, {"text": "  PETER SZOLOVITS: So\nlast time we talked about what medicine\ndoes, and today I want to take a deep\ndive into medical data. And I'm going to use as examples\na lot of stuff from the MIMIC database, which is\none of the databases that we're going to be\nusing in this class. Some of you are probably\nfamiliar with it, and some of you are not. And there are, I hope,\nsome takeaway lessons from this discussion. So for example, a\nfew years ago, when MIMIC-III was about\nto be released, I was playing with\nthe data, and I looked at the distribution\nof heart rates in the CareVue part\nof the database.", "id": "0UFwGJe6ubg_0", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  has intensive care data from\nabout 60-something thousand admissions to intensive care\nunits at the Beth Israel Deaconess Medical Center over\na period of about 12 years. And one of the\ntechnical difficulties that we encountered is that in\nthe middle of that time period. The hospitals shifted from\none information system that they used in their\nintensive care unit to another. CareVue is the old one. MetaVision is the new one. And of course, they're\nnot exactly compatible. So we'll see some\nexamples of that. So this is the old data. So this is from CareVue. And you look at that and\nsay, well, heart rates range from 40 to 200\nroughly, which is OK. But then there's\nthis funny thing. There are two peaks. So where, if ever,\ndo you see two peaks", "id": "0UFwGJe6ubg_1", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Not typical. And so my initial reaction was-- [LAUGHTER] So then I looked\na little closer, and I said, hmm, what\ndo the heart rates look like from these two systems? And if you look in CareVue,\nyou see the picture that I just showed you. And if you look\nin MetaVision, you see this other picture,\nwhich looks more like what you would normally expect. And so I'm sitting there\nscratching my head going, OK, there must be some\ndifference between these. It's not that simultaneous\nwith the switchover of the hospital from one\ninformation system to another. Physiology of people\nchanged, and all of a sudden some\nsubset of people started having\nfaster heart rates.", "id": "0UFwGJe6ubg_2", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  But if you think about\nthat what subset of people have faster heart rates? AUDIENCE: Athletes. PETER SZOLOVITS: Hmm? AUDIENCE: Babies? AUDIENCE: If you're\nin a stress test. PETER SZOLOVITS: Unh-hmm. AUDIENCE: Is it children? PETER SZOLOVITS: Yeah, kids. So I said, hmm, interesting. So anyway, if you look\nat the statistics, you see that the mean heart\nrate in CareVue is 108, and the mean heart rate\nin MetaVision is 87. But of course, means\nare not that meaningful when you look at these\nbimodal distributions. So then I said, well, what\nif we just look at adults? So we look at people from age\ngreater than 1 up to age 90. And I'll say a word\nabout that in a minute. And I look at those\ntwo distributions. They look pretty close.", "id": "0UFwGJe6ubg_3", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So that means that\nthe number of patients of different ages\nin the adult group is similar in the two data sets. But if I don't exclude the\nvery young or the very old, then I see this\nfunny distribution where I have suppressed\nages greater than 90 but not the young. And what you see is\nthat in CareVue there's this giant spike at age 0. So what happened at the hospital\nis that under the old system it was also being used in the\nNICU, the Neonatal Intensive Care Unit. And the new system was not\nbeing used in the NICU. And therefore, they didn't\ncapture data about babies. And in fact, if you\nlook at age versus heart rate of the entire\npopulation, you", "id": "0UFwGJe6ubg_4", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So here are the adults that\nwe've been talking about, and here are the babies. And sure enough, they\nhave higher heart rates. And then here are these\n300-year-old people. [LAUGHTER] You go, wow, I don't think I'm\ngoing to have a heart rate when I'm 300 years old. So who are those people? Anybody have a clue? Yeah? AUDIENCE: Entry errors. PETER SZOLOVITS: Sorry? AUDIENCE: Entry errors? PETER SZOLOVITS: There\nare too many of them. Yeah, entry errors is\nalways a possibility, but there's quite a\nfew data points there. Yeah? AUDIENCE: [INAUDIBLE] PETER SZOLOVITS: Close. It's not quite missing data. So HIPAA, the Health Insurance\nPortability and Accountability Act, defines a set of\ncriteria about protecting personal health information. And one of the things\nyou are not allowed to do", "id": "0UFwGJe6ubg_5", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  who is 90 years old or older. And the reason is because\nthe number of 97-year-olds is pretty small. And so if I tell you that\nWilly is 97 years old, then you're going to be able\nto pick him out of a population relatively easily, and so\nit's prohibited to say that. So as a result, everybody\nwho's 90 or older gets labeled as being 300\nyears old in the database. It's an artifact. It's like back in my youth, I\nworked as a computer programmer at a health sciences\ncomputing facility at UCLA. And we used to have a\nconvention that missing data was represented by 99999. And of course, if you average\nthat into a real data set, you get garbage, which\npeople did regularly.", "id": "0UFwGJe6ubg_6", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  running into one of those. If you look at just the\nadults, the two systems actually look very similar. So the blue and red\ndots, or the two systems, and I've drawn the trend\nlines between them, and you can see that\nthey're very similar. So it looks like\nas you get older, your heart rate\ndeclines very slightly. But it does so equally\nin the two data sets. Yeah? AUDIENCE: On the previous\nslide, beyond 300, it looks like they're\nolder than 300? PETER SZOLOVITS: Well that's\nbecause the ages there are computed at the time that\nthe heart rate is measured. And so if you are\n300 years old when you're admitted to\nthe hospital, if you stay in the hospital\nfor six months, then you're 300\nand 1/2 years old by the time of that measurement. [LAUGHS] So that's why\nthere are data points to the right of 300. Yeah, good catch.", "id": "0UFwGJe6ubg_7", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And of course, they do\nhave higher heart rates. And here here are the oldsters. So actually, there are\npeople out to 310 years old because maybe they were\ndischarged from the hospital. And then at age\n100, they came back. You know, maybe they\nwere 90 years old at the time they were initially\nadmitted 10 years later. They came back, and we\nrecorded more data about them, and so this is all\nrelative to that 300. OK, so that's just one example. And the lesson\nthere is be careful when you look at data because\nit can really easily fool you 'cause there are all kinds\nof funny things about the way it's collected, about\nthese artifactual things like 300-year-old\npatients and so on. So here's a catalog\nof the types of data that are available to us. So we have the typical kind of\nelectronic health record data", "id": "0UFwGJe6ubg_8", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  demographics, age, sex,\nsocioeconomic status, insurance type, language,\nreligion, living situation, family structure,\nlocation, work, et cetera. We have vital signs-- your weight, your height,\nyour pulse, respiration rate, body temperature, et cetera. So these are\ntypically the things that if you ever go\nto a doctor's office, or you go into a\nhospital, the nurse will take you aside and weigh\nyou and measure your height and check your blood pressure\nand take your temperature and stuff like that. These are standard\nvital signs, and so we have lots of those recorded. Medications--\nprescription medications, over-the-counter\ndrugs, illegal drugs if you're willing not to\nlie to your health care provider, alcohol. Again, one of my\nearliest days, I was hanging out with a\ncardiologist at Tufts Medical", "id": "0UFwGJe6ubg_9", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  who looks kind of terrible. And we're talking to her-- well, the doctor\nis talking to her. I'm trying to stay\nout of the way. And he says, so do\nyou drink alcohol? And she says, oh, no,\nnever touch the stuff. And then we talk some\nmore, and we go out of the patient's room. And the doctor turns to me\nout of earshot of the patient and says, oh, she's\na chronic drunk. I said, well, how do you know? And he says, well,\nfrom lab tests, from the appearance of her\nskin, from her general demeanor, from various sort of\nineffable factors. And so patients lie. They really do because they\ndon't want to tell you things.", "id": "0UFwGJe6ubg_10", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So there is this whole field\ncalled med red, medication reconciliation, which is the\nhospitals or the doctors' offices attempt to figure\nout what medications you're actually taking. So I'm a member of\nthe MIT health plan, and if I sign into my\nhealth plan account, it tells me that I'm\ntaking some pills that I got 12 years ago as part\nof a laboratory test, where I took two pills\nwhich were supposed to have some\nphysiological effect, and then they measured that. And I've never\ngotten another pill and never taken one\nsince then, nor would it be particularly good for me. But it's still on my\nrecord, and there's no notice of it ever\nhaving been discontinued. And that's a real\nproblem because if you're taking care of a\npatient, you'd like", "id": "0UFwGJe6ubg_11", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  and it's hard to know. Then lab tests-- so\nthis is the things that you imagine\nthat we do a lot of, and these are components of\nthe blood and the urine mainly, but also of the stool, saliva,\nspinal fluid, fluid taken off the belly, joint fluid,\nbone marrow, stuff coming out of your lungs. It's anything and\nany place where you can produce\nsome specimen, they can send it to a lab and\nmeasure things in it, and they measure lots and lots\nof different kinds of things. And these are often useful. Pathology, qualitative and\nquantitative examination of any body tissue, for\nexample, biopsy samples or surgical scraps. You know, if they\ndo an operation, they cut something out\nof you, that typically winds up on a\npathologist's bench,", "id": "0UFwGJe6ubg_12", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  and that's, again,\nuseful information. Microbiology--\never since Pasteur, we know that organisms\ncause disease. And so we're quite interested\nin knowing what organisms are growing inside your body. And typically, testing is not\nonly to identify the organism but also to figure\nout which antibiotics it's sensitive to\nand insensitive to. And so you'll see things like\nreports of sensitivity testing at various dilutions. In other words, they try\nto give a strong dose of an antibiotic a\nweek weaker dose a week or dose a weaker\ndose a week or dose to see which is the\nminimum level of dosing that's enough to\nkill the bacteria. There's a comma missing there,\nbut input, output of fluids", "id": "0UFwGJe6ubg_13", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  in the hospital, often get\neither dehydrated or over hydrated. And neither of those\nis good for you, and so trying to keep track\nof what's going into you and what's coming out\nof you is important. Then there are tons of notes. So an important one that we're\ngoing to look at in this class is discharge summaries. So these are the\ntypically long notes that are written at the\nend of a hospitalization. So this is a summary of why you\ncame in, what they did to you, the main things they\ndiscovered about you, and then plans for what to\ndo after your discharge. Where are you going to go? What drugs are you\ngoing to be taking? When are you supposed to come\nback for follow up, et cetera. I'll show you an excruciatingly\nlong one of those later in the lecture today.", "id": "0UFwGJe6ubg_14", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  nurses, various\nspecialties, consultants. The referring physician--\nif somebody sends you to the hospital, that\ndoctor will usually write a note saying this\nis what I'm interested in. Here's why I'm sending\nin the patient. There are letters back to the\nreferring physician saying, OK, this is what we found out. Here's the answer to the\nquestion you were asking. There are emergency\ndepartment notes. So that's often the first\ncontact between the patient and the health care system. So these are all important. And then there's tons\nand tons of billing data. So remember the EHR\nsystems were initially designed by accountants. And they were designed for\nthe purpose of billing. And so we capture a lot of\ndata about formalized ways of describing the\ncondition of the patient", "id": "0UFwGJe6ubg_15", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  to submit the right bills. You obviously want to bill\nthrough it as much as possible. But you have to be able to\njustify the bills that you submit because insurance\ncompanies and Medicare and Medicaid don't have\na good sense of humor. And if you submit bills for\nthings that you can't justify, then you get penalized. And then there are\nadministrative data like, which service are you on? So this this is occasionally\na confusing thing. You can go into the hospital\nand have heart problems, but it turns out that\nthe heart intensive care unit, the cardiac intensive care\nunit, is full up with patients. But there's an extra bed in the\npulmonary intensive care unit, and so they stick\nyou in that unit, but you're still on\nthe cardiology service.", "id": "0UFwGJe6ubg_16", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  still have to take care of. Transfers are when you get\ntransferred from one place to another in the hospital. Imaging data-- so I'm not going\nto talk about that much today, but there are X-rays,\nultrasound, CT, MRI, PET scans, retinal scans, endoscopy,\nphotographs of your skin and stuff like that. So this is all imaging\ndata, and there's been a tremendous\namount of progress recently in applying\nmachine learning techniques to try to interpret\nthe contents of these data. So these are also\nvery important. And then there's the whole\nquantified self movement. I mean, how many of you\nwhere an activity tracker? Only about 1/3? I'm surprised at\na place like MIT. [LAUGHTER] So you know, we measure\nsteps and elevation change", "id": "0UFwGJe6ubg_17", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And you can record\nvital signs and diet and your blood sugar,\nespecially if you're diabetic; allergies, allergic incidents. There's all this mindfulness,\nmood, sleep, pain, sexual activity. And then people have\ndeveloped this idea of N of 1 experiments. For example, I had\na student some years ago who suffered from psoriasis. It's a grody\ncondition of the skin. And the problem is there\nare no good cures for it. And so people who\nsuffer from psoriasis try all kinds of things. You know, they stop\neating nonce for a while, or they douse\nthemselves with vinegar. Or they do whatever crazy\nthing comes to mind. And we don't have a good theory\nfor how to treat this disease. But on the other hand, some\nthings work for some people.", "id": "0UFwGJe6ubg_18", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  has been developed that says,\nwhen you try these things, act like a scientist. Have hypotheses. Take good notes. Collect good data. Be cognizant of things\nlike onset periods, where you know you may have to drip\nvinegar on yourself for a week before you see any effect. So if that doesn't do a thing\nafter one day, don't stop. And furthermore, if you stop\nthen don't start something new immediately because\nyou will then be confused about whether this\nis the effect of the thing you were on before or the\nnew thing that you're trying. So there's all sorts\nof ideas like that. So this is a slide from\nour paper on MIMIC-III. And it gives you a kind\nof overview of what's going on with the patient. So if you look at this-- I'm going to point\nwith my hands--", "id": "0UFwGJe6ubg_19", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  This patient starts\noff at full code. That means that if something\nbad happens to him, he wants everything to be\ndone to try to save him. And he winds up in\ncomfort measures only, which means that if\nsomething bad happens to him, he wants to die-- or his family does\nif he's unconscious. So what else do we\nknow about this guy? Well GCS is the\nGlasgow Coma Score. And it's a way of\nquantifying people's level of consciousness. And you see that\nat the beginning this patient is oriented,\nand then gets confused. And finally, is only making\nincomprehensible words or sounds. Motor, he's able\nto obey commands. Eventually, he's\nonly able to flex", "id": "0UFwGJe6ubg_20", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So he's no longer conscious. Eye movements-- he's able\nto follow you spontaneously. He's able to orient to speech. And eventually\norientation at all. So this is clearly somebody\nwho's going downhill quickly and, in fact, dies at\nthe end of this episode. Now, we then look\nat labs so we can see what is their level of\nplatelets at about the time that they're measured,\ntheir creatinine level, their white blood cell count,\nthe neutrophils percentage, et cetera. And there's not every possible\ndata point on the slide. This is just illustrative. The next section is medications. So the person is on morphine. They're on Vancomycin,\nwhich is an antibiotic. Piperacillin-- I don't\nknow what that is. Does somebody know? AUDIENCE: Antibiotic.", "id": "0UFwGJe6ubg_21", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  AUDIENCE: It's antibiotic. PETER SZOLOVITS: OK. Sodium chloride 9%, So that's\njust keeping him hydrated. Amiodarone and dextrose. So dextrose is giving\nhim some energy. And then these are the\nvarious measurements. So you see the heart rate,\nfor example, is up pretty high and is going up near the end. The oxygen saturation\nstarts off pretty good. But here we're\ndown to 60% or 50% O2 sat, which is supposed\nto be above about 92 in order to be\nconsidered reasonable. So again, this is a\nvery consistent picture of things going very badly wrong\nfor this particular patient. So this is all the\ndata in the database. Now, if you want to try to\nanalyze some of this stuff, you can say, well,\nlet's look at the ages at the time of the last lab\nmeasurement in the database.", "id": "0UFwGJe6ubg_22", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So we can see that many of the\nICU population are fairly old. There's a relatively small\nnumber of young people and then a growing number of\nolder people in both females and males. If we look at age at\nadmission by gender-- so this is age at admission not\nage at the time the last lab measurement was done-- it's a pretty similar curve. So we see that\nfemales were 64.21 at time of last lab measurement;\n63.5 at the time of admission. So we can look at demographics,\nand demographics typically includes these kinds of factors,\nwhich I've mentioned before.", "id": "0UFwGJe6ubg_23", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  between this and, for\nexample, the age distribution, we see that if you look at the\ndifferent admission types-- so you can be either\nadmitted for an emergency for some urgent\ncare or electively. And it doesn't seem to make\na whole lot of difference, at least in the means of the\npopulation age distribution. On the other hand, if you\nlook at insurance type and, say, who's\npaying the bills, there is a big difference\nin the age distributions. Now, why do you think that\nprivate insurance drops way off at about 65? AUDIENCE: Isn't insurance\nalways covered for everyone by the state health? PETER SZOLOVITS: It's\nbecause of Medicare. So Medicare covers people\nwho are 65 years old.", "id": "0UFwGJe6ubg_24", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  I was talking to somebody at\nan insurance company who's a bit cynical, and\nhe said suppose that you see a 63-year-old\npatient who's developing type 2 diabetes, what\nshould you do for him? Well, there are\nstandard things you should do for\nsomebody developing type 2 diabetes, like get\nhim to eat better, get him to lose weight, get\nhim to exercise more, et cetera, et cetera. But his cynical answer\nwas absolutely nothing. Why? Well it's very\ncheap to do nothing. Most people who\ndevelop type 2 diabetes don't get real sick\nin the next two years. And by the time\nthis patient is 65, he'll be the government's\nresponsibility, not the insurance company's. Nice. AUDIENCE: Yeah.", "id": "0UFwGJe6ubg_25", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  are insured by Medicare or\nMedicaid, not that surprising. Self-pay is a\npretty small number because it's insanely expensive\nto pay for your own health care. What about where you came from? Were you referred\nfrom a clinic, or were you an emergency room admit? Or were you referred\nfrom an HMO or et cetera? And other than a transfer from\na skilled nursing facility or transfer within the\nfacility, within the hospital, it doesn't make much difference. The averages there\nand the distributions look moderately similar. If you're coming from a\nskilled nursing facility, if you are in a skilled\nnursing facility, you're probably old because\nyounger people don't typically", "id": "0UFwGJe6ubg_26", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And I'm not sure why\ntransfers within the facility are significantly\nyounger ages, but that's true from the MIMIC data. What about age at\nadmission by language? So some people speak English. Some people speak not available. Some people speak\nSpanish, et cetera. So it turns out the\nRussians are the oldest. And that may have to do\nwith immigration patterns, or I don't know exactly why. But that's what the data show. If you do it by\nethnicity, it turns out that African-Americans,\non the whole, are somewhat\nyounger than whites. And Hispanics are\nsomewhat younger yet.", "id": "0UFwGJe6ubg_27", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  need intensive care earlier\nin life than whites. So this is a topic that's\nvery hot right now, discussions about how bias\nmight play into health care. Yeah? AUDIENCE: What does\nunable to obtain mean? PETER SZOLOVITS: It\njust means that somebody refused to say what\ntheir ethnicity was. AUDIENCE: When they\nwere asked this? PETER SZOLOVITS: Yeah. I think. I'm not positive. AUDIENCE: So just to confirm. This also represents Boston's\npopulation dynamics too, right? PETER SZOLOVITS: It's\nthe catchment basin of the Beth Israel\nDeaconess Hospital, which is Boston clearly. But there are-- it turns out\nthat a lot of North Shore people go to Mass General, and\nso different hospitals have different catchment basins. AUDIENCE: Does it have anything\nto do with like, is this just the ICU?", "id": "0UFwGJe6ubg_28", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  PETER SZOLOVITS: These are\nall people who at some point were in the ICU. So these are the\nsicker patients. Yeah? AUDIENCE: So just\nwant to double-check there's a higher proportion of\nblack, African American people in the population here as\nwell because the red is higher than the others? PETER SZOLOVITS: No, actually-- I don't remember if\nI have that graph-- I think this is cumulative. AUDIENCE: Oh, OK. PETER SZOLOVITS:\nSo most people are white for whatever definition\nof white we're using. And I think it's only the\nincrement that you see on top. All right, how about\nmarital status? Well, according to this,\nit's bad to be single.", "id": "0UFwGJe6ubg_29", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  I'm not sure why\nit's true for the ICU because if you don't have\nanybody at home to take care of you when you get\nsick, it seems reasonable that you'd be more likely\nto wind up in the hospital. But I don't know why you'd\nwind up in intensive care. Yeah? AUDIENCE: Isn't it possible\nthat those are also single people are probably\nyounger than married people, and those are probably\nyounger than-- PETER SZOLOVITS: Yes, yeah. AUDIENCE: [INAUDIBLE] people. PETER SZOLOVITS: Yeah,\nthat's probably also right. So here's an\ninteresting question, a little bit related\nto something you'll see on the next problem set. So could we predict\nin-hospital mortality from just these\ndemographic features? So I'm using a tool\nin language called", "id": "0UFwGJe6ubg_30", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  and I've set it up to do\nbasically logistic regression. And it says I'm\npredicting whether you die in the hospital based on\nthese demographic factors. And it turns out that\nthe only ones that are highly significant are age. So that's not surprising,\nthat older people are more likely to die\nthan younger people. It's generally true. And if I'm unable to\nobtain your ethnicity, or I don't know your ethnicity,\nthen you're more likely to die. I have no clue why\nthat might be the case. And other things are\nnot as significant. So if you speak\nSpanish or English, you're slightly\nless likely to die. You see a negative\ncontribution here. And if you speak Russian, you're\nslightly less likely to die.", "id": "0UFwGJe6ubg_31", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  but it is at the p\nequal 0.06 level. And marriage doesn't seem\nto make much difference in predicting whether\nyou're going to die or not. Now, remember, this\nis ICU patients. And we're looking at\nin-hospital mortality. AUDIENCE: For\nethnicity, can they learn that at any\npoint in this study, or just right at the beginning? Or do you know? Because I don't know. PROFESSOR: I don't know. AUDIENCE: Because\nit could be that unable to obtain means that they\ndied before we can ask them. PROFESSOR: No,\nbecause there wouldn't be that many of those\npeople, I think. There are not that\nmany people who don't live past the intake interview. And they do ask them. AUDIENCE: [INAUDIBLE] PROFESSOR: Yeah, that\nwould be an example.", "id": "0UFwGJe6ubg_32", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  statistically. OK. Well, so I've already\nmentioned that there is this problem of having\nmoved from CareVue view to MetaVision just in\nthe MIMIC database. But of course, this is\na much bigger problem around the country\nand around the world, because every hospital has its\nown way of keeping records. And wouldn't it be nice\nif we had standards? And of course, there's\nthis funny phrase, the wonderful thing\nabout standards is that there's so\nmany to choose from. So for example, if you look\nat prescriptions in the MIMIC database, here are two\nparticular prescriptions for subject number\n57139 admitted on admission ID 155470. And so they have the same start\ndate but different end dates.", "id": "0UFwGJe6ubg_33", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  and the other is for clobetasol\npropionate 0.05% cream. That's a skin lotion thing for-- I think it's a\nsteroid skin cream. So if you look in\nthe BI's database, they have their own\nprivate formulary code where this thing is\nacet325 and this thing is clob.05C30, right? And if you look,\nthere there's also something called a GSN, which\nis some commercial coding system for drugs. Maybe having to do with\nwho their drug supplier is at the hospital. And these have different codes. There's the National Drug Code,\nwhich is an FDA assigned nine", "id": "0UFwGJe6ubg_34", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  in, and what's its strength. And so you get these. Then there's a human\nreadable description that says Tylenol comes\nin 325 milligram tablets. And the clobetasol\ncomes in 30 gram tubes. And the dose is supposed\nto be 325 to 650, i.e. one to two tablets\nmeasured in milligrams. The dose here is one\napplication, whatever that is. I don't know what\nthe 0.01 means. And this is a tablet\nand that's a tube. And this is taken orally. That's administered\non the skin, right? So this is a local database. AUDIENCE: For a doctor,\nthey just [INAUDIBLE]", "id": "0UFwGJe6ubg_35", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  It wasn't true when\nthe MIMIC database started being collected. And the BI was\nrelatively late in moving to that compared to some of the\nother hospitals in the Boston area. Each hospital has\nits own digitorata for what it thinks\nis most important. And I think the BI just didn't\nprioritize it as much as some of the other hospitals. OK, so then I said, well, if\nyou look at prescriptions, how often are they given? So remember, we have\nabout 60,000 ICU stays. And so iso-osmotic dextrose\nwas given 87,000 times to various people. Sodium chloride\n0.9 percent flush. Do you know what that is? Have you ever had an IV? So periodically,\nthe nurse comes by", "id": "0UFwGJe6ubg_36", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  to make sure that it\nhasn't clogged up. That's what that is. Insulin, SW. I don't know. Salt water? I don't know what SW is. Magnesium sulfate,\ndextrose five in water. Furosimide is a diuretic. Potassium chloride\nreplenishes potassium that people are often low on. And then you go, so why is\nthere this D5W and that D5W? And that's probably some\ndata in the system, OK? One of them has an NDC\ncode associated with it and the other one doesn't\nbut probably should. Yeah. AUDIENCE: I was actually\ngoing to ask, does yours mean that they're standard\nacross hospitals or just that we\ndon't have the data? PROFESSOR: The NDC code should\nbe standard across the country,", "id": "0UFwGJe6ubg_37", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  But not every hospital\nuses them, OK? And for the ones\nthat say zero, I'm not sure why they're\nnot associated with a code in this\nhospital's database. OK, next most common,\nyou see normal saline, 0.9 percent sodium chloride. So that was the same stuff\nas the flush solution but this time not\nbeing used for flush. Metoprolol is a beta blocker. Here's another insulin this time\nwith an NDC code, et cetera. I love bag and vial, OK? So these are not\nexactly medications. A bag is literally like a baggy\nthat they put something into, and a vial is\nliterally something that they put pills in.", "id": "0UFwGJe6ubg_38", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Because they get to\ncharge for it, OK? And I don't know\nwhat the charge is, but it wouldn't\nsurprise me if you're paying $5 for a plastic\nbag to put something in. OK, so if we say, well, how\nmany pharmacy orders are there per admission at this hospital,\nand the answer is a lot. So if you look at-- it's a very long\ntailed distribution, goes out to about 2,500. But you see, if I blow up just\nthe numbers up to about 200, there's a very large\nnumber of people with two prescriptions filled,\nand then a fairly declining number with more. And then it's a very long tail. So can you imagine 2,500\nthings prescribed for you during a hospital stay?", "id": "0UFwGJe6ubg_39", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  so NDC is probably the\nbest of the coding systems. And it's developed by the FDA. The picture up on\nthe top right shows that the first four digits\nare the so-called labeler. That's usually the person\nwho produced the drugs, or at least the person\nwho distributes them. The second four digit number\nis the form of the drug, so whether it's capsules, or\ntablets, or liquid, or whatever and the dose. And then the last two digits\nare a package code which translates into the total number\nof doses that are in a package, right? So this is a godsend. And all of the robotic\npharmacies and so on rely on using this kind\nof information nowadays.", "id": "0UFwGJe6ubg_40", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  and so there's now a-- they added an extra digit,\nbut they didn't do it systematically, and\nso sometimes they added an extra\ndigit to the labeler and sometimes to\nthe product code. And so there is a\nnightmare of translations between the old codes\nand the new codes. And you have to have a\ncode dictionary in order to do it properly and so on. OK, well, if that\nweren't good enough, the International Council\nfor the Harmonization of Technical requirements for\nPharmaceuticals for Human Use developed another coding\nsystem called MedDRA, which is also used in various places. And this is an\ninternational standard, which is, of course,\nincompatible with the NDC. CPT is the Common Procedural\nTerminology, which we'll", "id": "0UFwGJe6ubg_41", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And they have a\nsubrange of their codes which also correspond to\nmedication administration. And so this is yet another way\nof coding giving medicines. And then the HCPCS\nis yet another set of codes for specifying\nwhat medicines you've given to somebody. And then I had mentioned this\nGSN number, which apparently the Beth Israel uses. This as a commercial coding\nsystem from a company called First Databank\nthat is in the business of trying to produce standards. But in this case,\nthey're producing ones that are pretty redundant\nwith other existing standards. But nevertheless, for\nhistorical reasons, or for whatever reasons,\npeople are using these. OK, enough of drugs.", "id": "0UFwGJe6ubg_42", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  If you look in MIMIC,\nthere are three tables. There's procedures ICD, which\nhas ICD-9 codes for about a quarter million procedures. There's CPT events, which\nhas about half a million, 600,000 events that are\ncoded in the CPT terminology. And then MetaVision, the\nnewer of the two systems, has about a quarter\nmillion procedure events that are coded in that system. So some examples, here's the\nmost common ICD-9 procedure codes. So ICD-9 code 3893 of which\nthere are 14,000 instances is venous catheterization,\nnot elsewhere classified. So what's venous\ncatheterization?", "id": "0UFwGJe6ubg_43", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Very common. You show up at a hospital. Before they ask you your name,\nthey stick an IV in your arm. That's a billable event, too. Then insertion of an\nendotracheal tube, you know, if you're having\nany problems like that, they stick something\ndown your throat. Ventral infusion of concentrated\nnutritional substances, so if you're not able\nto eat, then they feed you through a\nstomach tube, OK? So that's what that is. Continuous invasive\nmechanical ventilation for less than 96\nconsecutive hours, so this is being put\non a ventilator that's breathing for you, et cetera. So you see that there is\na very long tail of these. So those are the ICD-9 codes. Now, CPT has its\nown procedure codes", "id": "0UFwGJe6ubg_44", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So for example, this is\nthe medicine subsection, and it shows you\nthe kinds of drugs that you're being\nadministered that are involved in dialysis,\nor psychiatry, or vaccines, or whatever. And then here are the surgical\nand the radiological codes. And there's tons and\ntons of detail on these. Yeah. AUDIENCE: So how can they put\nthese codes as 1,000 to 1,022? This is really\nannoying for anyone-- PROFESSOR: No, these\nare categories. So if you drill down,\nthere's a fanout of that tree and you get down to\nindividual codes. Just as a nasty surprise, CPT\nis owned by the American College of Physicians, and\nthey could sue me if I showed you the actual codes\nbecause they're copyrighted.", "id": "0UFwGJe6ubg_45", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  It's crazy. OK, so if you look\nat the number of all of these codes\nper admission, you see a distribution like this. Or if I separate\nthem out, you see that there are more ICD-9\ncodes and fewer of the CPT and the codes that\nare in MetaVision. But they look somewhat similar\nin their distributions. OK, lab measurements. So you send off a sputum\nsample, blood, urine, piece of your brain, something. They stick it in some goo and\nmeasure something about it. So what is it that\nthey're measuring? Well, it turns out\nthat hematocrit is the most common measurement. So this is how much\nhemoglobin is in your blood,", "id": "0UFwGJe6ubg_46", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  for sick people. And the second most\nimportant is potassium, then sodium creatinine,\nchloride, urea nitrogen, bicarbonate, et cetera. So this is a long, long\nlist of different things that can be measured, and all\nthe stuff is in the database. So for example, here's patient\nnumber two in the database. And on July 17 of 2138, this\nis part of the deidentification process to make it\ndifficult to figure out who the patient actually is. This person got a\ntest for their blood and they reported\natypical lymphocytes. So there are a couple\nof interesting things to note here.", "id": "0UFwGJe6ubg_47", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So this is a\nqualitative measure, so there's no value\nassociated with it. Just the fact of the label tells\nyou what the result of the test was. The other thing\nthat's interesting is this last column,\nwhich is LOINK, and I'll say a word\nabout that in a minute-- actually right now. So LOINK is the Logical\nObservation Identifiers Names and Codes. It was developed\nby our colleagues at Regenstrief Clinic in Indiana\nabout 15 years ago, maybe 20 years ago at this point. And the attempt was to say every\ndifferent type of laboratory test ought to have\na unique name, and they ought to\nbe hierarchical so that if you have, for\nexample, three different ways of measuring serum\npotassium, that they're", "id": "0UFwGJe6ubg_48", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  distinct from each\nother, because there may be circumstances under\nwhich the errors that you get from one measurement\nversus another are different. And so this is the standard way. If you send off your\nblood sample to a lab, they send back a string\nlike this to the hospital or to your doctor's\noffice that says, it's coded in this\nOBX coding system, and here is the\nLOINK code, and this is the SNOMED interpretation. And so this string is the\nway that your hospital's EHR or your doctor's office\nsystem figures out what the result of the test was. HL7 is this 30-something\nyear old organization that has been working on\nstandardizing stuff like this. And LOINK is part of\ntheir standardization.", "id": "0UFwGJe6ubg_49", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  how many tests per admission? Again, a huge, long\ntail up to about 15,000 for a very small\nnumber of patients. If you look at lab\ntests per admission, you can do a log\ntransform and get something that looks like a\nmore reasonable distribution. By the way, that's a very\ngeneric lesson when we're going to do analyses\nof these data, is that, often, doing a transform of some\nsort, like in this case, a log, takes some funny\nlooking distribution and turns it into something\nthat looks plausibly normal, which is better for a\nlot of the techniques we use. Yeah. AUDIENCE: [INAUDIBLE]\nmeans the same thing? Like, for instance-- PROFESSOR: Yes. AUDIENCE: --hematocrit\n[INAUDIBLE] PROFESSOR: Yes AUDIENCE: --same? PROFESSOR: Yes AUDIENCE: Always same? PROFESSOR: Yes, that's the whole\nidea of creating the standard.", "id": "0UFwGJe6ubg_50", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  successfully adopted. OK, chart events. So these are the things\nthat nurses typically enter at the bedside. And so there are 5.1,\n5.2 million heart rates measured in the MIMIC database. And calprevslig is an artifact. It exists in every record. And it's some calibration\nsomething or other that doesn't mean anything. I've never been able to\nfigure out exactly what it is. SPO2 is the partial pressure\nof oxygen in your blood. If you use a pulse oximeter,\nthat's what that's measuring. Respiratory rate, heart rhythm,\nectopy type, dot, dot, dot. Now, you might be troubled\nby the fact that here is heart rate again, right? But I've already shown you\nthis, that heart rate in CareVue", "id": "0UFwGJe6ubg_51", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  coded under different\ncodes in the joint system that we created out of\nthose two databases. And so you have to take\ncare of figuring out what's what if you're\ntrying to analyze this data. Not only do we have that problem\nof different age distributions across the two\ndifferent data sets, but we also just have\nthe mechanical problem that there will be things with\nthe same label that may or may not represent the same\nmeasurement at different times in the system. OK, this is the number of chart\nentries per admission, again, on a log scale. So you see that\nthere are about 10 to the 3.5 chart entries\nper admission, so thousands of admissions, of chart\nevents per admission.", "id": "0UFwGJe6ubg_52", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So Foley catheter\nallows your bladder to drain without your\nhaving consciously to go to the bathroom, so\nthey collect that information. There are 1.9 million recordings\nof how much fluid came out of your bladder. Chest tubes will drain\nstuff out of your chest if you have congestion. Urine is if you pee regularly,\nstool out, et cetera. And again, I'm not\nsure I understand what the difference is between\nurine out Foley versus Foley. They may be the\nsame thing but one from CareVue and\none from MetaVision, so again, typical\nkinds of problems. If you look at the number of\noutput events per admission, you're seeing on the\norder of 100, roughly.", "id": "0UFwGJe6ubg_53", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  you should also track\ninputs, and so they do. And so D5W is this dextrose\nin water, 0.9 percent normal saline. Propofol is an anesthetic. Insulin, heparin, blood\nthinner, et cetera. Fentanyl is, I think, an\nopioid, if I remember right. So these are various things\nthat are given to people. And they affect the\nvolume of the person. So this is an attempt to\nkeep the person in balance and keep track of that. MetaVision inputs are\nclassified somewhat differently but they have similar\nkinds of data. And if you combine\nthem, you get, again, a distribution on\na log scale that shows that there are\non the order of 10", "id": "0UFwGJe6ubg_54", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  a few input events, because\nthis is recorded periodically. Now, the paper that I-- yeah. AUDIENCE: What's\nthe input again? Is that when you come to the\nhospital and get admitted or-- PROFESSOR: No, no, no. It's an input into you. So it's like you drink\na glass of water, the nurse is supposed\nto record it. Although, she doesn't always\nbecause she may not notice it. But if they hang an IV bag\nand pour a liter of liquid into you, they do\nrecord that, OK? All right, so I had you\nread this interesting paper and a discussion\nprior to that paper, because one of the authors\nis a former student of mine. And I know one of the\nother guys pretty well. And the former\nstudent, Zak Kohane,", "id": "0UFwGJe6ubg_55", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  and was explaining to me\nthat he ran into a venture capitalist who\ndiscovered that there is an interesting physiological\nvariation in the abnormality of lab tests that\nare done at night. And he suspected that there\nwas a diurnal variation that lab tests actually\nbecome more abnormal at night than they do during the day. And Zak, who is not only a\ncomputer science PhD but also a practicing doctor,\nturns to him and says, you're an idiot, right? Who has their blood drawn\nat 3 o'clock in the morning. It's typically not\nhealthy people, right? So this is another of these\nnice confounding stories where, if you have a test done\nin the middle of the night,", "id": "0UFwGJe6ubg_56", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So he and Griffin recruited\ntheir third author and went off and did\na very large scale study of this question, which\nis what the paper that I asked you to read reports on. And so I said,\nwell, I wonder if I could reproduce that study\nin the MIMIC database. And the answer, just in\ncase you get your hopes up, was no, in large part\nbecause we just don't have the right kind of data. So there are not\nthat many white blood counts that were measured in\nthe MIMIC database, for example. But if you look at the-- this is MIMIC data. And if you say,\nwhat's the fraction of abnormal white blood\ncount values by hour-- so this is midnight to midnight. And each hour, there's some\nfraction of these test results that are abnormal. And sure enough, what you\nsee is that, at 5 o'clock", "id": "0UFwGJe6ubg_57", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  is abnormal than at 3\no'clock in the afternoon, OK, which is consistent with\nZak's peremptory comment about the guy being an idiot. So once again, I\nsaid, well, can we build a really simple\nmodel that predicts who's going to die in the\nhospital in this case? That's the easiest\none to predict because I have that data. We could get three-year\nsurvival data, which is what they were looking at. But it's harder and it runs\ninto censoring problems of what happens if the\nperson was hospitalized less than three years before\nthe end of our data collection period and so on. And so I avoided that. But what this is showing you\nis, for each of the hours, zero to 24, what is the\nnumber of measurements?", "id": "0UFwGJe6ubg_58", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  is the fraction of those\nmeasurements that's abnormal, OK? So I said, well,\nlet's just throw it into a logistic\nregression model. And what comes out is\nsomething really weird, which is that a few particular\nhours are significant, but most of them are not. And that looks like\nnoise to me, right? Because you wouldn't\nexpect that, at 8 o'clock in the morning, the fact that\nyou had something measured matters. But at 9 o'clock in the\nmorning, it doesn't. That doesn't seem sensible. So I don't think there's\nenough signal here. And in fact, when I looked at\nthe number of white blood count", "id": "0UFwGJe6ubg_59", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  so false means people lived\nand true means they died. But you see that there's not\na whole lot of difference between the distributions. But you also see that the\nnumber of white blood counts is relatively small\nin this database. And so I think we just don't\nhave enough data to do it. On the other hand, if you look\nat a panel of different drugs, you look at mean values of\nblood urea nitrogen or calcium chloride, CO2, et cetera,\nyou see that there is variation across time. So there is some\nsort of variance that's either caused by\nthe diurnal physiology of the human body or by the\nroutine practice of medicine, about when people choose\nto take lab measurements. And in fact, if you look at the\nfraction of high end low lab", "id": "0UFwGJe6ubg_60", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And in particular, if you\nlook at white blood counts, you see that the fraction of\nhigh values goes up at night and the fraction of\nlow values goes down at night, right, which\nis consistent with what they saw as well. There is another\nway to measure it, which is, instead of\nusing normal ranges, the lab actually gives\nyou a call that says, is this value\nnormal, low, or high? And we can use that. That's a little bit\nmore subtle because it depends on calibration\nof the equipment and is updated as the\ncalibration changes. So that's probably a\nlittle bit more accurate. But you see essentially\nthe same phenomenon here. But if you look at the\ndistributions of when measurements are done\nthat turn out to be normal", "id": "0UFwGJe6ubg_61", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  there is a lot of similarity\nbetween the normal and the abnormal curves of when\nthose measurements are taken. So we're not seeing that. OK, let me race\nthrough to the end. This is my heartbeat\nfrom my watch. You can actually\ndownload the stuff and put it in your\nfavorite analysis engine and take a look. So here I was running\nacross the Harvard bridge. And if you look at my heart rate\nvariability over the 30 seconds or so, you see that\nthe interbeat interval ranges from about\n550 to about 600 and whatever 20 milliseconds. And so you could calculate\nmy heart rate variability, which is thought to be an\nindicator of heart health and so on. You can calculate\nthat I was running", "id": "0UFwGJe6ubg_62", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  my heart was beating at a pace\nof about 100 beats per minute. So you know there's all\nsorts of information like that available. Now, as I said, I'm not\ngoing to get into this today, but this was a very\nsuccessful recently published paper where\nthey're able to take a look at images of the lung. So this is a transverse\nscan of the lung. And they have a deep\nlearning machine that is able to identify\nthese two yellow marked things as pulmonary emboli as opposed\nto these other things that are just random\nflecks in the tissue. And I can't do that by eyeball. Maybe a good radiologist\nmight be able to, but this is claimed in\nthe paper to outperform decent radiologists already. This was one of\nthe articles that led Geoff Hinton to make this\nrather stupid pronouncement", "id": "0UFwGJe6ubg_63", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  because the profession will be\nover by the time they get fully trained, which I don't believe. They may do different things,\nbut they won't go away. This was a slide from Ron\nKikinis at the Brigham, and they're using\nautomated techniques of analyzing white\nmatter in order to identify lupus lesions. So lupus is a bad\ndisease that shows up in these magnetic resonance\nimages in certain ways. The last thing I want to\ntalk about today is notes. So my students did a little\nexercise last semester where we tried to see how good\nis the average ape, namely", "id": "0UFwGJe6ubg_64", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And so we took a bunch of\ncases from the MIMIC data set, blinded to the question\nof whether the person lived or died. We gave the data to people in\na kind of visualization tool, sort of like the one that\nI showed you earlier, that summarizes the case, and\nthen also gave people access to the notes, the deidentified\nnotes about those cases, to see whether people could\npredict, better than a coin flip, whether somebody\nwas going to live or die. And the answer is yes,\nslightly better, OK? Not immensely better\nbut slightly better. And furthermore, it looks\nlike, by giving them feedback, so as they're looking\nat these cases and trying to make\nthe prediction, they make a prediction,\nyou tell them if they were right or wrong, we learn.", "id": "0UFwGJe6ubg_65", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  than random, right? It's kind of interesting. OK, so one of the\nthings I discovered is that, at least\nwhen I was playing the monkey in this\nexercise, I found the notes to be immensely useful,\nmuch more useful than the trend lines\nof laboratory data. Partly, it's because I'm\nused to reading English. I'm not so used to reading\ngraphs of laboratory data. But part of it is that there is\na level of human understanding that is transmitted\nin the nursing notes and in the discharge summaries\nand so on that you don't get from just looking at raw data. And so there is very\nmuch the sense, which we're going to talk about\nin a couple of weeks, of how can we take advantage\nof that information, extract it, and use it\nin the kinds of modeling that we want to do? So in MIMIC, if\nyou look, we have", "id": "0UFwGJe6ubg_66", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  and more nursing notes, and\nelectrocardiogram reports, and doctor's notes, and\ndischarge summaries, and echocardiograms,\nrespiratory, et cetera. And if you look at\nthe distribution of the lengths of these,\nthese are, unfortunately, not on the same scale. But the discharge\nsummary is the thing that's written at the time\nyou leave the hospital. So this is sort of the\nsummary of everything that happened to you during\nyour hospitalization. And it's long. So, you know, it goes up\nto like 30,000 characters. You know, it's a short story,\nnot so short short story. Nursing notes tend\nto be shorter. They run up to about\n3,000 characters. This other set of\nnursing notes, which I think comes from the other\nsystem, is a little bit longer. It goes up to about 5,000.", "id": "0UFwGJe6ubg_67", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  They go up to about 10,000,\n15,000 characters, typically. And there are various\nother kinds of notes. So I just wanted to\nshow you a few of these. Here's a brief nursing note. So this is a patient who is\nhypotensive but not in shock. Patient remains\non this drug drip at 0.75 micrograms per\nkilogram per minute, no titration needed\nat this time. Their blood pressure is\nstable at more than 100. Their mean arterial pressure\nis 65, greater than 65. Wean them from this drug\npresumably if it's tolerated. A wound infection, so\nanterior groin area open and oozing moderate amounts\nof thin, pink-tinged serous fluid. Patient's stooling with small\namounts of stool on something and dangerously close to\nthe open wound, et cetera.", "id": "0UFwGJe6ubg_68", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  She just went in,\nsaw the patient-- by the way, I say\nshe, but probably a vast majority of nurses\nin Boston area hospitals really are women, but there\nare some male nurses-- and will record sort\nof a snapshot of what's going on with the patient. What are the concerns? In principle, this is\ngoing to be useful not only as a part of the medical\nrecord, but also when this nurse goes off shift and\nthe next nurse comes on shift. Then this is a recording of\nwhat the state of the patient was the last time they\nwere seen by the nurse. In reality, the nurses tend\nto tell each other verbally rather than relying on\nthe written version. I remember one time talking to\na nurse in an intensive care unit in another part of\nthe country, and I said, so whoever reads\nyour notes, and she", "id": "0UFwGJe6ubg_69", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  people responsible\nfor trying to assess the quality of care\nthey're giving, and lawyers when\nthere's a lawsuit. And she was very happy because\nshe had saved the hospital 10 million dollars by\nhaving carefully recorded that some procedure had been\ndone to a patient who then had a bad outcome and was suing\nthe hospital for their neglect in not having done this. But because it was\nin the note, that was proof that it\nactually had been done, and therefore the\nhospital wasn't liable. But there is a lot of\ninformation in here. Now, I'm going to show you many\npages of a typical discharge summary. So this is somebody\non the surgery service who came in complaining\nof leg pain, redness, and swelling\nsecondary to infection", "id": "0UFwGJe6ubg_70", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So she had surgery-- I think she. Yeah, female. She had surgery which\ndidn't heal well, so major surgical or\ninvasive procedure, incision and drainage and pulse\nirrigation of the left groin, and left above-knee popliteal\nsite incisions with exploration of bypass graft, and excision of\nthe entire left common femoral artery to above-knee blah,\nblah, blah, blah blah, blah. So this is what they did. History of the present illness-- she's a 45-year-old\nwoman who underwent the left femoral, a.k.a. doctor something or other\nwith PTFE, whatever that is, over a month ago\non a certain date. By the way, these\nbracketed asterisked things are where we've taken out\nidentifying information", "id": "0UFwGJe6ubg_71", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  She had been doing\nwell post-operatively and was seen in the clinic six\ndays prior to presentation. At this time, she acutely\ndeveloped nausea, vomiting, fevers, and progressive\nredness, swelling, pain of her left\nthigh, et cetera, OK? So that's just page\none of many pages. Yeah. AUDIENCE: Just a question. Is this completely [INAUDIBLE]\ninformation [INAUDIBLE] patient's name or date? PROFESSOR: Not in this system. There are people-- Henry Chueh at\nMass General spent 10 years building a system that\nhad autocomplete and so on. And some doctors liked it\nand some doctors hated it. And the MGH threw out\nall of their old systems in order to buy Epic,\nand so it's gone. It was like 10 years\nof work down the drain. But it was not a\nspectacular success. Because whenever you\nhave auto complete,", "id": "0UFwGJe6ubg_72", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And people are very\ncreative, and they always want to type something\nthat you didn't anticipate. So it's hard to support it. AUDIENCE: What is Epic? That's like the new-- PROFESSOR: Epic is\na big company that has been winning all the\nrecent contests for installing electronic medical\nrecord systems. Remember in my last\nlecture, I showed that we're reaching about 100% saturation? So they've been winning a lot\nof the installation deals. And they're getting\na lot of the subsidy. The estimate I heard was that\nPartners Healthcare, which is MGH at the Brigham and a\ncouple of other hospitals, spent somewhere on the\norder of two billion dollars installing the system. So that included all\nthe customizations and all the training and\nall the administrative stuff that went with it. But that's a huge\namount of money.", "id": "0UFwGJe6ubg_73", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  PROFESSOR: OK, so we have\npast medical history-- pack a day smoker,\nabused cocaine but says she stopped\nsix months ago, has asthma, type 2 diabetes. Social history, family history. These are of the\nphysical exam results. So it's giving you a lot of\ninformation about the person. Description of the wound\ndown at the bottom. Pertinent lab results. So these are copied out\nof the laboratory tables. Yeah. AUDIENCE: Just to double\ncheck with the drug results-- PROFESSOR: Sorry? AUDIENCE: Just to double\ncheck with the drug results two slides back-- PROFESSOR: Yeah AUDIENCE: It said-- so it\nhas the fake dates of 2190 up there. PROFESSOR: Yep. AUDIENCE: So the fact that there\nwas a positive test in 2187 would mean a year ago. PROFESSOR: Yeah. AUDIENCE: So that's\nthe medication.", "id": "0UFwGJe6ubg_74", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  maintains the relative dates\nbut not the absolute dates. So these are results,\nagain, copied out of the laboratory database\ninto the discharge summary. Brief hospital course, and\nthen a review of systems, so what's going on\nneurologically, cardiovascular, pulmonary, GI, GU, et cetera. Infectious disease, endocrine,\nhematology, prophylaxis. And at the time was\ndischarged, the patient was doing well, no fever\nand stable vital signs, tolerating a regular\ndiet, ambulating, voiding without assistance, and\npain was well controlled. Medications on\nadmission, so this was the medication\nreconciliation. Discharge medication, so this is\nwhat she's being sent home on.", "id": "0UFwGJe6ubg_75", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  follow up service,\nand she's going home. And the discharge\ndiagnosis is infected left femoral popliteal bypass\ngraft and the condition. And these are the instructions\nto the patient that say, you know, here's what you can\ndo, here is when you should come back and tell us if\nsomething is going wrong, et cetera. And here's what you should\nreport if it happens. You know, if you have sudden\nsevere bleeding or swelling, do this. Follow up with doctor\nsomebody or other. Call his clinic at this number\nto schedule an appointment and then follow up with doctor\nsomebody else in two weeks. I think this is the same one.", "id": "0UFwGJe6ubg_76", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So you saw in David's\nintroductory lecture a reference to Odyssey, which\nis a standard method of encoding the kind of data that\nwe're talking about today. There is a likelihood that\nthe next release of the MIMIC database will adopt the Odyssey\nformats rather than the-- yeah. David's shaking his\nhead, wondering why. Me, too. AUDIENCE: Odyssey hasn't handled\nclinical notes very well yet. PROFESSOR: Well, so, you\nknow, what always happens, as you say, I'm going to\nadopt the standard asterisk with the following extensions. And that's probably\nwhat's going to happen. But it means that the\ncentral tables, you know, the ICD-9 code tables and\nthe drug tables, some things like that, are likely to\nwind up adopting the formats of the Odyssey database.", "id": "0UFwGJe6ubg_77", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  the Fast Health\nInteroperability Resources. So HL7 is the\nstandards organization that had a tremendous\nsuccess in the early 1990s in solving the problem of how\nto allow laboratories to report lab data back to the\nhospitals or the clinics that ordered the labs. And that character\nstring with the up arrows and the vertical bars and\nso on that I showed you before that had LOINK encoded\nin it is that standard. That's called HL7 Version 2. It's still in use very widely,\nthey then got ambitious and suffered second\nsystem syndrome, which is they decided to\nbuild HL7 Version 3, which I used to teach in\na class here 10 years ago. But one of my friends who\nworks for a company that", "id": "0UFwGJe6ubg_78", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  that describes what you\nneed to know in order to implement that system. And as a result,\nnobody was doing it. So FHIR is a gross\nsimplification of that that starts\noff and says, if a doctor refers a\nnew patient to you, what is the minimum set of data\nthat you need to know in order to take care of that person? And FHIR tries to provide just\nthat subset of all of the data. It has become a standard mainly\nbecause, after Congress spent $42 billion dollars\nor so bribing people into buying these\ninformation systems, they got mad that the\ninformation systems they bought couldn't talk to each other. And so they called in,\non the carpet, the heads of these IT companies,\nhealth IT companies,", "id": "0UFwGJe6ubg_79", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  them promise that there\nwould be interoperability. They promised. And out of that came FHIR. It was probably simultaneously\ndeveloped but they adopted it. And so now, in principle,\nit's possible to exchange data between different\nhospitals, at least to the level of that degree\nof harmonization of the data. In reality, the\ncompanies don't want you to do that because they\nlike there to be friction in not being able to\ntake all your data to a different hospital, because\nit is more likely to leave you at the one that you're at. So there is complicated\nsocioeconomic kinds of issues in all this. But at least the standard exists\nand is becoming more and more widely deployed as long as\nCongress pays attention. It's ugly. So here is what a patient\nlooks like, right? It's the usual\nunreadable XML garbage.", "id": "0UFwGJe6ubg_80", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  that can turn it into JSON\nand simpler representations. And so that's pretty common. So the terminologies that exist\nare LOINK, NBC, ICD-9 and 10. SNOMED I didn't\ntalk about today. DSM-5 is the Diagnostic\nand Statistical Manual for Psychiatrists. That's used as a\ncommon coding method for describing\npsychiatric disease. And there are many\nmore of these. There's something called\nthe Unified Medical Language Systems Metathesaurus from the\nNational Library of Medicine that integrates about 180 of\nthese different terminologies. And so there is a\nnice one-stop shop where you can get all\nthese things from them. So takeaway lessons,\nknow your data.", "id": "0UFwGJe6ubg_81", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  comes up over and over again. And doing machine learning\nand analysis on data that you don't understand\nis likely to lead you to false conclusions. Harmonization is difficult\nand time consuming. And there are lots of\nthings for which we just don't have standards,\nand so everybody develops their own representations. I had a PhD student about a\ndecade ago who, in his thesis, wrote that he spent about\nhalf his time cleaning data. And I gave that thesis\nto another student who started a few years\nlater who read it, and he comes to me just\nawestruck and he says, what? He only spent half\nhis time cleaning? Unfortunately, that's roughly\nwhere we are in this field. So sorry to be a\ndowner, but that's the current state of the art.", "id": "0UFwGJe6ubg_82", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  some models with\nthese kinds of data and showing you what\nwe can accomplish.", "id": "0UFwGJe6ubg_83"}, {"text": "  DAVID SONTAG: Today\nwe'll be talking about risk stratification. After giving you\na broad overview of what I mean by\nrisk stratification, we'll give you a\ncase study which you read about in your\nreadings for today's lecture coming from early detection\nof type 2 diabetes. And I won't be, of course,\nrepeating the same material you read about it in your readings. Rather I'll be giving\nsome interesting color around what are some of\nthe questions that we need to be thinking about as\nmachine learning people when we try to apply machine\nlearning to problems like this. Then I'll talk about\nsome of the subtleties. What can go wrong with machine\nlearning based approaches to risk stratification? And finally, the last\nhalf of today's lecture is going to be a discussion. So about 3:00 PM, you'll see\na man walk through the door.", "id": "_shuV1tJbTU_0", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  He is a professor at\nBrigham Women's Hospital. He also has a startup\ncompany called Sift, which is\nworking on applying risk stratification now, and\nthey have lots of clients. So they've been really\ndeep in the details of how to make this stuff work. And so we'll have an interview\nbetween myself and him, and we'll have\nopportunity for all of you to ask questions as well. And that's what I hope will\nbe the most exciting part of today's lecture. Then going on beyond\ntoday's lecture, we're now in the beginning of\na sequence of three lectures on very similar topics. So next Thursday,\nwe'll be talking about survival modeling. And you can think about it as\nan extension of today's lecture, talking about what you should\ndo if your data has centering, which I'll define\nfor you shortly. Although today's\nlecture is going to be a little bit\nmore high level, next Thursday's\nlecture is where we're", "id": "_shuV1tJbTU_1", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  about how one should\ntackle machine learning problems with centered data. And then the following\nlecture after that is going to be on\nphysiological data, and that lecture will also be\nmuch more technical in nature compared to the first couple\nof weeks of the course. So what is risk stratification? At a high level, you think\nabout risk stratification as a way of taking in\nthe patient population and separating out\nall of your patients into one of two or\nmore categories. Patients with high\nrisk, patients with low risk,\nand maybe patients somewhere in the middle. Now the reason why we might\nwant to do risk stratification is because we\nusually want to try to act on those predictions. So the goals are\noften one of coupling those predictions with\nknown interventions. So for example, patients\nin the high risk pool-- we will attempt to do\nsomething for those patients", "id": "_shuV1tJbTU_2", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Now risk stratification is\nquite different from diagnosis. Diagnosis often has very,\nvery stringent criteria on performance. If you do a mis-diagnosis\nof something, that can have very\nsevere consequences in terms of patients being\ntreated for conditions that they didn't need\nto be treated for, and patients dying because they\nwere not diagnosed in time. Risk stratification you\nthink of as a little bit more fuzzy in nature. We want to do our best job\nof trying to push patients into each of these categories--\nhigh risk, low risk, and so on. And as I'll show you\nthroughout today's lecture, the performance characteristics\nthat we'll often care about are going to be a bit different. We're going to look a\nbit more at quantities", "id": "_shuV1tJbTU_3", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Of the patients we say are high\nrisk, what fraction of them are actually high risk? And in that way, it differs\na bit from diagnosis. Also as a result of the\ngoals being different, the data that's used is\noften very different. In risk stratification, often we\nuse data which is very diverse. So you might bring in\nmultiple views of a patient. You might use auxiliary data\nsuch as patients' demographics, maybe even socioeconomic\ninformation about a patient, all of which\nvery much affect their risk profiles but may not be used\nfor a unbiased diagnosis of the patient. And finally in today's\neconomic environment, risk stratification\nis very much targeted towards reducing cost of\nthe US health care setting. And so I'll give you\na few examples of risk stratification, some of which\nhave cost as a major goal", "id": "_shuV1tJbTU_4", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  The first example is that\nof predicting an infant's risk of severe morbidity. So this is a premature baby. My niece, for example, was\nborn three months premature. It was really scary for my\nsister and my whole family. And the outcomes of patients\nwho are born premature have really changed dramatically\nover the last century. And now patients who are\nborn three months premature, like my niece, actually can\nsurvive and do really well in terms of long term outcomes. But of the many\ndifferent inventions that led to these improved\noutcomes, one of them was having a very good\nunderstanding of how risky a particular infant might be. So a very common\nscore that's used", "id": "_shuV1tJbTU_5", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  generally speaking, is\nknown as the Apgar score. For example when\nmy son was born, I was really excited when\na few seconds after my son was delivered, the nurse\ntook out a piece of paper and computed the Apgar score. I studied that, really\ninteresting, right? And then I got back to some\nother things that I had to do. But that score isn't actually\nas accurate as it could be. And there is this\npaper, which we'll talk about in a week and a\nhalf, by Suchi Saria who's a professor at Johns Hopkins,\nwhich looked at how one could use a machine learning based\napproach to really improve our ability to predict\nmorbidity in infants. Another example,\nwhich I'm pulling from the readings for today's\nlecture, has to do with-- for patients who come\ninto the emergency department with a heart related\ncondition, try to understand", "id": "_shuV1tJbTU_6", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Or is it safe enough to\nlet that patient go home and be managed by\ntheir primary care physician or their cardiologist\noutside of the hospital setting? Now that paper, you might have\nall noticed, was from 1984. So this isn't a new concept. Moreover, if you look\nat the amount of data that they used in that study,\nit was over 2,000 patients. They had a nontrivial number\nof variables, 50 something variables. And they used a non-trivial\nmachine learning algorithm. They used logistic regression\nwith a feature selection built in to prevent themselves\nfrom over fitting to the data. And the goal there was\nvery much cost oriented. So the premise was that\nif one could quickly decide these patients\nwho've just come to the ER", "id": "_shuV1tJbTU_7", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  then we'll be able to reduce the\nlarge amount of cost associated with those admissions\nto coronary care units. And the final example\nI'll give right now is that of predicting likelihood\nof hospital readmission. So this is something which is\ngetting a real lot of attention in the United States health care\nspace over the last few years because of penalties which\nthe US government has imposed on hospitals who have a\nlarge number of patients who have been released\nfrom the hospital, and then within the next 30\ndays readmitted to the hospital. And that's part\nof the transition to value based care, which Pete\nmentioned in earlier lectures. And so the premise is that\nthere are many patients who are hospitalized but are\nnot managed appropriately on discharge or after discharge. For example, maybe this patient\nwho has a heart condition wasn't really clear\non what they should", "id": "_shuV1tJbTU_8", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  For example, what medications\nshould they be taking? When should they follow up\nwith their cardiologist? What things they should\nbe looking out for, in terms of warning\nsigns that they should go back to the hospital\nor call their doctor for. And as a result of that\npoor communication, it's conjectured that these\npoor outcomes might occur. So if we could figure\nout which of the patients are likely to have\nthose readmissions, and if we could predict that\nwhile the patients are still in the hospital, then\nwe could change the way that discharge is done. For example, we could send\na nurse or a social worker to talk to the patient. Go really slowly through\nthe discharge instructions. Maybe after the\npatient is discharged, one could have a nurse\nfollow up at the patient's home over the next few weeks. And in this way, hopefully\nreduce the likelihood of that readmission.", "id": "_shuV1tJbTU_9", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And this is going to be really\na discussion throughout the rest of today's lecture. What's changed since\nthat 1984 article which you read for today's readings? Well, the traditional approaches\nto risk stratification are based on scoring systems. So I mentioned to you\na few minutes ago, the Apgar scoring\nsystem is shown here. You're going to say for each\nof these different correct criteria-- activity,\npulse, grimace, appearance, respiration-- you look at the baby, and you\nsay well, activity is absent. Or maybe they're\nactive movement. Appearance might be pale or\nblue, which would get 0 points, or completely pink\nwhich gets 2 points. And for each one\nof these answers, you add up the\ncorresponding points. You get a total\nnumber of points. And you look over\nhere and you say, OK, well if you have\na 0 to 3 points, the baby is at severe risk. If they have 7 to 10 points,\nthen the baby is low risk.", "id": "_shuV1tJbTU_10", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  have been very carefully\nderived through studies not dissimilar to the one that\nyou read for today's readings, and which are actually widely\nused in the health care system today. But the times have been\nchanging quite rapidly in the last 5 to 10 years. And now, what most of the\nindustry is moving towards are machine learning\nbased methods that can work with a much higher\ndimensional set of features and solve a number\nof key challenges of these early approaches. First-- and this is perhaps\nthe most important aspect, they can fit more easily\ninto clinical workflows. So the scores I\nshowed you earlier are often done manually. So one has to think\nto do the score. One has to figure out what\nthe corresponding inputs are.", "id": "_shuV1tJbTU_11", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  not used as frequently\nas they should be. Second, the new machine\nlearning approaches can get higher\naccuracy potentially, due to their ability to\nuse many more features than the traditional pitches. And finally, they can be\nmuch quicker to drive. So all of the traditional\nscoring systems had a very long research\nand development process that led to their adoption. First, you gather the data. Then you build the models. Then you check the models. Then you do an evaluation\nin one hospital. Then you do a prospective\nevaluation in many hospitals. And each one of those\nsteps takes a lot of time. Now with these machine\nlearning based approaches, it raises the possibility of\na research assistant sitting in a hospital, or in a\ncomputer science department, saying oh, I think it would\nbe really useful to derive", "id": "_shuV1tJbTU_12", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  You take data that's available. You apply your machine\nlearning algorithm. And even if it's a condition\nor an outcome which occurs very infrequently,\nif you have access to a large enough\ndata set you'll be able to get enough\nsamples in order to actually predict that\nsomewhat very narrow outcome. And so as a result, it\nreally opens the door to rethinking about the way\nthat risk stratification can be used. But as a result, there\nare also new dangers that are introduced. And we'll talk about some\nof those in today's lecture, and we'll continue\nto talk about those in next Thursday's lecture. So these models are being\nwidely commercialized. Here is just an example\nfrom one of many companies that are building risk\nstratification tools. This is from Optum. And what I'm showing\nyou here is the output from one of their models\nwhich is predicting COPD related hospitalizations.", "id": "_shuV1tJbTU_13", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So for all of the\npatients who are of interest to that hospital,\nthey will score the patient-- using either one of\nthe scores I showed you earlier, the manual ones, or\nmaybe a machine learning based model-- and they'll be put into one\nof these different categories depending on the risk level. And then one can dig in deeper. So for example, you could\nclick on one of those buckets and try to see well, who\nare the patients that are highest at risk. And what are some potentially\nimpactible aspects of those patients' health? Here, I'm showing you for a\nslightly different problem that are predicting high\nrisk diabetes patients. And you see that\nfor each patient, we're listing the\nnumber of A1C tests, the value of the last A1C test,\nthe day that it was performed. And in this way, you could\nnotice oh, this patient", "id": "_shuV1tJbTU_14", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  But look, they haven't\nbeen tracking their A1C. Maybe they have\nuncontrolled diabetes. Maybe we need to get\nthem into the clinic, get their blood tested,\nsee whether maybe they need a change in\nmedication, and so on. So in this way, we can\nstratify the patient population and think about\ninterventions that can be done for that subset of them. So I'll move now into a case\nstudy of early detection of type 2 diabetes. The reason why this\nproblem is of importance is because it's\nestimated that there are 25% of patients with\nundiagnosed type 2 diabetes in the United States. And that number is\nequally large as you go to many other\ncountries internationally. So if we can find patients who\ncurrently have diabetes or are likely to develop\ndiabetes in the future, then we could attempt\nto impact them. So for example, we could\ndevelop new interventions that can prevent those\npatients from worsening", "id": "_shuV1tJbTU_15", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  For example, weight loss\nprograms or getting patients on first line diabetic\ntreatments like Metformin. But the key problem which\nI'll be talking about today is really, how do you find\nthat at risk population? So the traditional\napproach to doing that is very similar to\nthat Apgar score. This is a scoring\nsystem used in Finland which asks a series of\nquestions and has points associated with each answer. So what's the age\nof the patient? What's their body mass index? Do they eat vegetables, fruit? Have they ever taken anti\nhypertension medication? And so on, and you get a\nfinal score out, right? Lower than 7 would\nbe 1 in 100 risk of developing type 2 diabetes. Higher than 20 is\nvery high risk. 1 in 2 people will\ndevelop type 2 diabetes in the next 10 years. But as I mentioned,\nthese scores haven't had the impact that we had\nhoped that they might have. And the reason really\nis because they", "id": "_shuV1tJbTU_16", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  as they should be. So what we will be\nthinking through is, can we change the way in which\nrisk stratification is done? Rather than it having\nto be something which is manually done, when\nyou think to do it, we can make it now\npopulation wide. We could, for example,\ntake data that's already available from\na health insurance company, use machine learning. Maybe we don't have access\nto all of those features I showed you earlier. Maybe we don't know\nthe patient's weight, but we will use machine\nlearning on the data that we do have to try\nto find other surrogates of those things we\ndon't have, which might predict diabetes risk. And then we can apply\nit automatically behind the scenes for\nmillions of different patients and find the high\nrisk population and perform interventions\nfor those patients. And by the way, the work that\nI'm telling you about today is work that really came\nout of my lab's research in the last few years. So this is an example going\nback to the set of stakeholders,", "id": "_shuV1tJbTU_17", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  This is an example of\na risk stratification being done at the payer level. So the data which is going\nto be used for this problem is administrative data,\ndata that you typically find in health insurance companies. So I'm showing you here a single\npatient's timeline and the type of data that you\nwould expect to be available for that\npatient across time. In red, it's showing\ntheir eligibility records. When had they been enrolled\nin that health insurance? And that's really important,\nbecause if they're not enrolled in the health\ninsurance on some month, then the lack of\ndata for that patient isn't because nothing happened. It's because we just don't\nhave visibility into it. It's missing. In green, I'm showing\nmedical claims which are associated with\ndiagnosis codes that Pete talked\nabout last week, procedure codes, CPT codes. We know what the specialist\nwas that the patient went to see, like cardiologists,\nprimary care physician,", "id": "_shuV1tJbTU_18", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  We know where the\nservice was performed, and we know when\nit was performed. And then from pharmacy, we have\naccess to medication records shown in the top right there. We know what medication\nwas prescribed, and we have it coded\nto the NDC code-- National Drug Code, which\nPete talked about again last Tuesday. We know the number\nof days' supply of the medication, the number\nof refills that are available still, and so on. And finally, we have\naccess to laboratory tests. Now traditionally, health\ninsurance companies only know what\ntests were performed because they have to pay for\nthat test to be performed. But more and more, health\ninsurance companies are forming partnerships\nwith companies like Quest and LabCorps to\nactually get access also to the results of\nthose lab tests. And in the data set that\nI'll tell you about today, we actually do have those\nlab test results as well. So what are these elements\nfor this population?", "id": "_shuV1tJbTU_19", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So if we look at the\ntop diagnosis codes, for example, we'll see that\nof 135,000 patients who had laboratory data,\nthere were over 400,000 different diagnosis\ncodes for hypertension. You'll notice that's greater\nthan the number of people. That's because they occurred\nmultiple times across time. Other common diagnosis codes\nincluded hyperlipidemia, hypertension, type 2 diabetes. And you'll notice that\nthere's actually quite a bit of interesting detail here. Even in diagnosis codes,\nyou'll find things that sound more like\nsymptoms-- like fatigue, which is over here. Or you also have records of\nprocedures, in many cases. Like they got a\nvaccination for influenza. Here's another example. This is now just\ntelling you something about the broad statistics\nof laboratory tests in this population.", "id": "_shuV1tJbTU_20", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  are all the most\npopular lab tests. And that's not surprising,\nbecause often there is a panel called the CBC\npanel which is what you would get in your annual physical. And that has many of these\ntop laboratory test results. But then as you look\ndown into the tail, there are many other\nlaboratory test results that are more specialized in nature. For example,\nhemoglobin A1C is used to track roughly 3 month\naverage of blood glucose and is used to understand a\npatient's diabetes status. So that's just to give\nyou a sense of what is the data behind the scenes. Now let's think, how\ndo we really derive-- how do we tackle-- how do we formulate this\nrisk stratification problem as a machine learning problem? Well today, I'll give you one\nexample of how to formulate it as a machine learning problem. But in Tuesday's lecture, I'll\ntell you several other ways.", "id": "_shuV1tJbTU_21", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to binary classification. We're going to go back in time. We're going to pretend\nit's January 1, 2009. We're going to say suppose\nthat we had run this risk stratification algorithm on\nevery single patient on January 1, 2009. We're going to\nconstruct features from the data in the past,\nso the past few years. We're going to predict\nsomething about the future. And there many things\nyou could attempt to predict about the future. I'm showing you here 3\ndifferent prediction tasks corresponding to\ndifferent gaps-- a 0 year gap, a 1 year\ngap, and a 2 year gap. And for each one\nof these, it asks will the patient newly\ndevelop type 2 diabetes in that prediction window? So for example, for\nthis prediction task we're going to exclude patients\nwho have developed type 2 diabetes between 2009 and 2011. And we're only going to count\nas positives patients who get newly diagnosed with type 2\ndiabetes between 2011 and 2013.", "id": "_shuV1tJbTU_22", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  want to include a\ngap in the model is because often,\nthere's label leakage. So if you look at\nthe very top set up, often what happens\nis a clinician might have a really good\nidea that the patient might be diabetic, but it's not\nyet coded in a way which our algorithms can pick up. And so on January 1,\n2009 the primary care physician for the patient might\nbe well aware that this patient is diabetic, might already\nbe doing interventions based on it. But our algorithm\ndoesn't know that, and so that patient,\nbecause of the signals that are present in the\ndata, is going to at the very top of\nour prediction list. We're going to say\nthis patient is someone you should be going after. But that's really not\nan interesting patient to be going after, because\nthe clinicians are probably already doing interventions that\nare relevant for that patient. Rather, we want to find the\npatients where the diabetes", "id": "_shuV1tJbTU_23", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And so this is one of the\nsubtleties that really arises when you try to use\nretrospective clinical data to derive your labels to\nuse within machine learning for risk stratification. So in the result\nI'll tell you about, I'm going to use a 1 year gap. Another problem is that the\ndata is highly censored. So what I mean by\ncensoring is that we often don't have full visibility\ninto the data for a patient. For example, patients\nmight have only come into the health insurance in\n2013, and so January 1, 2009 we have no data on them. They didn't even exist\nin the system at all. So there are two\ntypes of censoring. One type of censoring is\ncalled left censoring. It means when we don't\nhave data to the left, for example in the feature\nconstruction window. Another type of censoring\nis called right censoring. It means when we don't\nhave data about the patient", "id": "_shuV1tJbTU_24", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And for each one of\nthese in our work here, we tackle it\nin a different way. For left centering, we're\ngoing to deal with it. We're going to say OK, we might\nhave limited data on patients. But we will use whatever data is\navailable from the past 2 years in order to make\nour predictions. And for patients who have less\ndata available, that's fine. We have sort of a more\nsparse feature vector. For right centering,\nit's a little bit more challenging to deal with\nin this binary reduction, because if you don't\nknow what the label is, it's really hard to use\nwithin, for example, a supervised machine\nlearning approach. In Tuesday's lecture,\nI'll talk about a way to deal with right censoring. In today's lecture, we're\ngoing to just ignore it. And the way that\nwe'll ignore it is by changing the inclusion\nand exclusion criteria. We will exclude patients for\nwhom we don't know the label. And to be clear, that could\nbe really problematic. So for example, imagine if you\ngo back to this picture here.", "id": "_shuV1tJbTU_25", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And imagine that if we only have\ndata on a patient up to 2011, we remove them from\nthe data set, OK? Because we don't have full\nvisibility into the 2010 to 2012 time window. Well, suppose that exactly\nthe day before the patient was going to be removed\nfrom the data set-- right before the data\ndisappears for the patient because, for example, they\nmight change health insurers-- they were diagnosed\nwith type 2 diabetes. And maybe the reason\nwhy they changed health insurers had\nto do with them being diagnosed with type 2 diabetes. Then we've excluded that\npatient from the population, and we might be really biasing\nthe results of the model, by now taking away a whole\nset of the population where this model would've been\nreally important to apply.", "id": "_shuV1tJbTU_26", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and how that changes the\ngeneralizability of the model you get is something that should\nbe at the top of your mind. So the machine\nlearning algorithm used in that paper\nwhich you've read is L1 regularized\nlogistic regression. One of the reasons for using L1\nregularized logistic regression is because it provides a way to\nuse a high dimensional feature set. But at the same time, it allows\none to do feature selection. So I'll go more into detail\non that in just a moment. All of you should be familiar\nwith the idea of formulating machine learning as an\noptimization problem where you have\nsome loss function, and you have some\nregularization term-- w, in this case, as the\nweights of your linear model, which we're trying to learn. For those of you who've seen\nsupport vector machines before,", "id": "_shuV1tJbTU_27", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  called L2 regularization\nwhere we'll be putting a penalty on the\nL2 norm of the weight vector. Instead, what we\ndid in this paper is used L1 regularization. So this penalty is\ndefined over here. It's summing over the\nfeatures and looking at the absolute value\nfor each of the weights and summing those up. So one of the reasons\nwhy L1 regularization has what's known as a\nsparsity benefit can be explained\nby this picture. So this is just a\ndemonstration by sketch. Suppose that we're trying\nto solve this optimization problem here. So this is the level set\nof your loss function. It's a quadratic function. And suppose that\ninstead of adding on your regularization\nas a second term to your optimization\nproblem, you were", "id": "_shuV1tJbTU_28", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So you might say we're\ngoing to minimize the loss subject to the L1\nnorm of your weight vector being less than 3. Well, then what I'm showing\nyou here is weight space. I'm showing you 2 dimensions. This x-axis is weight 1. This y-axis is weight 2. And if you put an L1\nconstraint-- for example, you said that the sum of the\nabsolute values of weight 1 and weight 2 have\nto be equal to 1-- then the solution space has\nto be along this diamond. On the other hand, if you put\nan L2 constraint on your weight vector, then it would correspond\nto this feasibility space. For example, this\nwould say something like the L2 norm over the weight\nvector has to be equal to 1. So it would be a ball,\nsaying that the radius has to always be equal to 1. So suppose now you're\ntrying to minimize that objective function,\nsubject to the solution having", "id": "_shuV1tJbTU_29", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  were optimizing the L2 norm,\nversus living on this diamond, which is what would happen if\nyou're optimizing the L1 norm. Well, the optimal\nsolution is going to be in essence\nthe closest point along the circle,\nwhich gets as close as possible to the middle\nof that level set. So over here, the\nclosest point is that 1. And you'll see that this point\nhas a non-zero w1 and w2. Over here, the closest\npoint is over here. Notice that has a zero value of\nw1 and a non-zero value of w2, thus it's found a sparser\nsolution than this one. So this is just to give you\nsome intuition about why using L1 regularization\nresults in sparse solutions to your optimization problem. And that could be\nbeneficial for two purposes.", "id": "_shuV1tJbTU_30", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  where there exists a very\ngood risk model that uses a small number of features. And to point out,\nthat's not a crazy idea that there might exist\na risk model that uses a small number\nof features, right? Remember, think back\nto that Apgar score or the FINDRISC, which was used\nto predict diabetes in Finland. Each of those had only\n5 to 20 questions. And based on the answers\nto those 5 to 20 questions, one could get a pretty good\nidea of what the risk is of that patient, right? So the fact that there might\nbe a small number of features that are together\nsufficient is actually a very reasonable prior. And it's one reason why L1\nregularization is actually very well suited to these types\nof risk stratification problems on this type of data. The second reason is\none of interpretability. If one wants to\nthen ask, well, what are the features that actually\nwere used by this model", "id": "_shuV1tJbTU_31", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  When you find only\n20 or a few features, you can enumerate all of them\nand look to see what they are. And in that way,\nunderstand what is going on into the\npredictions that are made. And that also has\na very big impact when it comes to translation. So suppose you built a model\nusing data from this health insurance company. And this health\ninsurance company just happened to have access\nto a huge number of features. But now you want to go somewhere\nelse and apply the same model. If what you've\nlearned is a model with only a few\nhundred features, you're able to dwindle it down. Then it provides an opportunity\nto deploy your model much more easily. The next place you\ngo to, you only need to get access\nto those features in order to make\nyour predictions. So I'll finish up in\nthe next 5 minutes in order to get to our\ndiscussion with Leonard. But I just want to recap\nwhat are the features that", "id": "_shuV1tJbTU_32", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that we use. So the features\nthat we used here were ones that were\ndesigned to take into consideration that\nthere is a lot of missing data for patients. So rather than think through\ndo we impute this feature, do we not impute this\nfeature, we simply look to see were these features\never observed? So we choose our\nfeature space in order to already account for the fact\nthat there's a lot missing. For example, we look to see\nwhat types of specialists has this doctor seen in the\npast, been to in the past? For every possible\nspecialist, we put a 1 in the corresponding\ndimension if the patient has seen that type of\nspecialist and 0 otherwise. For the top 1,000 most\ncommon medications, we look to see has the patient\never taken his medication, yes or no? And again, 0 or 1 in the\ncorresponding dimension. For laboratory\ntests, that's where we do something which is\na little bit different. We look to see, first of\nall, was a laboratory test", "id": "_shuV1tJbTU_33", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And then we say OK, if\nit was administered, was the result ever low, out\nof bounds on the lower side? Was the result ever high? Was the result ever normal? Is the value increasing? Is the value decreasing? Is the value fluctuating? I noticed that each\none of these quantities is well-defined,\neven for patients who don't ever have\nany laboratory test results available, right? The answer would be 0, it\nwas never administered. And 0, it was never low. 0, it was never high, and so on. OK? AUDIENCE: Is the\nvalue increasing? Is it every time, or\nhow do you define? DAVID SONTAG: So\nincreasing here-- first of all, if there\nis only a single value observed then it's 0. If there were at least 2 values\nobserved, then you look to see was there ever any adjacent\npair of observations where the second one was\nhigher than the first one? That's the way it\nwas defined here. AUDIENCE: Then it has\nincreased and then decreased.", "id": "_shuV1tJbTU_34", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  DAVID SONTAG: Correct. That's what we did here. And it's extremely\nsimple, right? So there are lots of better\nways that you could do this. And in fact, this\nis an example which we'll come back to perhaps a\nlittle bit in the next lecture and then more in\nsubsequent lectures when we talk about using recurrent\nneural networks to try to summarize time series data. Because one could imagine\nthat using such an approach could actually automatically\nlearn such features. AUDIENCE: Just to double\ncheck, is fluctuating one of the other two [INAUDIBLE]? DAVID SONTAG: Fluctuating\nis exactly the scenario that was just described. It can go up, and\nthen it goes down. Has to do both, yeah. Yep? AUDIENCE: It said in the first\nquestion, [INAUDIBLE] together. Was the test ever\nadministered [INAUDIBLE]?? And the value you\nhave there is 1. DAVID SONTAG: Correct. So indeed, there is a\nhuge amount of correlation between these features. If any of these were 1, then\nthis is also going to be 1.", "id": "_shuV1tJbTU_35", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  DAVID SONTAG: Yeah,\nbut you would still want to include this 1 in here. So imagine that all\nof these were 0. You don't know if they're 0\nbecause these things didn't happen or because the\ntest was never performed. AUDIENCE: Are the\nlow, high, normal-- DAVID SONTAG: They're just\nbinary indicators here, right? AUDIENCE: Doesn't it have\nto fit into one category? DAVID SONTAG: Well, no. Oh, I see what you're saying. So you're saying if the\nresult was ever present, then it would be at\nleast 1 of these 3. Maybe. It gets into some of the\ntechnical details which I don't remember right now. It was a good question. And this is the next most\nreally important detail. The way I just\ndescribed this, there was no notion of time in that. But of course when\nthese things happened can be really important. So the next thing we\ndo is we re-compute all of these features for\ndifferent time buckets.", "id": "_shuV1tJbTU_36", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  for the last 24\nmonths of history, and then for all of\nthe past history. And we can catenate together\nall of those feature vectors and what you get out. In this case, it was\nsomething like a 42,000 dimensional feature vector. By the way, it's 42,000\ndimensional and not higher because the features that\nwe used for diagnosis codes for this paper were\nnot temporal in nature. And one could easily\nmake them temporal in nature, in which case it'd\nbe more like 60,000 features. I'm going to skip over\nthe deriving labels and get back to that next time. I just want to briefly\ntalk about how does one evaluate these types of models. And I'll give you one\nview on evaluations, and shortly we'll hear a\nvery different type of view. So here, what I'm showing\nyou are the variables that have been selected by the\nmodel and have non-zero weight.", "id": "_shuV1tJbTU_37", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  which is used by the model. It's not surprising\nbecause we're trying to predict is the\npatient likely to develop type 2 diabetes. Now you might ask, if a\npatient has a diagnosis code for impaired\nfasting glucose aren't they already diabetic? Shouldn't they\nhave been excluded? And the answer is no,\nbecause there are also patients who are pre-diabetic\nin this data set, who have been intentionally\nincluded because we don't know which of them\nare going to go on to develop type 2 diabetes. And so this is an indicator that\nthe patient has been previously flagged as being pre-diabetic. And it obviously\nmakes sense that would be at the very top of\nthe predictive variables. But there are also\nmany things that are a little bit less obvious. For example, here\nwe see obstructive sleep apnea and\nesophageal reflux as being chosen by the model\nto be predictive of the patient developing type 2 diabetes. What we would conjecture is\nthat those variables, in fact, act as surrogates for\nthe patient being obese.", "id": "_shuV1tJbTU_38", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  claims. And so with this\nvariable, despite the fact that the patient might be\nobese, if this variable is not observed then patients who\nare obese often have what's called sleep apnea. So they might stop breathing\nfor short periods of time during their sleep. And so that then would\nbe a sign of obesity. So I talked about how\nthe criteria which we use to evaluate risk\nstratification models are a little bit different\nfrom the criteria used to evaluate diagnosis models. Here I'll tell you one of the\nmeasures that we often use, and it's called positive\npredictive value. So what we'll do is\nlook at after you've learned your model. Look at the top 100 predictions,\ntop 1,000 predictions, top 10,000 predictions,\nand look to see", "id": "_shuV1tJbTU_39", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  type 2 diabetes. Now of course, this is\ndone using held up data. Now the reason why you might be\ninterested in different levels is because you might want to\ntarget different interventions depending on the risk and cost. For example, a very\nlow cost intervention-- one of the ones that we did--\nwas sending a text message to patients who are suspected\nto have high risk of developing type 2 diabetes. If they've not been to see their\neye doctor in the last year, we send them a text\nmessage saying maybe you want to go see your eye doctor. Remember, you get\na free eye checkup. And this is a very\ncheap intervention, and it's a very\nsubtle intervention. The reason why it\ncan be effective is because patients who\ndevelop type 2 diabetes, once that diabetes progresses\nit leads to something called diabetic\nretinopathy, which is often caught in an eye exam. And so that could\nbe one mechanism for patients to be diagnosed.", "id": "_shuV1tJbTU_40", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So you take the 10,000\nmost risky people. You apply the\nintervention for them, and you look to see\nwhich of those people actually had developed\ndiabetes in the future. In the model that I showed\nyou, 10% of that population went on to develop\ntype 2 diabetes 1 to 3 years from then. The comparison point I'm\nshowing you here, this blue bar, is if you used a\nmodel which is derived using a very small\nnumber of features, so not a machine\nlearning based approach. And there, only 6%\nof the people went on to develop type 2 diabetes\nfrom the top 10,000. On the other hand,\nother interventions you might want to do\nare much more expensive. So for example,\nyou might only be able to do that\nintervention for 100 people because it costs so much money,\nand you have a limited budget as a health insurer. And so for those people,\nyou could ask well, what is the positive predictive\nvalue of those top 100 predictions? And here, that was 15%\nusing the machine learning based model and less\nthan half of that using the more traditional approach.", "id": "_shuV1tJbTU_41", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  There's a lot more that\nI can and will say. But I'll have to get to it\nin next Thursday's lecture, because I'd like our\nguest to come down, and we will have a\nbit of a discussion. To be clear, this\nis the first time that we've ever had this\ntype of class interaction which is why, by the way,\nI ran a little bit late. I hadn't ever done\nsomething like this before. So it's an experiment. Let's see what happens. So, do you say Leonard? LEONARD D'AVOLIO: Len's fine. DAVID SONTAG: Len, OK. So Len, could you please\nintroduce yourself? LEONARD D'AVOLIO: Sure. My name is Len D'Avolio. I'm an assistant professor\nat Harvard Medical School. I am also the CEO and founder\nof a company called Sift. Do you want a little\nbit of background or no? DAVID SONTAG: Yeah, a\nlittle bit of background. LEONARD D'AVOLIO: Yeah, so\nI've spent probably the last 15 years or so trying to help\nhealth care learn from its data in new ways.", "id": "_shuV1tJbTU_42", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  I would say health care\nfor both societal, but also just from a where we're at\nwith our ability to use data standpoint is a great place for\nyou guys to invest your time. I've been doing\nthis for government, in academia as a researcher,\npublishing papers. I've been doing\nthis for non-profits in this country\nand a few others. But every single project\nthat I've been a part of has been an effort to bring\nin data that has always been there, but we haven't been\nable to learn from until now. And whether that's\nat the VA building out there, genomic\nscience infrastructure, recruiting and enrolling\na million veterans to donate their\nblood and their EMR, or at Ariadne Labs over out\nof Harvard School of Public Health and the Brigham,\nimproving childbirth in India-- it's all about how can we get a\nlittle bit better over and over again to make health care\na better place for folks. DAVID SONTAG: So tell me,\nwhat is risk stratification", "id": "_shuV1tJbTU_43", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Defining that I found to be\none of the most difficult parts of today's lecture. LEONARD D'AVOLIO: Well, thank\nyou for challenging me with it. [LAUGHTER] So it's a rather\ngeneric term, and I think it depends entirely on the\nproblem you're trying to solve. And every time I go\nat this, you really have to ground\nyourself in the problem that you're trying to solve. Risk could be running out of a\nmedical supply in an operating room. Risk could be an Apgar score. Risk could be from\npre-diabetic to diabetic. Risk could be an older person\nfalling down in their home. So really, what is it to me? I'm very much caught up\nin the tools analogy. These are wonderful\ntools with which a skilled craftsman surrounded\nby others that have skills could go ahead and solve\nvery specific problems. This is a hammer. It's one that we\nspend a lot of time refining and applying to\nsolve problems in health care.", "id": "_shuV1tJbTU_44", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  where your company\nhas been applying risk stratification today\nat a very high level. And then we'll choose on of\nthem to dive a bit deeper into. LEONARD D'AVOLIO: Sure. So the way we\ndescribe what we do is it's performance improvement. And I'm just giving you\na little background, because it'll tell you which\nproblems I'm focused on. So it's performance\nimprovement, and to be candid, the types of things we like to\nimprove the performance of are how do we keep people\nout of the hospital. I'm not going to soapbox\non this too much, but I think it matters. Like the example that\nyou gave that you were employed to help solve was\nby an insurer, and insurance companies-- there's probably 30\nindustries in health care. It's not one industry. And every one of them has\ndifferent and oftentimes competing incentives. And so the most\nlogical application for these technologies is to\nhelp do preventative things.", "id": "_shuV1tJbTU_45", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of health care is\nfinancially incentivized to do preventative things. The rest are the\nhospitals and the clinics. And when you think\nof health care, you probably think of those\ntypes of organizations. They don't typically pay to keep\nyou out of those facilities. DAVID SONTAG: So as\na company, you know, you've got to make\na profit of entry. So you need to focus\non the ones where there's a financial incentive. LEONARD D'AVOLIO:\nYou focus on where there's a financial incentive. And in my case, I wanted\nto build a company where the financial\nincentive aligned with keeping people healthy. DAVID SONTAG: So what are\nsome of these examples? LEONARD D'AVOLIO: Sure. So we do a lot with\nolder populations. With older\npopulations, it becomes very important to understand who\ncare managers should approach, because their risk\nlevels are rising. A lot of risk stratification,\nthe old way that you described, identifies people that are\nalready at their most acute.", "id": "_shuV1tJbTU_46", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  You're getting\nattention because you are at the absolute\npeak of your acuity. We're trying to help care\nmanagement organizations find people that are rising risk. And even when we do\nthat, we try to get-- I mean, the power of\nthese technologies is to move away from\none size fits all. So when we think\nabout rising risk, we think about in a\nbehavioral health environment, it is the rising risk of\nan inpatient psychiatric admission. That is a very\nspecific application. There are things\nwe can do about it. As opposed to risk, which\nif you think about what's being done in other\nindustries, Amazon does not consider us all consumers. There are individuals\nthat are very likely to react to certain\noffers at certain times. And so we're trying to\nbring this sort of more granular approach into health\ncare, where we sit with teams and they're used to just\nhaving generic risk scores. We're trying to help them think\nthrough which older people are", "id": "_shuV1tJbTU_47", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  We do work in diabetes\nalso, so which children with type 1\ndiabetes shouldn't just be scheduled for an\nappointment every 3 months, but you should go\nto them right now? So those are some examples, but\nthe themes are very consistent. It's helping organizations\nmove away from rather generic, one size fits all toward\nwhat are the more actionable. So even graduation from care\nmanagement, because now you should be having serious illness\nconversations because you're nearing end of life, or\npalliative care referrals, or hospice referrals. DAVID SONTAG: OK, so I want\nto choose a single one to dive into. And I want to choose one that\nyou've worked on the longest and where you're already doing\nat least the initial parts of an evaluation of it. And so I think when we\ntalked on the phone, psyche ER was one\nof those examples. Tell us a bit about that one. LEONARD D'AVOLIO: Yeah. Well, I'll just walk you through\nthe problem to be solved. DAVID SONTAG: Please, yeah. LEONARD D'AVOLIO: Sure. So we work with a large\nbehavioral health care", "id": "_shuV1tJbTU_48", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  They are contracted\nby health plans, in effect, to treat people that\nhave mental health challenges. And the traditional way\nof identifying anyone for care management is\nagain, you get a risk score. When you sort the highest\nranking in terms of odds ratio variables, it's because\nyou were already admitted, because you're older, because\nyou have more medications. So they were using\na similar approach, finding the most acute people. So the very first thing we\ndo in all of our engagements is an understanding. Where is the\ngreatest opportunity? And this has very little to\ndo with machine learning. It's just what's\nhappening today? Where are these\nthings happening? Who is caring for these folks? Everyone wants to reduce\nhospital admissions. But there's a difference\nbetween hospital admissions because you're not\ntaking your meds, and hospital admissions because\nyou're addicted to opioids,", "id": "_shuV1tJbTU_49", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  have chronic complex\nbipolar schizophrenia. So we wanted to first\nunderstand well, where is the greatest cost? What types of things are\nhappening most frequently? And then you want to have the\nclinical team tell you well, these are the types\nof resources we have. We have people that can\naddress these issues, or we have\ninterventions designed to solve these problems. And so you bring together where\nis the greatest possible return on your investment\nfrom both a data standpoint, a financial\nstandpoint, but also and we can do\nsomething about it. After you do that,\nit's only then-- after you have full agreement\nfrom executive teams-- that this is the very\nnarrow thing that we think we can address. Then we begin to\napply machine learning to try to solve the problem. DAVID SONTAG: So what\ndid that funnel lead to? What did you decide was\nthe thing to address? LEONARD D'AVOLIO:\nYeah, it was tried to reduce inpatient\npsychiatric admissions.", "id": "_shuV1tJbTU_50", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  just because it came out\nof this tradition of 30 day readmissions-- has always been thought of\nin terms of 30 days out. But when we\ninterviewed the teams, they said actually for\nthis particular condition it takes us more like 90 days\nto be able to have an impact. And so that clinical\nunderstanding mixed with what we have\nthe resources to address, that's what steers then the\napplication of machine learning to solve a specific problem. DAVID SONTAG: OK, so psychiatric\ninpatient admission-- so these are patients who come to the\nER for some psychiatric related problem, and then\nwhen they're in the Er they're admitted\nto the hospital. They're in the\nhospital for anywhere from a day to a few days. And you want to\nfind when are those going to happen in the future? LEONARD D'AVOLIO: Yeah. DAVID SONTAG: What type of\ndata is useful for that? LEONARD D'AVOLIO: Sure. You don't have to just get\nthrough the ED, though.", "id": "_shuV1tJbTU_51", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  DAVID SONTAG: Got it. So what kind of data is most\nuseful for predicting that? LEONARD D'AVOLIO: Yeah. So I think a philosophy\nthat you all should take is whatever data\nyou have, it should be your competitive advantage\nin solving the problem. And that's different\nin the way this has been done where folks have\nmade an algorithm somewhere else, and then they're\ncoming and telling you, hey, as long as you have claims\ndata, then plug in my variables and I can help you. Our approach-- and this is sort\nof derived from my interest from the start in solving\nthe problem and try to make the tools work faster-- is whatever data\nyou have, we will bring it in and consider it. What ultimately then wins\nis dependent on the problem. But you would not be\nsurprised to learn that there is some value in claims data. You put labs up there. There's a lot of value in labs. When it comes to\nbehavioral health, and this is where you really\nhave to understand health care,", "id": "_shuV1tJbTU_52", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  There is a stigma attached\nto carrying diagnosis codes that would describe you\nas having mental health challenges. And so claims alone is not\nsufficient for that reason. We find a lot of lift\nfrom care management. So when you have a care\nmanager, that care manager is assessing you and you are\nfilling out forms and serving you and giving you\ndifferent types of sort of functional\nassessments or activities of daily living assessments. That data turns out\nto be very powerful. And then, a dark horse that most\npeople aren't used to using, we get a lot of lift\nout of the clinicians whether it's the psychiatrist\nor care manager's notes. So there is value in the written\ndescriptions of a nurse's or a care manager's\nimpressions of what's wrong, what has been done, what\nhasn't been done, and so on. DAVID SONTAG: So tell me a bit\nabout the development process.", "id": "_shuV1tJbTU_53", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  You at least have that in words. You have your data in one place. Then what? LEONARD D'AVOLIO: Yeah. Well, you wouldn't be surprised. The very first\nthing we do is just try to throw a logistic\nregression at it. We want the story to\nmake sense to begin with, and we're always\nlooking for the simplest solution to the problem. Then the team sort of iterates\nback and forth through based on how this data looks and\nthe characteristics of it-- the density, the sparsity-- based on what we\nunderstand about this data, these guys are in\nand out of the plan. So we may have issues with\ndata not existing in the time windows that you had described. Then they're working their way\nthrough algorithms and feature selection approaches that\nseem to fit for the data that we have. DAVID SONTAG: But what error\nmetrics do you optimize for? LEONARD D'AVOLIO: You're\ngoing to have to ask them. It's been too long. DAVID SONTAG: OK. [LAUGHTER] LEONARD D'AVOLIO:\nI'm 10 years out of being allowed to write code.", "id": "_shuV1tJbTU_54", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  where we have to be--\nthis is a big deal. We have to be able to translate. We do positive predictive\nvalue, obviously. And I like the way you describe\nthat, because a lot of folks that have been trained in\nstatistics for medicine, whether it's\nepidemiology or the like, are always looking for an r\nsquared or an area under ROC. And we have to help them\nunderstand that you can only care for so many people. So you don't really care\nwhat the area under ROC is for a population of, for this\nclient, 300,000 in the one plan that we were serving. You really care about\nfor the top 100 or 200, and really that number should be\nderived based on your capacity. DAVID SONTAG: Yeah. LEONARD D'AVOLIO: So if I can\ngive you 7 out of 10 for 100, you might go knock\non their door. But for, let's say, between\n1,000 and 2,000 that number goes down to 4 out of 10. Maybe you should go with a\nless expensive intervention.", "id": "_shuV1tJbTU_55", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  understand what they're seeing\nand how to interpret it, and helping them\nconnect it back to what they're going to do with it. And then I think probably,\nin courses to follow, you'll go into all\nof the challenges with interpretability\nand the like. But they all exist. DAVID SONTAG: So tell me a\nbit about how it's deployed. So once you build a model,\nhow do you get your client to start using it? LEONARD D'AVOLIO: Yeah. So you don't start getting them\nready when the model's ready. I've learned the hard way that's\nfar too late to involve them in the process. And in fact, the one bullet\nyou had up here that I didn't completely agree\nwith was this idea that these approaches are\neasier to plug into a workflow. Putting a number into an\nelectronic health record may be easier.", "id": "_shuV1tJbTU_56", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  just that the number\nappears at the right time. It's the culture of getting-- put it this way. These care managers have spent\nthe last 20, 30 years learning who needs their help, and\neverything about their training and their experience is to\ncare for the people that are most acute. All of the red\nflags are going off. And here comes a bunch of\nnerds and computer science people that are\nsuggesting that no, rather than your\nintuition and experience of 30 years you should trust\nwhat a computer says to do. DAVID SONTAG: So\nthere are two parts I want to understand better. LEONARD D'AVOLIO: Sure. DAVID SONTAG: First, how\nyou deal with that problem, and second, I\nactually am curious about the technical details. Do you give them predictions\non a piece of paper? Do you use APIs? LEONARD D'AVOLIO: Yeah. Well, let me answer\nthe technical one first because it's a faster answer. You remember at the\nbeginning of this, I said health care\nis pretty immature", "id": "_shuV1tJbTU_57", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So it's never a piece\nof paper, but it can be an Excel spreadsheet\ndelivered via secure FTP once a month, because\nthat's all they're able to take right now based\non their state of affairs. It can be a real\ntime call to an API. What we learn to do informing a\ncompany serving health care is do not create a new interface. Do not create a new log in. Accommodate whatever\nworkflow and systems they already have in place. So build for flexibility\nas opposed to giving them something else to log into. You have very little time. And the other\nthing is clinicians hate their information\ntechnology. They love their phones, but they\nhate what their organization forces them to use. Now that may be a\ngross generalization, but I don't think\nit's too far off. Data is sort of a\nfour letter word.", "id": "_shuV1tJbTU_58", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  the students have been\nlearning about things like FHIR and so on. Are these any of the\nAPIs that you use? LEONARD D'AVOLIO: No. So those are technologies\nwith enormous potential. You put up a paper that\ndescribed a risk stratification algorithm from 1984. That paper, I'm sure, was\nsupported with evidence that it could make\na big difference. I'm getting awfully close to\nstanding on a soapbox again, but you have to understand\nthat health care is paid for based on delivering care. And the more complex the care\nis, the more you get paid. And I'm not telling you this,\nI'm kind of sharing with them. You know that. So the idea that a\ntechnology like FHIR would open up EHRs\nto allow people to just kind of drop\nthings in or out, thereby taking away the monopoly\nthat the electronic health", "id": "_shuV1tJbTU_59", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  these are tough investments for\nthe electronic health record vendor to make. They're being forced by\nthe federal government. And they saw the writing on the\nwall, so they're moving ahead. And there's great\nexamples coming out of Children's, Ken\nMandl and the like, where some progress\nhas been made. But I live in right now, I\nhave to get this done inside of the health care of today. And very few of\nthe organizations that we not just work with\nbut would even talk to are in a position,\nlike FHIR ready. In 5 years, I think\nI'll be telling you-- DAVID SONTAG: Hopefully\nsomething different, yeah. All right, so can you briefly\nanswer that first question about what do you have to give\naround a prediction in order for it to be acted\nupon effectively? LEONARD D'AVOLIO: Yes. So the very first thing\nyou have to do is-- so we invite the\nclinical team to be part of the project\nfrom the very beginning. It's just really important. If you show up with a\nprediction, you've lost.", "id": "_shuV1tJbTU_60", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Remember, I say\nwe're triangulating what they can and\ncan't do, and what might matter what might not. They are literally\npart of the team. And as we're moving through,\nhow would one evaluate whether or not this works? We show them, these are\nsome of the people we found. Oh yeah, that makes sense. I know Mr. Smith. And so it's a real show and\ntell process from the start. DAVID SONTAG: So once\nyou get closer to that, after development phase\nhas been done, then what? LEONARD D'AVOLIO: After\nthe development phase, if you've done a great job\nyou get away from the show me what variable mattered\non a per patient basis. So you can show folks the\nodds ratios on a model is easy enough to produce. You can show people these\nare the features that matter at the model level. Where this gets tougher is all\nof health care is used to Apgar scores which are\nbased on 5 things. We all know what they are. And the machine\nlearning results, the models that we\nhave been talking about in behavioral health--", "id": "_shuV1tJbTU_61", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  is over 3,700\nvariables with at least a little bit of a contribution. So how do you square up the\nculture of 5 to 7 variables? And in fact, I gave\nyou the variables and you ran the hypothesis\ntesting algorithm versus more of an\ninductive approach, where thousands of\nvariables are actually contributing incrementally. And it's a double edged\nsword, because you could never show somebody 3,700 variables. But if you show them 3\nor 4, then the answer is, well that's obvious. I knew that. DAVID SONTAG: Right, like the\nimpaired fasting glucose one. LEONARD D'AVOLIO: Yes, exactly. So really, I just\npaid you to tell me that somebody who has been\nadmitted is likely to readmit. You know, that's the challenge. So striking that\nbalance between-- really, it's education\nmore than anything, because I don't think\nthat an algorithm created", "id": "_shuV1tJbTU_62", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  support where it can\npresent you 2 or 3 that you could rely upon and then\nmake informed decisions. And part of the\neducation process is we also say forget\nabout the number. If I were to give you this\nperson, what would you do next? And the answer is always, well\nI would look at their chart. The analogy we use that we\nfind is helpful is this is GPS, right? GPS isn't going to give you like\na magic, underground highway that we didn't know about. It's going to suggest the roads\nthat you're familiar with. The advantage it has\nis that unlike you in the car as you're driving,\nit's just aware of more than you are and it can do\nthe math a little bit faster than you can. And so it's going to\ngive you a suggestion, and it's going to tell\nyou more often than not, in your situation, I'm going\nto save you a few minutes. DAVID SONTAG: Yeah. LEONARD D'AVOLIO: Now\nyou're still the driver. You could still decide to\ntake 93 South and so be it.", "id": "_shuV1tJbTU_63", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that you really like the view on\nMemorial Drive versus Storrow, and so you're going to do that. And so we try to help people\nunderstand that it just has access to a little\nbit more than you do, and it's going to get you\nthere a little bit faster. DAVID SONTAG: All right,\nI'm going to stop you here because I want to leave\nsome time for some questions from the audience. So I'll make the\nfollowing request. Try to keep it to\nquick responses so we can get to as many\nquestions as we can. AUDIENCE: How much\nis there a worry that certain demographic\ngroups are under diagnosed and have less access to care? And then, would have a\nlower risk edification, and then potentially\nbe de-prioritized? How do you think\nabout adjusting that? LEONARD D'AVOLIO: Yeah, so\nthat was a great question. I'll try to answer it very fast. DAVID SONTAG: And could\nyou repeat the question", "id": "_shuV1tJbTU_64", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  [LAUGHTER] LEONARD D'AVOLIO: Yeah. I mean, models can be\nbiased by experience. And do you worry about\nsmaller size populations being overlooked? Safe to say, is that fair? DAVID SONTAG: And the question\nwas also about the training data that you used. LEONARD D'AVOLIO: Well,\nthat's what I implied. DAVID SONTAG: Yeah, OK. LEONARD D'AVOLIO: OK. So all right, this work we're\ndoing in behavioral health-- and we've done this in a\nfew other environments-- if there is a different\ndemographic for which you would do something different and they\nmay be lost in the shuffle, we do bring that\nto their attention. DAVID SONTAG: Next question! Is there someone\nin the back there? LEONARD D'AVOLIO:\nYou went too fast. DAVID SONTAG: OK, over here. AUDIENCE: How do you\nevaluate [INAUDIBLE]?? Would you be\nwilling to sacrifice the data of [INAUDIBLE] to\nre-approve the [INAUDIBLE]?? DAVID SONTAG: I'm going\nto repeat the question. You talked about how it's like\nreading tea leaves to just show a couple of\nthe top features anyway from a linear model. So why not just get rid of\nall that interpretability", "id": "_shuV1tJbTU_65", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Does that open the door to\nthat possibility for you? LEONARD D'AVOLIO: You're\nsaying get rid of all the interpretability. I think the question was are\nyou willing to trade performance for interpretability. DAVID SONTAG: Yes. LEONARD D'AVOLIO: And that\ncould be an answer to it. Just throw it out. So if I can get our\npartners to the point where they truly understand\nwhat we're doing here and they have been part\nof evaluating the model, success is when\nthey don't need to-- on a per patient, who\nneeds my help basis-- see the 3,000 variables. But that does mean that as\nyou're building the model, you will show them the patients. You will show them\nthe variables. So that's what I\ntry to walk them to. DAVID SONTAG: So it's about\nbuilding up trust as you go. LEONARD D'AVOLIO: Absolutely. That being said in\nsome situations, depending on whether it's\nclinically appropriate-- I mean, if I'm in the\nhundredth percentile here, but interpretability\ncan get me pretty far, I'm willing to make that trade. And that's the difference. Don't fall in love\nwith the hammer, right? Fall in love with\nbuilding the home, and then you're easy\nenough to just swap it out.", "id": "_shuV1tJbTU_66", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Over there. AUDIENCE: Yeah, how\nmuch time do you spend engaging with\n[INAUDIBLE] and physicians before staring to sort\nof build your model. LEONARD D'AVOLIO: So\nactually, first we spend time with the CEO\nand the CFO and the CMO-- chief medical, chief\nexecutive, chief financial. Because if there isn't at\nleast a 5 to 1 financial return for solving this\nproblem, you will never make it all the\nway down the chain to doing something that matters. And so what I have learned\nis the math is fantastic. We can model all\nsorts of fun things. But if I can't figure out how\nit makes them or saves them-- we have like a $5\nmillion mark, right? For the size of\nour company, if I can't help you make 5 million,\nI know you won't pay me. So we start there. As soon as we have\nfigured out that there is money to be made or\nsaved in getting these folks the right care at\nthe right time, then yes the clinicians\nare on the team. We have what's called a working\ngroup-- project manager,", "id": "_shuV1tJbTU_67", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  We have a team and a\ncommunication structure that embeds the clinician. And we have clinicians\non the team. DAVID SONTAG: I think you'll\nfind in many different settings that's what it\nreally takes to get machine learning implemented. You have to have working groups\nof administration, clinicians, users, and engineers,\nand others. Over here there's a question. AUDIENCE: Actually, it's a\nquestion for both of you, so about the data connection. So I know as people, we try\nto connect all kinds of data to train the machine\nlearning model. But when you have some\npreliminary model, can you have some\ninsights to guide you to target certain\ndata, so that you can know that this\nnew information can be very informative\nfor prediction tasks or even design data experiments? DAVID SONTAG: So I'll\nrepeat the question.", "id": "_shuV1tJbTU_68", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Could we use data\ndriven approaches to find what data we should get? LEONARD D'AVOLIO: So we're\ndoing this right now. There's a popular thing\nin the medical industry. Everyone's really fired up about\nsocial determinants of health, and so that has been branded\nand marketed and sold. And so now customers are\nsaying to us, well hey, do you have social\ndeterminants of health data? And that's interesting to\nme, because they've never looked at anything but claims. And now they're suggesting\ngo buy a third party data set which may not add more\nvalue than simply having the zip code. And we say of course, we\ncan bring in new data. We bring in weather pattern. We bring in all\nkinds of funny data when the problem calls for it. That's the easy part. The real challenge\nis will it add value? Should we invest our time\nand energy in doing this? So if you've got all kinds of\nfantastic data, run with it and then see where\nyou fall short. The data just doesn't tell\nyou, now go out and get a different type of data. If the performance is\nlow clinically and based on intuition, it makes sense\nthat another data source", "id": "_shuV1tJbTU_69", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Then we'll try it. If it's free, we'll\ntry it quicker. If it costs money, we'll\ntalk to the client about it. DAVID SONTAG: For both of\nthose, I'll give you my answer to that question. If you have a high dimensional\nenough starting place, often that can give you a\nhint of where to go next. So in the example\nI showed you there, even though obesity is very\nseldom coded in claims data, we saw that it still showed\nup as a useful feature, right? So that then hints\nto us, well maybe if we got higher\nquality obesity data it would be an\neven better model. And so sometimes you can\nuse that type of trick. There is a question over here. AUDIENCE: We use\ncodes to [INAUDIBLE] by calculating how\nmuch the hospital will gain by limiting [INAUDIBLE]? DAVID SONTAG: OK, so this is\ngoing to be the last question that we're going to end on. And it really has to do with\none of evaluation and thinking about the impact of\nan intervention based on their predictions. How much does that causal\neffect show up in both the way", "id": "_shuV1tJbTU_70", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  the effect of your predictions? LEONARD D'AVOLIO: Yeah. So the most important\nthing to know is no customer will ever pay\nyou for a positive predictive value. They don't care, right? They care about will you\nhelp them save or make money solving a problem. So cost effectiveness\nstarts at the beginning. But the nice thing about a\npositive predictive value approach-- and there's\nso much literature that can tell you what does the\naverage cost of certain things having happened. So the very first part of any\nengagement for us is well, you guys are here. This is the cost of being there. If you improved by 10%, if\nwe can get approval to that, then we start to model. And we say well look, of the\ntop 100 people 70 of them are the right people. Multiply that by\nthe potential cost. If you think you can prevent\n10 of those terrible things from occurring, that's\nworth this much. So cost effectiveness\ndata is at the start. It's in the modeling stage. And then at the end,\nwe never show them", "id": "_shuV1tJbTU_71", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  We show them the baseline. We say baseline\nactivities outcomes-- where were you,\nwhat are you doing, and then did it\nmake a difference. And the last part is always\nin dollars and cents, too. DAVID SONTAG: Although Len\ndidn't mention it here, he also does quite\nsome work when trying to think through\nthis causal effect. And we talked about how you\nuse propensity matching, for example, in your work. We won't be able to get into\nthat in today's discussion, but we'll come back\nto those questions when we talk about causal\ninference in a few weeks. That's all for today, thanks.", "id": "_shuV1tJbTU_72"}, {"text": "  [CLICK] [SQUEAK] [PAGES RUSTLING] [MOUSE DOUBLE-CLICKS] PROFESSOR: So today we'll be\ncontinuing along the theme of risk stratification. I'll spend the first half\nto 2/3 of today's lecture continuing where we\nleft off last week before the discussion. I'll talk about how does\none derive the labels that one uses within a\nsupervised machine learning approach. I'll continue talking\nabout how one evaluates risk stratification models. And then I'll talk about\nsome of the subtleties that arise when you\nwant to use machine learning for health\ncare, specifically for risk stratification. And I think that's\ngoing to be one of the most interesting\nparts of today's lecture. In the last third\nof today's lecture, I'll be talking about\nhow one can rethink the supervised machine\nlearning problem, not to be a\nclassification problem, but be something closer\nto a regression problem. And one now thinks about not\nwill someone, for example,", "id": "wqI_z1yumzY_0", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  but when precisely will\nthey develop diabetes-- so the time to event. Then one has to start to\nreally think very carefully about the censoring issues\nthat I alluded to last week. And so I'll formalize\nthose notions in the language of\nsurvival modeling. And I'll talk about how one\ncan do maximum likelihood estimation in that setting, and\nhow one should do evaluation in that setting. So in our lecture\nlast week, I gave you this example of risk\nstratification for type 2 diabetes. The goal, just to remind\nyou, was as follows. 25% of people in\nthe United States have undiagnosed\ntype 2 diabetes. If we could take\nhealth insurance claims data that's available\nfor everyone who has health\ninsurance, and use that to predict who, in the\nnear-term-- next one to three years-- is likely to be newly\ndiagnosed with type 2 diabetes, then we could use it to\nrisk-stratify patient population. We could use that, then, to\nfigure out who is most at risk,", "id": "wqI_z1yumzY_1", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to try to get them diagnosed\nand get them started on treatment if relevant. But what I didn't\ntalk much about was where did those\nlabels come from. How do we know that someone had\na diabetes onset in that window that I show up there on the top? So what are the answers? I mean, all of you should have\nread the paper by Razavian. And then also you should\nhopefully have some ideas. Thoughts? A hint-- it was in\nsupplementary material. How did we define a\npositive case in that paper? Yep. AUDIENCE: Drugs they were on. PROFESSOR: Drugs they were on. OK, yeah, so for example,\nmetformin, glucose-- sorry, insulin. AUDIENCE: I think they did\ninclude metformin actually.", "id": "wqI_z1yumzY_2", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Because metformin is often used\nfor alternative indications. But there are many\nmedications, such as insulin, which are used pretty\nexclusively for treating diabetes. And so you can look\nto see, does a patient have a record of taking one\nof these diabetic medications in that window that we're\nusing to define the outcome? If you see a record\nof a medication, you might conjecture, this\npatient probably has diabetes. But what about it they don't\nhave any medication listed in that time window? What could you conclude then? Any ideas? Yeah. AUDIENCE: If you look\nat the HBA1C value, and you know the normal range,\nand if you see the [INAUDIBLE] above like 7.5 or 7. PROFESSOR: So you're giving\nme an alternative approach, not looking at medications,\nbut looking at laboratory test results. Look at their HBA1C\nresults, which measures approximately an\naverage of three-month glucose", "id": "wqI_z1yumzY_3", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And if that's out of range,\nthen they're diabetic. And that's, in\nfact, usually used as a definition of diabetes. But that didn't answer\nmy original question. Why is just looking at diabetic\nmedications not enough? AUDIENCE: Some of the\ndiabetic medications can be used to treat\nother conditions. PROFESSOR: Sometimes\nthere's ambiguity in diabetic medications. But we've sort of\ndealt with that already by trying to\nchoose an unambiguous set. What are other reasons? AUDIENCE: You're starting\nwith the medicine at the onset of diabetes [INAUDIBLE]. PROFESSOR: Oh, that's a\nreally interesting point-- not the one I was thinking\nabout, but I like it-- which is that a patient might\nhave been diagnosed with type 2 diabetes, but they,\nfor whatever reason, in that communication\nbetween provider and patient, they decided we're not going\nto start treatment yet. So they might not yet be\non treatment for diabetes, yet the whole health\ncare system might be very well aware that\nthe patient is diabetic, in which case doing these\ninterventions for that patient", "id": "wqI_z1yumzY_4", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Yep, another reason? AUDIENCE: So a lot of\npeople are just not diagnosed for diabetes. So they have it. So one label means that\nthey have diabetes, and the other label is a\ncombination of people who have and don't have diabetes. PROFESSOR: So the\npoint was, often you just might not be\ndiagnosed for diabetes. That, unfortunately,\nis not something that we're going to\nable to solve here. It is an issue, but we\nhave no solution for it. No, rather there's a different\npoint that I want to get at, which is that this\ndata has biases in it. So even if a patient is\non a diabetes medication, for whatever reason--\nmaybe they are paying cash for those medications. If they're paying cash\nfor those medications, then there's not going to be any\nrecord for the patient taking those medications in the\nhealth insurance claims. Because the health insurer\ndidn't have to pay for it. But the reason that you gave is\nalso a very interesting reason. And both of them are valid. So for all of these reasons,\njust looking at the medications alone is going to\nbe insufficient. And as was just\nsuggested a moment ago,", "id": "wqI_z1yumzY_5", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  does the patient have an\nabnormal blood glucose value or HBA1C value would\nalso provide information. So it's non-trivial, right? And part of what you're going\nto be doing in your next problem set, problem set 2, is going\nto be thinking through how does one actually do this cohort\nconstruction, not just what is your inclusion/exclusion\ncriteria, but also how do you\nreally derive those labels from that data set. Now the traditional answer\nto this has two steps to it. Step 1 is to actually\nmanually label some patients. So you take a few\nhundred patients, and you go through their data. You actually look at\ntheir data, and decide, is this patient diabetic\nor are they not diabetic? And the reason why you have to\ndo that is because often what you might think of is obvious-- like, oh, if they're on diabetes\nmedication, they're diabetic-- has flaws to it. And until you really dig\ndown and look at the data, you might not recognize that\nthat criteria has a flaw in it. So that chart review is\nreally an essential part", "id": "wqI_z1yumzY_6", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Then the second\nstep is, how do you generalize to get that\nlabel now for everyone in your population. And again, there, there are\nusually two different types of approaches. The first approach is to\ncome up with some simple rule to try to then\nextrapolate to everyone. For example, if they have,\nA, diabetes medication, or an abnormal lab\ntest result, that would be an example of a rule. And then you could then\napply that to everyone. But even those rules can\nbe really tricky to derive. And I'll show you some examples\nof that in just a moment. And as we know,\nmachine learning is sometimes good as an alternative\nfor coming up with a rule. So there's often now\na second approach to this being more\nand more commonly used in the literature, which is to\nactually use machine learning itself to derive the labels. And this is a bit subtle,\nbecause it's machine learning for machine learning. So I want to break that\ndown for one second.", "id": "wqI_z1yumzY_7", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  you want to know\nis not, at time T, what's going to happen at\ntime T plus W and onwards-- that's the original\nmachine learning task that we set out to solve-- but rather, given\neverything you know about the patient,\nincluding the future data, is this patient newly diagnosed\nwith diabetes in that window that I show in black there,\nbetween T plus W and onward. OK? So for example, this\nmachine learning problem, this new machine\nlearning problem, could take, as input, lab\ntest results, and medications, and a whole bunch of other data. And you then use the few\nexamples you labeled in step 1 to try to predict, is\nthis patient currently diabetic or not. You then use that\nmodel to extrapolate to the whole population. And now you have\nyour outcome label. It might be a little\nbit imperfect, but hopefully it's much\nbetter than what you could have gotten with a rule. And then, now\nusing those outcome labels, you solve your original\nmachine learning problem. Is that clear?", "id": "wqI_z1yumzY_8", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  AUDIENCE: I have one. PROFESSOR: Yep. AUDIENCE: How do you\nevaluate yourself then, if you have these\nlabels that were produced with machine learning,\nwhich are probabilistic? PROFESSOR: So that's where this\nfirst step is really important. You've got to get\nground truth somehow. And of course once you\nget that ground truth, you create a train-and-validate\nset of that ground truth. You run your machine learning\nalgorithm with the trained one. You'd look at its\nperformance metrics on that validate set for the\nlabel prediction problem. And that's how you\nget confidence in it. But let's try to break\nthis down a little bit. So first of all, what does this\nchart review step look like? Well, if it's an\nelectronic health record system, what you often do is you\nwill pull up Epic, or Cerner, or whatever the\ncommercial EHR system is. And you will actually start\nlooking at the patient data. You'll read notes written\nby previous doctors about this patient. And you'll look at\ntheir blood test results across time, medications\nthat they're on. And from that you\ncan usually tell pretty coherent story what's\ngoing on with your patient. Of course even better-- or\nthe best way to get data is to do a prospective study.", "id": "wqI_z1yumzY_9", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  in the room when a patient\nwalks into a provider. And they talk to the\npatient, and they take down really very clear notes\nwhat this patient has, what they don't have. But that's usually too\nexpensive to do prospectively. So usually what we do is\ndo this retrospectively. Now, if you're working with\nhealth insurance claims data, you usually don't have the\nluxury of looking at notes. And so what, in my group, we\ntype typically do is we build, actually, a visualization tool. And by the way, I'm a\nmachine learning person. I don't know anything\nabout visualization. Neither do I claim\nto be good at it. But you can't do the machine\nlearning work unless you really understand your data. So we had to build this tool\nin order to look at the data, in order to try to do that\nfirst step of understanding, did we even characterize\ndiabetes correctly. So I'm not going\ngo deep into it. By the way, you\ncan download this. It's an open source tool. But ballpark what I'm showing\nyou here is one patient's data. I'm showing on\nthis x-axis, time, going from April to December.", "id": "wqI_z1yumzY_10", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So in orange are\ndiagnosis codes that were recorded for the patient. In green are procedure codes. In blue are laboratory tests. And if you see, on a\ngiven line, multiple dots along that same line, it\nmeans that same lab test was performed multiple times. And you could click on it to\nsee what the results were. And in this way, you could start\nto tell a coherent story what's going on with your patient. All right, so tools\nlike this is what you're going to\nneed to able to do that first step from something\nlike health insurance claims data. Now, traditionally,\nthat first step, which then leads you to\nlabel some data, and then, from there, you go and\ncome up with these rules, or do a machine learning\nalgorithm to get the label, usually that's a\npaper in itself. Of course, not of interest to\nthe computer science community, but of extreme interest to\nthe health care community. So usually there's a first\npaper, academic paper, which evaluates this process\nfor deriving the label,", "id": "wqI_z1yumzY_11", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  you could do with that label,\nsuch as the machine learning problem we originally\nset out to solve. So let's look at an example\nof one of those rules. Here is a rule, to derive\nfrom health insurance claims data whether a patient\nhas type 2 diabetes. Now, this isn't quite the same\none that we used in that paper, but it gets the idea across. First you look to see, did\nthe patient have a diagnosis code for type 1 diabetes. If the answer is\nno, you continue. If the answer is yes,\nyou've sort of ruled out. Because you say, OK, this\npatient's abnormal blood test results are because they\nhave type 1 diabetes, not type 2 diabetes. Type 1 diabetes\nusually is what you can think of as\njuvenile diabetes, is diagnosed much earlier. And there's a different\nmechanism behind it. Then you look at other things-- OK, is there a\ndiagnosis code for type 2 diabetes somewhere\nin the patient's data?", "id": "wqI_z1yumzY_12", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  is there a medication,\nan Rx, for type 1 diabetes in the data. If the answer is no, you\ncontinue down this way. If the answer is\nyes, you go this way. A yes of a type 1\ndiabetes medication doesn't alone rule\nout the patient. Because maybe the\nsame medications are used for type\n1 as for type 2. So there's some other\nthings you need to do there. Right, you can see that this\nstarts to really quickly become complicated. And these manual-based\napproaches end up having pretty\nbad positive-- so they're designed\nusually to have pretty high positive predictive value. But they end up having\npretty bad recall, in that they don't end up\nfinding all of the patients. And that's really why the\nmachine-learning-based approaches end up\nbeing very important for this type of problem. Now, this is just one example\nof what I call a phenotype. I call this a phenotype. That's just what the\nliterature calls it. It's a phenotype\nfor type 2 diabetes. And the word, phenotype,\nin this context is exactly the same\nthing as the label. Yep.", "id": "wqI_z1yumzY_13", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  PROFESSOR: For example, if the\nHA1C result is 6.5 or higher, you might say the\npatient has diabetes. AUDIENCE: OK, so this is a\nlab result, not a medical-- PROFESSOR: Correct,\nyeah, thanks. Other questions. AUDIENCE: What's the\nphenotype, which part exactly is the phenotype,\nlike, the whole thing? PROFESSOR: The\nwhole thing, yeah. So the construction,\nwhere you say-- you follow this\ndecision tree, and you get to a conclusion, which\nis case, which means, yes they're type 2 diabetic. And if ever you don't reach this\npoint, then the answer is no, they're not type 2 diabetic. That's what I mean\nby-- so that labeling is what we're calling the\nphenotype of type 2 diabetes. Now later in the\nsemester, people will use the word, phenotype,\nto mean something else. It's an overloaded term. But this is what it's called\nin this context as well. Now here's an example\nof a website-- it's from the PheKB project-- where you will\nfind tens to close to 100 of these\nphenotypes that have been arduously created\nfor a whole range", "id": "wqI_z1yumzY_14", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  OK, so if you go\nto this website, and you click on any\none of these conditions, like appendicitis,\nautism, cataracts, you'll see a different diagram\nof this sort I just showed you. So this is a real thing. This is something that the\nmedical community really needs to do in order to try\nto derive the label that we can then use in our\nmachine learning task. AUDIENCE: I'm just curious,\nis the lab value ground truth? Like if somebody has\ndiabetes, then they must have [INAUDIBLE]. It means they have been\ndiagnosed, and they must have-- PROFESSOR: Well,\nso, for example, you might have an\nabnormal glucose value for a variety of reasons. One reason is because\nyou might have what's called gestational\ndiabetes, which is diabetes that's\ninduced due to pregnancy. But those patients\ntypically-- well, although it's a\npredictive factor, they don't always have\nlong-term type 2 diabetes. So even the\nlaboratory test alone doesn't tell the whole story. AUDIENCE: You could be\ndiagnosed without having", "id": "wqI_z1yumzY_15", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  PROFESSOR: That's\nmuch less common here. The story will\nchange in the future, because there will be a\nwhole range of new diagnosis techniques that might use\nnew modalities, like gene expression, for example. But typically, today, the\nanswer is yes to that. Yep. AUDIENCE: So if these\nare made by doctors, does that mean, for\nevery single disease, there's one\ndefinitive phenotype? PROFESSOR: These\nare usually made by health outcomes\nresearchers, which usually have clinicians on their team. But the type of people who\noften work on these often come from the field of\nepidemiology, for example. And so what was\nyour question again? AUDIENCE: Is there\njust one phenotype for every single disease? PROFESSOR: Is\nthere one phenotype for every different disease? In the ideal world, you'd\nhave at least one phenotype for every single disease\nthat could possibly exist. Now, of course, you\nmight be interested in different aspects. Like you might be\ninterested in not knowing just does the\npatient have autism, but where they are on\ntheir autism spectrum.", "id": "wqI_z1yumzY_16", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  do they have it\nnow, but you also might want to know\nwhen did they get it. So there's a lot of subtleties\nthat could go into this. But building these\nup is really slow. And validating them to\nmake sure that they're going to work\nacross multiple data sets is really challenging, and\nusually is a negative result. And so it's been a\nvery slow process to do this manually,\nwhich has led me and many others to start\nthinking about the machine learning approaches for\nhow to do it automatically. AUDIENCE: Just as a\nfollow-up, is there any case where there's,\nlike, five autism phenotypes, for example, or\nmultiple competing ones? PROFESSOR: Yes. So there are often many\ndifferent such rule-based systems that give you\nconflicting results. Yes, that happens all the time. AUDIENCE: Can these\nrule-based systems provide an estimate of when\ntheir condition was onset? PROFESSOR: Right,\nso that's getting at one of the subtleties I just\nmentioned-- can these tell you when the onset happened? They're not typically\ndesigned to do that, but one can come up\nwith one to do it. And so one way to\ntry to do that is", "id": "wqI_z1yumzY_17", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And then you can imagine\napplying those rules in a sliding window\nto the patient data to see, when is the first\ntime that it triggers. And that would be\none way to try to get a sense of when onset was. But there's a lot of\nsubtleties to that, too. So I'm going to move on now. I just want to give it some\nsense of what that deriving the labels ends up looking like. Let's now turn to evaluation. So a very commonly used\napproach in this field is to compute what's known as\nthe Receiver-Operator Curve, or ROC curve. And what this looks\nat is the following. First of all, this\nis well-defined for a binary\nclassification problem. For a binary\nclassification problem when you're using a\nmodel that outputs, let's say, a probability\nor some continuous value, then you could use that\ncontinuous valid prediction. If you wanted to\nmake a prediction, you usually threshold it, right? So you say, if it's greater than\n0.5, it's a prediction of 1.", "id": "wqI_z1yumzY_18", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  But here we might be interested\nin not just what minimizes, let's say, 0-1 loss,\nbut you might also be interested in\ntrading off, let's say, false positives for\nfalse negatives. And so you might choose\ndifferent thresholds. And you might want\nto quantify how do those trade-offs look\nfor different choices of those thresholds of this\ncontinuous value prediction. And that's what the ROC\ncurve will show you. So as you move along the\nthreshold, you can compute, for every single threshold,\nwhat is the true positive rate, and what is the\nfalse positive rate. And that gives you a number. And you try all\npossible thresholds, that gives you a curve. And then you can compare curves\nfrom different machine learning algorithms. For example, here,\nI'm showing you, in the green line, the\npredictive model obtained by using what we're calling the\ntraditional risk factors, so something like eight\nor 10 different risk factors for type 2 diabetes\nthat are very commonly used in the literature. Versus in blue, it's\nshowing you what", "id": "wqI_z1yumzY_19", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  logistic regression model\nwith no domain knowledge, just sort of throw in\nthe bag of features. And you want to be up there. You want to be in\nthat top left corner. That's the goal here. So you would like\nthat blue curve to be up there, and then\nall the way to the right. Now, one way to try to\nquantify in a single number how useful any one\nROC curve is is by looking at what's called\nthe area under the ROC curve. And mathematically, this is\nexactly what you'd expect. This area is the area\nunder the ROC curve. So you could just\nintegrate the curve, and you get that number out. Now, remember, I\ntold you you want to be in the upper\nleft quadrant. And so the goal\nwas to get an area under the ROC curve of a 1. Now, what would a random\nprediction give you?", "id": "wqI_z1yumzY_20", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So if you're to just\nflip a coin and guess-- what do you think? AUDIENCE: 0.5. PROFESSOR: 0.5? AUDIENCE: [INAUDIBLE] PROFESSOR: Well, so I was a\nlittle bit misleading when I said you just flip a coin. You got to flip a coin with\nsort of different noise rates. And each one of\nthose will get you sort of a different\nplace along this curve. And if you look at\nthe curve that you get from these\nrandom guesses, it's going to be the straight\nline from 0 to 1. And as you said, that will\nthen have an AUC of 0.5. So 0.5 is going to\nbe random guessing. 1 is perfect. And your algorithm is going\nto be somewhere in between. Now, of relevance to the\nrest of today's lecture is going to be an\nalternative definition-- alternative way of computing\nthe area under the ROC curve. So one way to compute it\nis literally as I said. You create that curve,\nand you integrate to get the area under it.", "id": "wqI_z1yumzY_21", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  I'm not going to give\nyou the derivation here, but you can look\nit up on Wikipedia. One can show mathematically\nthat an equivalent way of computing the\narea under the ROC curve is to compute the probability\nthat an algorithm will rank a positive-labeled\npatient over a negative-labeled patient. So mathematically\nwhat I'm talking about is the following thing. You're going to sum over\npairs of patients where-- I'm going to call x1 is a\npatient with label y1 equals 1. And x2 is a patient\nwith label y-- actually, I'll call it-- yeah, with label x2 equals 1. So these are two\ndifferent patients. I think I'm going to rewrite\nit like this-- xi and xj, just for generality's sake. So we're going to sum this\nup over all choices of i", "id": "wqI_z1yumzY_22", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So that should say yj equals 0. And then you're\ngoing to look at-- what you want to happen,\nlike suppose that you're using a linear model here. So your prediction is given\nto you by, let's say, w.xj. What you want is that this\nshould be smaller than w.xi. So the j data point,\nremember, was the one that got the 0-th and\nthe i-th data point is the one that got the 1 label. So we want the score of the\ndata point that should've been 1 to be higher than the score\nof the data point which should've gotten the label 0. And you just count up-- this\nis an indicator function. You just count up how many of\nthose were correctly ordered.", "id": "wqI_z1yumzY_23", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of comparisons that you do. And it turns out that that\nis exactly equal to the area under the ROC curve. And it really makes clear\nthat this is a notion that really cares about ranking. Are you getting the ranking\nof patients correct? Are you ranking\nthe ones who should have been given 1 higher\nthan the ones that should have gotten the label 0. And importantly,\nthis whole measure is actually invariant\nto the label imbalance. So you might have a very\nimbalanced data set. But if you were to\nre-sample with now making it a balanced data set, your\nAUC of your predictive model wouldn't change. And that's a nice\nproperty to have when it comes to evaluating\nsettings where you might have artificially created\na balanced data set for computational concerns. Even though the true setting\nis imbalanced, there at least", "id": "wqI_z1yumzY_24", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  the same in both settings. On the other hand, it also\nhas lots of disadvantages. Because often you don't\ncare about the performance of the whole entire curve. Often you care about particular\nparts along the curve. So for example, in\nlast week's lecture, I argued that really\nwhat we often care about is just the positive\npredictive value for a particular threshold. And we want that to be as high\nas possible for as few people as possible. Like, find the 100\nmost risky people, and look at what\nfraction of them actually developed\ntype 2 diabetes. And that setting, what\nyou're really looking at is this part of the curve. And so it turns out\nthere are generalizations of area under the curve that\nfocus on parts of the curve. And that goes by the\nname of partial AUC. For example, if you just\nintegrated from 0 to, let's say, 0.1 of\nthe curve, then you could still get a number to\ncompare two different curves, but it's focusing on the area\nof that curve that's actually relevant for your\npredictive purposes, for your task at hand.", "id": "wqI_z1yumzY_25", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  characteristic curves. Any questions? Yep. AUDIENCE: Could you talk more\nabout what the drawbacks were of using this. Does the class imbalance--\nis the class imbalance, then, always a positive effect? PROFESSOR: So the thing is, when\nyou want to use this approach, depending on how you're\nusing the [INAUDIBLE],, you might not be\nable to tolerate a 0.8 false positive rate. So in some sense, what's going\non in this part of the curve might be completely\nirrelevant for your task. And so one of the algorithms--\nso one of these curves-- might look like it's\ndoing really, really well over here, and\npretty poorly over here. But if you're looking at the\nfull area under the ROC curve, you won't notice that. And so that's one\nof the big problems. Yeah. AUDIENCE: And when would you\nuse this versus precision recall or-- PROFESSOR: Yeah, so a\nlot of the community is interested in\nprecision recall curves.", "id": "wqI_z1yumzY_26", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to receiver-operator\ncurves, have the property that they are not invariant\nto class imbalance, which in many settings is of\ninterest, because it will allow you to capture\nthese types of quantities. I'm not going to go into\ndepth about your reasons for one or the other. But that's something that\nyou could read up about, and I encourage you to\npost to Piazza about, and we have\ndiscussion on Piazza. So the last evaluation quantity\nthat I want to talk about is known as calibration. And calibration, as\nI've defined it here, has to do with binary\nclassification problems. Now, before you dig\ninto this figure, which I'll explain in a\nmoment, let me just give you the gist of what\nI mean by calibration. Suppose that your model\noutputs a probability. So you do logistic regression. You get a probability out. And your model says,\nfor these 10 patients, that their likelihood of dying\nin the next 48 hours is 0.7.", "id": "wqI_z1yumzY_27", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  If you were on the receiving\nend of that result, so you heard that,\n0.7, what should you expect about those 10 people? What fraction of\nthem should actually die in the next 48 hours? Everyone could scream out loud. [INTERPOSING VOICES] PROFESSOR: So seven of them. Seven of the 10 you would expect\nto die in the next 48 hours if the probability for all of\nthem that was output was 0.7. All right, that's what\nI mean by calibration. So if, on the other\nhand, what you found was that only\none of them died, then it would be a very weird\nnumber that you're outputting. And so the reason why this\nnotion of calibration, which I'll define formally\nin a second, is so important, is when you're out\nputting a probability, and when you don't really\nknow how that probability is going to be used.", "id": "wqI_z1yumzY_28", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And you knew that\nall that mattered was the actual prediction, 1\nor 0, then that would be fine. But often predictions\nin machine learning are used in a much\nmore subtle way. Like for example,\noften your doctor might have more information\nthan your computer has. And so often they might\nwant to take the result that your computer\npredicts, and weigh that against other evidence. Or in some settings,\nit's not just weighting about other evidence. Maybe it's also about\nmaking a decision. And that decision\nmight take exertion-- a utility, for example,\na patient preference for suffering versus getting\na treatment that could have big, adverse consequences. And that's something\nthat Pete is going to talk about much more\nlater in the semester, I think, how to formalize that notion. But at this point, I just want\nto sort of get out the point", "id": "wqI_z1yumzY_29", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And having the\nprobabilities be meaningful is something that\none can now quantify. So how do we quantify it? Well, one way to\ntry to quantify it is to create the\nfollowing prompt. Actually, we'll\ncall it a histogram. So on the x-axis is the\npredicted probability. So that's what I meant by p-hat. On the y-axis is the\ntrue probability. It's what I mean when I say\nthe fraction of individuals with that predicted\nprobability that actually got the positive outcome. That's going to be the y-axis. So I'll call that\nthe true probability. And what we would like\nto see is that this", "id": "wqI_z1yumzY_30", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  be equal. And in the example\nI gave, remember I said that there\nwere a bunch of people with 0.7 probability\npredicted, but for whom only one out of them actually\ngot the positive end. So that would have been\nsomething like over here. Whereas you would have\nexpected it to be over there. So you might ask, how\ndo I create such a plot from finite data? Well, a common way to do\nso is to bin your data. So you'll create intervals. So this bin is the\nbin from 0 to 0.1. This bin is the bin from\n0.1 to 0.2, and so on. And then you'll look to see,\nOK, how many people for whom the predicted probability\nwas between 0 and 0.1", "id": "wqI_z1yumzY_31", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And you'll get a number out. And now here's where\nI can go to this plot. That's exactly what\nI'm showing you here. So for now, ignore the\nbar charts and the bottom, and just look at the line. So let's focus on\njust the green line. Here I'm showing you\nseveral different models. For now, just focus\non the green line. So the green line, by the way,\nnotice it looks pretty good. It's almost a straight line. So how did I compute it? Well, first of all,\nnotice the number of ticks are 1, 2, 3, 4,\n5, 6, 7, 8, 9, 10. OK, so there are 10\npoints along this line. And each of those corresponds\nto one of these bins. So the first point\nis the 0 to 0.1 bin. The second point is the\n0.1 to 0.2 bin, and so on. So that's how it computed this. The next thing you\nnotice is that I have confidence intervals. And the reason I compute\nthese confidence intervals is because sometimes\nyou just might not have that much data\nin one of these bins. So for example, suppose\nyour algorithm almost never said that someone has a\npredictive probability of 0.99.", "id": "wqI_z1yumzY_32", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  you're not going to know what\nfraction of those individuals actually went on to\ndevelop the event. And you should be looking\nat sort of the confidence interval of this line,\nwhich should take that into consideration. And a different way to try to\nunderstand that notion, now looking at the\nnumbers, is what I'm showing you in the bar\ncharts in the bottom. On the bar charts,\nI'm showing you the number of individuals or\nthe fraction of individuals who actually got that\npredicted probability. So now let's start\ncomparing the lines. So the blue line shown\nhere is a machine learning algorithm which\nis predicting infection in the emergency rooms. It's a slightly different\nproblem than the diabetes one we looked at earlier. And it's using a bag of words\nmodel from clinical text. The red line is using\njust chief complaint.", "id": "wqI_z1yumzY_33", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that you get at one\npoint of time in the ER. So it's using very\nlittle information. And you can see that both models\nare somewhat well calibrated. But the intervals--\nthe confidence intervals of both the\nred and the purple lines gets really big towards the end. And if you look at\nthese bar charts, it explains why,\nbecause the models that use less information end\nup being much more risk-averse. So they will never predict\na very high probability. They will always sort of\nstay in this lower regime. And that's why we have very\nbig confidence intervals there. OK, so that's all I want\nto say about evaluation. And I won't take any\nquestions on this right now, because I\nreally want to get on to the rest of the lecture. But again, if you have any\nquestions, post to Piazza, and I'm happy to discuss\nthem with you offline.", "id": "wqI_z1yumzY_34", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to reduce risk stratification\nto binary classification. I've told you how to\nderive the labels. I've given you one example\nof machine learning algorithm you can use, and I talked to\nyou about how to evaluate it. What could possibly go wrong? So let's look at some examples. And these are a small number of\nexamples of what could possibly go wrong. There are many more. So here's some data. I'm showing you--\nfor the same problem we looked at before,\ndiabetes onset, I'm showing you the prevalence of\ntype 2 diabetes as recorded by, let's say, diagnosis\ncodes across time. All right, so over here is 1980. Over here is 2012. Look at that. It is not a flat line. Now, what does that mean? Does that mean that the\npopulation is eating much more", "id": "wqI_z1yumzY_35", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  are becoming diabetic? That would be one\nplausible answer. Another plausible explanation\nis that something has changed. So in fact I'm showing you\nwith these blue lines, well, in fact, there was a change\nin the diagnostic criteria for diabetes. And so now the patient\npopulation actually didn't change much\nbetween, let's say, this time point at\nthat time point. But what really led\nit to this big uptick, according to one theory, is\nbecause the diagnostic criteria changed. So who we're calling\ndiabetic has changed. Because diseases are,\nat the end of the day, a human-made concept, you know,\nwhat do we call some disease. And so the data is\nchanging, as you see here. Let me show you another example. Oh, by the way, so the\nconsequence of that is that", "id": "wqI_z1yumzY_36", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  for example, if you use\none of those phenotyping algorithms I showed you\nearlier, the rules-- what the label is\nderived for over here might be very different\nfrom the label that's derived from over\nhere, particularly if it's using data such\nas diagnosis codes that have changed in\nmeaning over the years. So that's one consequence. There'll be other consequences\nI'll tell you about later. Here's another example. And by the way, this notion\nis called non-stationarity, that the data is\nchanging across time. It's not stationary. Here's another example. On the x-axis again\nI'm showing you time. Here each column is a\nmonth, from 2005 to 2014. And on the y-axis, for every\nsort of row of this table, I'm showing you a\nlaboratory test. And here we're not looking at\nthe results of the lab test, we're only looking\nat what fraction of-- at how many lab\ntests of that type", "id": "wqI_z1yumzY_37", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And now you might expect\nthat, broadly speaking, the number of glucose tests,\nthe number of white blood cell count tests, the number of\nneutrophil tests and so on might be pretty constant\nacross time, on average, because you're averaging\nover lots of people. But indeed what you see\nhere is that, in fact, there is a huge amount\nof non-stationarity. Which tests are\nordered dramatically changes across time. So for example you see\nthis one line over here, where it's all blue, meaning\nno one is ordering the test, until this point in time,\nwhen people start using it. What could that be? Any ideas? Yeah. AUDIENCE: [INAUDIBLE] PROFESSOR: So the test was used\nless, or really, in this case, not used at all. And then suddenly it was used. Why might that happen? In the back. AUDIENCE: A new test.", "id": "wqI_z1yumzY_38", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Suddenly we come up with a\nnew diagnostic test, a new lab test. And we can start using it,\nwhere it didn't exist before. So obviously there was\nno data on it before. What's another reason why it\nmight have suddenly showed up? Yep. AUDIENCE: It could be like\nannual check-ups become mandatory, or that it's part of\nthe test admission at hospital. Like, it's an additional test. PROFESSOR: I'll stick\nwith your first example. Maybe that test\nbecomes mandatory. OK, so maybe there's\na clinical guideline that is created at this\npoint in time, right there. And health insurers\ndecide we're going to reimburse for this test\nat this point in time. And the test might've\nbeen really expensive. So no one would have\ndone it beforehand. And now that the health\ninsurance companies are going to pay for it,\nnow people start doing it. So it might have\nexisted beforehand. But if no one would pay for\nit, no one would use it. What's another reason why you\nmight see something like this,", "id": "wqI_z1yumzY_39", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Notice, here in\nthe middle, there's this huge gap in the middle. What might have explained that? AUDIENCE: [INAUDIBLE] PROFESSOR: Hold on. Yep, over here. AUDIENCE: Maybe your\npatient population is mostly of a certain age,\nand coverage for something changes once your age\ncrosses a threshold. PROFESSOR: Yeah, so\none explanation-- I think it's not plausible\nin this data set, but it is plausible\nfor some data sets-- is that maybe your\npatients at time 0 were all of exactly\nthe same age. So maybe there's some\namount of alignment. And suddenly, at this\npoint in time, let's say, women only get, let's say,\ntheir annual mammography once they turn a certain age. And so that might be one reason\nwhy you would see nothing until one point in time. And maybe that would\nchange across time as well.", "id": "wqI_z1yumzY_40", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  That's not true, but let's say. So that's one explanation. In this case, it\ndoesn't make sense, because the patient\npopulation is very mixed. So you could think about it as\nbeing roughly at steady state. So they're not-- you'll have\npatients of all ages here. What's another reason? Someone raised their\nhand over here. Yep. AUDIENCE: Yeah, I was\njust going to say, maybe the EMR shut\ndown for awhile, and so they were only\ndoing stuff on paper, and they only were able\nto record 4 things. PROFESSOR: Ding\nding ding ding ding. Yes, that's right. So maybe the EMR shut down. Or in this case,\nwe had data issues. So this data was\nacquired somehow. For example, maybe\nit was required through a contract\nwith something like Webquest or LabCorp. And maybe, during that\nfour-month interval, there was contract negotiation. And so suddenly we\ncouldn't get the Data for that time period. Or maybe our databases\ncrashed, and we suddenly", "id": "wqI_z1yumzY_41", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  This happens, and this\nhappens all the time, and not just the\nhealth care industry, but other industries as well. And as a result of those\nsystemic-type changes, your data is also going to be\nnon-stationary across time. So now we've seen three or\nfour different explanations for why this happens. And the reality is really\na mixture of all of these. And just as in the previous-- so in the previous example,\nnotice how what really changed here is that\nthe derived labels might change meaning across time. Now the significance\nof the features used in the machine\nlearning models would really change across time. And that's one of the\nconsequences of this, particular if you're driving\nfeatures from lab test values. Here's one last example. Again, on the x-axis\nhere, I have time. On the y-axis here, I'm\nshowing the number of times that you observed some\ndiagnosis code of some kind. This cyan line is ICD-9 codes.", "id": "wqI_z1yumzY_42", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  You might remember that Pete\nmentioned in an earlier lecture that there was a big shift from\nICD-9 coding to ICD-10 coding at some point. When was that time? It was precisely this time. And so if you think about the\nfeature vector that you would derive for your machine\nlearning problem, you would have one feature\nfor all ICD-9 codes, and one-- a whole set of features\nfor all ICD-10 codes. And those ICD-9-based\nfeatures are going to be-- they're going\nto be used quite a bit in this time period. And then suddenly they're\ngoing to be completely sparse in this time period. And ICD-10 features\nstart to become used. And you could imagine that\nif you did machine learning using just ICD-9\ndata, and then you tried to apply your model\nat this point in time, it's going to do horribly,\nbecause it's expecting features that it no longer has access to. And this happens all the time. And in fact, what\nI'm describing here is actually a major problem for\nthe whole health care industry. For the next five\nyears, everyone is going to grapple\nwith this problem,", "id": "wqI_z1yumzY_43", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  learning, but their\nhistorical data is very different from\ntheir recent data. So now, in the face of all of\nthis non-stationarity that I just described, did we do\nanything wrong in the diabetes risk stratification problem\nthat I told you about earlier? Thoughts. That was my paper, by the way. Did I make an error? Thoughts. Don't be afraid. I'm often wrong. I'm just asking\nspecifically about the way I evaluated the models. Yep. AUDIENCE: This wasn't\nan error, but one thing, like if I was a doctor\nI would like to see is the sensitivity to-- like, the inclusion\ncriteria if I", "id": "wqI_z1yumzY_44", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Like most people, they have\ncompared to having either Rx or [INAUDIBLE] then\nkind of evaluating the-- PROFESSOR: So understanding\nthe robustness to changing the data a\nbit is something that would be of a lot of interest. I agree. But that's not\nimmediately suggested by the non-stationarity results. Not something that's suggested\nby non-stationarity results. Our TA in the front\nrow has an idea. Yeah, let's hear it. AUDIENCE: The train\nand test distributions were drawn from the\nsame-- or the train and tests were drawn from\nthe same distribution. PROFESSOR: So in the way that\nwe did our evaluation there, we said, OK, we're going to set\nit up such that on January 1, 2009, we're predicting\nwhat's going to happen in the following three years. And we segmented our\npatient population into train, validate, and\ntest, but at all times, using that same setup, January\n1 2009, as the prediction time.", "id": "wqI_z1yumzY_45", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  We want to apply\nthis model today. And I computed an area\nunder the ROC curve. I computed positive\npredictive values using that retrospective data. And I handed those\noff to my partners. And they might hope that those\nnumbers are reflective of what their models would do today. But because of these issues\nI just told you about-- for example, that\nthe number of people who have type 2 diabetes,\nand even the definition of it has changed. Because of the fact that\nthe laboratory-- ignore this part over here. That's just a fluke. But the fact, because\nof the laboratory tests that were\navailable during training might be different from the\nones that are available now, and because of the fact that\nwe have only ICD-10 data now, and not ICD-9, for\nall of those reasons, our predictive\nperformance is going to be really horrible\nnow, Particularly because of this last issue\nof not having ICD-9s. Our predictive model is\ngoing to work horribly now if it was trained on\ndata from 2008 or 2009.", "id": "wqI_z1yumzY_46", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that if we used the validation\nset up that we had done there. So I wrote that paper when\nI was young and naive. [AUDIENCE CHUCKLING] I'm a little bit\nmore gray-haired now. And so in our more recent\nwork-- for example, this is a paper which\nwe're working on right now, done by a master's student\nof mine, Helen Zhou, and is looking at predicting\nantibiotic resistance, now we're a little bit smarter\nabout over evaluation setup. And we decided to set it up\na little bit differently. So what I'm showing\nyou now is the way that we chose,\ntrained, validated and test for our population. So we segmented our data. So the x-axis here is time,\nand the y-axis here are people. So you can think of each person\nas being a different row. And you can imagine that we\nrandomly sorted the rows. What we did is we segmented our\ndata into these four quadrants. The first two quadrants, we\nused for train and validate.", "id": "wqI_z1yumzY_47", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  in the training set as we\ndo in the validate set. That's important for\nanother quantity which I'll talk about in a minute. So we used this data\nfor train and validate. And that's, again,\nvery similar to the way we did it in the diabetes paper. But now, for testing,\nwe use this future data. So we used data\nfrom 2014 to 2016. And one can imagine two\ndifferent quadrants. You might be\ninterested in knowing, for the same patients for\nwhom you made predictions on during training, how\nwould your predictions do for those same people at\ntest time in the future data. And that's assuming that\nwhat we're predicting is something that's much\nmore myopic in nature. In this case it was\npredicting, are they going to be resistant\nto some antibiotic? But you can also look at it\nfor a completely different set of patients, for\npatients who are not used during training at all. And suppose that this 2\nbucket isn't used at all,", "id": "wqI_z1yumzY_48", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  using the future data for that. And the advantage\nof this setup is that it can really help you\nassess non-stationarity. So if your model\nreally took advantage of features that were\navailable in 2007, 2008, 2009, but weren't available\nin 2014, you would see a big drop\nin your performance. Looking at the\ndrop in performance from your validate set\nin this time period, to your test set from\nthat time period, that drop in performance\nwill be uniquely attributed to the non-stationarity. So it's a good way\nto diagnose it. Yep. AUDIENCE: Just\nsome clarification on non-stationarity-- is it the\nfact that certain data is just lost altogether,\nor is it the fact that it's just\nencoded differently, and so then it's difficult\nto get that mapping correct? PROFESSOR: Both. Both of these happen. So I have a big\nresearch program now which is asking not just how-- so this is how you can\nevaluate and recognize there's a problem. But of course there's a really\ninteresting research question, which is, how can you make\nuse of the non-stationarity. Right, so for example,\nyou had ICD-9/ICD-10 data.", "id": "wqI_z1yumzY_49", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Is there a way to use it? So the naive answer, which is\nwhat the community is largely using today, is come\nup with a mapping. Come up with a manual\nmapping from ICD-9 to ICD-10 so that you can sort of\nmanually transform your data into this new format\nsuch that the models you learn from this older time\nis useful in the future time. That's the boring\nand simple answer. But I think we could\ndo much better. For example, we can learn new\nrepresentations of the data. We can learn that\nmapping directly in order to optimize\nfor your sort of most recent performance. And there's a whole bunch more\nthat we can talk about later. Yep. AUDIENCE: [INAUDIBLE]\nnon-stationary change, this will [INAUDIBLE]\ndoes not ensure robustness to the future. PROFESSOR: Correct. So this allows you to detect\nthat a non-stationarity has happened. And it allows you to say\nthat your model is going to generalize to 2014-2016.", "id": "wqI_z1yumzY_50", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to generalize to 2016-2018. And so how do you do that? How do you have\nconfidence in that? Well, that's a really\ninteresting research question. We don't have good\nanswers to that today. From a practical perspective,\nthe best I can offer you today is, build in these checks\nand balances all the time. So continuously sort\nof evaluate how you're doing on the most recent data. And if you see big\nchanges, throw a red flag. Build more checks and balances\ninto your deployment process. If you see a bunch of\npatients who are getting predicted probabilities\nof 1, and in the past, you'd never predicted\nprobability 1, that might tell you something. Then much later in the semester,\nwe'll talk about robust machine learning approaches,\nfor example, approaches that have\nbeen designed to be robust against adversaries. And those type of\napproaches as well will allow you to be much more\nrobust to particular types of data set shift, of\nwhich non-stationarity is one example. But it's a big,\nopen research field. Yep. AUDIENCE: So just to make sure I\nhave the understanding correct,", "id": "wqI_z1yumzY_51", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  from the old data set to the new\ndata set, like the encodings, would it still be\nOK, like the results you get on the future data set? PROFESSOR: If you could\ndo a perfect mapping, and it's one to one, and the\ndistributions of those things also didn't change, then yeah. Really what you need to assess\nis, is there data set shift? Is your training\ndistribution, after mapping, the same as your\ntesting distribution? If the answer is\nyes, you're all good. If you're not,\nyou're in trouble. Yep. AUDIENCE: What seems to be the\ntest set of traits set here? Or what [INAUDIBLE]? PROFESSOR: So 1 is using\ndata only from 2007-2013, 3 is using data\nonly from 2014-2016. AUDIENCE: But in the case,\nlike, the output we care about happened in, like,\n2007-2013, then that observation would be\nnot-- it wouldn't be useful. PROFESSOR: Yeah, so for\nthe diabetes problem, there's also just\ninclusion/exclusion criteria that you have to deal with. For what I'm showing you here,\nI'm talking about a setting where you might be making\nmultiple predictions", "id": "wqI_z1yumzY_52", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So it's a much more\nmyopic prediction task. But one could come\nup with an analogy to this for the\ndiabetes setting. Like, for example, just hold out\nhalf of the patients at random. And then for your training\nset, use data up to 2009, and evaluate on data\nonly up to 2013. And for your test set, pretend\nas if it was January 1, 2013, and look at\nperformance up to 2017. And so that would be-- you're changing your prediction\ntime to use more recent data. So the next subtlety is-- it's a name that I put on to it. This isn't a standard name. This is what I'm calling\nintervention-tainted outcomes. And so the example here came\nfrom your reading for today.", "id": "wqI_z1yumzY_53", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  for health care predicting\npneumonia risk in hospital 30-day admissions from KDD 2015. So in that paper,\nthey give an example-- it's a very old example-- of trying to use\na predictive model to understand a patient's\nrisk of mortality when they come\ninto the hospital. And what they learned-- and\nthey used a rule-based learning algorithm-- and\nwhat they discovered was a rule that said if\nthe patient has asthma, then they have\nlow risk of dying. So these are all patients\nwho have pneumonia. So a patient who comes in\nwith pneumonia and asthma has a lower risk of\ndying than a patient who comes in with pneumonia and does\nnot have a history of asthma. OK, that's what this rule says. And this paper argued\nthat there's something wrong with that learned model. Any of you remember\nwhat that was? Someone who hasn't\ntalked today, please. Yeah, in the back. AUDIENCE: It was that\nthose with asthma", "id": "wqI_z1yumzY_54", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So that means that they had\na higher chance of survival. PROFESSOR: Patients with asthma\nhad more aggressive treatment. In particular, they\nmight have been admitted to the\nintensive care unit for more careful vigilance. And as a result, they\nhad better outcomes. Yes, that's exactly right. So the real story behind this\nis that risk stratification, as we talked about\nthe last couple weeks, it's used to drive\ninterventions. And those interventions, if\nthey happened in the past data, would change the outcomes. So in this case,\nyou might imagine using the learned\npredictive model to say, a new patient comes in,\nthis new patient has asthma, and so we're going to\nsay they're low risk. And if we took a naive action\nbased on that prediction, we might say, OK,\nlet's send them home. They're at low risk of dying. But if we did that, we\ncould be killing people. Because the reason\nwhy they were low risk is because they had those\ninterventions in the past. So here's what's going\non in that picture. You have your\ndata, X. And you're", "id": "wqI_z1yumzY_55", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  emergency department triage. You want to predict\nsome outcome Y, let's say, whether the patient\ndies at some defined point in the future. Now, the challenge is that, as\nstated in the machine learning tasks that you saw there,\nall you had access to was X and Y, the covariance of\nthe features and the outcome. And so you're\npredicting Y from X, but you're marginalizing\nover everything that happens in between, in\nthis case, the treatment. So the good outcomes,\npeople surviving, might have been due to\nwhat's going on in between. But what's going on\nin between is not even observed in the\ndata necessarily. So how do we address\nthis problem? Well, the first thing I\nwant you to think about is, can we even recognize\nthat this is a problem? And that's where\nthat article really suggests that using an\nunintelligible model, a model that you can introspect and\ntry to understand a little bit, is actually really important\nfor even recognizing", "id": "wqI_z1yumzY_56", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And this is a\ntopic which we will talk about in a lecture towards\nthe end of the semester in much more-- Jack will talk about\nalgorithms for interpreting machine learning models. So that's important. You've got to recognize\nwhat's going on. But what do you do about it? So here are some hacks. Hack number 1--\nmodify the model. This is the solution that is\nproposed in the paper you read. They said, OK, if it's a\nsimple rule-based prediction that the learning\nalgorithm outputs to you, you could see the rule\nthat doesn't make sense, you could use your\nclinical insight to recognize it\ndoesn't make sense. You might even be able to\nexplain why it happened. And then you just\nremove that rule. So you manually modify the model\nto push it towards something that's more sensible. All right, so that's\nwhat was suggested. And I think it's nonsense. I don't think that's ever\ngoing to work in today's world. In today's world of\nhigh-dimensional models, there's always going to be\nsurrogates which are somehow", "id": "wqI_z1yumzY_57", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that you will not\neven recognize. And it will be really hard\nto modify it in the way that you want. Maybe it's impossible using the\nsimple approach, by the way. Another interesting\nresearch question-- how do you actually\nmake this work in a high-dimensional setting? But for now, let's say we\ndon't know how to do it in a high-dimensional setting. So what are your other choices? Hack number 2 is to redefine\nthe outcome altogether, to change what\nyou're predicting. So for example, if you\ngo back to this picture, and instead of\ntrying to predict Y, death, if you could try to find\nsome surrogate for the thing you care about, which\nis pre-treatment, and you predict\nthat thing instead, then you'll be back in business. And so, for example, in one\nof the optional readings for-- or actually I think in the\nsecond required reading for today's class,\nit was a paper about risk revocation\nfor sepsis, which is often caused by infection. And what they show\nin that article is that there are laboratory\ntest results, such as lactate,", "id": "wqI_z1yumzY_58", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  a hint that this patient\nmight be on a path to clinical deterioration. And that test might precede\nthe interventions to try to take care of that condition. And so if you instead\nchange your outcome to be predicting that\nsurrogate, then you're getting around this problem\nthat I just pointed out. Now, a third hack is from\none of the optional readings from today's lecture,\nthis paper by Suchi Saria and her students, from\nScience Translational Medicine 2015. It's a really\nwell-written paper. I highly recommend reading it. In that paper, they suggest\nformalizing the problem as one of censoring,\nwhich is what we'll be talking about for\nthe very last third of today's lecture. In particular, what\nthey say is suppose you see that a patient is\ntreated for the condition. Let's say they're\ntreated for sepsis. Then if the patient is\ntreated for that condition, then we don't know what would\nhave happened to them had they", "id": "wqI_z1yumzY_59", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So we don't observe the outcome,\ndeath given no treatment. And so we're going to treat\nit as an unknown outcome. And for patients who were\nnot treated, but ended up dying due to sepsis, then\nthey're not censored. And what I'll show you in\nthe later part of the class is how to learn\nfrom censored data. So this is another\nformalization which tries to address this\nproblem that we pointed out. Now, I call these hacks\nbecause, really, I think what we should be\ndoing is formalizing it using the language of causality. Once you do this\nintrospection and you realize that there is\ntreatment, in fact, you should be rethinking\nabout the problem as one of now having three\nquantities of interest. There's the patient, everything\nyou know about them at triage. That's the X-variable\nI showed you before. There's the outcome,\nlet's say, Y. And then there's\nthat everything that happened in between, in\nparticular the interventions that happened in between. We'll call that\nT, for treatment. And the question\nthat one would like", "id": "wqI_z1yumzY_60", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  for the patient is one of,\nwill admission to the ICU, which is the intervention\nthat we're considering here, will that lower the likelihood\nof death for the patient? And now when I say lower,\nI don't mean correlation, I mean causation. Will it actually lower the\npatient's risk of dying? I think we need to hit\nthese questions on the head with actually thinking\nabout causality to try to formalize this properly. And if you do that,\nthis will be a solution which will generalize to the\nhigh-dimensional settings that we care about\nin machine learning. And this will be a topic that\nwe'll talk really in-depth after spring break. But I wanted to give you this\nas one motivation for why it's so important-- there\nare many other reasons-- to really think about it\nfrom a causal perspective. OK, so subtlety number 3-- there's been a ton of hype in\nthe media about deep learning and health care. A lot of it is very\nwell warranted.", "id": "wqI_z1yumzY_61", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  in areas ranging from\nradiology and pathology to interpretation of\nEKGs are all really being transformed by\ndeep learning algorithms. But the problems\nI've been telling you about for the last\ncouple of weeks, of doing risk stratification on\nelectronic health record data, such as taxed notes,\nsuch as lab test results and vital signs,\ndiagnosis codes, that's a different story. And in fact, if you look\nclosely at all of the papers, all the papers that\nhave been published in the last few years\nthat have been trying to apply the gauntlet of\ndeep learning algorithms at those problems, in fact,\nthe gains are very small. And so what I'm showing you\nhere is just one example of such a paper. This is a paper that received\na lot of media attention. It's a Google paper\ncalled \"Scalable and Accurate Deep Learning with\nElectronic Health Records.\" And if you go across\nthe United States, if you go\ninternationally, you talk", "id": "wqI_z1yumzY_62", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  they're all going to be\ntelling you about this paper. They've all read it,\nthey've all heard about it, and they all want to use it. But what is this actually doing? What's going on\nbehind the scenes? Well, this paper\nuses the same sorts of data we've been\ntalking about. It takes vitals, notes,\norders, medications, thinks about it as a\ntimeline, summarizes it, then uses a recurrent neural network. It also uses attentional\narchitectures. And there's some pretty\nsmart people on this paper-- you know, Greg\nCorrado, Jeff Dean, are all co-authors\nof this paper. They know what they're doing. All right, so they use\nthese algorithms to predict a number of downstream\nproblems-- readmission risk, for example, 30-day\nreadmission, like you read about in your\nreadings for this week. And they see they get\npretty good predictions. But if you go to the\nsupplementary material, which is a bit hard to find, but\nhere's the link for all of you, and I'll post it to my slides. And if you look at\nthe very last figure", "id": "wqI_z1yumzY_63", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  see something interesting. So here are those\nthree different tasks that they studied--\ninpatient mortality prediction, 30-day readmission,\nlength-of-stay prediction. The first line each\nof these buckets is what your deep\nlearning algorithm does. Over here, they have\ntwo different hospitals. I think it might\nhave been University of Chicago and Stanford. And they're showing the area\nunder the ROC curve, which we've talked about,\nperformance for each of these tasks for\ntheir best models. And in the parentheses, they\ngive confidence intervals-- let's say something like 95%\nconfidence intervals-- for area under the ROC curve. Now, the second\nline that you see is called full-feature\nenhanced baseline. It's using the\nsame data, but it's using something very close\nto the feature represetnation that you saw in the\npaper by Narges Razavian, so that paper on\ndiabetes prediction that I told you about and\nwe've been criticizing. So it's using that\nL1-regularized logistic regression with a\nsmart set of features.", "id": "wqI_z1yumzY_64", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  is that the results are not\nphysically significantly different. So let's look at the first\none, hospital A, deep learning, 0.95 AUC. This L1-regularized\nlogistic regression, 0.93. 30-day readmission,\n0.77, 0.75, 0.86, 0.85. And the confidence intervals\nare all overlapping. So what's going on? So I think what you're\nseeing here, first of all, is a recognition by the machine\nlearning community that-- in this case, a late recognition\nthat simpler approaches tend to work well with\nthis type of data. I don't think this was the\nfirst thing that they tried. They tried probably the deep\nlearning algorithms first. Second, we're all\ngrasping at this, and we all want to come up\nwith these better algorithms, but so far we're\nnot doing that well. And I'll tell you more\nabout that in just a second. But before I finish\nwith the slide,", "id": "wqI_z1yumzY_65", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  You might come home\nfrom this and say, you know what, it's\nnot that much better, but it's a little bit better-- 0.95 to 0.93. Suppose it was tight\nconfidence intervals, there might be a few\npatients whose lives you could save with that. But because all the issues I've\ntold you about up until now, of non-stationary, for\nexample, those gains disappear. In many cases, they even\nreverse when you actually go to deploy these models\nbecause of that data set shift for non-stationarity. It so happens that\nthe simpler models tend to generalize better\nwhen your data changes on you. And this is nicely\nexplored in this paper from Kenneth Jung and Nigam\nShah in Journal of Biomedical Informatics, 2015. So this is something that\nI want you to think about. Now let's try to answer why. Well, the areas where\nwe've been seeing recurrent neural networks\ndoing really well-- in, for example,\nspeech recognition, natural language processing,\nare areas where, often-- for example, you're\npredicting what", "id": "wqI_z1yumzY_66", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  the previous few words\nare pretty predictive. Like, what is the next\n[PAUSES] that I'm going to say? What is it? AUDIENCE: Word. PROFESSOR: Word,\nright, and you knew that, right, because it was\npretty obvious to predict that. And so the models that\nare good at predicting for that type of\ndata, it doesn't mean that they should\nbe good for predicting for a different type\nof sequential data. Sequential data\nwhich, by the way, lives in many\ndifferent time scales. Patients who are hospitalized,\nyou get tons of data for them at a time, and then\nyou might go months without any data on them. Data with lots of missing data. Data with multivariate\nobservations at each point in time,\nnot just a single word at that point in time. All right, so it's\na different setting. And we shouldn't expect that\nthe same architectures that have been developed\nfor other problems will generalize immediately\nto these problems. Now, I do conjecture\nthat there are lots of nonlinear\nattractions where deep neural networks\ncould be very powerful at predicting for. But I think they're subtle. And I don't think that we\nhave enough data currently to deal with the fact\nthat the data is messy", "id": "wqI_z1yumzY_67", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  We just can't find\nthem right now. But this shouldn't mean that\nwe're not going to find them a few years from now. I think this deservedly is\na very interesting research direction to work on. And a final reason\nto point out is that the features that are\ngoing into these types of models are actually really\ncleverly-chosen features. A laboratory test result,\nlike looking at your A1C-- what is A1C? So it's something that\nhad been developed over decades and decades of\nresearch, where you recognize that looking at a\nparticular protein is actually informative as\nsomething about a patient's health. So the features that we're\nusing that go into these models were designed-- first, they were designed\nfor humans to look at. And second, they were\ndesigned to really help you with decision-making, or\nlargely independent features from other information that\nyou have about a patient. And all of those\nare reasons, really, I think why we're\nobserving these subtleties. OK, so for the last\n10 minutes of class-- I'm going to have\nto hold questions, because I want to get\nthrough all the material.", "id": "wqI_z1yumzY_68", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  For the last 10\nminutes of class, I want to change\ngears a little bit, and talk about\nsurvival modeling. So often we want want to\ntalk about predicting time to some event. So this red dot here-- sorry, this black line here\nis what I mean by an event. That event might be, for\nexample, a patient dying. It might mean a married\ncouple getting divorced. It might mean the day that\nwhat you graduate from MIT. And the red dot here\ndenotes censored events. So for whatever\nreason, we don't have data on this patient, patient\nS3, after time step 4. They were censored. So we do know that\nthe event didn't occur prior to time step 4. But we don't know\nif and when it's going to occur\nafter time step 4, because we have\nmissing data there.", "id": "wqI_z1yumzY_69", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So you might ask, why not\njust use classification-- like binary classification--\nin this setting? And that's exactly\nwhat we did earlier. We thought about formalizing\nthe diabetes risk stratification problem as looking to see\nwhat happens years 1 to 3 after the time of prediction. That was with a gap of one year. And there a couple\nof reasons why that's perhaps not what\nyou really wanted to do. First, you have less data\nto use during training. Because you've suddenly\nexcluded patients for whom-- or to differently--\nif you have patients for whom they were censored\nduring that time window, you're throwing them out. So you have fewer\ndata points there. That was part of our\ninclusion/exclusion criteria. Also, when you go to\ndeploy these models,", "id": "wqI_z1yumzY_70", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  is going to develop type 2\ndiabetes between one and three years from now. But in fact what happened is\nthey develop type 2 diabetes 3.1 years from now. So your model would\ncount this as a negative. Or it would be a false positive. The prediction would\nbe a false positive. But in reality, your model\nwasn't actually that bad. We did pretty well. We didn't quite get\nthe right range, but they did get\ndiagnosed diabetes right outside that time window. And so your measures\nof performance are going to be pessimistic. You might be doing\nbetter than you thought. Now, you can try to\naddress these two challenges in many ways. You can imagine a multi-task\nlearning framework where you try to predict\nwhat's going to happen one to two years from\nnow, what's going to happen two to three years\nfrom now, three to four, and so on. Each of those are different\nbinary classification models. You might try to tie\ntogether the parameters of those models via a\nmulti-task learning formulation. And that will get you closer\nto what you care about. But what I'll tell you about\nin the last five minutes is a much more elegant approach\nto trying to deal with that.", "id": "wqI_z1yumzY_71", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So that leads to\nmy second point-- why not just treat this\nas a regression problem? Predict time to event. You have some continuous\nvalued outcome, the time until\ndiagnosis diabetes. Just try to minimize\nmean squared-- minimize your\nsquared error trying to predict that\ncontinuous value. Well, the first\nchallenge to think about is, remember where that\nmean squared error loss function came from. It came from thinking\nabout your data as coming from a\nGaussian distribution. And if you do maximum likelihood\nestimation of this Gaussian distribution, it\nturns out to look like minimizing a squared loss. So it's making a lot of\nassumptions about the outcome. For one, it's making\nthe assumption that outcome could be\nnegative or positive. A Gaussian distribution\ndoesn't have to be positive. But here we know that T\nis always non-negative. In addition, there\nmight be long tails. We might not know exactly\nwhen the patient's going to develop\ndiabetes, but we know it's not going to be now.", "id": "wqI_z1yumzY_72", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And that may also look\nvery non-Gaussian. So typical regression approaches\naren't quite what you want. But there's another\nreally important problem, which is that if you naively\nremove those censored points-- like, what do you do for the\nindividuals where you never observe the time-- where the never get diabetes,\nbecause they were censored? Well, if you just remove those\nfrom your learning algorithm, then you're biasing\nyour results. So for example, if you\nthink about the average age of diabetes onset, if you only\nlook at people who actually were observed to\nget diabetes, it's going to be much closer to now. Because obviously the\npeople who were censored are people who got it much\nlater from the censoring time. So that's another\nserious problem. So the way they we're\ntrying to formalize this mathematically is as follows. Now we should think about\nhaving data which has, again, features x, outcome-- what we usually call Y for\nthe outcome in regression, but here I'll call\nit capital T, because of the time to the event. And now we have an\nadditional variable-- so it's no longer a\ntwo-point, now it's a triple--", "id": "wqI_z1yumzY_73", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And b is going to be a binary\nvariable, which is saying, was this individual\ncensored-- was the time, t, denoting a censoring\nevent, or was it denoting the actual event happening? So it's distinguishing\nbetween the red and the black. So black is b equals 0. Red is b equals 1. OK, so now we can talk\nabout learning a density, P of t, which I'll\nalso call f of t, which is the probability\nof death at time t. And associated with\nany density, of course, is the cumulative\ndensity function, which is the integral from 0\nto any point of the density. Here we'll actually look\nat 1 minus the CDF, what's called the survival function. So it's looking at probability\nof T, actual time of the event, being larger than some\nquantity, little t. And that's, of course,\njust the integral of the density from\nlittle t to infinity. All right, so this is\nthe survival function.", "id": "wqI_z1yumzY_74", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  You want to know,\nis the patient going to be diagnosed with diabetes\ntwo or more years from now. So pictorially, what\nyou're interested in is something like this. You want to estimate these\nconditional distributions. So I call it\nconditional because you want to condition on the\ncovariant to individual x. So what I'm showing\nyou, this black line, is your density, little f of t. And this white area\nhere, the integral from little t to infinity,\nmeaning all this white area, is capital S of t. It's the probability of\nsurviving longer than time little t. OK, so the first thing\nyou might do is say, we get these data,\nthese tuples, and we want to try to estimate\nthat function, little f, the probability of\ndeath at some time. Or, equivalently, you\nmight want to estimate the survival time, capital S\nof t, which is the CDF version. And these two are related to\nanother just by some calculus.", "id": "wqI_z1yumzY_75", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  is a non-parametric method\nfor estimating that survival probability, capital S of t. So this is the probability\nthat an individual lives more than some time period. So first I'll explain\nto you this plot, then I'll tell you how to compute it. So the x-axis of\nthis plot is time. The y-axis is this survival\nproperty, capital S of t. It's the probability\nthat an individual lives more than this amount of time. I think this x-axis is in days,\nso 500, 1,000, 1,500, 2,000. This figure, by the way, was\ncreated by one of my students who's studying a multiple\nmyeloma data set. So you could then ask, well,\nunder what covariants do you want to compute this survival? So here, this method\nI'll tell you about, is very good for when you\ndon't have any features. So all you want\nto do is estimate that density by itself. And of course you\ncould apply a method", "id": "wqI_z1yumzY_76", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So what I'm showing\nyou here is applying it for two different populations. Suppose there's just a\nsingle binary feature. And we're going to apply\nit to the x equals 0 and to x equals 1. That gets you two\ndifferent curves out. But here the estimator is going\nto work independently for each of the two populations. So what you see here\non this red line is for the x equals\n0 population. We see that, at time 0, everyone\nis alive, as you would expect. And at time 1,000,\nroughly 60% individuals are still alive for time 1,000. And that sort of stays constant. Now you see that, for\nthe other subgroup, the x equals 1 subgroup, again,\ntime step 0, as you would expect, everyone is alive. But they survive much longer. At time step 1,000, over\n75% of them are still alive. And of course of interest here\nis also confidence balance. I'm not going to tell\nyou how can you do that, but it's in some of\nthe optional readings. And by the way, there are\nmore optional readings given", "id": "wqI_z1yumzY_77", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And so you see that there is\na statistically significant difference between x\nequals 1 and x equals 0. These people seem\nto be surviving longer than these people. And you get that\nimmediately from this curve. So how do we compute that? Well, we take those\nobserved times, those capital Ts, and here\nI'm going to call them just y. I'm going to sort them. So these are sorted times. And I don't care whether they\nwere censored or not censored. So y is just all of the times\nfor all of the patients, whether they are\ncensored or not. dK I want you think about as 1. It's the number of events\nthat occurred at that time. So if everyone had a unique\ntime of censoring or death, then dK is always 1. K is indexing one\nof these things. n of K is the number\nof individuals alive and uncensored\nby the K-th time point. Then what this estimator\nsays is that S of t--", "id": "wqI_z1yumzY_78", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  is given to you by\nthe product over K such that y of K is\nless than or equal to t. So it's going over the\nobserved times up to little t, of 1 minus the ratio of 1 over-- so I'm thinking about dK as 1-- 1 over the number of people\nwho are alive and uncensored by that time. And that has a very\nintuitive definition. And one can prove that\nthis estimator gives you a consistent estimator\nof the number of people who are alive-- sorry, the number of survival\nprobability at any one point in time for censored data. And that's critical. This works for censored data. So I'm past time today. So I'll finish the last few\nslides on Tuesday's lecture. So that's all for today.", "id": "wqI_z1yumzY_79"}, {"text": "  DAVID SONTAG: So I'll\nbegin today's lecture by giving a brief recap\nof risk stratification. We didn't get to finish talking\nsurvival modeling on Thursday, and so I'll go a little\nbit more into that, and I'll answer some\nof the questions that arose during our discussions\nand on Piazza since. And then the vast majority\nof today's lecture we'll be talking about a new topic-- in particular, physiological\ntime series modeling. I'll give two examples of\nphysiological time series modeling-- the first one\ncoming from monitoring patients in intensive care units,\nand the second one asking a very different\ntype of question-- that of diagnosing patients'\nheart conditions using EKGs. And both of these\ncorrespond to readings that you had for\ntoday's lecture, and we'll go into much\nmore depth in these-- of those papers today, and\nI'll provide much more color", "id": "lLhfDSOwWtU_0", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So just to briefly remind you\nwhere we were on Thursday, we talked about how one could\nformalize risk stratification instead of as a classification\nproblem of what would happen, let's say, in some\npredefined time period, rather thinking about\nrisk stratification as a regression question,\nor regression task. Given what you know about\na patient at time zero, predicting time to event-- so for example, here the\nevent might be death, divorce, college graduation. And patient one-- that event\nhappened at time step nine. Patient two, that event\nhappened at time step 12. And for patient four, we don't\nknow when that event happened, because it was censored. In particular, after\ntime step seven, we no longer get to view\nany of the patients' data, and so we don't know when\nthat red dot would be-- sometime in the future or never. So this is what we mean by\nright censor data, which is precisely what survival\nmodeling is aiming to solve.", "id": "lLhfDSOwWtU_1", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  AUDIENCE: You flipped the x on-- DAVID SONTAG: Yeah,\nI realized that. I flipped the x and the o\nin today's presentation, but that's not relevant. So f of t is the\nprobability of death, or the event occurring\nat time step t. And although in this\nslide I'm showing it as an unconditional\nmodel, in general, you should think about this\nas a conditional density. So you might be conditioning\non some covariates or features that you have for that\npatient at baseline. And very important\nfor survival modeling and for the next\nthings I'll tell you are the survival function,\nto note it as capital S of t. And that's simply 1 minus the\ncumulative density function. So it's the probability\nthat the event occurring, which is time-- which is denoted\nhere as capital T, occurs greater\nthan some little t. So it's this function,\nwhich is simply given to you by\nthe integral from 0 to infinity of the density.", "id": "lLhfDSOwWtU_2", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  On the x-axis is time. The y-axis is the\ndensity function. And this black curve is\nwhat I'm denoting as f of t. And this white area is capital s\nof c, the survival probability, or survival function. Yes? AUDIENCE: So I just\nwant to be clear. So if you were to\nintegrate the entire curve, [INAUDIBLE] by infinity you're\ngoing to be [INAUDIBLE].. DAVID SONTAG: In the\nway that I described it to here, yes, because we're\ntalking about the time to event. But often we might be in\nscenarios where the event may never occur, and so that-- you can formalize that in\na couple of different ways. You could put that at point\nmass at s of infinity, or you could simply say that\nthe integral from 0 to infinity is some quantity less than 1. And in the readings\nthat I'm referencing in the very bottom of\nthose slides-- it shows you", "id": "lLhfDSOwWtU_3", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  I'm telling you about here\nto deal with that scenario where the event may never occur. But for the purposes\nof my presentation, you can assume that\nthe event will always occur at some point. It's a very minor modification\nwhere you, in essence, divide the densities by a constant,\nwhich accounts for the fact that it wouldn't integrate\nto one otherwise. Now, a key question\nthat has to be solved when trying to use a parametric\napproach to survivor modeling is, what should that\nf of t look like? What should that density\nfunction look like? And what I'm showing you here\nis a table of some very commonly used density functions. What you see in\nthese two columns-- on the right hand column is the\ndensity function f of t itself. Lambda denotes some\nparameter of the model. t is the time. And on this second middle\ncolumn is the survival function. So this is obtained for these\nparticular parametric forms", "id": "lLhfDSOwWtU_4", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to infinity. This is the analytic\nsolution for that. And so these go by common\nnames of exponential, weeble, log-normal, and so on. And critically, all of\nthese have support only on the positive real numbers,\nbecause the event can ever occur at negative time. Now, we live in a\nday and age where we no longer have to make\nstandard parametric assumptions for densities. We could, for example, try\nto formalize the density as some output of some\ndeep neural network. But if we don't use a\nparametric approach, so there are two ways\nto try to do that. One way to do that would\nbe to say that we're going to model the post-- the distribution, f of t, as one\nof these things, where lambda or whatever the\nparameters of distribution are given to by the\noutput of, let's", "id": "lLhfDSOwWtU_5", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So that would be one approach. A very different\napproach would be a non-parametric distribution\nwhere you say, OK, I'm going to define f of\nt extremely flexibly, not as one of these forms. And there one runs into a\nslightly different challenge, because as I'll show\nyou in the next slide, to do maximum\nlikelihood estimation of these distributions\nfrom censor data, one needs to get-- one needs\nto make use of this survival function, s of t. And so if you're\nf if t is complex, and you don't have a nice\nanalytic solution for s of t, then you're going to\nhave to somehow use a numerical approximation\nof s of t during limiting. So it's definitely\npossible, but it's going to be a little bit more effort. So now here's where I'm going\nto get into maximum likelihood estimation of these\ndistributions, and to define for you\nthe likelihood function, I'm going to break it down\ninto two different settings. The first setting\nis an observation which is uncensored, meaning\nwe do observe when the event--", "id": "lLhfDSOwWtU_6", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And in that case, the\nprobability of the event-- it's very simple. It's just probability of the\nevent occurring at capital-- at capital T, random\nvariable T, equals a little t-- is just f or t. Done. However, what happens\nif, for this data point, you don't observe when the event\noccurred because of censoring? Well, of course, you could just\nthrow away that data point, not use it in your\nestimation, but that's precisely what we mentioned\nat the very beginning of last week's lecture-- was\nthe goal of survival modeling to not do that,\nbecause if we did that, it would introduce bias\ninto our estimation procedure. So we would like to be able\nto use that observation that this data\npoint was censored, but the only information we\ncan get from that observation is that capital\nT, the event time, must have occurred\nsome time larger", "id": "lLhfDSOwWtU_7", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  is little t here. So we don't know precisely\nwhen capital T was, but we know it's something larger than\nthe observed centering time little of t. And that, remember, is precisely\nwhat the survival function is capturing. So for a censored\nobservation, we're going to use capital S of\nt within the likelihood. So now we can then combine these\ntwo for censored and uncensored data, and what we get is the\nfollowing likelihood objective. This is-- I'm showing you here\nthe log likelihood objective. Recall from last week that\nlittle b of i simply denotes is this observation\ncensored or not? So if bi is 1, it means\nthe time that you're given is the time of the\ncensoring event. And if bi is 0, it means\nthe time you're given is the time that\nthe event occurs. So here what we're going to do\nis now sum over all of the data points in your data\nset from little i equals 1 to little n of bi\ntimes log of probability", "id": "lLhfDSOwWtU_8", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of probability under\nthe uncensored model. And so this bi is just going\nto switch on which of these two you're going to use for\nthat given data point. So the learning objective for\nmaximum likelihood estimation here is very similar\nto what you're used to in learning distributions\nwith the big difference that, for censored\ndata, we're going to use the survival function\nto estimate its probability. Are there any questions? And this, of course,\ncould then be optimized via your\nfavorite algorithm, whether it be stochastic\ngradient descent, or second order\nmethod, and so on. Yep? AUDIENCE: I have a\nquestion about the a kind of side project. You mentioned that we\ncould use [INAUDIBLE].. DAVID SONTAG: Yes. AUDIENCE: And then combine it\nwith the parametric approach. DAVID SONTAG: Yes. AUDIENCE: So is that\ntrue that we just still have the parametric\nassumption that we kind of map the input to the parameters? DAVID SONTAG: Exactly. That's exactly right. So consider the following\npicture where for--", "id": "lLhfDSOwWtU_9", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And this is f of t. You can imagine\nfor any one patient you might have a\ndifferent function. You might-- but they might all\nbe of the same parametric form. So they might be\nlike that, or maybe they're shifted a little bit. So you think about\neach of these three things as being from the\nsame parametric family of distributions, but\nwith different means. And in this case,\nthen the mean is given to as the output of\nthe deep neural network. And so that would be the\nway it would be used, and then one could just back\npropagate in the usual way to do learning. Yep? AUDIENCE: Can you\nrepeat what b sub i is? DAVID SONTAG: Excuse me? AUDIENCE: Could you\nrepeat what b sub i is? DAVID SONTAG: b sub i is just an\nindicator whether the i-th data point was censored\nor not censored. Yes? AUDIENCE: So [INAUDIBLE] equal\nit's more a probability density function [INAUDIBLE].", "id": "lLhfDSOwWtU_10", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  AUDIENCE: Yeah, but\n[INAUDIBLE] probability. No, for the [INAUDIBLE] it's\nprobability density function. DAVID SONTAG Yes, so just to-- AUDIENCE: [INAUDIBLE] DAVID SONTAG: Excuse me? AUDIENCE: Will\nthat be any problem to combine those\ntwo types there? DAVID SONTAG: That's\na very good question. So the observation was that\nyou have two different types of probabilities used here. In this case, we're\nusing something like the cumulative\ndensity, whereas here we're using the probability\ndensity function. The question was, are these\ntwo on different scales? Does it make sense\nto combine them in this type of linear fashion\nwith the same weighting? And I think it does make sense. So think about a setting where\nyou have a very small time range.", "id": "lLhfDSOwWtU_11", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  It's something in\nthis time range. In the setting of\nthe censored data, where that time range could\npotentially be very large, your model is providing-- your log probability\nis somehow going to be much more flat,\nbecause you're covering much more probability mass. And so that\nobservation, I think, intuitively is likely\nto have a much-- a bit of a smaller effect on\nthe overall learning algorithm. These observations-- you know\nprecisely where they are, and so as you deviate from that,\nyou incur the corresponding log loss penalty. But I do think\nthat it makes sense to have them in the same scale. If anyone in the room has done\nwork with [INAUDIBLE] modeling and has a different answer\nto that, I'd love to hear it.", "id": "lLhfDSOwWtU_12", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  will answer this\nquestion differently. I'm going to move on for now. So the remaining question that\nI want to talk about today is how one evaluates\nsurvival models. So we talked about binary\nclassification a lot in the context of\nrisk stratification in the beginning, and we talked\nabout how area under the ROC curve is one measure of\nclassification performance, but here we're doing more-- something more akin to\nregression, not classification. A standard measure that's\nused to measure performance is known as the C-statistic,\nor concordance index. Those are one in the same-- and is defined as follows. And it has a very\nintuitive definition. It sums over pairs\nof data points that can be compared\nto one another, and it says, OK, what\nis the likelihood", "id": "lLhfDSOwWtU_13", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  occurs before an event-- another event. And what you want is that the\nlikelihood of the event that, on average, in essence,\nshould occur later should be larger than the event\nthat should occur earlier. I'm going to first illustrate\nit with this picture, and then I'll work\nthrough the math. So here's the picture, and\nthen we'll talk about the math. So what I'm showing you here\nare every single observation in your data set,\nand they're sorted by either the censoring\ntime or the event time. So by black, I'm illustrating\nuncensored data points. And by red, I'm denoting\ncensored data points. Now, here we see that\nthis data point-- the event happened before this\ndata point's censoring event. Now, since this data\npoint was censored,", "id": "lLhfDSOwWtU_14", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  sometime into the far future. So what we would want\nis that the model gives that the probability that\nthis event happens by this time should be larger\nthan the probability that this event\nhappens by this time, because this actually\noccurred first. And these two are comparable\ntogether-- to each other. On the other hand, it\nwouldn't make sense to compare y2 and y4,\nbecause both of these were censored data\npoints, and we don't know precisely when they occurred. So for example, it could\nhave very well happened that the event 2\nhappened after event 4. So what I'm showing you here\nwith each of these lines are the pairwise\ncomparisons that are actually possible to make. You can make pairwise\ncomparisons, of course, between any pair of events\nthat actually did occur,", "id": "lLhfDSOwWtU_15", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  between censored events and\nevents that occurred before it. Now, if you now look at this\nformula, the formula in this indicate-- this is looking\nat an indicator of survival functions between pairs of data\npoints, and which pairs of data points? It was precisely those\npairs of data points, which I'm showing comparisons\nof with these blue lines here. So we're going to sum over i\nsuch that bi is equal to 0, and remember that means it\nis an uncensored data point. And then we look at-- we look at yi compared to\nall other yj that's great-- that has a value greater than--\nboth censored and uncensored. Now, if your data had no\nsensor data points in it, then you can verify that,\nin fact, this corresponds-- so there's one other\nassumption one has to make, which is that-- suppose that your\noutcome is binary.", "id": "lLhfDSOwWtU_16", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  from this, imagine that\nyour density function looked a little bit like this,\nwhere it could occur either at time 1 or time 2. So something like that. So if the event can\noccur at only two times, not a whole range\nof times, then this is analogous to\na binary outcome. And so if you have\na binary outcome like this and no censoring,\nthen, in fact, that C-statistic is exactly equal to the\narea under the ROC curve. So that just connects it a\nlittle bit back to things we're used to. Yep? AUDIENCE: Just to make\nsure that I understand. So y1 is going to be\nwe observed an event, and y2 is going to be we\nknow that no event occurred until that day? DAVID SONTAG: Every dot\ncorresponds to one event, either censored or not. AUDIENCE: Thank you.", "id": "lLhfDSOwWtU_17", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  In this figure, they're\nsorted by the time of either the censoring\nor the event occurring. So I talked to-- when I talked about\nC-statistic, it-- that's one way to measure\nperformance of your survival modeling, but you\nmight remember that I-- that when we talked about\nbinary classification, we said how area\nunder there ROC curve in itself is very\nlimiting, and so we should think through\nother performance metrics of relevance. So here are a few other\nthings that you could do. One thing you could\ndo is you could use the mean squared error. So again, thinking about\nthis as a regression problem. But of course, that\nonly makes sense for uncensored data points. So focus just in the\nuncensored data points, look to see how well\nwe're doing at predicting when the event occurs. The second thing one could\ndo, since you have the ability to define the likelihood\nof an observation, censored or not censored,\none could hold out data,", "id": "lLhfDSOwWtU_18", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of that held-out data. And the third thing\nyou could do is you can-- after learning\nusing this survival modeling framework, one could then turn\nit into a binary classification problem by, for\nexample, artificially choosing time ranges, like\ngreater than three months is 1. Less than three months is 0. That would be one\ncrude definition. And then once you've\ndone a reduction to a binary\nclassification problem, you could use all of\nthe existing performance metrics they're used to thinking\nabout for binary classification to evaluate the\nperformance there-- things like positive\npredictive value, for example. And you could, of course,\nchoose different reductions and get different\nperformance statistics out. So this is just a\nsmall subset of ways to try to evaluate\nsurvivor modeling, but it's a very,\nvery rich literature. And again, on the\nbottom of these slides, I pointed you to\nseveral references that you could go\nto to learn more. The final comment\nI wanted to make is that I only told\nyou about one estimator", "id": "lLhfDSOwWtU_19", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  estimator. But there is a whole\nother estimation approach for survival modelings, which\nis very important to know about, that are called partial\nlikelihood estimators. And for those of you who have\nheard of Cox proportional hazards models-- and I\nknow they were discussed in Friday's recitation-- that's an example\nof a class of model that's commonly used within this\npartial likelihood estimator. Now, at a very intuitive level,\nwhat this partial likelihood estimator is doing is it's\nworking with something like the C-statistic. So notice how the C-statistic\nonly looks at relative orderings of events-- of their event occurrences. It doesn't care about exactly\nwhen the event occurred or not. In some sense,\nthere's a constant. There's-- in this\nsurvival function, which could be divided out from\nboth sides of this inequality,", "id": "lLhfDSOwWtU_20", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And so one could think\nabout other ways of learning these models by\nsaying, well, we want to learn a survival\nfunction such that it gets the ordering\ncorrect between data points. Now, such a survival function\nwouldn't do a very good job. There's no reason it would\ndo any good at getting the precise time of\nwhen an event occurs, but if your goal were\nto just figure out what is the sorted order\nof patients by risk so that you're going to do an\nintervention on the 10 most risky people, then getting that\norder incorrect is going to be enough, and that's\nprecisely the intuition used behind these partial\nlikelihood estimators-- so they focus on something\nwhich is a little bit less than the original\ngoal, but in doing so, they can have much better\nstatistical complexity, meaning the amount of\ndata they need in order to fit this models well. And again, this is\na very rich topic. All I wanted to do is\ngive you a pointer to it so that you can go read more\nabout it if this is something of interest to you.", "id": "lLhfDSOwWtU_21", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of the most important points\nthat we discussed last week was about non-stationarity. And there was a question\nposted to Piazza, which was really interesting,\nwhich is how do you actually deal with non-stationarity. And I spoke a lot\nabout it existing, and I talked about\nhow to test for it, but I didn't say what\nto do if you have it. So I thought this was such\nan interesting question that I would also talk about\nit a bit during lecture. So the short answer is, if\nyou have to have a solution that you deploy\ntomorrow, then here's the hack that sometimes works. You take your most recent data,\nlike the last three months' data, and you hope\nthat there's not much non-stationarity\nwithin last three months. You throw out all\nthe historical data, and you just train using\nthe most recent data. So a bit unsatisfying,\nbecause you might have now extremely\nlittle data left to learn with, but if you have enough volume,\nit might be good enough.", "id": "lLhfDSOwWtU_22", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  from a research perspective\nis how could you optimally use that historical data. So here are three\ndifferent ways. So one way has to\ndo with imputation. Imagine that the way in which\nyour data was non-stationary was because there\nwere, let's say, parts of time when certain\nfeatures were just unavailable. I gave you this example last\nweek of laboratory test results across time, and I showed\nyou how there are sometimes these really big\nblocks of time where no lab tests are available,\nor very few are available. Well, luckily we live in a world\nwith high dimensional data, and what that means is there's\noften a lot of redundancy in the data. So what you could imagine\ndoing is imputing features that you observed\nto be missing, such that the missingness\nproperties, in fact, aren't changing as much\nacross time after imputation. And if you do that as\na pre-processing step, it may allow you\nto make use of much more of the historical data.", "id": "lLhfDSOwWtU_23", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  has to do with\ntransforming the data. Instead of imputing\nit, transforming it into another representation\naltogether, such that that presentation is\ninvariant across time. And here I'm giving\nyou a reference to this paper by Ganin et al\nfrom the Journal of Machine Learning Research 2016,\nwhich talks about how to do domain and variant\nlearning of neural networks, and that's one\napproach to do so. And I view those two as being\nvery similar-- imputation and transformations. A second approach is\nto re-weight the data to look like the current data. So imagine that you\ngo back in time, and you say, you know what? I ICD-10 codes, for\nsome very weird reason-- this is not true, by the way-- ICD-10 codes in\nthis untrue world happen to be used between\nMarch and April of 2003. And then they weren't\nused again until 2015. So instead of throwing away\nall of the previous data, we're going to\nrecognize that those--", "id": "lLhfDSOwWtU_24", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  was actually drawn from a very\nsimilar distribution as what we're going to be\ntesting on today. So we're going to weight those\ndata points up very much, and down weight the\ndata points that are less like the ones from today. That's the intuition behind\nthese re-weighting approaches, and we're going to talk\nmuch more about that in the context of\ncausal inference, not because these two have\nto do with each other, but they have-- they end up\nusing a very similar technique for how to deal with datas\nthat shift, or covariate shift. And the final technique\nthat I'll mention is based on online\nlearning algorithms. So the idea there is that there\nmight be cut points, change points across time. So maybe the data looks one\nway up until this change point, and then suddenly\nthe data looks really different until\nthis change point, and then suddenly\nthe data looks very different on into the future. So here I'm showing you there\nare two change points in which", "id": "lLhfDSOwWtU_25", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  What these online learning\nalgorithms do is they say, OK, suppose we were\nforced to make predictions throughout this\ntime period using only the historical\ndata to make predictions at each point in time. Well, if we could\nsomehow recognize that there might\nbe these shifts, we could design\nalgorithms that are going to be robust to those shifts. And then one could try to\nanalyze-- mathematically analyze those algorithms\nbased on the amount of regret they would have to, for example,\nan algorithm that knew exactly when those changes were. And of course, we don't\nknow precisely when those changes were. And so there's a whole field of\nalgorithms trying to do that, and here I'm just give me one\ncitation for a recent work. So to conclude risk\nstratification-- this is the last slide here. Maybe ask your\nquestion after class. We've talked about\ntwo approaches for formalizing risk\nstratification-- first as binary classification.", "id": "lLhfDSOwWtU_26", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And in the regression\nframework, one has to think about\ncensoring, which is why we call it survival modeling. Second, in our examples,\nand again in your homework assignment that's\ncoming up next week, we'll see that\noften the variables, the features that are most\npredictive make a lot of sense. In the diabetes case, we said-- we saw how patients having\ncomorbidities of diabetes, like hypertension, or\npatients being obese were very predictive of\npatients getting diabetes. So you might ask yourself, is\nthere something causal there? Are those features that are very\npredictive in fact causing-- what's causing the patient\nto develop type 2 diabetes? Like, for example,\nobesity causing diabetes. And this is where I\nwant to caution you. You shouldn't interpret these\nvery predictive features", "id": "lLhfDSOwWtU_27", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  not when one starts to work\nwith high dimensional data, as we do in this course. The reason for that\nis very subtle, and we'll talk about that in\nthe causal inference lectures, but I just wanted to\ngive you a pointer now that you shouldn't\nthink about it in that way. And you'll understand\nwhy in just a few weeks. And finally we talked about ways\nof dealing with missing data. I gave you one\nfeature representation for the diabetes case,\nwhich was designed to deal with missing data. It said, was there any\ndiagnosis code 250.01 in the last three months? And if there was, you have a 1. If you don't, 0. So it's designed to\nrecognize that you don't have information, perhaps,\nfor some large chunk of time in that window. But that missing data\ncould also be dangerous", "id": "lLhfDSOwWtU_28", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  which is then going to result in\nyour test distribution looking different from your\ntraining distribution. And that's where approaches\nthat are based on imputation could actually be very valuable,\nnot because they improve your predictive accuracy\nwhen everything goes right, but because they might improve\nyour predictive accuracy when things go wrong. And so one of your readings\nfor last week's lecture was actually an example of\nthat, where they used a Gaussian process model to impute much of\nthe missing data in a patient's continuous vital\nsigns, and then they used a recurrent neural\nnetwork to predict based on that imputed data. So in that case, there are\nreally two things going on. First is this robustness\nto data set shift, but there's a\nsecond thing, which is going on as well,\nwhich has to do with a trade-off between\nthe amount of data you have and the complexity of\nthe prediction problem. By doing imputations,\nsometimes you", "id": "lLhfDSOwWtU_29", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and simpler algorithms might\nsucceed where otherwise they would fail because not\nhaving enough data. And that's something\nthat you saw in that last week's reading. So I'm done with\nrisk stratification. I'll take a one minute breather\nfor everyone in the room, and then we'll start\nwith the main topic of this lecture, which is\nphysiological time-series modeling. Let's say started. So here's a baby that's\nnot doing very well. This baby is in the\nintensive care unit. Maybe it was a premature infant. Maybe it's a baby who\nhas some chronic disease, and, of course, parents\nare very worried. This baby is getting\nvery close monitoring.", "id": "lLhfDSOwWtU_30", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  In number one here, it's\nillustrating a three probe-- three lead ECG, which we'll be\ntalking about much more, which is measuring its heart, how\nthe baby's heart is doing. Over here, this number\nthree is something attached to the baby's foot,\nwhich is measuring its-- it's a pulse oximeter, which\nis measuring the baby's oxygen saturation, the amount\nof oxygen in the blood. Number four is a probe which\nis measuring the baby's temperature and so on. And so we're really taking\nreally close measurements of this baby, because\nwe want to understand how is this baby doing. We recognize that there might\nbe really sudden changes in the baby's state\nof health that we want to be able to recognize\nas early as possible. And so behind the scenes,\nnext to this baby, you'll, of course, have a\nhuge number of monitors, each of the monitors showing\nthe readouts from each", "id": "lLhfDSOwWtU_31", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And this type of data is really\nprevalent in intensive care units, but you'll also\nsee in today's lecture how some aspects of\nthis data are now starting to make its way\nto the home, as well. So for example, EKGs are now\navailable on Apple and Samsung watches to help understand-- help to help with\ndiagnosis of arrhythmias, even for people at home. And so from this\ntype of data, there are a number of really important\nuse cases to think about. The first one is to\nrecognize that often we're getting really noisy\ndata, and we want to try to infer the true signal. So imagine, for example,\nthe temperature probe. The baby's true\ntemperature might be 98.5, but for whatever reason-- we'll\nsee a few reasons here today-- maybe you're getting\nan observation of 93. And you didn't know. Is that actually the\ntrue baby temperature? In which case we-- it would be in a lot of trouble. Or is that an anomalous reading?", "id": "lLhfDSOwWtU_32", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  things. And in other cases, we are\ninterested in not necessarily fully understanding what's going\non with the baby along each of those axes, but we\njust want to use that data for predictive purposes,\nfor risk stratification, for example. And so the type of\nmachine learning approach that we'll take here will depend\non the following three factors. First, do we have\nlabel data available? For example, do we\nknow the ground truth of what the baby's\ntrue temperature was, at least for a few of the\nbabies in the training set? Second. Do we have a good mechanistic\nor statistical model of how this data might\nevolve across time? We know a lot about\nhearts, for example. Cardiology is one of\nthose fields of medicine where it's really well studied. There are good\nsimulators of hearts, and how they beat\nacross time, and how that affects the electrical\nstimulation across the body.", "id": "lLhfDSOwWtU_33", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  or statistical\nmodels, that can often allow one to trade off not\nhaving much label data, or just not having\nmuch data period. And it's really\nthese three points which I want to illustrate\nthe extremes of in today's lecture-- what do you do when you\ndon't have much data, and what you do\nwhen-- what you can do when you have a ton of data. And I think it's going to\nbe really informative for us as we go out into the world\nand will have to tackle each of those two settings. So here's an example of two\ndifferent babies with very different trajectories. One in the x-axis here\nis time in seconds. The y-axis here-- I think seconds, maybe minutes. The y-axis here is beats per\nminute of the baby's heart rate, and you see in\nsome cases it's really fluctuating a lot up and down. In some cases, it's sort of\ngoing in a similar-- in one direction, and in all cases,\nthe short term observations are very different from the\nlong range trajectories.", "id": "lLhfDSOwWtU_34", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to think about is one\nof trying to understand, how do we deconvolve between the\ntruth of what's going on with, for example, the\npatient's blood pressure or oxygen versus interventions\nthat are happening to them? So on the bottom\nhere, I'm showing examples of interventions. Here in this oxygen\nuptake, we notice how between roughly 1,000\nand 2,000 seconds suddenly there's no signal whatsoever. And that's an example of\nwhat's called dropout. Over here, we see a\ndifferent type of-- the effect of a\ndifferent intervention, which is due to a\nprobe recalibration. Now, at that time,\nthere was a drop out followed by a sudden\nchange in the values, and that's really happening\ndue to a recalibration step. And in both of\nthese cases, what's going on with the individual\nmight be relatively constant across time,\nbut what's being observed", "id": "lLhfDSOwWtU_35", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So we want to ask\nthe question, can we identify those\nartifactual processes? Can we identify that these\ninterventions were happening at those points in time? And then, if we\ncould identify them, then we could potentially\nsubtract their effect out. So we could impute the\ndata, which we know-- now know to be missing, and then\nhave this much higher quality signal used for some\ndownstream predictive purpose, for example. And the second reason why\nthis can be really important is to tackle this problem\ncalled alarm fatigue. Alarm fatigue is one of the\nmost important challenges facing medicine today. As we get better and better\nin doing risk stratification, as we come up with more and\nmore diagnostic tools and tests, that means these red flags\nare being raised more and more", "id": "lLhfDSOwWtU_36", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And each one of these has some\nassociated false positive rate for it. And so the more tests you have-- suppose the false\npositive rate is kept constant-- the more tests\nyou have, the more likely it is that the union\nof all of those is going to be some error. And so when you're in\nan intensive care unit, there are alarms going\noff all the time. And something that happens\nis that nurses end up starting to ignore those\nalarms, because so often those alarms are\nfalse positives, are due to, for\nexample, artifacts like what I'm showing you here. And so if we had techniques,\nsuch as the ones we'll talk about right now,\nwhich could recognize when, for example, the sudden drop\nin a patient's heart rate is due to an artifact and not\ndue to the patient's true heart rate dropping-- if we had enough\nconfidence in that-- in distinguishing\nthose two things, then we might not decide\nto raise that red flag.", "id": "lLhfDSOwWtU_37", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and that then might reduce\nthe amount of alarm fatigue. And that could have a very\nbig impact on health care. So the technique which\nwe'll talk about today goes by the name of switching\nlinear dynamical systems. Who here has seen\na picture like this on-- this picture on\nthe bottom before. About half of the room. So for the other\nhalf of the room, I'm going to give\na bit of a recap into probabilistic modeling. All of you are now familiar\nwith general probabilities. So you're used to thinking\nabout, for example, univariate Gaussian\ndistributions. We talked about how one\ncould model survival, which was an example of\nsuch a distribution, but for today's\nlecture, we're going to be thinking now about\nmultivariate probability", "id": "lLhfDSOwWtU_38", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  In particular, we'll be thinking\nabout how a patient's state-- let's say their true\nblood pressure-- evolves across time. And so now we're interested in\nnot just the random variable at one point in time, but\nthat same random variable at the second point in\ntime, third point in time, fourth point in time, fifth\npoint in time, and so on. So what I'm showing\nyou here is known as a graphical model, also\nknown as a Bayesian network. And it's one way of illustrating\na multivariate probability distribution that has particular\nconditional independence properties. Specifically, in\nthis model, one node corresponds to one\nrandom variable. So this is describing a\njoint distribution on x1 through x6, y1 through y6. So it's this\nmultivariate distribution on 12 random variables.", "id": "lLhfDSOwWtU_39", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  denotes that, at test time, when\nwe use these models, typically these y variables are observed. Whereas our goal is usually\nto infer the x variables. Those are typically unobserved,\nmeaning that our typical task is one of doing posterior\ninference to infer the x's given the y's. Now, associated with\nthis graph, I already told you the nodes correspond\nto random variables. The graph tells us how is this\njoint distribution factorized. In particular, it's\ngoing to be factorized in the following way-- as the product over\nrandom variables of the probability of\nthe i-th random variable. I'm going to use z to just\ndenote a random variable. Think of z as the\nunion of x and y. zi conditioned on the parents-- the values of the parents of zi.", "id": "lLhfDSOwWtU_40", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and in particular for this\ngraphical model, which goes by the name\nof a Markov model, it has a very specific\nfactorization. And we're just going to read\nit off from this definition. So we're going to go in\norder-- first x1, then y1, then x2, then y2,\nand so on, which is going based on\na root to children transversal of this graph. So the first random\nvariable is x1. Second variable is y2, and\nwhat are the parents of y-- sorry, what are\nthe parents of y1. Everyone can say out loud. AUDIENCE: x1. DAVID SONTAG: x1. So y1 in this factorization\nis only going to depend on x1.", "id": "lLhfDSOwWtU_41", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  What are the parents of x2? Everyone say out loud? AUDIENCE: x1. DAVID SONTAG: x1. Then we have y2. What are the parents of y2. Everyone say out loud. AUDIENCE: x2. DAVID SONTAG: x2 and so on. So this joint\ndistribution is going to have a particularly\nsimple form, which is given to by this\nfactorization shown here. And this factorization\ncorresponds one to one with the particular graph in\nthe way that I just told you. And in this way, we can define\na very complex probability distribution by a number of much\nsimpler conditional probability distributions. For example, if each of the\nrandom variables were binary, then to describe\nprobability of y1 given x1, we only need two numbers. For each value of\nx1, either 0 or 1, we give the probability\nof y1 equals 1. And then, of course, probably y1\nequals 0 is just 1 minus that. So we can describe that very\ncomplicated joint distribution", "id": "lLhfDSOwWtU_42", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Now, the reason why I'm\ndrawing it in this way is because we're making some\nreally strong assumptions about the temporal\ndynamics in this problem. In particular, the\nfact that x3 only has an arrow from\nx2 and not from x1 implies that x3 is\nconditionally independent of x1. If you knew x2's value. So in some sense, think\nabout this as cutting. If you're to take\nx2 out of the model and remove all edges\nincident on it, then x1 and x3 are now\nseparated from one another. They're independent. Now, for those of you who\ndo know graphical models, you'll recognize that that type\nof independent statement that I made is only true\nfor Markov models, and the semantics\nfor Bayesian networks are a little bit different. But actually for this\nmodel, it's-- they're one", "id": "lLhfDSOwWtU_43", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So we're going to make\nthe following assumptions for the conditional\ndistributions shown here. First, we're going to suppose\nthat xt is given to you by a Gaussian distribution. Remember xt-- t is\ndenoting a time step. Let's say 3-- it only\ndepends in this picture-- the conditional\ndistribution only depends on the previous\ntime step's value, x2, or xt minus 1. So you'll notice how\nI'm going to say here xt is going to\ndistribute as something, but the only random\nvariables in this something can be xt minus 1, according\nto these assumptions. In particular, we're\ngoing to assume that it's some Gaussian\ndistribution, whose mean is some linear transformation\nof xt minus 1, and which has a fixed\ncovariance matrix q. So at each step of this process,\nthe next random variable", "id": "lLhfDSOwWtU_44", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  where you're moving according\nto some Gaussian distribution. In a very similar\nway, we're going to assume that yt is drawn also\nas a Gaussian distribution, but now depending on xt. So I want you to think\nabout xt as the true state of the patient. It's a vector that's\nsummarizing their blood pressure, their\noxygen saturation, a whole bunch of\nother parameters, or maybe even just one of those. And y1 are the observations\nthat you do observe. So let's say x1 is the\npatient's true blood pressure. y1 is the observed\nblood pressure, what comes from your monitor. So then a reasonable\nassumption would be that, well, if\nall this were equal, if it was a true\nobservation, then y1 should be very close to x1. So you might assume that\nthis covariance matrix is-- the covariance is-- the\nvariance is very, very small.", "id": "lLhfDSOwWtU_45", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And of course, if it's\na noisy observation-- like, for example, if the probe\nwas disconnected from the baby, then y1 should have\nno relationship to x1. And that dependence on the\nactual state of the world I'm denoting here by these\nsuperscripts, s of t. I'm ignoring that right\nnow, and I'll bring that in in the next slide. Similarly, the relationship\nbetween x2 and x1 should be one which captures\nsome of the dynamics that I showed in the previous\nslides, where I showed over here now this is the patient's\ntrue heart rate evolving across time, let's say. Notice how, if you\nlook very locally, it looks like there are some\nvery, very big local dynamics. Whereas if you\nlook more globally, again, there's some smoothness,\nbut there are some-- again,", "id": "lLhfDSOwWtU_46", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And so those-- that\ndrift has to somehow be summarized in this model\nby that A random variable. And I'll get into more detail\nabout that in just a moment. So what I just showed\nyou was an example of a linear dynamical\nsystem, but it was assuming that there were\nnone of these events happening, none of these\nartifacts happening. The actual model\nthat we were going to want to be able\nto use then is going to also\nincorporate the fact that there might be artifacts. And to model that,\nwe need to introduce additional random\nvariables corresponding to whether those artifacts\noccurred or not. And so that's now this model. So I'm going to let these S's-- these are other\nrandom variables, which are denoting\nartifactual events. They are also\nevolving with time. For example, if there's\nartifactual factual event at three seconds, maybe there's\nalso an artifactual event at four seconds. And we like to model the\nrelationship between those.", "id": "lLhfDSOwWtU_47", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And then the way that we\ninterpret the observations that we do get depends\non both the true value of what's going on\nwith the patient and whether there was an\nartifactual event or not. And you'll notice\nthat there's also an edge going from\nthe artifactual events to the true values\nto note the fact that those interventions\nmight actually be affecting the patient. For example, if you\ngive them a medication to change their blood\npressure, then that procedure is going to affect the next time\nstep's value of the patient's blood pressure. So when one wants\nto learn this model, you have to ask yourself,\nwhat types of data do you have available? Unfortunately, it's very hard\nto get data on both the ground truth, what's going\non with the patient,", "id": "lLhfDSOwWtU_48", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Instead, what we actually have\nare just these observations. We get these very noisy blood\npressure draws across time. So what this paper does is\nit uses a maximum likelihood estimation approach,\nwhere it recognizes that we're going to be\nlearning from missing data. We're going to explicitly\nthink of these x's and the s's as latent variables. And we're going to\nmaximize the likelihood of the whole entire model,\nmarginalizing over x and s. So just maximizing the marginal\nlikelihood over the y's. Now, for those of you who have\nstudied unsupervised learning before, you might recognize\nthat as a very hard learning problem. In fact, it's-- that\nlikelihood is non-convex. And one could imagine all sorts\nof a heuristics for learning, such as gradient descent,\nor, as this paper uses, expectation maximization, and\nbecause of that non-convexity, each of these\nalgorithms typically", "id": "lLhfDSOwWtU_49", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So this paper uses EM,\nwhich intuitively iterates between inferring those missing\nvariables-- so imputing the x's and the s's given\nthe current model, and doing posterior inference\nto infer the missing variables given the\nobserved variables, using the current model. And then, once you've\nimputed those variables, attempting to refit the model. So that's called the\nm-step for maximization, which updates the model and\njust iterates between those two things. That's one learning\nalgorithm which is guaranteed to reach a\nlocal maxima of the likelihood under some regularity\nassumptions. And so this paper\nuses that algorithm, but you need to be\nasking yourself, if all you ever\nobserve are the y's, then will this algorithm\never recover anything close to the true model? For example, there\nmight be large amounts of non-identifiability here.", "id": "lLhfDSOwWtU_50", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of the s's, and you'd get a\nsimilar likelihood on the y's. That's where bringing in domain\nknowledge becomes critical. So this is going to be an\nexample where we have no label data or very little label data. And we're going to do\nunsupervised learning of this model, but\nwe're going to use a ton of domain knowledge\nin order to constrain the model as much as possible. So what is that\ndomain knowledge? Well, first we're\ngoing to use the fact that we know that a true heart\nrate evolves in a fashion that can be very well modeled by\nan autoregressive process. So the autoregressive process\nthat's used in this paper is used to model the\nnormal heart rate dynamics. In a moment, I'll tell you how\nto model the abnormal heart", "id": "lLhfDSOwWtU_51", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And intuitively-- I'll\nfirst go over the intuition, then I'll give you the math. Intuitively what it\ndoes is it recognizes that this complicated signal can\nbe decomposed into two pieces. The first piece shown here\nis called a baseline signal, and that, if you\nsquint your eyes and you sort or ignore the\nvery local fluctuations, this is what you get out. And then you can\nlook at the residual of subtracting this signal,\nsubtracting this baseline from the signal. And what you get\nout looks like this. Notice here it's around 0 mean. So it's a 0 mean signal with\nsome random fluctuations, and the fluctuations\nare happening here at a much faster rate than-- and for the original baseline. And so the sum of bt and\nthis residual is a very-- it looks-- is exactly equal\nto the true heart rate.", "id": "lLhfDSOwWtU_52", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  This we can model by\na random walk with-- which goes very\nslowly, and this we can model by a random walk\nwhich goes very quickly. And that is exactly what I'm\nnow going to show over here on the left hand side. bt, this baseline\nsignal, we're going to model as a Gaussian\ndistribution, which is parameterized as a function\nof not just bt minus 1, but also bt minus\n2, and bt minus 3. And so we're going to be\ntaking a weighted average of the previous few time steps,\nwhere we're smoothing out, in essence, the observation--\nthe previous few observations. If you were to-- if you're being a\nkeen observer, you'll notice that this is no\nlonger a Markov model.", "id": "lLhfDSOwWtU_53", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  this then corresponds to a\nsecond order Markov model, because each random variable\ndepends on the previous two time steps of the Markov chain. And so after-- so you would\nmodel now bt by this process, and you would\nprobably be averaging over a large number\nof previous time steps to get this smooth property. And then you'd model xt minus bt\nby this autoregressive process, where you might,\nfor example, just be looking at just the\nprevious couple of time steps. And you recognize\nthat you're just doing much more random fluctuations. And then-- so that's how one\nwould now model normal heart rate dynamics.", "id": "lLhfDSOwWtU_54", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  this is an example of\na statistical model. There is no\nmechanistic knowledge of hearts being\nused here, but we can fit the data of normal\nhearts pretty well using this. But the next question and\nthe most interesting one is, how does one now\nmodel artifactual events? So for that, that's where some\nmechanistic knowledge comes in. So one models that\nthe probe dropouts are given by recognizing\nthat, if a probe is removed from the baby, then\nthere should no longer be-- or at least if you-- after\na small amount of time, there should no longer\nbe any dependence on the true value of the baby. For example, the blood pressure,\nonce the blood pressure probe is removed, is no longer\nrelated to the baby's true blood pressure. But there might be some delay\nto that lack of dependence. And so-- and that is going\nto be encoded in some domain knowledge. So for example, in\nthe temperature probe,", "id": "lLhfDSOwWtU_55", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  it starts heating up again--\nor it starts cooling, so assuming that the ambient\ntemperature is cooler than the baby's temperature. So you take it off the baby. It starts cooling down. How fast does it cool down? Well, you could assume\nthat it cools down with some exponential decay\nfrom the baby's temperature. And this is something\nthat is very reasonable, and you could\nimagine, maybe if you had label data for just\na few of the babies, you could try to fit the\nparameters of the exponential very quickly. And in this way, now, we\nparameterize the conditional distribution of the temperature\nprobe, given both the state and whether the artifact\noccurred or not, using this very simple\nexponential decay. And in this paper, they give\na very similar type of-- they make similar types\nof-- analogous types of assumptions for all of\nthe other artifactual probes. You should think about\nthis as constraining these conditional distributions\nI showed you here. They're no longer allowed to\nbe arbitrary distributions,", "id": "lLhfDSOwWtU_56", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to try to maximize the marginal\nlikelihood of the data, you've now constrained\nit in a way that you hopefully are\nmoved on to identifyability of the learning problem. It makes all of the\ndifference in learning here. So in this paper,\ntheir evaluation did a little bit of fine\ntuning for each baby. In particular, they assumed\nthat the first 30 minutes near the start consists\nof normal dynamics so that's there\nare no artifacts. That's, of course,\na big assumption, but they use that to try to\nfine tune the dynamic model to fine tune it for each\nbaby and for themselves. And then they looked\nat the ability to try to identify\nartifactual processes. Now, I want to go a little\nbit slowly through this plot, because it's quite interesting. So what I'm showing\nyou here is a ROC curve of the ability to\npredict each of the four", "id": "lLhfDSOwWtU_57", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  For example, at any\none point in time, was there a blood sample\nbeing taken or not? At any one point\nin time, was there a core temperature disconnect\nof the core temperature probe? And to evaluate it,\nthey're assuming that they have some label data\nfor evaluation purposes only. And of course, you want to be\nat the very far top left corner up here. And what we're showing here\nare three different curves-- the very faint\ndotted line, which I'm going to trace out with\nmy cursor, is the baseline. Think of that as a\nmuch worse algorithm. Sorry. That's that line over there. Everyone see it? And this approach are\nthe other two lines. Now, what's differentiating\nthose other two lines corresponds to the particular\ntype of approximate inference algorithm that's used.", "id": "lLhfDSOwWtU_58", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  the true value of the x's\ngiven your noisy observations in the model given here is\nactually a very hard inference problem. Mathematically, I\nthink one can show that it's an NP-hard\ncomputational problem. And so they have to\napproximate it in some way, and they use two different\napproximations here. The first approximation\nis based on what they're calling a Gaussian\nsum approximation, and it's a deterministic\napproximation. The second approximation is\nbased on a Monte Carlo method. And what you see here is that\nthe Gaussian sum approximation is actually dramatically better. So for example, in\nthis blood sample one, that the ROC curve looks like\nthis for the Gaussian sum approximation. Whereas for the Monte\nCarlo approximation, it's actually\nsignificantly lower. And this is just to\npoint out that, even in this setting, where\nwe have very little data,", "id": "lLhfDSOwWtU_59", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of how one does the\nmath-- in particular, the proximate\ninference-- can make a really big difference in the\nperformance of this system. And so it's something\nthat one should really think deeply about, as well. I'm going to skip that\nslide, and then just mention very briefly this one. This is showing an\ninference of the events. So here I'm showing you\nthree different observations. And on the bottom here,\nI'm showing the prediction of when artifact-- two different\nartifactual events happened. And these predictions\nwere actually quite good, using this model. So I'm done with that\nfirst example, and-- and the-- just to recap\nthe important points of that example, it was that\nwe had almost no label data.", "id": "lLhfDSOwWtU_60", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  statistical model with some\ndomain knowledge built in, and that can go really far. So now we'll shift gears to\ntalk about a different type of problem involving\nphysiological data, and that's of detecting\natrial fibrillation. So what I'm showing you\nhere is an AliveCore device. I own one of these. So if you want to drop\nby my E25 545 office, you can-- you can\nplay around with it. And if you attach it\nto your mobile phone, it'll show you your electric\nconductance through your heart as measured through\nyour two fingers touching this device\nshown over here. And from that, one can try to\ndetect whether the patient has atrial fibrillation. So what is atrial fibrillation? Good question. It's [INAUDIBLE].", "id": "lLhfDSOwWtU_61", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  They defined atrial fibrillation\nas a quivering or irregular heartbeat, also\nknown as arrhythmia. And one of the big\nchallenges is that it could lead to blood clot,\nstroke, heart failure, and so on. So here is how a\npatient might describe having atrial fibrillation. My heart flip-flops,\nskips beats, feels like it's banging\nagainst my chest wall, particularly when I'm\ncarrying stuff up my stairs or bending down. Now let's try to look\nat a picture of it. So this is a normal heartbeat. Hearts move-- pumping like this. And if you were to\nlook at the signal", "id": "lLhfDSOwWtU_62", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  it would look like this. And it's roughly corresponding\nto the different-- the signal is corresponding\nto different cycles of the heartbeat. Now for a patient who\nhas atrial fibrillation, it looks more like this. So much more obviously abnormal,\nat least in this figure. And if you look at the\ncorresponding signal, it also looks very different. So this is just to give you\nsome intuition about what I mean by atrial fibrillation. So what we're going to try\nto do now is to detect it. So we're going to\ntake data like that and try to classify it into a\nnumber of different categories. Now this is something which\nhas been studied for decades, and last year, 2017,\nthere was a competition run by Professor Roger\nMark, who is here", "id": "lLhfDSOwWtU_63", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  how good are we at\ntrying to figure out which patients have different\ntypes of heart rhythms based on data that\nlooks like this? So this is a normal\nrhythm, which is also called a sinus rhythm. And over here it's atrial-- this is an example one patient\nwho has atrial fibrillation. This is another type of rhythm\nthat's not atrial fibrillation, but is abnormal. And this is a noisy recording--\nfor example, if a patient's-- doesn't really have their\ntwo fingers very well put on to the two leads\nof the device. So given one of these\ncategories, can we predict-- one of these signals,\ncould predict which category it came from? So if you looked at\nthis, you might recognize that they look a bit different. So could some of\nyou guess what might be predictive features\nthat differentiate one of these signals\nfrom the other? In the back?", "id": "lLhfDSOwWtU_64", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of one of the peaks the QRS\ncomplex are [INAUDIBLE].. DAVID SONTAG: So\nspeak in English for people who don't know\nwhat these terms mean. AUDIENCE: There is one\nlarge piece, which can-- probably we can consider one\nmV and there is another peak, which is sort of like-- they have reverse polarity\nbetween normal rhythm and [INAUDIBLE]. DAVID SONTAG: Good. So are you a cardiologist? AUDIENCE: No. DAVID SONTAG: No, OK. So what the student\nsuggested is one could look for sort\nof these inversions to try to describe it a\nlittle bit differently. So here you're suggesting\nthe lack of those inversions is predictive of\nan abnormal rhythm. What about another feature\nthat could be predictive? Yep? AUDIENCE: The spacing\nbetween the peaks is more irregular with the AF. DAVID SONTAG: The\nspacing between beats is more irregular\nwith the AF rhythm. So you're sort of\nlooking at this. You see how here\nthis spacing is very", "id": "lLhfDSOwWtU_65", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Whereas in the normal\nrhythm, sort of the spacing looks pretty darn regular. All right, good. So if I was to show you\n40 examples of these and then ask you to\nclassify some new ones, how well do you think\nyou'll be able to do? Pretty well? I would be surprised if\nyou couldn't do reasonably well at least distinguishing\nbetween normal rhythm and AF rhythm, because there seem to be\nsome pretty clear signals here. Of course, as you get\ninto alternatives, then the story gets\nmuch more complex. But let me dig in\na little bit deeper into what I mean by this. So let's define\nsome of these terms. Well, cardiologists have studied\nthis for a really long time, and they have-- so\nwhat I'm showing you here is one heart cycle. And they've-- you can put names\nto each of the peaks that you would see in a regular heart\ncycle-- so that-- for example, that very high peak is\nknown as the R peak. And you could look at, for\nexample, the interval--", "id": "lLhfDSOwWtU_66", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  You could look at the interval\nbetween the R peak of one beat and the R peak of\nanother peak, and define that to be the RR interval. In a similar way,\none could take-- one could find different\ndistinctive elements of the signal-- by the way, each-- each time step\ncorresponds to the heart being in a different position. For a healthy heart, these\nare relatively deterministic. And so you could look at\nother distances and derive features from those\ndistances, as well, just like we were talking\nabout, both within a beat and across beats. Yep? AUDIENCE: So what's\nthe difference between a segment and\nan interval again? DAVID SONTAG: I don't\nknow what the difference between a segment\nand an interval is. Does anyone else know? I mean, I guess the\ninterval is between probably the heads of peaks, whereas\nsegments might refer to within a interval. That's my guess. Does someone know better?", "id": "lLhfDSOwWtU_67", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that's a good enough\nunderstanding. The point is this\nis well understood. One could derive\nfeatures from this. AUDIENCE: By us. DAVID SONTAG: By us. So what would a traditional\napproach be to this problem? So this is-- I'm pulling this figure\nfrom a paper from 2002. What it'll do is it'll\ntake in that signal. It'll do some filtering of it. Then it'll run a peak\ndetection logic, which will find these\npeaks, and then it'll measure intervals between\nthese peaks and within a beat. And it'll take\nthose computations or make some\ndecision based on it. So that's a\ntraditional algorithm, and they work pretty reasonably. And so what do I mean\nby signal processing? Well, this is an\nexample of that. I encourage any of you to go\nhome today and try to code up", "id": "lLhfDSOwWtU_68", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  It's not that hard, at\nleast not to get an OK one. You might imagine\nkeeping a running tab of what's the highest\nsignal you've seen so far. Then you look to see what\nis the first time it drops, and the second time-- and the\nnext time it goes up larger than, let's say, the previous-- suppose that one of-- you want to look for when the\ndrop is-- the maximum value-- recent maximum\nvalue divided by 2. And then you-- then you reset. And you can imagine in this\nway very quickly coding up a peak finding algorithm. And so this is just,\nagain, to give you some intuition behind what a\ntraditional approach would be. And then you can very\nquickly see that that-- once you start to look at\nsome intervals between peaks, that alone is often good\nenough for predicting whether a patient has\natrial fibrillation. So this is a figure\ntaken from paper in 2001 showing a single\npatient's time series.", "id": "lLhfDSOwWtU_69", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  their heart beats across time. The y-axis is just\nshowing the RR interval between the previous beat\nand the current beat. And down here in the\nbottom is the ground truth of whether the patient\nis assessed to have-- to be in-- to have a normal\nrhythm or atrial fibrillation, which is noted as this\nhigher value here. So these are AF rhythms. This is normal. This is AF again. And what you can see is that\nthe RR interval actually gets you pretty far. You notice how it's\npretty high up here. Suddenly it drops. The RR interval\ndrops for a while, and that's when\nthe patient has AF. Then it goes up again. Then it drops again, and so on. And so it's not deterministic,\nthe relationship, but there's definitely a lot\nof signal just from that. So you might say,\nOK, well, what's", "id": "lLhfDSOwWtU_70", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  a little bit more? So flash backwards from 2001 to\n1970 here at MIT, studied by-- actually, no, this is not MIT. This is somewhere else, sorry. But still 1970-- where they\nused a Markov model very similar to the Markov models\nwe were just talking about in the previous example to model\nwhat a sequence of normal RR intervals looks like versus\nwhat a sequence of abnormal, for example, AF RR\nintervals looks like. And in that way,\none can recognize that, for any one\nobservation of an RR interval might not by itself be\nperfectly predictive, but if you look at sort\nof a sequence of them for a patient with\natrial fibrillation, there is some common\npattern to it. And you can-- one can detect it\nby just looking at likelihood of that sequence under each\nof these two different models, normal and abnormal.", "id": "lLhfDSOwWtU_71", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  approaches for-- for predicting\natrial fibrillation. This is the paper I\nwanted to say from MIT. Now 1991, this is also\nfrom Roger Mark's group. Now this is a neural network\nbased approach, where it says, OK, we're going to take\na bunch of these things. We're going to derive a\nbunch of these intervals, and then we're going to throw\nthat through a black box supervised machine\nlearning algorithm to predict whether a\npatient has AF or not. So these are very-- first of all, there are\nsome simple approaches here that work reasonably well. Using neural networks in this\ndomain is not a new thing, but where are we as a field? So as I mentioned, there was\nthis competition last year, and what I'm showing\nyou here-- the citation is from one of the\nwinning approaches. And this winning approach\nreally brings the two paradigms together. It extracts a large number\nof expert derived features-- so shown here. And these are exactly\nthe types of things", "id": "lLhfDSOwWtU_72", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  interval of regular rhythms,\nmax RR irregularity measure. And there's just a whole\nrange of different things that you can imagine manually\nderiving from the data. And you throw all\nof these features into a machine\nlearning algorithm, maybe a random forest, maybe a\nneural network, doesn't matter. And what you get out is a\nslightly better algorithm than what if you\nhad just come up with a simple rule on your own. That was the winning\nalgorithm then. And in the summary paper, they\nconjectured that, well, maybe it's the case that they were-- they'd expected that\nconvolutional neural networks would win. And they were surprised that\nnone of the winning solutions involved convolution\nneural networks. And they conjectured that may be\nthe reason why is because maybe with these 8,000 patients\nthat they had [INAUDIBLE] that just wasn't enough to give the\nmore complex models advantage. So flip forward now to\nthis year and the article", "id": "lLhfDSOwWtU_73", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  where the Stanford\ngroup now showed how a convolutional neural\nnetwork approach, which is, in many ways, extremely\nnaive-- all it does is it takes the\nsequence data in. It makes no attempt at trying\nto understand the underlying physiology, and just\npredicts from that-- can do really, really well. And so there are\ncouple of differences that I want to emphasize\nto the previous work. First, the censor is different. Whereas the previous work\nused this alive core censor, in this paper from\nStanford, they're using a different censor\ncalled the Zio patch, which is attached to the human\nbody and conceivably much less noisy. So that's one big difference. The second big difference\nis that there's dramatically more data. Instead of 8,000\npatients to train from, now they have over\n90,000 records from 50,000 different\npatients to train from. The third major\ndifference is that now, rather than just trying to\nclassify into four categories--", "id": "lLhfDSOwWtU_74", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  now we're going\nto try to classify into 14 different categories. We're, in essence, breaking\napart that other class into much finer grain\ndetail of different types of abnormal rhythms. And so here are some of\nthose other abnormal rhythms, things like complete\nheart block, and a bunch of other\nnames I can't pronounce. And from each one of these,\nthey gathered a lot of data. And that actually did-- so it's not described\nin the paper, but I've talked to\nthe authors, and they did-- they gathered this data\nin a very interesting way. So they sort of-- they did\ntheir training iteratively. They looked to see\nwhere their errors were, and then they went and gathered\nmore data from patients with that subcategory. So many of these\nother categories are very under-- might\nbe underrepresented in the general population,\nbut they actually gather a lot of\npatients of that type in their data set for\ntraining purposes.", "id": "lLhfDSOwWtU_75", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  making a very big difference. So what is their\nconvolutional network? Well, first of all,\nit's a 1-D signal. So it's a little bit\ndifferent from the con nets you typically see\nin computer vision, and I'll show you an\nillustration of that in the next slide. It's a very deep model. So it's 34 layers. So the input comes in on the\nvery top in this picture. It's passed through\na number of layers. Each layer consists of\nconvolution followed by rectified linear\nunits, and there is sub sampling at every\nother layer so that you go from a very wide signal-- so a very long-- I can't remember how long-- 1 second long signal\nsummarized down into sort of much-- just many\nsmaller number of dimensions, which you then have a sort\nof fully connected layer at the bottom to do\nfor your predictions. And then they also have\nthese shortcut connections, which allow you to pass\ninformation from earlier layers down to the very\nend of the network,", "id": "lLhfDSOwWtU_76", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And for those of you who\nare familiar with residual networks, it's the same idea. So what is a 1D convolution? Well, it looks a\nlittle bit like this. So this is the signal. I'm going to just approximate\nit by a bunch of 1's and 0's. I'll say this is a 1. This is a 0. This is a 1, 1, so on. A convolutional network has\na filter associated with it. That filter is then\napplied in a 1D model. It's applied in\na linear fashion. It's just taken a dot product\nwith the filter's values, with the values of the\nsignal at each point in time. So it looks a little\nbit like this, and this is what you get out. So this is the convolution\nof a single filter with the whole signal. And the computation I did\nthere-- so for example, this first number came\nfrom taking the dot product of the first three numbers-- 1, 0, 1-- with the filter. So it's 1 times 2 plus 3 times\n0 plus 1 times 1, which is 3.", "id": "lLhfDSOwWtU_77", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  was computed in the same way. And I usually have you figure\nout what this last one is, but I'll leave that\nfor you to do at home. And that's what a\n1D convolution is. And so they have-- they do this\nfor lots of different filters. Each of those filters might\nbe of varying lengths, and each of those will\ndetect different types of signal patterns. And in this way, after\nhaving many layers of these, one can, in an\nautomatic fashion, extract many of the same types\nof signals used in that earlier work, but also be much\nmore flexible to detect some new ones, as well. Hold your question,\nbecause I need to wrap up. So in the paper\nthat you read, they talked about how\nthey evaluated this. And so I'm not going to go\ninto much depth in it now. I just want to point out\ntwo different metrics that they used. So the first metric\nthey used was what they called a\nsequential error metric. What that looked at is you\nhad this very long sequence for each patient, and\nthey labeled different one", "id": "lLhfDSOwWtU_78", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  into abnormal,\nnormal, and so on. So you could ask,\nhow good are we at labeling each of\nthe different points along the sequence? And that's the sequence metric. The different-- the second\nmetric is the set metric, and that looks at,\nif the patient has something that's abnormal\nanywhere, did you detect it? So that's, in essence,\ntaking an or of each of those 1\nsecond intervals, and then looking\nacross patients. And from a clinical\ndiagnostic perspective, the set metric might be\nmost useful, but then when you want to\nintrospect and understand where is that happening,\nthen the sequential metric is important. And the key take home message\nfrom the paper is that, if you compared the model's\npredictions-- this is, I think, using an f1 metric-- to what you would get from\na panel of cardiologists, these models are doing as well,\nif not better than these panels of cardiologists. So this is extremely exciting. This is technology--\nor variance of this is technology that you're\ngoing to see deployed now.", "id": "lLhfDSOwWtU_79", "title": "6. Physiological Time-Series", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  these Samsung watches, I don't\nknow exactly what they're using, but I\nwouldn't be surprised if they're using\ntechniques similar to this. And you're going to see much\nmore of that in the future. So this is going to be\nreally the first example in this course so\nfar of something that's really been deployed. And so in summary,\nwe're very often in the realm of not enough data. And in this lecture today,\nwe gave two examples how you can deal with that. First, you can try to use\nmechanistic and statistical models to try to work\nin settings where you don't have much data. And in other extremes,\nyou do have a lot of data, and you can try to\nignore that, and just use these black box approaches.", "id": "lLhfDSOwWtU_80"}, {"text": "  PETER SZOLOVITS: OK. So today and next\nTuesday, we're talking about the role of natural\nlanguage processing in machine learning in health care. And this is going to\nbe a heterogeneous kind of presentation. Mainly today, I'm going to\ntalk about stuff that happened or that takes\nadvantage of methods that are not based on neural\nnetwork representations. And on Tuesday, I'm\ngoing to speak mostly about stuff that does depend on\nneural network representations, but I'm not sure where the\nboundary is going to fall. I've also invited\nDr. Katherine Liao over there, who will\njoin me in a question", "id": "IiD3YZkkCmE_0", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  ago with David. Kat is a rheumatologist in the\nPartners HealthCare system. And you'll actually be\nhearing about some of the work that we've done\ntogether in the past before we go to the interview. So roughly, the outline\nof these two lectures is that I want to talk\na little bit about why we care about clinical text. And then I'm going to talk\nabout some conceptually very appealing, but practically\nnot very feasible methods that involve\nanalyzing these narrative texts as linguistic entities,\nas linguistic objects in the way that a linguist\nmight approach them. And then we're going to talk\nabout what is very often done,", "id": "IiD3YZkkCmE_1", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  well, we may not be able to\nunderstand exactly everything that goes on in the\nnarratives, but we can identify certain words\nand certain phrases that are very highly indicative\nthat the patient has a certain disease,\na certain symptom, that some particular\nthing was done to them. And so this is a lot\nof the bread and butter of how clinical research\nis done nowadays. And then I'll go on to\nsome other techniques. So here's an example. This is a discharge\nsummary from MIMIC. When you played with MIMIC, you\nnotice that it's de-identified. And so names and\nthings are replaced with square brackets, star,\nstar, star kinds of things. And here I have replaced--\nwe replaced those with synthetic names. So Mr. Blind isn't\nreally Mr. Blind,", "id": "IiD3YZkkCmE_2", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  But I wanted something\nthat read like real text. So if you look at something\nlike this, you see that Mr. Blind is a 79-year-old\nwhite white male-- so somebody repeated a word-- with a history of diabetes\nmellitus and inferior MI, who underwent open repair of\nhis increased diverticulum on November 13 at some-- again, that's not the\nname of the actual place-- medical center. And then he developed\nhematemesis, so he was spitting up\nblood, and was intubated for respiratory distress. So he wasn't breathing well. So these are all really\nimportant things about what happened to Mr. Blind. And so we'd like to be able\nto take advantage of this.", "id": "IiD3YZkkCmE_3", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  quantitative version of this,\nKat and I worked on a project back around 2010\nwhere we were looking at trying to understand what\nare the genetic correlates of rheumatoid arthritis. And so we went to the research\npatient data repository of Mass General and the\nBrigham Partners HealthCare, and we said, OK, who\nare the patients who have been billed for a\nrheumatoid arthritis visit? And there are many thousands\nof those people, OK? And then we selected\na random set of I think 400 of those patients. We gave them to\nrheumatologists, and we said, which of these people actually\nhave rheumatoid arthritis?", "id": "IiD3YZkkCmE_4", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So what would you guess is\nthe positive predictive value of having a billing code\nfor rheumatoid arthritis in this data set? I mean, how many people\nthink it's more than 50%? OK, that would be\nnice, but it's not. How many people think\nit's more than 25%? God, you guys are getting\nreally pessimistic. Well, it also isn't. It turned out to be something\nlike 19% in this cohort. Now, before you\nstart calling, you know, the fraud\ninvestigators, you have to ask yourself why is\nit that this data is so lousy, right? And there's a systematic reason,\nbecause those billing codes", "id": "IiD3YZkkCmE_5", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  what's wrong with the patient. They were created\nin order to tell an insurance company\nor Medicare or somebody how much of a payment is\ndeserved by the doctors taking care of them. And so what this means is\nthat, for example, if I clutch my chest and go, uh,\nand an ambulance rushes me over to Mass General and they\ndo a whole bunch of tests and they decide that I'm\nnot having a heart attack, the correct billing\ncode for that visit is myocardial infarction. Because of course\nthe work that they have to do in order to figure\nout that I'm not having a heart attack is the same\nas the work they would have had to do to figure\nout that I was having a heart attack. And so the billing codes-- we've talked about\nthis a little bit before-- but they are a very\nimperfect representation of reality.", "id": "IiD3YZkkCmE_6", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  What if we insisted\nthat you have three billing codes for\nrheumatoid arthritis rather than just one. And that turned out to raise\nthe positive predictive value all the way up to 27%. So we go, really? How could you get\nbilled three times? Right? Well, the answer is that you\nget billed for, you know, every aspirin you\ntake at the hospital. And so for example,\nit's very easy to accumulate three billing\ncodes for the same thing because you go see a\ndoctor, the doctor bills you for a rheumatoid\narthritis visit, he or she sends you\nto a radiologist to take an X-ray of your\nfingers and your joints. That bill is another\nbilling code for RA. The doctor also\nsends you to the lab to have a blood\ndraw so that they", "id": "IiD3YZkkCmE_7", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  That's another billing code\nfor rheumatoid arthritis. And it may be that all\nof this is negative and you don't actually\nhave the disease. So this is something that's\nreally important to think about and to remember when you're\nanalyzing these data. And so we started off\nin this project saying, well, we need to get a\npositive predictive value more on the order of 95%,\nbecause we wanted a very pure sample of people who really\ndid have the disease because we were going to take blood\nsamples from those patients, pay a bunch of money to\nthe Broad to analyze them, and then hopefully\ncome up with a better understanding of\nthe relationship between their genetics\nand their disease. And of course, if you talk to\na biostatistician, as we did,", "id": "IiD3YZkkCmE_8", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of that database, then\nwe're going to get meaningless results from it. So that's the goal here. So what we did is\nto say, well, if you train a data set that tries\nto tell you whether somebody really has rheumatoid\narthritis or not based on just codified data. So codified data are things like\nlab values and prescriptions and demographics and stuff\nthat is in tabular form. Then we were getting a positive\npredictive value of about 88%. We said, well, how\nwell could we do by, instead of looking\nat that codified data, looking at the narrative text in\nnursing notes, doctor's notes,", "id": "IiD3YZkkCmE_9", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Could we do as well or better? And the answer turned out\nthat we were getting about 89% using only the natural language\nprocessing on these notes. And not surprisingly, when\nyou put them together, the joint model\ngave us about 94%. So that was definitely\nan improvement. So this was published in 2010,\nand so this is not the latest hot off the bench results. But to me, it's a\nvery compelling story that says there is real value\nin these clinical narratives. OK, so how did we do this? Well, we took about four\nmillion patients in the EMR.", "id": "IiD3YZkkCmE_10", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that they have at\nleast one ICD-9 code for rheumatoid\narthritis, or that they've had an anti-CCP titer\ndone in the lab. And then we-- oh,\nit was 500, not 400. So we looked at\n500 cases, which we got gold standard readings on. And then we trained\nan algorithm that predicted whether this\npatient really had RA or not. And that predicted about 35-- well, 3,585 cases. We then sampled a validation\nset of 400 of those. We threatened our\nrheumatologists with bodily harm if they\ndidn't read all those cases and give us a gold\nstandard judgment.", "id": "IiD3YZkkCmE_11", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  They were actually\nreally cooperative. And there are some\ndetails here that you can look at in the\nslide, and I had a pointer to the\noriginal paper if you're interested in the details. But we were looking\nat ICD-9 codes for rheumatoid arthritis\nand related diseases. We excluded some\nICD-9 codes that fall under the general\ncategory of rheumatoid diseases because they're not\ncorrect for the sample that we were interested in. We dealt with this\nmultiple coding by ignoring codes that happened\nwithin a week of each other so that we didn't get this\nproblem of multiple bills from the same visit. And then we looked for\nelectronic prescriptions of various sorts.", "id": "IiD3YZkkCmE_12", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and anti-cyclic\ncitrullinated peptide, if I pronounced that correctly. And another thing we found,\nnot only in this study but in a number of others,\nis it's very helpful just to count up\nhow many facts are on the database about\na particular patient. That's not a bad proxy for\nhow sick they are, right? If you're not very\nsick, you tend to have a little bit of data. And if you're sicker, you\ntend to have more data. So these were the\ncohort selection. And then for the\nnarrative text, we used a system that was built\nby Qing Zeng and her colleagues at the time-- it\nwas called HITex. It's definitely not\nstate of the art today. But this was a system\nthat extracted entities from narrative text and did\na capable job for its era.", "id": "IiD3YZkkCmE_13", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and pathology reports, discharge\nsummaries, operative reports. And we also extracted\ndisease diagnosis notes, mentions from the same\ndata, medications, lab data, radiology\nfindings, et cetera. And then we had augmented the\nlist that came with that tool with the sort of hand-curated\nlist of alternative ways of saying the same thing in\norder to expand our coverage. And we played with\nnegation detection because, of course, if a note\nsays the patient does not have x, then you don't want to\nsay the patient had x because x was mentioned. And I'll say a few more\nwords about that in a minute. So if you look at\nthe model we built", "id": "IiD3YZkkCmE_14", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  you find is that there\nare positive and negative predictors, and the\npredictors actually are an interesting\nmix of ones based on natural language processing\nand ones that are codified. So for example, you have\nrheumatoid arthritis. If a note says the patient\nhas rheumatoid arthritis, that's pretty good\nevidence that they do. If somebody is characterized\nas being seropositive, that's again good evidence. And then erosions and so on. But they're also\ncodified things, like if you see that the\nrheumatoid factor in a lab test was negative, then-- actually, I don't\nknow why that's-- oh, no, that counts against-- OK. And then various exclusions. So these were the\nthings selected", "id": "IiD3YZkkCmE_15", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And I showed you\nthe results before. So we were able to get a\npositive predictive value of about 0.94. Yeah? AUDIENCE: In a the\nprevious slide, you said standardized\nregression coefficients. So why did you standardize? Maybe I got the words wrong. Just on the previous\nslide, the-- PETER SZOLOVITS: I think-- so the regression coefficients\nin a logistic regression are typically just\nodds ratios, right? So they tell you whether\nsomething makes a diagnosis more or less likely. And where does it\nsay standardized?", "id": "IiD3YZkkCmE_16", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  PETER SZOLOVITS: Oh,\nregression standardized. I don't know why it\nsays standardized. Do you know why it\nsays standardized? KATHERINE LIAO:\nCouple of things. One is, when you\nrun an algorithm right on your data\nset, you can't port it using the same\ncoefficients because it's going to be different for each one. So we didn't want people to feel\nlike they can just add it on. The other thing, when\nyou standardize it, is you can see the relative\nweight of each coefficient. So it's kind of a measure. Not exactly of how important\neach coefficient was. That's our way of--\nif you can see, we ranked it by the standardized\nregression coefficient. So NL PRA is up top at 1.11. So that has the highest weight. Whereas the other DMARDs lend\nit only a little bit more. PETER SZOLOVITS: OK. Yes? AUDIENCE: The variables\nwhere NL PRA, where it says rheumatoid\narthritis in the test, were these presence of\nor if they're count?", "id": "IiD3YZkkCmE_17", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Assuming it's present. So the negation\nalgorithm hopefully would have picked up\nif it said it's absent and you wouldn't\nget that feature. All right? So here's an interesting thing. This group, I was not involved\nin this particular project, said, well, could we replicate\nthe study at Vanderbilt and at Northwestern University? So we have colleagues\nin those places. They also have electronic\nmedical record systems. They also are interested\nin identifying people with rheumatoid arthritis. And so Partners had\nabout 4 million patients, Northwestern had 2.2,\nVanderbilt had 1.7. And we couldn't run exactly the\nsame stuff because, of course, these are different systems. And so the medications,\nfor example,", "id": "IiD3YZkkCmE_18", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And the natural language\nqueries were also extracted in different\nways because Vanderbilt, for example, already\nhad a tool in place where they would\ntry to translate any text in their\nnotes into UMLS less concepts, which we'll talk\nabout again in a little while. So my expectation, when\nI heard about this study, is that this would\nbe a disaster. That it would simply\nnot work because there are local effects,\nlocal factors, local ways that people\nhave of describing patients that I thought would be very\ndifferent between Nashville, Chicago, and Boston. And much to my surprise, what\nthey found was that, in fact, it kind of worked.", "id": "IiD3YZkkCmE_19", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that the way the data\nwas extracted out of the notes and clinical\nsystems was different, was fairly similar. Now, one thing that\nis worrisome is that the PPV of our\nalgorithm on our data, the way we calculated PPV, they\ncalculated PPV in this study, came in lower than the way we\nhad done it when we found it. And so there is a\ntechnical reason for it, but it's still\ndisturbing that we're getting a different result. The technical reason\nis described here. Here, the PPV is estimated from\na five-fold cross validation of the data, whereas\nin our study, we had a held out data set\nfrom which we were calculating the positive predictive value.", "id": "IiD3YZkkCmE_20", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  It's not that we made\nsome arithmetic mistake. But this is interesting. And what you see is that if\nyou plot the areas under-- or if you plot the ROC\ncurves, what you see is that training on\nNorthwestern data and testing on either\nPartners or Vanderbilt data was not so good. But training on either\nPartners or Vanderbilt data and testing on any of the others\nturned out to be quite decent. Right? So there is some generality\nto the algorithm. All right, I'm going to\nswitch gears for a minute. So this was from an old paper\nby Barrows from 19 years ago. And he was reading nursing\nnotes in an electronic medical", "id": "IiD3YZkkCmE_21", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And he came up with\na note which has exactly that text on the left\nhand side in the nursing note. Except it wasn't nicely\nseparated into separate lines. It was all run together. So what does that mean? Anybody have a clue? I didn't when I\nwas looking at it. So here's the interpretation. So that's a date. IPN stands for\nintern progress note. SOB, that's not what\nyou think it means. It's shortness of breath. And DOE is dyspnea on exertion. So this is difficulty breathing\nwhen you're exerting yourself, but that has\ndecreased, presumably", "id": "IiD3YZkkCmE_22", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And the patient's vital\nsigns are stable, so VSS. And the patient is afebrile, AF. OK? Et cetera. So this is harder than reading\nthe Wall Street Journal because the Wall\nStreet Journal is meant to be readable by\nanybody who speaks English. And this is probably not meant\nto be readable by anybody except the person\nwho wrote it or maybe their immediate\nfriends and colleagues. So this is a real issue\nand one that we don't have a very good solution for yet. Now, what do you use NLP for? Well, I had mentioned that one\nof the things we want to do is to codify things\nthat appear in a note. So if it says\nrheumatoid arthritis, we want to say, well,\nthat's equivalent", "id": "IiD3YZkkCmE_23", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  We might want to use\nnatural language processing for de-identification of data. I mentioned that before. You don't, MIMIC, the only way\nthat Roger Mark's group got permission to release\nthat data and make it available for\npeople like you to use is by persuading\nthe IRB that we had done a good enough\njob of getting rid of all the identifying\ninformation in all of those records so that\nit's probably not technically impossible, but\nit's very difficult to figure out who the patients\nactually were in that cohort, in that database. And the reason we ask you\nto sign a data use agreement is to deal with\nthat residual, you know, difficult but not\nnecessarily impossible because of correlations\nwith other data. And then you have\nlittle problems", "id": "IiD3YZkkCmE_24", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  the first Huntington is\nprotected health information because it's a patient's name. The second Huntington\nis actually an important medical fact. And so you wouldn't want\nto get rid of that one. You want to determine\naspects of each entity. Its time, its location,\nits degree of certainty. You want to look\nfor relationships between different entities that\nare identified in the text. For example, does one precede\nanother, does it cause it, does it treat it, prevent\nit, indicate it, et cetera? So there are a whole bunch\nof relationships like that that we're interested in. And then also, for certain\nkinds of applications, what you'd really like\nto do is to identify what part of a textual record\naddresses a certain question. So even if you can't\ntell what the answer is,", "id": "IiD3YZkkCmE_25", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and say, oh, this\ntells me about, in this case, the\npatient's exercise regimen. And then summarization\nis a very real challenge as well, especially because\nof the cut and paste that has come about as a result of\nthese electronic medical record systems where, when a nurse\nis writing a new note, it's tempting and\nsupported by the system for him or her to just\ntake the old note, copy it over to a new note, and\nthen maybe make a few changes. But that means that\nit's very repetitive. The same stuff is recorded\nover and over again. And sometimes that's\nnot even appropriate because they may not\nhave changed everything that needed to be changed. The other thing to keep\nin mind is that there are two very different tasks. So for example, if I'm\ndoing de-identification,", "id": "IiD3YZkkCmE_26", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  in order to see whether it's\nprotected health information. But there are often aggregate\njudgments that I need to make, where many of the words\ndon't make any difference. And so for example, one\nof the first challenges that we ran back\nin 2006 was where we gave people medical\nrecords, narrative text records from a bunch\nof patients and said, is this person a smoker? Well, you can imagine that\nthere are certain words that are very helpful like\nsmoker or tobacco user or something like that. But even those are\nsometimes misleading. So for example, we\nsaw somebody who happened to be a researcher\nworking on tobacco mosaic virus", "id": "IiD3YZkkCmE_27", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And then you have\ninteresting cases like the patient quit\nsmoking two days ago. Really? Are they a smoker or not? And also, aggregate judgment is\nthings like cohort selection, where it's not every\nsingle thing that you need to know about this patient. You just need to know if\nthey fit a certain pattern. So let me give you a\nlittle historical note. So this happened to be work\nthat was done by my PhD thesis advisor, the gentleman whose\npicture is on the slide there. And he published\nthis paper in 1966 called English for the Computer\nin the Proceedings of the Fall Joint Computer Conference. This was the big computer\nconference of the 1960s.", "id": "IiD3YZkkCmE_28", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to process English is to\nassume that there is a grammar, and any English text\nthat you run across, you parse according\nto this grammar. And that each parsing\nrule corresponds to some semantic function. And so the picture that\nemerges is one like this. Where if you have\ntwo phrases and they have some syntactic relationship\nbetween them, then you can map each phrase to its meaning. And the semantic relationship\nbetween those two meanings is determined by the syntactic\nrelationship in the language. So this seems like a\nfairly obvious idea, but apparently nobody had tried\nthis on a computer before. And so Fred built, over the\nnext 20 years, computer systems,", "id": "IiD3YZkkCmE_29", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And he was, in fact,\nable to build systems that were used by researchers\nin areas like anthropology, where you don't have nice coded\ndata and where a lot of stuff is in narrative text. And yet he was able to\nhelp one anthropologist that I worked with at Caltech\nto analyze a database of about 80,000 interviews that he had\ndone with members of the Gwembe Tonga tribe, who lived in the\nvalley that is now flooded by the Zambezi River Reservoir\non the border of Zambia and Zimbabwe. That was fascinating. Again, he became very well\nknown for some of that research. In the 1980s I was\namused to see that SRI--", "id": "IiD3YZkkCmE_30", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  but used to stand for\nStanford Research Institute-- built a system called\nDiamond Diagram, which was intended\nto help people interact with the\ncomputer system when they didn't know a command\nlanguage for the computer. So they could express what\nthey wanted to do in English and the English\nwould be translated into some semantic\nrepresentation. And from that, the right thing\nwas triggered in the computer. So these guys, Walker\nand Hobbs, said, well, why don't we apply this idea\nto natural language access to medical text? And so they built a system\nthat didn't work very well, but it tried to do this\nby essentially translating", "id": "IiD3YZkkCmE_31", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  calculus representation\nof what they saw, and then a process for that system. The original Diamond\nDiagram system that was built for people\nwho were naive computer users and didn't know\ncommand languages actually had a\nvery rigid syntax. And so what they\ndiscovered is that people are more adaptable\nthan computers and that they could adapt\nto this rigid syntax. How many of you have\nGoogle Home or Amazon Echo or Apple something or\nother that you deal with? Well, so it's\ntraining you, right?", "id": "IiD3YZkkCmE_32", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  but you're more adaptable. And so you quickly learn that\nif you phrase things one way, it understands you, and if you\nphrase things a different way, it doesn't understand you. And you learn how to phrase it. So that's what these\nguys are relying on, is that they can\nget people to adopt the conventions that the\ncomputer is able to understand. The most radical\nversion of this was a guy named de Heaulme,\nwho I met in 1983 in Paris. He was a doctor Le\nPitie Salpetriere, which is one of these\nmedieval hospitals in Paris. And it's wonderful place,\nalthough when they built it, it was just a place to\ndie because they really couldn't do much for you. So de Heaulme convinced\nthe chief of cardiology at that hospital that he would\ndevelop an artificial language", "id": "IiD3YZkkCmE_33", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  He would teach this\nto all of the fellows and junior doctors in\nthe cardiology department at the hospital. And they would be required\nby the chief, which is very powerful in France, to\nuse this artificial language to write notes instead of\nusing French to write notes. And they actually\ndid this for a month. And when I met de\nHeaulme, he was in the middle of analyzing the\ndata that he had collected. And what he found was\nthat the language was not expressive enough. There were things\nthat people wanted to say that they couldn't say\nin this artificial language he had created. And so he went back\nto create version two, and then he went back to\nthe cardiologist and said, well, let's do this again.", "id": "IiD3YZkkCmE_34", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So the experiment\nwas not repeated. OK, so back to term spotting. Traditionally, if\nyou were trying to do this, what you would\ndo is you would sit down with a bunch of medical experts\nand you would say, all right, tell me all the\nwords that you think might appear in a note that are\nindicative of some condition that I'm interested in. And they would give\nyou a long list. And then you'd do grep, you'd\nsearch through the notes for those terms. OK? And if you want it to\nbe really sophisticated, you would use an\nalgorithm like NegEx, which is a negation expression\ndetector that helps get rid of things that are not true. And then, as people\ndid this, they", "id": "IiD3YZkkCmE_35", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of doing this. And so a whole industry\ndeveloped of people saying that not only\nshould we use the terms that we got originally\nfrom the doctors who were interested in\ndoing these queries, but we can define a machine\nlearning problem, which is how do we learn\nthe set of terms that we should actually\nuse that will give us better results than just\nthe terms we started with? And so I'm going to talk about\na little bit of that approach. First of all, for negation,\nWendy Chapman, now at Utah, but at the time at Pittsburgh,\npublished this paper in 2001 called A Simple\nAlgorithm for Identifying the Gated Findings of Diseases\nin Discharge Summaries. And it is indeed a\nvery simple algorithm.", "id": "IiD3YZkkCmE_36", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  You find all the UMLS\nterms in each sentence of a discharge summary. So I'll talk a little\nbit about that. But basically, it's\na dictionary look up. You look up in this very large\ndatabase of medical terms and translate them into\nsome kind of expression that represents what\nthat term means. And then you find two\nkinds of patterns. One pattern is a negation phrase\nfollowed within five words by one of these UMLS terms. And the other is a UMLS term\nfollowed within five words by a negation phrase, different\nset of negation phrases. So if you see no\nsign of something, that means it's not present. Or if you see ruled out,\nunlikely something, then it's not present.", "id": "IiD3YZkkCmE_37", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And post modifiers if you say\nsomething declined or something unlikely, that also indicates\nthat it's not present. And then they hacked up a\nbunch of exceptions where, for example, if you\nsay gram negative, that doesn't mean that it's negative\nfor whatever follows it or whatever precedes it, right? Et cetera. So there are a\nbunch of exceptions. And what they found\nis that this actually, considering how\nincredibly simple it is, does reasonably well. So if you look at sentences\nthat do not contain a negation phrase and looked\nat 500 of them, you find that you get a\nsensitivity and specificity of 88% and 52% for\nthose that don't", "id": "IiD3YZkkCmE_38", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Of course, the sensitivity\nis 0 and the specificity is 100% on the baseline. And if you use\nNegEx, what you find is that you can significantly\nimprove the specificity over the baseline. All right? And you wind up with\na better result, although not in all schemes. So what this means is that\nvery simplistic techniques can actually work\nreasonably well at times. So how do we do\nthis generalization? One way is to take advantage\nof related terms like hypo- or hypernyms, things\nthat are subcategories or super categories of a word. You might look for those\nother associated terms. For example, if you're looking\nto see whether a patient has a certain disease,\nthen you can do", "id": "IiD3YZkkCmE_39", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  if I see a lot of symptoms\nof that disease mentioned, then maybe the disease\nis present as well. So the recursive\nmachine learning problem is how best to identify\nthe things associated with the term. And this is generally\nknown as phenotyping. Now, how many of you\nhave used the UMLS? Just a few. So in 1985 or '84, the\nnewly appointed director of the National\nLibrary of Medicine, which is one of\nthe NIH institutes, decided to make a big\ninvestment in creating this unified medical language\nsystem, which was an attempt to take all of the terminologies\nthat various medical professional societies\nhad developed", "id": "IiD3YZkkCmE_40", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  they called a meta-thesaurus. So it's not really a\nthesaurus because it's not completely well integrated,\nbut it does include all of this terminology. And then they spent a lot\nof both human and machine resources in order\nto identify cases in which two\ndifferent expressions from different terminologies\nreally meant the same thing. So for example, myocardial\ninfarction and heart attack really mean exactly\nthe same thing. And in some\nterminologies, it's called acute myocardial\ninfarction or acute infarct or acute, you know, whatever. And they paid people\nand they paid machines to scour those entire\ndatabases and come up with the mapping\nthat said, OK, we're going to have some concept,\nyou know, see 398752--", "id": "IiD3YZkkCmE_41", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to that particular concept. And then they mapped\nall those together. So that's an enormous\nhelp in two ways. It helps you normalize databases\nthat come from different places and that are\ndescribed differently. It also tells you, for\nnatural language processing, how it is-- it gives you a treasure\ntrove of ways of expressing the same conceptual idea. And then you can\nuse those in order to expand the kinds of phrases\nthat you're looking for. So there are, as of\nthe current moment, there are about 3.7\nmillion distinct concepts in this concept base. There are also hierarchies\nand relationships that are imported from all\nthese different sources", "id": "IiD3YZkkCmE_42", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And then over the whole thing,\nthey created a semantic network that says there are 54\nrelations and 127 types, and every concept\nunique identifier is assigned at least\none semantic type. So this is very useful for\nlooking through this stuff. Here are the UMLS semantic\nconcepts of various-- or the semantic types. So you see that the most\ncommon semantic type is this T061, which stands\nfor therapeutic or preventive procedure. And there are 260,000 of those\nconcepts in the meta-thesaurus. There are 233,000\nfindings, 172,000 drugs,", "id": "IiD3YZkkCmE_43", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  amino acid peptide or\nprotein, invertebrate. So the data does not come\nonly from human medicine but also from veterinary\nmedicine and bioinformatics research and all over the place. But you see that these\nare a useful listing of appropriate semantic\ntypes that you can then look for in such a database. And the types are\nhierarchically organized. So for example,\nthe relations are organized so there's an\neffects relation which has sub-relations, manages,\ntreats, disrupts, complicates, interacts with, or prevents. Something like\nbiological function can be a physiologic function\nor a pathologic function. And again, each of\nthese has subcategories.", "id": "IiD3YZkkCmE_44", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  is labeled with at least\none of these semantic types, and that helps to identify\nthings when you're looking through the data. There are also some\ntools that deal with the typical\nlinguistic problems, that if I want to say\nbleeds or bleed or bleeding, those are really all\nthe same concept. And so there are these\nlexical variant generator that helps us normalize that. And then there is the\nnormalization function that takes some statement like\nMr. Huntington was admitted, blah, blah, blah,\nand normalizes it into lowercase alphabetized\nversions of the text, where things are translated into\nother potential meanings,", "id": "IiD3YZkkCmE_45", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So for example, notice\nthis one says was, but one of its translations\nis be because was is just a form of be. This can also get\nyou in trouble. I ran into a problem where\nI was finding beryllium in everybody's medical\nrecords because it also knows that b-e is an\nabbreviation for beryllium. And so you have to\nbe a little careful about how you use this stuff. There is an online tool where\nyou can type in something and it says weakness of\nthe upper extremities. And it says, oh, you\nmean the concept proximal weakness, upper extremities. And then it has a relationship\nto various contexts and it has siblings and it\nhas all kinds of other things that one can look up. I built a tool a\nfew years ago where", "id": "IiD3YZkkCmE_46", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  it tries to color code\nthe types of things that it found in that summary. And so this is\nusing a tool called MetaMap, which again\ncomes from the National Library of Medicine,\nand a locally built UMLS look up tool that in\nthis particular case finds exactly the same\nmappings from the text. And so you can look through\nthe text and say, ah, OK, so no indicates negation\nand urine output is a kind of one\nof these concepts. If you moused over\nit, it would show you. OK, I think what I'm going\nto do is stop there today so that I can invite Kat to\njoin us and talk about A, what's", "id": "IiD3YZkkCmE_47", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  by clinicians and\nclinician researchers. Kat? OK, well, welcome, Kat. KATHERINE LIAO: Thank you. PETER SZOLOVITS: Nice\nto see you again. So are the techniques\nthat were represented in that paper from\nnine years ago still being used today\nin research settings? KATHERINE LIAO: Yeah. So I'd say yes, the\nbare bones of platform-- that pipeline is being used. But now I'd say we're\nin version five. Actually, you were on\nthat revision list. But we've done a\nlot of improvements to actually automate\nthings a little more. So the rate limiting\nfactor in phenotyping is always the clinician. Always getting that label,\ndoing the chart review, coming up with that term list. So I don't know if you want me\nto go into some of the details on what we've been doing. PETER SZOLOVITS:\nYeah, if you would. KATHERINE LIAO:\nKind of plugs it in. So if you recall\nthat diagram, there were several steps, where\nyou started with the EMR.", "id": "IiD3YZkkCmE_48", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Then you get this data mart,\nand then you start training. You had to select a random\n500, which is a lot. It's a lot of\nchart review to do. It is a lot. So our goal was to reduce\nthat amount of chart review. And part of the\nway to reduce that is reducing the feature space. So one of the things that\nwe didn't know when we first started out was how many gold\nstandard labels did we need and how many\nfeatures did we need and which of those features\nwould be important. So by features, I mean ICD\ncodes, a diagnosis code, medications, and all\nthat list of NLP terms that might be related\nto the condition. And so now we have ways to\ntry to whittle down that list before we even use those\ngold standard labels. And so let me think about-- this is NLP. The focus here is on NLP. So there are a couple of\nways we're doing this. So one rate limiting step\nwas getting the clinicians to come up with a\nlist of terms that", "id": "IiD3YZkkCmE_49", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  You can imagine if you\nget five doctors in a room to try to agree on a\nlist, it takes forever. And so we tried to get\nthat out of the way. So one thing we\nstarted doing was we took just common things that\nare freely available on the web. Wikipedia, Medline,\nthe Merck Manual that have medical information. And we actually now\nprocess those articles, look for medical\nterms, pull those out, map them to concepts, and\nthat becomes that term list. Now, that goes into-- so now instead of, if you\nthink about in the old days, we came up with the list, we\nhad ICD lists and term lists, which got mapped to a concept. Now we go straight\nto the article. We kind of do majority\nvoting with the articles. We take five articles,\nif three out of five mention it more than\nx amount of time, we say that could\npotentially be important. So that's the term list. Get the clinicians\nout of that step. Well, actually, we\ndon't train yet. So now instead of\ntraining right away in the gold standard\nlabels, we train on a silver standard label.", "id": "IiD3YZkkCmE_50", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  but sometimes we use\nthe main NLP [INAUDIBLE] Because sometimes there is\nno code for the phenotype we're interested in. So that's kind of\nsome of the steps that we've done to automate\nthings a little bit more and formalize that pipeline. So in fact, the\npipeline is now part of the Partners Biobank, which\nis a Partner's Healthcare. As Pete mentioned, it's Mass\nGeneral and Brigham Women's Hospital. They are recruiting patients\nto come in and get the blood sample, link it with their\nnotes so people can do research on linked EHR data\nand blood sample. So this is the pipeline\nthey used for phenotyping. Now I'm over at the Boston\nVA along with Tianxi. And this is the pipeline\nwe're laying down for also the Million Veterans\nprogram, which is even bigger. It's a million\nvets and they have EHR data going back decades. So it's pretty exciting.", "id": "IiD3YZkkCmE_51", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  I mean, this study that we\nwere talking about today was for rheumatoid arthritis. What other diseases are being\ntargeted by this phenotyping approach? KATHERINE LIAO: So\nall kinds of diseases. There's a lot of things\nwe learn, though. The phenotyping approach is\nbest suited, the pipeline that we-- the base\npipeline is best suited for conditions that have\na prevalence of 1% or higher. So rheumatoid arthritis is\nkind of at that lower bound. Rheumatoid arthritis is a\nchronic inflammatory joint disease. It affects 1% of the population. But it is the most common\nautoimmune joint disease. Once you go to rare\ndiseases that are episodic that don't happen-- you know, not only\nis it below 1%, but only happens\nonce in a while-- this type of approach\nis not as robust. But most diseases are above 1%. So at the VA, we've kind\nof laid down this pipeline", "id": "IiD3YZkkCmE_52", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And they're running\nthrough acute stroke, myocardial infarction, all\nkinds of these-- diabetes-- just really a lot of\nall the common diseases that we want to study. PETER SZOLOVITS: Now,\nyou were mentioning that when you identify\nsuch a patient, you then try to get a\nblood sample so that you can do genotyping on them. Is that also common\nacross all these diseases or are there\ndifferent approaches? KATHERINE LIAO: Yeah,\nso it's interesting. 10 years ago, it\nwas very different. It was very expensive\nto genotype a patient. It was anywhere between\n$500 to $700 per patient. PETER SZOLOVITS: And that was\njust for single nucleotide polymorphism. KATHERINE LIAO: Yes,\njust for a snip. So we had to be very careful\nabout who we selected. So 10 years ago, what\nwe did is we said, OK, we have 4 million\npatients and partners. Who has already\nwith good certainty? Then we select those patients\nand we genotype them. Because it costs\nso much, you didn't want to genotype someone\nwho didn't have RA. Not only would it alter the--", "id": "IiD3YZkkCmE_53", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  it would just be\nlike wasted dollars. The interesting thing is\nthat the change has happened. And we can completely\nthink of a different way of approaching things. Now you have these biobanks. You have something like\nthe VA MVP or UK Biobank. They are being\nsystematically recruited, blood samples are\ntaken, they're genotyped with no study in mind. Linked with the EHR. So now I walk into the VA, it's\na completely different story. 10 years later, I'm\nat the VA and I'm interested in identifying\nrheumatoid arthritis. Interesting enough,\nthis algorithm ports well over there, too. But now we tested our\nnew method on there. But now, instead of saying, I\nneed to identify these patients and get the genotype, all the\ngenotypes are already there. So it's a completely different\napproach to research now. PETER SZOLOVITS: Interesting. So the other question\nthat I wanted", "id": "IiD3YZkkCmE_54", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to questions from\nthe audience is, so this is all focused on\nresearch uses of the data. Are there clinical\nuses that people have adopted that use\nthis kind of approach to trying to read the note? We had fantasized\ndecades ago that, you know, when you get a\nreport from a pathologist, that somehow or other, a\nmachine learning algorithm using natural\nlanguage processing would grovel over it, identify\nthe important things that came out, and then\neither incorporate that in decision support or in\nsome kind of warning systems that drew people's attention\nto the important results as opposed to the unimportant ones. Has any of that happened? KATHERINE LIAO: I think\nwe're not there yet, but I feel like we're so much\ncloser than we were before. That's probably how you\nfelt a few decades ago.", "id": "IiD3YZkkCmE_55", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  EHR weren't really widely\nadopted until the HITEx Act in 2010. So a lot of systems are actually\nnow just getting their EHR. And the reason that we've had\nthe luxury of playing around with the data is because\nPartners was ahead of the curve and had developed an EHR. The VA happened to have an EHR. But I think first-- because research and clinical\nmedicine is very different. Research, if you mess up\nand you misclassify someone with a disease, it's OK, right? You just lose power\nin your study. But in the clinical\nsetting, if you mess up, it's a really big deal. So I think the bar\nis much higher. And so one of our goals\nwith all this phenotyping is to get it to that point\nwhere we feel pretty confident. We're not going to say someone\nhas or hasn't a disease, but we are, you\nknow, Tianxi and I have been planning\nthis grant where, what's outputted\nfrom this algorithm is a probability of disease.", "id": "IiD3YZkkCmE_56", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And so what we want to\ntest is what threshold is that probability\nthat you would want to tell a clinician\nthat, hey, if you're not thinking about rheumatoid\narthritis in this patient-- this is particularly\nhelpful in places where they're in\nremote locations where there aren't\nrheumatologist available-- you should\nbe thinking about it and maybe, you know,\nconsidering referring them or speaking to a rheumatologist\nthrough telehealth, which is also something. There's a lot of things\nthat are changing that are making\nsomething like this fit much more into the workflow. PETER SZOLOVITS: Yeah. So you're as optimistic\nas I was in the 1990s. KATHERINE LIAO: Yes. I think we're getting-- we'll see. PETER SZOLOVITS: Well,\nyou know, it will surely happen at some point. Did any of you go\nto the festivities around the opening\nof the Schwarzman College of Computing?", "id": "IiD3YZkkCmE_57", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And health care does keep\ncoming up over and over again as one of the great\nopportunities. I profoundly believe that. But on the other hand, I've\nlearned over many decades not to be quite as optimistic\nas my natural proclivities are. And I think some of the\nspeakers here have not yet learned that same lesson. So things may take\na little bit longer. So let me open up the\nfloor to questions. KATHERINE LIAO: Yes? AUDIENCE: So the mapping\nthat you did to concepts, is that within the\nPartners system or is that something\nlike publicly available? And can you just\ntransfer that to the VA? Or like, when you do work\nlike, how much is proprietary and how much gets expanded up? KATHERINE LIAO: Yeah. So you're speaking about\nwhen we were trying to create that term\nlist and we mapped the terms to the concepts? AUDIENCE: And you\nwere using Wikipedia and three other sources. KATHERINE LIAO: Yeah. Yeah. So that's all out there.", "id": "IiD3YZkkCmE_58", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  We put our codes up on GitHub\nor CRAN for other people to play out and tests and break. So yeah, the terms are\nreally similar in UMLS. I don't know if you had a\nchance to look through it. They have a lot of keywords. So there is a general way to map\nkeywords to terms to concepts. So that's the basis\nof what we do. There may maybe a\nlittle bit more there, but there's nothing\nfancy behind it. And as you can\nimagine, because we're trying to go across\nmany phenotypes, when we think about mapping,\nit always has to be automated. Our first round was very\nmanual, incredibly manual. But now we try to use\nsystems that are available such as UMLS and\nother mapping methods. PETER SZOLOVITS: So what map-- presumably, you don't\nuse HITex today. KATHERINE LIAO: No. PETER SZOLOVITS: So\nwhich tools do you use? KATHERINE LIAO: Just thinking\nI had a two hour conversation with Oakridge about this.", "id": "IiD3YZkkCmE_59", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And it had to do with the\nfact that cTAKES, which is a really robust system,\nwas just too computationally intensive. And for the purposes\nof phenotyping, we didn't need that\nlevel of detail. What we really needed\nwas, was it mentioned, what's the concept,\nand the negation. And so NIAL is something\nthat we've been using and have kind of validated over\ntime with the different methods we've been testing. PETER SZOLOVITS:\nSo Tuesday, I'll talk a little bit\nabout that system and some of its successors. So you'll get a sense\nof how that works. I should mention also that\none of the papers that was on your reading\nlist is a paper out of David Sontag's group, which\nuses this anchorous concept. And that's very much\nalong the same lines. That it's a way of\ntrying to automate,", "id": "IiD3YZkkCmE_60", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  if the doctor's\nmention some term and you discover that that\nterm is very often used with certain other\nterms by looking at Wikipedia or at\nthe Mayo Clinic data or wherever your\nsources are, then that's a good clue that\nthat other term might also be useful. So this is a\nformalization of that idea as a machine learning problem. So basically, that\npaper talks about how to take some very\ncertain terms that are highly indicative\nof a disease and then use those\nas anchors in order to train a machine\nlearning model that identifies more terms that\nare also likely to be useful. So this notion of-- and David talked about a similar\nidea in a previous lecture, where you get a silver standard\ninstead of a gold standard. And the silver\nstandard can be derived", "id": "IiD3YZkkCmE_61", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  learning algorithm. And then you can use that in\nyour further computations. AUDIENCE: So what\nwas the process like for partnering with\nacademics and machine learning? So did you seek them out,\ndid they seek you out? Did you run into each\nother at the bus stop? How does that work? KATHERINE LIAO: Well,\nI was really lucky. There was a big study called\nThe Informatics for Integrating Biology and the Bedside Project\ncalled i2B2 led by Zak Kohane And so that was\nalready in place. And Pete had already been\npulled in and Tianxi. So what they basically\ndid was locked all us in a room for three\nhours every Friday. And it was like, what's the\nproblem, what's the question, and how do we get there. And so I think that\ninfrastructure was so helpful in bringing everyone\nto the table, because it's not easy\nbecause you're not rotating in the same space. And the way you think\nis very different. So that's how we did it. Now it's more mainstream. I think when we first\nstarted, everyone was--", "id": "IiD3YZkkCmE_62", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  They're like, what\nare you doing? R2D2? What's going on? Are you going off the\ndeep end over there? Because you know, the\ntype of research we do was more along the ways of\nclinical trials and clin-epi projects. But now, you know, we have-- I run a core at Brigham. So it's run out of the\nrheumatology division. And so we kind of try to\nconnect people together. I did post to our core the\nconsulting session here. But you know, if\nthere is interest, there's probably more\ngroups that are doing this, where we can kind of more\nformally have joint talks or connect people together. Yeah. But it's not easy. I have to say, it\ntakes a lot of time. Because when Pete put\nup that thing in what looked like a different\nlanguage, I mean, it didn't even occur to me that\nit was hard to read, right? So it's like, you know,\nyou're into these two different worlds. And so you have to work\nto meet in the middle,", "id": "IiD3YZkkCmE_63", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  PETER SZOLOVITS: It also\ntakes the right people. So I have to say\nthat Zak was probably very clever in bringing the\nright people to the table and locking those into that\nroom for three hours at a time because, for example,\nour biostatistician, Tianxi Cai just, you\nknow, she speaks AI or she has learned to speak AI. And there are still\nplenty of statisticians who just have allergic\nreactions to the kinds just things that we do,\nand it would be very difficult to work with them. So having the right\ncombination of people is also really I think critical. KATHERINE LIAO: As one\nof my mentors said, you have to kiss a lot of frogs. AUDIENCE: I wondering if you\ncould say a bit more about how you approached the\nalarm fatigue with how", "id": "IiD3YZkkCmE_64", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  versus clinical questions of how\nimportant this is versus even psychological questions\nof, I said is too often to a certain amount of people. They're going to\nstart [INAUDIBLE]?? KATHERINE LIAO: Yeah,\nyou've definitely hit the nail on the head of\none of the major barriers, or several things. The alarm fatigue\nis one of them. So EMRs became more\nprominent in 2010. But now, along with\nEMRs came a lot of regulations on physicians. And then came getting\nrid of our old systems for these new systems that\nare now government compliant. So Epic is this\nbig monster system that's being rolled out\nacross the country, where you literally have-- it's so complicated\nin places like Mayo. They hire scribes. The physicians\nsits in the office and there's another person\nwho actually listens in and types and then\nclicks all the buttons that you need to get\nthe information there. So alarm fatigue is definitely\none of the barriers.", "id": "IiD3YZkkCmE_65", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that the EMRs are so\nuser-unfriendly now. They're not built\nfor clinical care. They're built for billing. We have to be careful\nabout how we roll this out. And that's one reason why I\nthink things have been held up, actually. Not necessarily the science. It's the implementation part\nis going to be very hard. PETER SZOLOVITS: So that\nisn't new, by the way. I remember a class I taught\nin biomedical computing about 15 years ago. David Bates, who's the chief\nof general internal medicine or something at the\nBrigham, came in and gave a guest lecture. And he was describing\ntheir experience with a drug-drug\ninteraction system that they had implemented. And they purchased a data\nset from a vendor called First Databank that had\nscoured the literature and found all the instances\nwhere people had reported cases", "id": "IiD3YZkkCmE_66", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  had an apparent adverse event. So there was some\ninteraction between them. And they bought this\nthing, they implemented it, and they discovered that, on\nthe majority of drug orders that they were making through\ntheir pharmacy system, a big red alert would pop up\nsaying, you know, are you aware of the fact that there is\na potential interaction between this drug\nand some other drug that this patient is taking. And the problem is that the\nincentives for the company that curated this\ndatabase were to make sure they didn't miss\nanything, because they didn't want to be responsible\nfor failing to alarm. But of course,\nthere's no pushback saying that if you warn\non every second order, then no one's going to pay\nany attention to any of them. And so David's\nsolution was to get a bunch of the senior\ndoctors together", "id": "IiD3YZkkCmE_67", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  experienced at the hospital. And they cut this list of\nthousands of drug interactions down to 20. And they said, OK,\nthose are the only ones we're going to alarm on. KATHERINE LIAO: And\nthen they threw that out when Epic came in. So now I put in an order,\nI get like a list of 10 and I just click them all. So that's the problem. And the threshold\nis going to be-- so there's going\nto be an entire-- I think there's going\nto be entire methods development that's going to have\nto happen between figuring out where that threshold is and\nthe fatigue from the alarms. AUDIENCE: I have two questions. One is about [INAUDIBLE]. Like how did you approach that\nbecause we talk about this in other contexts in class? And the other one\nis, like, how can you inform other countries\n[INAUDIBLE] done here? Because, I mean, at\nthe end of the day, it's a global health issue.", "id": "IiD3YZkkCmE_68", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  even between the US and the UK. So all the mapping\nwe're doing here, how could that inform\nEHR or elsewhere? KATHERINE LIAO: Yeah. So let me answer the first one. The second one is\na work in progress. So ICD-10 came to the\nUS on October 1, 2015. I remember. It hurt us all. So we actually don't have that\nmuch information on ICD-10 yet. But it's definitely\nimpacted our work. So if you think about\nwhen Pete was pointing to the number of ICD counts\nfor ICD-9, for those of you who don't know, ICD-9 was\ndeveloped decades ago. ICD-10 maybe two decades ago. But what ICD-10 did was\nit added more granularity. So for rheumatoid\narthritis, I mentioned it's a systemic chronic\ninflammatory joint disease. We used to have a code that\nsaid rheumatoid arthritis. In ICD-10, it now says\nrheumatoid arthritis,", "id": "IiD3YZkkCmE_69", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  rheumatoid factor negative. And under each category\nis RA of the right wrist, RA of the left wrist, RA of\nthe right knee, left knee. Can you imagine? So we're clicking\noff all of these. And so, as it turns\nout, surprisingly-- we're about to publish\na small study now, is RA any more accurate now\nthey have all these granular-- it turns out, I\nthink we got annoyed because it's actually less\naccurate now than the ICD-9. So that's one thing. But that's, you know, only\ntwo or three years of data. I think it's going to\nbecome pretty equivalent. The other thing is,\nyou'll see an explosion in the number of ICD codes. So you have to think about how\ndo you deal with back October 1, 2015 when you\nhad one RA code, but after 2015, it depends\non when the patient comes in. They may have RA of the\nright wrist on one day, then on the left\nknee the other day. That looks like\na different code.", "id": "IiD3YZkkCmE_70", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to roll up. I think the biggest challenge\nright now is the mapping. So ICD-9, you know, doesn't\nmap directly to ICD-10 or back because there were\ndiseases that we didn't know when they developed\nICD-9 that exist in ICD-10. In ICD-10, they talk about\ndiseases in ways that weren't described in ICD-9. So when you're trying\nto harmonize the data, and this is actively something\nwe're dealing with right now at the VA, how do you\nnow count the ICD codes? How do you consider that\nsomeone has an ICD code for RA? So those are all things that\nare being developed now. CMS, Center for\nMedicaid and Medicare, again, this is for\nbilling purposes, has come up with a mapping\nsystem that many of us are using now,\ngiven what we have. PETER SZOLOVITS: And by\nthe way, the committee that is designing ICD-11 has\nbeen very active for years.", "id": "IiD3YZkkCmE_71", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Although, from\nwhat I understand-- KATHERINE LIAO: Are\nyou involved with that? PETER SZOLOVITS: No. But Chris Chute is or was. KATHERINE LIAO: Yes, I saw. I said, don't do it. PETER SZOLOVITS:\nWell, but actually, I'm a little bit\noptimistic because unlike the traditional\nICD system, this one is based on SNOMED,\nwhich has a much more logical structure. So you know, my\nfavorite ICD-10 code is closed fracture\nof the left femur due to spacecraft accident. KATHERINE LIAO: I didn't\neven know that existed. PETER SZOLOVITS:\nAs far as I know, that code has never\nbeen applied to anybody. But it's there just in case. Yeah. AUDIENCE: So wait,\nfor the ICD-11 you don't think take that\nlong to exist because it's a more logical system? PETER SZOLOVITS: So ICD-11-- well, I don't know\nwhat it's going to be because they\nhaven't defined it yet.", "id": "IiD3YZkkCmE_72", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  a combinatorial system. So it's more like a\ngrammar of descriptions that you can assemble according\nto certain rules of what assemblies make sense. And so that means\nthat you don't have to explicitly mention something\nlike the spacecraft accident one. But if that ever\narises, then there is a way to construct\nsomething that would describe that situation. KATHERINE LIAO: I ran\ninto Chris at a meeting and he said something\nalong the lines that he thinks it's going\nto be more NLP-based, even. I don't know. Is it going to be\nmore like a language? PETER SZOLOVITS: Well,\nyou need to ask him. KATHERINE LIAO:\nYeah, I don't know. He hints at it [INAUDIBLE]. I was like, OK, this\nwill be interesting. PETER SZOLOVITS: I\nthink it's definitely more like a language,\nbut it'll be more like the old Fred Thompson\nor the Diamond Diagram kind of language. It's a designed\nlanguage that you're", "id": "IiD3YZkkCmE_73", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  things appropriately. Or at least your billing\nclerk will have to learn it. Yeah? AUDIENCE: I know\nwe're towards the end. But I had a question about\nwhen a clinician is trying to label data, for\nexample, training data, are there any ambiguities\never, where sometimes this is definitely--\nthis person has RA. This person, I'm\nnot really sure. How do you take\nthat into account when you're actually\ntraining a [INAUDIBLE]?? KATHERINE LIAO: Yeah. So we actually have three\ncategories-- definite, possible, and no. So there is always ambiguity. And then you always want to\nhave more than one reviewer. So in clinical trials\nwhen you have outcomes, you have what we\ncall adjudication. So you have some\nkind of system where you have the first sit down, you\nhave to define the phenotype. Because not everybody\nis going to agree, even for a really clear disease,\nhow do you define the disease. What are the components\nthat has to happen. For that, they're usually for\nsocieties or classification criteria for research.", "id": "IiD3YZkkCmE_74", "title": "7. Natural Language Processing (NLP), Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  know, for coronary\nartery disease. And then it is having\nthose different categories in a very structured\nsystem for adjudicating. You know, blindly having two\nreviewers review 20, you know, let's say 20 of the\nsame notes and look at the integrated reliability. Yeah. That's a big issue. PETER SZOLOVITS: All right. I think we have expired. So Kat, thank you very much.", "id": "IiD3YZkkCmE_75"}, {"text": "  PETER SZOLOVITS: All right. Let's get started. Good afternoon. So last time, I started\ntalking about the use of natural language processing\nto process clinical data. And things went a\nlittle bit slowly. And so we didn't get through\na lot of the material. I'm going to try to\nrush a bit more today. And as a result, I have\na lot of stuff to cover. So if you remember,\nlast time, I started by saying that a\nlot of the NLP work involves coming up with\nphrases that one might be interested in to help\nidentify the kinds of data", "id": "lkO2ocJBsmI_0", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So that's a very simple method. But it's one that\nworks reasonably well. And then Kat Liao was\nhere to talk about some of the applications\nof that kind of work in what she's been doing\nin cohort selection. So what I want to\ntalk about today is more sophisticated\nversions of that, and then move on to more\ncontemporary approaches to natural language processing. So this is a paper\nthat was given to you as one of the\noptional readings last time. And it's work from\nDavid Sontag's lab, where they said, well, how do\nwe make this more sophisticated? So they start the same way. They say, OK, Dr.\nLiao, let's say, give me terms that are very\ngood indicators that I have the right kind of\npatient, if I find them in the patient's notes.", "id": "lkO2ocJBsmI_1", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So you don't want to use a term\nlike sick, because that's going to find way too many people. But you want to\nfind something that is very specific but that\nhas a high predictive value that you are going to\nfind the right person. And then what they\ndid is they built a model that tries to\npredict the presence of that word in the\ntext from everything else in the medical record. So now, this is an example of a\nsilver-standard way of training a model that says, well, I don't\nhave the energy or the time to get doctors to\nlook through thousands and thousands of records. But if I select these\nanchors well enough, then I'm going to get a high\nyield of correct responses from those.", "id": "lkO2ocJBsmI_2", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  model that learns to\nidentify those same terms, or those same records that\nhave those terms in them. And by the way, from\nthat, we're going to learn a whole\nbunch of other terms that are proxies for the\nones that we started with. So this is a way of\nenlarging that set of terms automatically. And so there are a bunch\nof technical details that you can find out\nabout by reading the paper. They used a relatively\nsimple representation, which is essentially a\nbag-of-words representation. They then sort of\nmasked the three words around the word that\nactually is the one they're trying to predict\njust to get rid of short-term\nsyntactic correlations. And then they built an\nL2-regularized logistic", "id": "lkO2ocJBsmI_3", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  the occurrence of this word? And then they expanded\nthe search vocabulary to include those\nfeatures as well. And again, there\nare tons of details about how to discretize\ncontinuous values and things like that that\nyou can find out about. So you build a\nphenotype estimator from the anchors and\nthe chosen predictors. They calculated a\ncalibration score for each of these\nother predictors that told you how well it predicted. And then you can build\na joint estimator that uses all of these. And the bottom line is\nthat they did very well. So in order to\nevaluate this, they looked at eight different\nphenotypes for which they had human judgment data. And so this tells\nyou that they're", "id": "lkO2ocJBsmI_4", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  for these different phenotypes. So that's quite good. They, in fact, were estimating\nnot only these eight phenotypes but 40-something. I don't remember the exact\nnumber, much larger number. But they didn't\nhave validated data against which to\ntest the others. But the expectation is that\nif it does well on these, it probably does well\non the others as well. So this was a very nice idea. And just to illustrate, if\nyou start with something like diabetes as a\nphenotype and you say, well, I'm going to\nlook for anchors that are a code of\n250 diabetes mellitus, or I'm going to\nlook at medication history for diabetic therapy-- so those are the silver-standard\ngoals that I'm looking at.", "id": "lkO2ocJBsmI_5", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  being in the cohort. And then they identify\nall these other features that predict those,\nand therefore, in turn, predict appropriate\nselectors for the phenotype that they're interested in. And if you look at the\npaper again, what you see is that this\noutperforms, over time, the standard supervised baseline\nthat they're comparing against, where you're getting much\nhigher accuracy early in a patient's visit to\nbe able to identify them as belonging to this cohort. I'm going to come back later to\nlook at another similar attempt to generalize from a core using\na different set of techniques. So you should see that in\nabout 45 minutes, I hope. Well, context is important. So if you look at a sentence\nlike Mr. Huntington was treated", "id": "lkO2ocJBsmI_6", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  on Huntington Avenue, each\nof those mentions of the word Huntington is different. And for example, if you're\ninterested in eliminating personally identifiable\nhealth information from a record like\nthis, then certainly you want to get rid of the\nMr. Huntington part. You don't want to get rid\nof Huntington's disease, because that's a\nmedically relevant fact. And you probably do want to\nget rid of Huntington Hospital and its location on\nHuntington Avenue, although those are not\nnecessarily something that you're prohibited\nfrom retaining. So for example, if you're\ntrying to do quality studies among different\nhospitals, then it would make sense to retain the\nname of the hospital, which is not considered identifying\nof the individual.", "id": "lkO2ocJBsmI_7", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  where we were trying to build\nan improved de-identifier. And here's the way\nwe went about it. This is a kind of kitchen\nsink approach that says, OK, take the text, tokenize it. Look at every single token. And derive things from it. So the words that\nmake up the token, the part of speech,\nhow it's capitalized, whether there's\npunctuation around it, which document\nsection is it in-- many databases have sort\nof conventional document structure. If you've looked at the\nmimic discharge summaries, for example, there's a\nkind of prototypical way in which that flows\nfrom beginning to end. And you can use that\nstructural information. We then identified a bunch of\npatterns and thesaurus terms.", "id": "lkO2ocJBsmI_8", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  to see if they matched some\nclinically meaningful term. We had patterns that\nidentified things like phone numbers and social\nsecurity numbers and addresses and so on. And then we did\nparsing of the text. So in those days,\nwe used something called the Link\nGrammar Parser, which, doesn't make a whole lot\nof difference what parser. But you get either a constituent\nor constituency or dependency parse, which gives you\nrelationships among the words. And so it allows you to\ninclude, as features, the way in which a word\nthat you're looking at relates to other\nwords around it. And so what we did is we\nsaid, OK, the lexical context includes all of the\nabove kind of information for all of the words that\nare either literally adjacent", "id": "lkO2ocJBsmI_9", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  focusing on, or that are\nlinked by within k links through the parse to that word. So this gives you a very\nlarge set of features. And of course, parsing\nis not a solved problem. And so this is an\nexample from that story that I showed you last time. And if you see, it comes\nup with 24 ambiguous parses of this sentence. So there are technical problems\nabout how to deal with that. Today, you could use\na different parser. The Stanford\nParser, for example, probably does a better\njob than the one we were using 14 years\nago and gives you at least more definitive answers.", "id": "lkO2ocJBsmI_10", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And so if you look\nat what we did, we said, well, here\nis the text \"Mr.\" And here are all the ways that\nyou can look it up in the UMLS. And it turns out to\nbe very ambiguous. So M-R stands not\nonly for mister, but it also stands for\nMagnetic Resonance. And it stands for a whole\nbunch of other things. And so you get huge\namounts of ambiguity. \"Blind\" turns out also to\ngive you various ambiguities. So it maps here to four\ndifferent concept-unique identifiers. \"Is\" is OK. \"79-year-old\" is OK. And then \"male,\" again, maps to\nfive different concept-unique identifiers. So there are all these\nproblems of over-generation", "id": "lkO2ocJBsmI_11", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And here's some more, but\nI'm going to skip over that. And then the learning\nmodel, in our case, was a support vector machine for\nthis project, in which we just said, well, throw in all the-- you know, it's\nthe kill them all, and God will sort them\nout kind of approach. So we just threw in\nall these features and said, oh, support\nvector machines are really good at picking\nout exactly what are the best features. And so we just relied on that. And sure enough, so you wind\nup with literally millions of features. But sure enough, it\nworked pretty well. And so Stat De-ID\nwas our program. And you see that on real\ndischarge summaries, we're getting precision\nand recall on PHI up around 98 and\n1/2%, 95 and 1/4%, which was much better than\nthe previous state of the art, which had been based on\nrules and dictionaries", "id": "lkO2ocJBsmI_12", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So this was a successful\nexample of that approach. And of course, this is usable\nnot only for de-identification. But it's also usable\nfor entity recognition. Because instead of\nselecting entities that are personally\nidentifiable health information, you could train it to select\nentities that are diseases or that are medications or\nthat are various other things. And so this was, in the\n2000s, a pretty typical way for people to approach\nthese kinds of problems. And it's still used today. There are tools around\nthat let you do this. And they work\nreasonably effectively. They're not state of\nthe art at the moment, but they're simpler than\nmany of today's state of the art methods. So here's another approach. This was something we published\na few years ago, where", "id": "lkO2ocJBsmI_13", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  could we predict\n30-day readmission for a psychiatric patient with\nany degree of reliability? That's a hard prediction. Willie is currently\nrunning an experiment where we're asking\npsychiatrists to predict that. And it turns out, they're\nbarely better than chance at that prediction. So it's not an easy task. And what we did is we said,\nwell, let's use topic modeling. And so we had this\ncohort of patients, close to 5,000 patients. About 10% of them\nwere readmitted with a psych diagnosis. And almost 3,000 of\nthem were readmitted with other diagnoses. So one thing this\ntells you right away is that if you're dealing\nwith psychiatric patients, they come and go to the\nhospital frequently.", "id": "lkO2ocJBsmI_14", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  of reimbursement policies of\ninsurance companies and so on. So of the 4,700, only 1,240 were\nnot readmitted within 30 days. So there's very\nfrequent bounce-back. So we said, well, let's try\nbuilding a baseline model using a support vector machine from\nbaseline clinical features like age, gender,\npublic health insurance as a proxy for\nsocioeconomic status. So if you're on Medicaid,\nyou're probably poor. And if you have\nprivate insurance, then you're probably an MIT\nemployee and/or better off. So that's a frequently used\nproxy, a comorbidity index that tells you sort\nof how sick you are from things other than\nyour psychiatric problems. And then we said,\nwell, what if we", "id": "lkO2ocJBsmI_15", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So we said, let's do\na TF-IDF calculation. So this is term frequency\ndivided by log of the document frequency. So it's sort of, how\nspecific is a term to identify a particular\nkind of condition? And we take the 1,000 most\ninformative words, and so there are a lot of these. So if you use 1,000\nmost informative words from these nearly\n5,000 patients, you wind up with something like\n66,000 words, unique words, that are informative\nfor some patient. But if you limit\nyourself to the top 10, then it only uses 18,000 words. And if you limit\nyourself to the top one, then it uses about 3,000 words. And then we said, well, instead\nof doing individual words,", "id": "lkO2ocJBsmI_16", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So topic modeling on all of\nthe words, as a bag of words-- so no sequence information,\njust the collection of words. And so we calculated\n75 topics from using LDA on all these notes. So just to remind\nyou, the LDA process is a model that says\nevery document consists of a certain mixture of topics,\nand each of those topics probabilistically\ngenerates certain words. And so you can build\na model like this, and then solve it using\ncomplicated techniques. And you'd wind up with topics,\nin this study, as follows. I don't know. Can you read these? They may be too small. So these are\nunsupervised topics.", "id": "lkO2ocJBsmI_17", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  it says patient, alcohol,\nwithdrawal, depression, drinking, and Ativan,\nETOH, drinks, medications, clinic inpatient, diagnosis,\ndays, hospital, substance, use treatment program, name. That's a de-identified\nuse/abuse problem number. And we had our experts\nlook at these topics. And they said, oh,\nwell, that topic is related to alcohol abuse,\nwhich seems reasonable. And then you see, on\nthe bottom, psychosis, thought features, paranoid\npsychosis, paranoia symptoms, psychiatric, et cetera. And they said, OK,\nthat's a psychosis topic. So in retrospect, you can\nassign meaning to these topics. But in fact, they're generated\nwithout any a priori notion of what they ought to be. They're just a\nstatistical summarization of the common co-occurrences\nof words in these documents.", "id": "lkO2ocJBsmI_18", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  used just the demographic\nand clinical variables, and you say, what's the\ndifference in survival, in this case, in\ntime to readmission between one set and\nanother in this cohort, and the answer is\nthey're pretty similar. Whereas, if you use\na model that predicts based on the baseline\nand 75 topics, the 75 topics that\nwe identified, you get a much\nbigger separation. And of course, this is\nstatistically significant. And it tells you\nthat this technique is useful for being\nable to improve the prediction of\na cohort that's more likely to be readmitted\nfrom a cohort that's less likely to be readmitted. It's not a terrific prediction.", "id": "lkO2ocJBsmI_19", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So you know, it's not like 0.99. But nevertheless, it\nprovides useful information. The same group of psychiatrists\nthat we worked with also did a study with a much larger\ncohort but much less rich data. So they got all\nof the discharges from two medical centers\nover a period of 12 years. So they had 845,000\ndischarges from 458,000 unique individuals. And they were looking for\nsuicide or other causes of death in these\npatients to see if they could predict\nwhether somebody is likely to try\nto harm themselves, or whether they're likely\nto die accidentally, which sometimes can't be\ndistinguished from suicide. So the censoring problems\nthat David talked about", "id": "lkO2ocJBsmI_20", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Because you lose\ntrack of people. It's a highly\nimbalanced data set. Because out of the\n845,000 patients, only 235 committed suicide, which is, of\ncourse, probably a good thing from a societal point\nof view but makes the data analysis hard. On the other hand, all-cause\nmortality was about 18% during nine years\nof a follow-up. So that's not so imbalanced. And then what they\ndid is they curated a list of 3,000 terms\nthat correspond to what, in the psychiatric literature,\nis called positive valence. So this is concepts like joy\nand happiness and good stuff, as opposed to negative valence,\nlike depression and sorrow and all that stuff. And they said, well, we can\nuse these types of terms in order to help distinguish\namong these patients.", "id": "lkO2ocJBsmI_21", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  for different quartiles of\nrisk for these patients, you see that there's a\npretty big difference between the different quartiles. And you can certainly\nidentify the people who are more likely to commit\nsuicide from the people who are less likely to do so. This curve is for suicide\nor accidental death. So this is a much\nlarger data set, and therefore the\nerror bars are smaller. But you see the same\nkind of separation here. So these are all\nuseful techniques. Now I'll to another approach. This was work by one of\nmy students, Yuon Wo, who was working with some\nlymphoma pathologists at Mass General. And so the approach\nthey took was", "id": "lkO2ocJBsmI_22", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  with lymphoma, can we\ntell what type of lymphoma they had from the\npathology report if we blank out the part of\nthe pathology report that says, \"I, the pathologist, think\nthis person has non-Hodgkin's lymphoma,\" or something? So from the rest of the context,\ncan we make that prediction? Now, Yuon took a kind of\ninteresting, slightly odd approach to it,\nwhich is to treat this as an unsupervised\nlearning problem rather than as a supervised\nlearning problem. So he literally\nmasked the real answer and said, if we just treat\neverything except what gives away the\nanswer as just data, can we essentially cluster that\ndata in some interesting way so that we re-identify the\ndifferent types of lymphoma?", "id": "lkO2ocJBsmI_23", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  is because lymphoma\npathologists keep arguing about how to\nclassify lymphomas. And every few years, they\nrevise the classification rules. And so part of his\nobjective was to say, let's try to provide an\nunbiased, data-driven method that may help identify\nappropriate characteristics by which to classify\nthese different lymphomas. So his approach was a tensor\nfactorization approach. You often see data\nsets like this that's, say, patient\nby a characteristic. So in this case,\nlaboratory measurements-- so systolic/diastolic blood\npressure, sodium, potassium, et cetera. That's a very vanilla\nmatrix encoding of data. And then if you add a\nthird dimension to it,", "id": "lkO2ocJBsmI_24", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  30 minutes later, 60 minutes\nlater, 90 minutes later, now you have a\nthree-dimensional tensor. And so just like you can\ndo matrix factorization, as in the picture above, where\nwe say, my matrix of data, I'm going to assume is generated\nby a product of two matrices, which are smaller in dimension. And you can train\nthis by saying, I want entries in\nthese two matrices that minimize the\nreconstruction error. So if I multiply these\nmatrices together, then I get back my\noriginal matrix plus error. And I want to\nminimize that error, usually root mean square, or\nmean square error, or something like that. Well, you can play the\nsame game for a tensor by having a so-called core\ntensor, which identifies", "id": "lkO2ocJBsmI_25", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  that dimension of your data. And then what you\ndo is the same game. You have matrices corresponding\nto each of the dimensions. And if you multiply\nthis core tensor by each of these\nmatrices, you reconstruct the original tensor. And you can train it again to\nminimize the reconstruction loss. So there are, again,\na few more tricks. Because this is\ndealing with language. And so this is a typical report\nfrom one of these lymphoma pathologists that says\nimmunohistochemical stains show that the follicles-- blah,\nblah, blah, blah, blah-- so lots and lots of details.", "id": "lkO2ocJBsmI_26", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  could be put into\nthis matrix tensor, this tensor factorization form. And what he did is to\nsay, well, let's see. If we look at a\nstatement like this, immuno stains show that\nlarge atypical cells are strongly positive for\nCD30, negative for these other surface expressions. So the sentence tells us\nrelationships among procedures, types of cells, and\nimmunologic factors. And for feature choice,\nwe can use words. Or we can use UMLS concepts. Or we can find various\nkinds of mappings. But he decided that\nin order to retain the syntactic relationships\nhere, what he would do is to use a graphical\nrepresentation that", "id": "lkO2ocJBsmI_27", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And so what you get is that\nthis creates one graph that talks about the strongly\npositive for CD30, large atypical cells, et cetera. And then you can factor\nthis into subgraphs. And then you also have\nto identify frequently occurring subgraphs. So for example,\nlarge atypical cells appears here, and also appears\nthere, and of course will appear in many other places. Yeah? AUDIENCE: Is this parsing\ndomain in language diagnostics? For example, did\nthey incorporate some sort of medical\ninformation here or some sort of linguistic-- PETER SZOLOVITS: So in\nthis particular study, he was using the Stanford\nParser with some tricks. So the Stanford\nParser doesn't know a lot of the medical words. And so he basically marked\nthese things as noun phrases.", "id": "lkO2ocJBsmI_28", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  doesn't do well with\nlong lists like the set of immune features. And so he would recognize\nthose as a pattern, substitute a single\nmade-up word for them, and that made the parser\nwork much better on it. So there were a whole\nbunch of little tricks like that in order to adapt it. But it was not a model\ntrained specifically on this. I think it's trained on\nWall Street Journal corpus or something like that. So it's general English. AUDIENCE: Those are things that\nhe did manually as opposed to, say, [INAUDIBLE]? PETER SZOLOVITS: No. He did it algorithmically,\nbut he didn't learn which algorithms to use. He made them up by hand. But then, of course,\nit's a big corpus. And he ran these\nprograms over it that did those transformations. So he calls it\ntwo-phase parsing.", "id": "lkO2ocJBsmI_29", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  in this section if you're\ninterested in the details. It's described there. So what he wound\nup with is a tensor that has patients on one\naxis, the words appearing in the text on another axis. So he's still using a\nbag-of-words representation. But the third axis is\nthese language concept subgraphs that we\nwere talking about. And then he does tensor\nfactorization on this. And what's interesting\nis that it works much better than I expected. So if you look at his technique,\nwhich he called SANTF, the precision and recall\nare about 0.72 and 0.854 macro-average and\n0.754 micro-average,", "id": "lkO2ocJBsmI_30", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  factorization results, which\nonly use patient by word or patient by subgraph, or,\nin fact, one where you simply do patient and concatenate\nthe subgraphs and the words in one dimension. So that means that this is\nactually taking advantage of the three-way relationship. If you read papers from\nabout 15, 20 years ago, people got very excited about\nthe idea of bi-clustering, which is, in modern terms,\nthe equivalent of matrix factorization. So it says given two\ndimensions of data, and I want to\ncluster things, but I want to cluster\nthem in such a way that the clustering\nof one dimension helps the clustering\nof the other dimension. So this is a formal way of doing\nthat relatively efficiently.", "id": "lkO2ocJBsmI_31", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So now I'm going to turn to\nthe last of today's big topics, which is language modeling. And this is really where\nthe action is nowadays in natural language\nprocessing in general. I would say that the\nnatural language processing on clinical data is\nsomewhat behind the state of the art in natural\nlanguage processing overall. There are fewer corpora\nthat are available. There are fewer\npeople working on it. And so we're catching up. But I'm going to lead\ninto this somewhat gently. So what does it mean\nto model a language? I mean, you could imagine\nsaying it's coming up with a set of parsing rules that\ndefine the syntactic structure of the language. Or you could imagine\nsaying, as we suggested", "id": "lkO2ocJBsmI_32", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  of semantic rules\nthat say a concept or terms in the language\ncorrespond to certain concepts and that they are\na combinatorially, functionally combined\nas the syntax directs, in order to give us a\nsemantic representation. So we don't know how to do\neither of those very well. And so the current,\nthe contemporary idea about language\nmodeling is to say, given a sequence of tokens,\npredict the next token. If you could do that\nperfectly, presumably you would have a good\nlanguage model. So obviously, you\ncan't do it perfectly. Because we don't always\nsay the same word after some sequence of\nprevious words when we speak. But probabilistically,\nyou can get close to that. And there's usually some\nkind of Markov assumption that says that the probability\nof emitting a token", "id": "lkO2ocJBsmI_33", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  only on n previous words\nrather than on all of history, on everything you've ever\nsaid before in your life. And there's a measure\ncalled perplexity, which is the entropy of the\nprobability distribution over the predicted words. And roughly speaking, it's\nthe number of likely ways that you could continue the\ntext if all of the possibilities were equally likely. So perplexity is often used, for\nexample, in speech processing. We did a study\nwhere we were trying to build a speech system that\nunderstood a conversation", "id": "lkO2ocJBsmI_34", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And we ran into real\nproblems, because we were using software that had\nbeen developed to interpret dictation by doctors. And that was very well trained. But it turned out-- we didn't\nknow this when we started-- that the language that doctors\nuse in dictating medical notes is pretty straightforward,\npretty simple. And so it's perplexity\nis about nine, whereas conversations are much\nmore free flowing and cover many more topics. And so its perplexity\nis about 73. And so the model that works\nwell for perplexity nine doesn't work as well\nfor perplexity 73. And so what this tells you about\nthe difficulty of accurately transcribing speech\nis that it's hard. It's much harder. And that's still not\na solved problem.", "id": "lkO2ocJBsmI_35", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So if you empirically\njust take all the words in all the literature of, let's\nsay, English, what you discover is that the n-th word\nis about one over n as probable as the first word. So there is a\nlong-tailed distribution. One thing you should\nrealize, of course, is if you integrate one over\nn from zero to infinity, it's infinite. And that may not be an\ninaccurate representation of language, because language\nis productive and changes. And people make up new words\nall the time and so on. So it may actually be infinite. But roughly speaking, there is\na kind of decline like this. And interestingly,\nin the brown corpus, the top 10 words make\nup almost a quarter", "id": "lkO2ocJBsmI_36", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So you write a lot of thes,\nofs, ands, a's, twos, ins, et cetera, and much less\nhematemesis, obviously. So what about n-gram models? Well, remember, if we make\nthis Markov assumption, then all we have to\ndo is pay attention to the last n tokens\nbefore the one that we're interested\nin predicting. And so people have generated\nthese large corpora n-grams. So for example, somebody,\na couple of decades ago, took all of\nShakespeare's writings-- I think they were\ntrying to decide whether he had\nwritten all his works or whether the earl\nof somebody or other was actually the guy\nwho wrote Shakespeare. You know about this controversy? Yeah. So that's why they\nwere doing it. But anyway, they\ncreated this corpus.", "id": "lkO2ocJBsmI_37", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  a vocabulary of about\n30,000 words and about 300,000 bigrams, and out of\n844 million possible bigrams. So 99.96% of bigrams\nwere never seen. So there's a certain regularity\nto his production of language. Now, Google, of course,\ndid Shakespeare one better. And they said, hmm, we can\ntake a terabyte corpus-- this was in 2006. I wouldn't be surprised if\nit's a petabyte corpus today. And they published this. They just made it available. So there were 13.6\nmillion unique words that occurred at least 200\ntimes in this tera-word corpus. And there were 1.2 billion\nfive-word sequences that occurred at least 40 times. So these are the statistics. And if you're interested,\nthere's a URL.", "id": "lkO2ocJBsmI_38", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So ceramics, collectibles,\ncollectibles-- I don't know-- occurred 55\ntimes in a terabyte of text. Ceramics collectibles fine,\nceramics collectibles by, pottery, cooking, comma, period,\nend of sentence, and, at, is, et cetera-- different number of times. Ceramics comes from\noccurred 660 times, which is reasonably large\nnumber compared to some of its competitors here. If you look at\nfour-grams, you see things like serve as the\nincoming, blah, blah, blah, 92 times; serve\nas the index, 223 times; serve as the\ninitial, 5,300 times. So you've got all\nthese statistics. And now, given those statistics,\nwe can then build a generator.", "id": "lkO2ocJBsmI_39", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Suppose I start with\nthe token, which is the beginning of a\nsentence, or the separator between sentences. And I say sample a\nrandom bigram starting with the beginning of\na sentence and a word, according to its probability,\nand then sample the next bigram from that word and\nall the other words, according to its\nprobability, and keep doing that until you hit\nthe end of sentence marker. So for example, here I'm\ngenerating the sentence, I, starts with I,\nthen followed by want, followed by two, followed\nby get, followed by Chinese, followed by food, followed\nby end of sentence. So I've just generated,\n\"I want to get Chinese food,\" which sounds\nlike a perfectly good sentence. So here's what's interesting. If you look back again\nat the Shakespeare corpus", "id": "lkO2ocJBsmI_40", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  you get stuff like\nat the top, \"To him swallowed confess here both. Which. Of save on trail for are ay\ndevice and rote life have.\" It doesn't sound terribly good. It's not very grammatical. It doesn't have that sort of\nShakespearean English flavor. Although, you do have words\nlike nave and ay and so on that are vaguely reminiscent. Now, if you go to\nbigrams, it starts to sound a little better. \"What means, sir. I confess she? Then all sorts, he\nis trim, captain.\" That doesn't make any sense. But it starts to\nsound a little better. And with trigrams, we get,\n\"Sweet prince, Falstaff shall die. Harry of Monmouth,\" et cetera.", "id": "lkO2ocJBsmI_41", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And if you go to quadrigrams,\nyou get, \"King Henry. What? I will go seek the\ntraitor Gloucester. Exeunt some of the watch. A great banquet\nserv'd in,\" et cetera. I mean, when I first saw this,\nlike 20 years ago or something, I was stunned. This is actually\ngenerating stuff that sounds vaguely\nShakespearean and vaguely English-like. Here's an example of generating\nthe Wall Street Journal. So from unigrams, \"Months\nthe my and issue of year foreign new exchanges\nSeptember were recession.\" It's word salad. But if you go to trigrams,\n\"They also point to ninety nine point six billion\nfrom two hundred four oh six three percent of the\nrates of interest stores as Mexico and Brazil.\"", "id": "lkO2ocJBsmI_42", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  writer on acid\nwriting this text. Because it has a little bit\nof the right kind of flavor. So more recently,\npeople said, well, we ought to be able to make use\nof this in some systematic way to help us with our\nlanguage analysis tasks. So to me, the first\neffort in this direction was Word2Vec, which\nwas Mikolov's approach to doing this. And he developed two models. He said, let's build a\ncontinuous bag-of-words model that says what\nwe're going to use is co-occurrence data on a\nseries of tokens in the text that we're trying to model. And we're going to\nuse a neural network model to predict the word\nfrom the words around it.", "id": "lkO2ocJBsmI_43", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  the parameters of that neural\nnetwork model as a vector. And that vector will be the\nrepresentation of that word. And so what we're\ngoing to find is that words that tend to\nappear in the same context will have similar\nrepresentations in this high-dimensional vector. And by the way,\nhigh-dimensional, people typically use like 300\nor 500 dimensional vectors. So there's a lot of-- it's a big space. And the words are\nscattered throughout this. But you get this\nkind of cohesion, where words that are\nused in the same context appear close to each other. And the extrapolation\nof that is that if words are used in the\nsame context, maybe", "id": "lkO2ocJBsmI_44", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So the other model\nis a skip-gram model, where you're doing\nthe prediction in the other direction. From a word, you're predicting\nthe words that are around it. And again, you are using\na neural network model to do that. And you use the\nparameters of that model in order to represent the\nword that you're focused on. So what came as a surprise\nto me is this claim that's in his original paper, which\nis that not only do you get this effect of locality\nas corresponding meaning but that you get relationships\nthat are geometrically represented in the space\nof these embeddings. And so what you\nsee is that if you take the encoding of the\nword man and the word woman and look at the vector\ndifference between them,", "id": "lkO2ocJBsmI_45", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  you get close to queen. And if you apply it uncle,\nyou get close to aunt. And so they showed a\nnumber of examples. And then people\nhave studied this. It doesn't hold\nit perfectly well. I mean, it's not like we've\nsolved the semantics problem. But it is a genuine\nrelationship. The place where it\ndoesn't work well is when some of these things are\nmuch more frequent than others. And so one of the examples\nthat's often cited is if you go, London is to\nEngland as Paris is to France, and that one works. But then you say as Kuala\nLumpur is to Malaysia, and that one doesn't\nwork so well. And then you go, as\nJuba or something is to whatever country\nit's the capital of.", "id": "lkO2ocJBsmI_46", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  there's very little\ndata on that. And so that doesn't\nwork so well. So there was this\nother paper later from van der Maaten\nand Geoff Hinton, where they came up with\na visualization method to take these\nhigh-dimensional vectors and visualize them\nin two dimensions. And what you see is that if\nyou take a bunch of concepts that are count concepts-- so 1/2, 30, 15, 5, 4, 2,\n3, several, some, many, et cetera-- there is a geometric\nrelationship between them. So they, in fact, do map to\nthe same part of the space. Similarly, minister, leader,\npresident, chairman, director, spokesman, chief,\nhead, et cetera form a kind of\ncluster in the space. So there's definitely\nsomething to this. I promised you that I would\nget back to a different attempt", "id": "lkO2ocJBsmI_47", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  that you want to use\nfor term-spotting and develop an automated way of\nenlarging that set of concepts in order to give you a\nricher vocabulary by which to try to identify cases\nthat you're interested in. So this was by some\nof my colleagues, including Kat, who\nyou saw on Tuesday. And they said,\nwell, what we'd like is the fully automated and\nrobust, unsupervised feature selection method that\nleverages only publicly available medical knowledge\nsources instead of VHR data. So the method that David's\ngroup had developed, which we talked about\nearlier, uses data from electronic\nhealth records, which means that you move\nto different hospitals and there may be\ndifferent conventions. And you might\nimagine that you have to retrain that sort of method,\nwhereas here the idea is", "id": "lkO2ocJBsmI_48", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  sources. So unlike that earlier model,\nhere they built a Word2Vec skip-gram model from about 5\nmillion Springer articles-- so these are published\nmedical articles-- to yield 500 dimensional\nvectors for each word. And then what they did is\nthey took the concept names that they were interested\nin and their definitions from the UMLS, and\nthen they summoned the word vectors for each\nof these words, weighted by inverse document frequency. So it's sort of a\nTF-IDF-like approach to weight different words. And then they went\nout and they said, OK, for every disease that's\nmentioned in Wikipedia, Medscape, eMedicine, the\nMerck Manuals Professional Edition, the Mayo Clinic\nDiseases and Conditions,", "id": "lkO2ocJBsmI_49", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  they used named entity\nrecognition techniques to find all the concepts that\nare related to this phenotype. So then they said, well,\nthere's a lot of randomness in these sources, and maybe\nin our extraction techniques. But if we insist that\nsome concept appear in at least three of\nthese five sources, then we can be pretty confident\nthat it's a relevant concept. And so they said,\nOK, we'll do that. Then they chose\nthe top k concepts whose embedding vectors are\nclosest by cosine distance to the embedding\nof this phenotype that they've calculated. And they say, OK,\nthe phenotype is going to be a linear combination\nof all these related concepts. So again, this is a bit\nsimilar to what we saw before. But here, instead of\nextracting the data from electronic medical\nrecords, they're", "id": "lkO2ocJBsmI_50", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  sources. And again, what you see is that\nthe expert-curated features for these five phenotypes,\nwhich are coronary artery disease, rheumatoid\narthritis, Crohn's disease, ulcerative colitis,\nand pediatric pulmonary arterial hypertension, they started\nwith 20 to 50 curated features. So these were the\nones that the doctors said, OK, these are the\nanchors in David's terminology. And then they expanded\nthese to a larger set using the technique that I just\ndescribed, and then selected down to the top n that\nwere effective in finding", "id": "lkO2ocJBsmI_51", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And this is a terrible graph\nthat summarizes the results. But what you're seeing is that\nthe orange lines are based on the expert-curated features. This is based on an earlier\nversion of trying to do this. And SEDFE is the technique\nthat I've just described. And what you see is that\nthe automatic techniques for many of these phenotypes\nare just about as good as the manually curated ones. And of course, they require\nmuch less manual curation. Because they're using this\nautomatic learning approach. Another interesting\nexample to return to the theme of\nde-identification is a couple of my\nstudents, a few years ago,", "id": "lkO2ocJBsmI_52", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  complicated architecture. So it starts with a\nbi-directional recursive neural network model that\nis implemented over the character sequences\nof words in the medical text. So why character sequences? Why might those be important? Well, consider a misspelled\nword, for example. Most of the character\nsequence is correct. There will be a bug in\nit at the misspelling. Or consider that a\nlot of medical terms are these compound\nterms, where they're made up of lots of\npieces that correspond to Greek or Latin roots. So learning those can\nactually be very helpful.", "id": "lkO2ocJBsmI_53", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  You then could\nconcatenate the results from both the left-running and\nthe right-running recursive neural network. And concatenate that with\nthe Word2Vec embedding of the whole word. And you feed that into another\nbi-directional RNN layer. And then for each word, you\ntake the output of those RNNs, run them through a feed-forward\nneural network in order to estimate the prob-- it's like a soft max. And you estimate the probability\nof this word belonging to a particular category of\npersonally identifiable health information. So is it a name? Is it an address? Is it a phone number? Is it or whatever? And then the top layer is a\nkind of conditional random field-like layer that imposes\na sequential probability", "id": "lkO2ocJBsmI_54", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  what's the next most likely\nthing that you're going to see? And so you combine that with\nthe probability distributions for each word in order to\nidentify the category of PHI or non-PHI for that word. And this did insanely well. So optimized by F1 score, we're\nup at a precision of 99.2%, recall of 99.3%. Optimized by recall,\nwe're up at about 98%, 99% for each of them. So this is doing quite well. Now, there is a non-machine\nlearning comment to make,", "id": "lkO2ocJBsmI_55", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  regulations, they\ndon't say that you must get rid of 99%\nof the personally identifying information in\norder to be able to share this data for research. It says you have to\nget rid of all of it. So no technique we\nknow is 100% perfect. And so there's a kind of\npractical understanding among people who\nwork on this stuff that nothing's\ngoing to be perfect. And therefore, that you can\nget away with a little bit. But legally, you're on thin ice. So I remember many years ago,\nmy wife was in law school. And I asked her at one point,\nso what can people sue you for? And she said,\nabsolutely anything. They may not win. But they can be a\nreal pain if you have", "id": "lkO2ocJBsmI_56", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And so this hasn't\nplayed out yet. We don't know if a\nde-identifier that is 99% sensitive\nand 99% specific will pass muster with people\nwho agree to release data sets. Because they're worried,\ntoo, about winding up in the newspaper or\nwinding up getting sued. Last topic for today-- so if you read this interesting\nblog, which, by the way, has a very good\ntutorial on BERT, he says, \"The year 2018 has been\nan inflection point for machine learning models handling\ntext, or more accurately, NLP. Our conceptual\nunderstanding of how best to represent words\nand sentences in a way that best captures underlying\nmeanings and relationships is rapidly evolving.\" And so there are a\nwhole bunch of new ideas", "id": "lkO2ocJBsmI_57", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  including ELMo, which learns\ncontext-specific embeddings, the Transformer architecture,\nthis BERT approach. And then I'll end with just\nshowing you this gigantic GPT model that was developed\nby the OpenAI people, which does remarkably better\nthan the stuff I showed you before in generating language. All right. If you look inside\nGoogle Translate, at least as of not\nlong ago, what you find is a model like this. So it's essentially an LSTM\nmodel that takes input words and munges them together\ninto some representation, a high-dimensional vector\nrepresentation, that summarizes everything that the model\nknows about that sentence", "id": "lkO2ocJBsmI_58", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Obviously, it has to be\na pretty high-dimensional representation, because your\nsentence could be about almost anything. And so it's important to\nbe able to capture all that in this representation. But basically, at\nthis point, you start generating the output. So if you're translating\nEnglish to French, these are English\nwords coming in, and these are French words\ngoing out, in sort of the way I showed you, where we're\ngenerating Shakespeare or we're generating Wall\nStreet Journal text. But the critical feature here\nis that in the initial version of this, everything\nthat you learned about this English sentence\nhad to be encoded in this one vector that got passed from\nthe encoder into the decoder, or from the source language into\nthe target language generator.", "id": "lkO2ocJBsmI_59", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  someone, namely these\nguys, came along and said, wouldn't it be nice\nif we could provide some auxiliary information\nto the generator that said, hey, which part of\nthe input sentence should you pay attention to? And of course, there's\nno fixed answer to that. I mean, if I'm translating\nan arbitrary English sentence into an arbitrary French\nsentence, I can't say, in general, look at the third\nword in the English sentence when you're generating the third\nword in the French sentence. Because that may or may\nnot be true, depending on the particular sentence. But on the other\nhand, the intuition is that there is such\na positional dependence and a dependence on what the\nparticular English word was that is an important component\nof generating the French word.", "id": "lkO2ocJBsmI_60", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  to passing along\nthe this vector that encodes the meaning\nof the entire input and the previous word that you\nhad generated in the output, in addition, we pass along this\nother information that says, which of the input words\nshould we pay attention to? And how much attention\nshould we pay to them? And of course, in the\nstyle of these embeddings, these are all represented\nby high-dimensional vectors, high-dimensional real\nnumber vectors that get combined with\nthe other vectors in order to produce the output. Now, a classical linguist\nwould look at this and retch. Because this looks nothing\nlike classical linguistics. It's just numerology that gets\ntrained by stochastic gradient", "id": "lkO2ocJBsmI_61", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  But from an engineering point\nof view, it works quite well. So then for a while, that\nwas the state of the art. And then last year, these\nguys, Vaswani et al. came along and said,\nyou know, we now have this complicated\narchitecture, where we are doing the\nold-style translation where we summarize everything\ninto one vector, and then use that to generate\na sequence of outputs. And we have this\nattention mechanism that tells us how\nmuch of various inputs to use in generating each\nelement of the output. Is the first of those\nactually necessary? And so they published this\nlovely paper saying attention is all you need,\nthat says, hey, you", "id": "lkO2ocJBsmI_62", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  model. Not only is it a\nuseful addition, but in fact, it can take the\nplace of the original model. And so the Transformer\nis an architecture that is the hottest thing\nsince sliced bread at the moment, that says,\nOK, here's what we do. We take the inputs. We calculate some\nembedding for them. We then want to\nretain the position, because of course, the sequence\nin which the words appear, it matters. And the positional encoding\nis this weird thing where it encodes using\nsine waves so that-- it's an orthogonal basis. And so it has nice\ncharacteristics. And then we run it\ninto an attention model that is essentially\ncomputing self-attention. So it's saying what-- it's like Word2Vec, except\nin a more sophisticated way.", "id": "lkO2ocJBsmI_63", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  and saying, which words is\nthis word most related to? And then, in order to\ncomplicate it some more, they say, well, we don't\nwant just a single notion of attention. We want multiple\nnotions of attention. So what does that sound like? Well, to me, it\nsounds a bit like what you see in convolutional\nneural networks, where often when you're\nprocessing an image with a CNN, you're not only applying\none filter to the image but you're applying a whole\nbunch of different filters. And because you\ninitialize them randomly, you hope that they\nwill converge to things that actually detect different\ninteresting properties of the image. So the same idea here-- that what they're\ndoing is they're", "id": "lkO2ocJBsmI_64", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  we initialize them randomly. They will evolve\ninto something that is most useful for helping us\ndeal with the overall problem. So then they run\nthis through a series of, I think, in Vaswani's paper,\nsomething like six layers that are just replicated. And there are additional things\nlike feeding forward the input signal in order to add it to\nthe output signal of the stage, and then normalizing,\nand then rerunning it, and then running it through\na feed-forward network that also has a bypass that combines\nthe input with the output of the feed-forward network. And then you do this\nsix times, or n times. And that then feeds\ninto the generator. And the generator then uses\na very similar architecture", "id": "lkO2ocJBsmI_65", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And then it samples from those\nin order to generate the text. So this is sort of\nthe contemporary way that one can do translation,\nusing this approach. Obviously, I don't have time to\ngo into all the details of how all this is done. And I'd probably\ndo it wrong anyway. But you can look at the paper,\nwhich gives a good explanation. And that blog that I\npointed to also has a pointer to another\nblog post by the same guy that does a pretty good job\nof explaining the Transformer architecture. It's complicated. So what you get out of the\nmulti-head attention mechanism is that-- here is one attention machine. And for example, the colors\nhere indicate the degree to which the encoding\nof the word \"it\"", "id": "lkO2ocJBsmI_66", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And you see that it's focused on\nthe animal, which makes sense. Because \"it,\" in\nfact, is referring to the animal in this sentence. Here they introduce\nanother encoding. And this one focuses on \"was\ntoo tired,\" which is also good. Because \"it,\" again, refers to\nthe thing that was too tired. And of course, by\nmulti-headed, they mean that it's doing\nthis many times. And so you're\nidentifying all kinds of different relationships\nin the input sentence. Well, along the same lines\nis this encoding called ELMo. People seem to like\nSesame Street characters. So ELMo is based on a\nbi-directional LSTM.", "id": "lkO2ocJBsmI_67", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  But what it does\nis, unlike Word2Vec, which built an embedding\nfor each type-- so every time the\nword \"junk\" appears, it gets the same embedding. Here what they're saying is,\nhey, take context seriously. And we're going to calculate\na different embedding for each occurrence\nin context of a token. And this turns out\nto be very good. Because it goes part\nof the way to solving the word-sense\ndisambiguation problem. So this is just an example. If you look at the word\n\"play\" in GloVe, which is a slightly more\nsophisticated variant of the Word2Vec approach,\nyou get playing, game, games, played, players, plays, player,\nplay, football, multiplayer. This all seems to\nbe about games.", "id": "lkO2ocJBsmI_68", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  that they got this from,\nthat's the most common usage of the word \"play.\" Whereas, using this\nbi-directional language model, they can separate\nout something like, \"Kieffer, the only\njunior in the group, was commended for his ability\nto hit in the clutch, as well as his all-around excellent play.\" So this is presumably\nthe baseball player. And here is, \"They\nwere actors who had been handed fat roles\nin a successful play.\" So this is a different\nmeaning of the word play. And so this embedding also\nhas made really important contributions to improving the\nquality of natural language processing by being able\nto deal with the fact that single words have multiple\nmeanings not only in English but in other languages. So after ELMo comes BERT, which\nis this Bidirectional Encoder", "id": "lkO2ocJBsmI_69", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So rather than using the LSTM\nkind of model that ELMo used, these guys say, well,\nlet's hop on the bandwagon, use the Transformer-based\narchitecture. And then they introduced\nsome interesting tricks. So one of the problems\nwith Transformers is if you stack them on\ntop of each other there are many paths from\nany of the inputs to any of the intermediate\nnodes and the outputs. And so if you're\ndoing self-attention, you're trying to figure\nout where the output should pay attention to the input,\nthe answer, of course, is like, if you're trying\nto reconstruct the input, if the input is present in\nyour model, what you will learn is that the\ncorresponding word is the right word for your output. So they have to prevent\nthat from happening. And so the way they do\nit is by masking off,", "id": "lkO2ocJBsmI_70", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  at that level. So what this is doing\nis it's a little bit like the skip-gram model\nin Word2Vec, where it's trying to predict the\nlikelihood of some word, except it doesn't know\nwhat a significant fraction of the words are. And so it can't overfit in the\nway that I was just suggesting. So this turned out\nto be a good idea. It's more complicated. Again, for the details,\nyou have to read the paper. I gave both the Transformer\npaper and the BERT paper as optional readings for today. I meant to give them\nas required readings, but I didn't do it in time. So they're optional. But there are a whole\nbunch of other tricks. So instead of using words,\nthey actually used word pieces. So think about syllables and\ndon't becomes do and apostrophe", "id": "lkO2ocJBsmI_71", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And then they discovered\nthat about 15% of the tokens to be masked seems to work\nbetter than other percentages. So those are the hidden tokens\nthat prevent overfitting. And then they do some\nother weird stuff. Like, instead of\nmasking a token, they will inject random other\nwords from the vocabulary into its place, again,\nto prevent overfitting. And then they look at\ndifferent tasks like, can I predict the next\nsentence in a corpus? So I read a sentence. And the translation is\nnot into another language. But it's predicting what the\nnext sentence is going to be. So they trained it on 800\nmillion words from something called the Books corpus\nand about 2 and 1/2", "id": "lkO2ocJBsmI_72", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And what they found\nwas that there is an enormous improvement\non a lot of classical tasks. So this is a listing of\nsome of the standard tasks for natural language processing,\nmostly not in the medical world but in the general NLP domain. And you see that you get things\nlike an improvement from 80%. Or even the GPT model\nthat I'll talk about in a minute is at 82%. They're up to about 86%. So a 4% improvement in\nthis domain is really huge. I mean, very often\npeople publish papers showing a 1% improvement. And if their corpus\nis big enough, then it's statistically\nsignificant, and therefore publishable. But it's not significant in the\nordinary meaning of the term", "id": "lkO2ocJBsmI_73", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  But doing 4% better\nis pretty good. Here we're going\nfrom like 66% to 72% from the earlier\nstate of the art-- 82 to 91; 93 to 94; 35 to\n60 in the CoLA task corpus of linguistic acceptability. So this is asking, I\nthink, Mechanical Turk people, for generated\nsentences, is this sentence a valid sentence of English? And so it's an\ninteresting benchmark. So it's producing really\nsignificant improvements all over the place. They trained two models of it. The base model is\nthe smaller one. The large model is just\ntrained on larger data sets. Enormous amount of computation\nin doing this training--", "id": "lkO2ocJBsmI_74", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  on some gigantic\ncluster of GPU machines. And so it's daunting,\nbecause you can't just crank this up on your\nlaptop and expect it to finish in your lifetime. The last thing I want to\ntell you about is this GPT-2. So this is from the\nOpenAI Institute, which is one of these\nphilanthropically funded-- I think, this one,\nby Elon Musk-- research institute\nto advance AI. And what they said is, well,\nthis is all cool, but-- so they were not using BERT. They were using the\nTransformer architecture but without the same\ntraining style as BERT. And they said, the\nsecret is going to be that we're going to apply\nthis not only to one problem", "id": "lkO2ocJBsmI_75", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So it's a multi-task\nlearning approach that says, we're going to\nbuild a better model by trying to solve a bunch of\ndifferent tasks simultaneously. And so they built\nenormous models. By the way, the task itself is\ngiven as a sequence of tokens. So for example, they\nmight have a task that says translate to French,\nEnglish text, French text. Or answer the question,\ndocument, question, answer. And so the system\nnot only learns how to do whatever\nit's supposed to do. But it even learns\nsomething about the tasks that it's being asked to work\non by encoding these and using them as part of its model. So they built four\ndifferent models. Take a look at the bottom one.", "id": "lkO2ocJBsmI_76", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  This is a very large model. And so it's a byte-level model. So they just said forget\nwords, because we're trying to do this multilingually. And so for Chinese,\nyou want characters. And for English, you might\nas well take characters also. And the system will, in\nits 1.5 billion parameters, learn all about the sequences of\ncharacters that make up words. And it'll be cool. And so then they look at a whole\nbunch of different challenges. And what you see is that the\nstate of the art before they did this on, for example,\nthe Lambada data set was that the perplexity of\nits predictions was a hundred.", "id": "lkO2ocJBsmI_77", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  is about nine. So that means that it's\nreduced the uncertainty of what to predict next\nridiculously much-- I mean, by more than\nan order of magnitude. And you get similar\ngains, accuracy going from 59% to 63% accuracy on a-- this is the children's\nsomething-or-other challenge-- from 85% to 93%-- so dramatic improvements\nalmost across the board, except for this\nparticular data set, where they did not do well. And what really blew\nme away is here's an application of this\n1.5 billion-word model that they built. So they\nsaid, OK, I give you a prompt, like the opening paragraph\nof a Wall Street Journal article or a Wikipedia article.", "id": "lkO2ocJBsmI_78", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  that I showed you before, that\njust uses the language model and picks the most\nlikely word to come next and emits that as the next word. So here is a prompt that says,\n\"A train carriage containing controlled nuclear materials\nwas stolen in Cincinnati today. Its whereabouts are unknown.\" By the way, this is made up. I mean, this is not\na real news article. And the system comes\nback with a completion that says, \"The incident\noccurred on the downtown train line, which runs from\nCovington and Ashland stations. In an email to\nOhio news outlets, the US Department\nof Energy said it's working with the Federal\nRailroad Administration to find the thief,\" et cetera. This looks astoundingly good. Now, the paper from\nwhich this comes-- this is actually from a\nblog, but they've also published a paper about it--", "id": "lkO2ocJBsmI_79", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  If you go to that page and\npick sample 1, 2, 3, 4, 5, 6, et cetera, you get\ndifferent examples that they claim are\nnot cherry-picked. And every one of\nthem is really good. I mean, you could imagine\nthis being an actual article about this actual event. So somehow or other,\nin this enormous model, and with this\nTransformer technology, and with the multi-task\ntraining that they've done, they have managed\nto capture so much of the regularity of\nthe English language that they can generate these\nfake news articles based on a prompt and make them\nlook unbelievably realistic. Now, interestingly,\nthey have chosen not to release that trained model. Because they're worried that\npeople will, in fact, do this, and that they will generate\nfake news articles all the time.", "id": "lkO2ocJBsmI_80", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  that is not nearly as good\nin terms of its realism. So that's the state of the\nart in language modeling at the moment. And as I say, the general domain\nis ahead of the medical domain. But you can bet\nthat there are tons of people who are sitting\naround looking at exactly these results and\nsaying, well, we ought to be able to\ntake advantage of this to build much better language\nmodels for the medical domain and to exploit them in order\nto do phenotyping, in order to do entity recognition,\nin order to do inference, in order to do\nquestion answering, in order to do any of\nthese kinds of topics. And I was talking to\nPatrick Winston, who is one of the good\nold-fashioned AI people, as he characterizes himself. And the thing that's a\nlittle troublesome about this", "id": "lkO2ocJBsmI_81", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  to do with anything\nthat we understand about language or about\ninference or about question answering or about anything. And so one is left with\nthis queasy feeling that, here is a wonderful engineering\nsolution to a whole set of problems, but\nit's unclear how it relates to the original goal\nof artificial intelligence, which is to understand something\nabout human intelligence by simulating it in a computer. Maybe our BCS\nfriends will discover that there are, in fact,\ntransformer mechanisms deeply buried in our brain. But I would be surprised\nif that turned out to be exactly the case. But perhaps there is\nsomething like that going on. And so this leaves an\ninteresting scientific conundrum of,\nexactly what have we learned from this type of very,\nvery successful model building?", "id": "lkO2ocJBsmI_82", "title": "8. Natural Language Processing (NLP), Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Thank you.", "id": "lkO2ocJBsmI_83"}, {"text": "  PETER SZOLOVITS: Fortunately,\nI have a guest today, Dr. Adam Wright, who will be doing\nan interview-style session and will answer\nquestions for you. This is Adam's bread\nand butter, exactly how to translate this kind of\ntechnology into the clinic. He's currently in the partner\nsystem at the Brigham, I guess. But he's about to become a\ntraitor and leave us in Boston and occupy a position at\nVanderbilt University, for which we wish him luck. But I'm glad that we caught him\nbefore he leaves this summer. OK, so quite frankly, I\nwish that I could tell you", "id": "ZQu2B3GyI_k_0", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that you're going to hear from\nme during the prepared part of my talk. And maybe Adam will cheer us\nup and make us more optimistic, based on his experience. So you may have\nnoticed that AI is hot. So HIMSS, for example, is the\nHealth Information Management Systems Society. It's a big-- they\nhold annual meetings and consist of a lot of\nvendors and a lot of academics. And it's one of these huge\ntrade show kinds of things, with balloons hanging over\nbooths and big open spaces. So for example,\nthey're now talking about a AI-powered health care. On the other hand, it's\nimportant to remember this graph. So this is the sort of\ntechnology adoption graph.", "id": "ZQu2B3GyI_k_1", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And what you see\nhere is that R&D-- that's us-- produces some\nwonderful, interesting idea. And then all of a sudden,\npeople get excited about it. So who are the people that\nget most excited about it? It's the people who\nthink they're going to make a fortune from it. And these are the so-called\nvulture capitalists-- venture capitalists. And so the venture\ncapitalists come in and they encourage\npeople like us to go out and found companies-- or if\nnot us, then our students to go found companies. And figure out how to\nturn this nascent idea into some important\nmoneymaking enterprise. Now the secret of\nventure capital is that they know that about 90%\nof the companies that they fund", "id": "ZQu2B3GyI_k_2", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  They're going to do very badly. And so as a result, what they\nhope for and what they expect-- and what the good\nones actually get-- is that one in 10 that\nbecomes successful makes so much money\nthat it makes up for all of the investment that\nthey poured into the nine out of 10 that do badly. So I actually\nremember in the 1990s, I was helping a\ngroup pitch a company to Kleiner Perkins, which\nis the big venture-- one of the big venture capital\nfunds in Silicon Valley. And we walked into\ntheir boardroom and they had a copy of\nthe San Jose Mercury News, which is the local newspaper for\nSilicon Valley, on their table. And they were just\nbeaming, because there was an article that said\nthat in the past year,", "id": "ZQu2B3GyI_k_3", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  had been by their company. But that's pretty good, right? If you get two winners\nand two really bad losers, you're making tons\nand tons of money. So they were in a good\nmood and they funded us. We didn't make them any money. So what you see on this\ncurve is that there is a kind of set of\nrising expectations that comes from the development\nof these technologies. And you have some\nearly adopters. And then you have the\nnewspapers writing about how this is the\nrevolution and everything will be different from here on out. Then you have some\nadditional activity beyond the early adopters. And then people start\nlooking at this and going, well, it really isn't as good\nas it's cracked up to be.", "id": "ZQu2B3GyI_k_4", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  there's some consolidation\nand some failures. And people have to go\nback to venture capital to try to get more\nmoney in order to keep their companies going. And then there's\na kind of trough, where people go, oh well, this\nwas another of these failed technological innovations. Then gradually, you start\nreaching what this author calls the slope of enlightenment,\nwhere people realize that, OK, it's not really as bad as\nwe thought it was when it didn't meet our lofty expectations. And then gradually,\nif it's successful, then you get multiple\ngenerations of the product and it does achieve adoption. The adoption almost\nnever reaches the peak that it was expected to reach at\nthe time of the top of the hype", "id": "ZQu2B3GyI_k_5", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  But it becomes useful. It becomes profitable. It becomes productive. Now I've been around long enough\nto see a number of these cycles go by. So in the 1980s, for example,\nat a time that was now jokingly referred to as AI summer-- where people were building\nexpert systems and these expert systems were going to just\nrevolutionize everything-- I remember going to a conference\nwhere the Campbell Soup Company had built an\nexpert system that was based on the expertise\nof some old timers who were retiring. And what this expert\nsystem did is it told you how to clean the vats of soup-- y know, these giant\nmillion-gallon things where they make soup-- when you're switching from\nmaking one kind of soup to another.", "id": "ZQu2B3GyI_k_6", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and you switch to\nmaking beef barley soup, you don't need to\nclean the vat at all. Whereas if you're switching\nfrom something like clam chowder to a consomme, then you need\nto clean it really well. So this was exactly the kind\nof thing that they were doing. And there were\nliterally thousands of these applications\nbeing built. At the top of the\nhype cycle, all kinds of companies, like Campbell's\nSoup and the airlines and everybody was investing\nhuge amounts of money into this. And then there was a kind\nof failure of expectations. These didn't turn out to be\nas good as people thought they were going to be,\nor as valuable as people thought they were going to be. And then all of a\nsudden came AI winter. So AI winter followed AI summer. There was no AI fall,\nexcept in a different sense", "id": "ZQu2B3GyI_k_7", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And all of a sudden,\nfunding dried up and the whole thing\nwas declared a failure. But in fact today, if you go\nout there and you look at-- Microsoft Excel has an expert\nsystem-based help system bundled inside it. And there are tons\nof such applications. It's just that now\nthey're no longer considered cutting-edge\napplications of artificial intelligence. They're simply considered\nroutine practice. So they've become\nincorporated, without the hype, into all kinds of\nexisting products. And they're serving\na very useful role. But they didn't make\nthose venture capital firms the tons of money\nthat they had hoped to make. There was a similar\nboom and bust cycle in the 2000s around the\ncreation of the worldwide web and e-commerce. OK, so e-commerce.", "id": "ZQu2B3GyI_k_8", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of expectations. Then around the year 2000,\nthere was a big crash, where all of a sudden\npeople realized that the value in\nthese applications was not as high as what\nthey expected it to be. Nevertheless, you know\nAmazon is doing just fine. And there are plenty of\nonline e-commerce sites that are in perfectly good\noperating order today. But it's no longer the same\nhype about this technology. It's just become an\naccepted part of the way that you do business\nin almost everything. Yeah. AUDIENCE: When you\nspeak of expert systems, does that mean\nrule-based systems? PETER SZOLOVITS: They were\neither rule-based or pattern matching systems. There were two basic kinds. I think a week from today,\nI'm going to talk about some of that and how it relates\nto modern machine learning. So we'll see some examples.", "id": "ZQu2B3GyI_k_9", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So I assume most of\nyou remember when Watson hit the big time by\nbeating the Jeopardy champions. This was back in the\nearly 2010s or something. I don't remember\nexactly which year. And they had, in fact, built\na really impressive set of technologies that\nwent out and read all kinds of online\nsources and distilled them into a kind of representation\nthat they could very quickly look up things when they\nwere challenged with a Jeopardy question. And then it had a\nsophisticated set of algorithms that would\ntry to find the best answer for some question. And they even had all kinds of\nbizarre special-purpose things. I remember there was a\nprobabilistic model that", "id": "ZQu2B3GyI_k_10", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  squares were most likely to\nbe on the Jeopardy board. And then they did a utility\ntheoretic calculation to figure out if they\ndid hit the Daily Double, what was the optimum\namount of money to bet, based on the\nmachine's performance, in order to optimize. They decided that\nhumans typically don't bet enough when they have\na chance on the Daily Double. So there was a lot of\nvery special-purpose stuff done for this. So this was a huge\npublicity bonanza. And IBM decided that next they\nwere going to tackle medicine. So they were going to\ntake this technology and apply it to medicine. They were going to read\nall of the medical journals and all of the electronic\nmedical records that they could\nget their hands on. And somehow this\ntechnology would again distill the right\ninformation, so that they", "id": "ZQu2B3GyI_k_11", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  except not stated in\nits funny backward way. Where you might say,\nOK, for this patient, what is the optimum therapy? And it would go out and\nuse the same technology to figure that out. Now that was a perfectly\nreasonable thing to try. The problem they ran\ninto was this hype cycle, that the people who\nmade this publicly-known were their marketing people\nand not their technical people. And the marketing people\noverpromised like crazy. They said surely\nthis is just going to solve all these problems. And we won't need anymore\nresearch in this area, because man, we got it. I'm overstating it, even from\nthe marketing point of view. And so Watson for Oncology used\nthis cloud-based supercomputer to digest massive\namounts of data.", "id": "ZQu2B3GyI_k_12", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So I'm going to go into\na little bit of detail about what some of\ntheir problems were. This is from an article\nin this journal, Statnews, which did an investigative piece\non what happened with Watson. So you know, they\nsay what I just said. Breathlessly promoting\nits signature brand, IBM sought to capture\nthe world's imagination and quickly zeroed in on\na high-profile target, which was cancer. So this was going to solve\nthe problem of some patient shows up, is\ndiagnosed with cancer, and you want to know how\nto treat this person. So this would use\nall of the literature and all of everything\nthat it had gathered from\nprevious treatments of previous patients. And it would give you\nthe optimal solution. Now it has not been a success. There are a few dozen hospitals\nthat have adopted the system.", "id": "ZQu2B3GyI_k_13", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And the foreigners\ncomplain that its advice is biased toward\nAmerican patients and American approaches. To me, the biggest problem\nis that they haven't actually published anything\nthat validates, in a scientific sense,\nthat this is a good idea. That it's getting\nthe right answers. My guess is the\nreason for this is because it's not getting\nthe right answers, a lot of the time. But that doesn't prevent\nmarketing from selling it. The other problem is that they\nmade a deal with Memorial Sloan Kettering-- which is one of\nthe leading cancer hospitals in the country-- to say, we're going to work with\nyou guys and your oncologists in order to figure out what\nreally is the right answer. So I think they tried to do\nwhat their marketing says that they're doing,\nwhich is to really derive", "id": "ZQu2B3GyI_k_14", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and looking at past cases. But I don't think that\nworked well enough. And so what they wound\nup doing is turning to real oncologists,\nsaying, what would you do under these circumstances? And so what they\nwound up building is something like a\nrule-based system that says, if you see the\nfollowing symptoms and you have the\nfollowing genetic defects, then this is the\nright treatment. So the promise\nthat this was going to be a machine learning system\nthat revolutionized cancer care by finding the\noptimal treatment really is not what they provided. And as the article\nsays, the system doesn't really\ncreate new knowledge. So it's AI only in the\nsense of providing a search engine that, when it\nmakes a recommendation, can point you to articles that\nare a reasonable reflection", "id": "ZQu2B3GyI_k_15", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Well, I'm going to stop\ngoing through this litany. But you'll see it in the\nslides, which we'll post. They had a big contract\nwith M.D. Anderson, which is another leading cancer\ncenter in the United States. M.D. Anderson spent about\n$60 million on this contract, implementing it. And they pulled the plug on it,\nbecause they decided that it just wasn't doing the job. Now by contrast, there was a\nmuch more successful attempt years ago, which was less driven\nby marketing and more driven by medical need. And the idea here was CPOE,\nstands for Computerized Physician Order Entry. The idea behind\nCPOE was that if you want to affect the\nbehavior of clinicians", "id": "ZQu2B3GyI_k_16", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  is to make sure that they are\ninteracting with the computer. So that when they\norder, for example, some insanely expensive drug,\nthe system can come back and say, hey, do you\nrealize that there's a drug that costs 1/100\nas much, which according to the clinical trials\nthat we have on record is just as effective as the\none that you've ordered? And so for example, here at\nthe Beth Israel many years ago, they implemented a\nsystem like that. And in the first\nyear, they showed that they saved something like\n$16 million in the pharmacy, just by ordering cheaper\nvariants of drugs that could have been very expensive. And they also found\nthat the doctors who were doing the ordering were\nperfectly satisfied with that, because they just didn't know\nhow expensive these drugs were. That's not one of the things\nthat they pay attention to.", "id": "ZQu2B3GyI_k_17", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that are driven by this. And again, here are\nsome statistics. You can reduce\nerror rates by half. You can reduce severe\nmedication errors by 88%. You can have a 70% reduction in\nantibiotic-related adverse drug events. You can reduce length of stay,\nwhich is another big goal that people go after. And at least if\nyou're an optimist, you can believe\nthese extrapolations that say, well, we could\nprevent 3 million adverse drug events at big city hospitals\nin the United States if everybody used\nsystems like this. So the benefits\nare that it prompts with warnings against possible\ndrug interactions, allergies,", "id": "ZQu2B3GyI_k_18", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  It can be kept up to date by\nsome sort of mechanism where people read the\nliterature and keep updating the databases\nthis is driven from. And it can do mechanical\nthings like eliminate confusion about drug names\nthat sound similar. Stuff like that. So the Leapfrog Group, which\ndoes a lot of meta analyses and studies of what's\neffective, really is behind this and\npushing it very strongly. Potential future\nbenefits, of course, are that if the kinds of\nmachine learning techniques that we talk about\nbecome widely used, then these systems can be\nupdated automatically rather than by manual review. And you can gain the advantages\nof immediate feedback as new information\nbecomes available. Now the adoption of CPOE was\nrecommended by the National", "id": "ZQu2B3GyI_k_19", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  They wanted every hospital\nto use this by 1999. And of course, it\nhasn't happened. So I couldn't find current\ndata, but 2014 data shows that CPOE, for example,\nfor medication orders, is only being used in\nabout 25% of the hospitals. And at that time, people were\nextrapolating and saying, well, it's not going to reach 80%\npenetration until the year 2029. So it's a very slow\nadoption cycle. Maybe it's gotten better. The other problem-- and one of\nthe reasons for resistance-- is that it puts additional\nstresses on people. So for example, this\nis a study of how pharmacists spend their time. So clinical time is useful.", "id": "ZQu2B3GyI_k_20", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  helping them figure\nout appropriate dosage for patients. Or they're talking to\npatients, explaining to them how to take their medications,\nwhat side effects to watch out for, et cetera. These distributive tasks--\nit's a funny term-- mean the non-clinical part\nof what they're doing. And what you see is\nthat hospitals that have adopted CPOE, they wind up\nspending a little bit more time on the distributive tasks\nand a little bit less time on the clinical tasks. Which is probably not\nin the right direction, in terms of what\npharmacists were hoping for out of systems like this. Now people have\nstudied the diffusion of new medical technologies. And I think I'll just\nshow you the graph. So this is in England, but this\nis the adoption for statins.", "id": "ZQu2B3GyI_k_21", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  statins is the drug that\nkeeps your cholesterol low. From the time they\nwere introduced until they were being\nused, essentially, at 100% of places was about\nfive and a half, six years. So reasonably fast. If you look at the adoption\nof magnetic resonance imaging technology, it took\nfive years for it to have any adoption whatsoever. And that's because it\nwas insanely expensive. So there were all\nkinds of limitations. You know, even in\nMassachusetts, you have to get permission\nfrom some state committee to buy a new MRI machine. And if another hospital in\nyour town already had one, then they would say, well,\nyou shouldn't buy one because you should be able to\nuse this other hospital's MRI machine. Same thing happened with CT. But as soon as those\nlimitations were lifted, boom.", "id": "ZQu2B3GyI_k_22", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Whereas stents, I\nactually don't know why they were delayed by that long. But this is for people with\nblockages in coronary arteries or other arteries. You can put in a\nlittle mesh tube that just keeps that artery open. And that adoption\nwas incredibly quick. So different things get\nadopted at different rates. Now the last topic I want\nto talk about before-- yeah. AUDIENCE: So what\nhappens in those years where you just have spikes? What's doing it? PETER SZOLOVITS: So\naccording to those authors, in the case of stents,\nthere were some champions of the idea of stenting\nwho went around and convinced their\ncolleagues that this was", "id": "ZQu2B3GyI_k_23", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So there was just an\nexplosive growth in it. In the other technologies,\nin the MRI case, money mattered a lot because\nthey're so expensive. Stents are relatively cheap. And in the case\nof statins, those are also relatively cheap. Or they've become cheap\nsince they went off patent. Originally, they were\nmuch more expensive. But there are still\nadoption problems. So for example, there\nwas a recommendation-- I think about 15, maybe\neven 20 years ago-- that said that anybody\nwho has had a heart attack or coronary\nartery disease should be taking beta blockers. And I don't remember what\nthe adoption rate is today, but it's only on\nthe order of a half. And so why?", "id": "ZQu2B3GyI_k_24", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  For reasons not\nquite understood, it reduces the probability\nof having a second heart attack by about 35%. So it's a really\ncheap protective way of keeping people healthier. And yet it just hasn't\nsuffused practice as much as people think it should have. All right. So how do we assure the\nquality of these technologies before we foist\nthem on the world? This is tricky. So John Ioannidis, a\nStanford professor, has made an extremely\nsuccessful career out of pointing out that most\nbiomedical research is crap. It can't be reproduced. And there are some\nfamous publications that show that people have\ntaken some area of biomedicine,", "id": "ZQu2B3GyI_k_25", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  studies. And they've gone to\nthe lab and they've tried to replicate\nthose studies. Half the time or\nthree-quarters of the time, they fail to do so. You go, oh my god,\nthis is horrible. It is horrible. Yeah. AUDIENCE: You mean like\nthey failed to do so, so they won't reproduce\nthe exact same results? Or what exactly-- PETER SZOLOVITS:\nWorse than that. So it's not that there\nare slight differences. It's that, for\nexample, a result that was shown to be statistically\nsignificant in one study, when they repeat the\nstudy, is no longer statistically significant. That's bad, if you base policy\non that kind of decision. So Ioannidis has a\nsuggestion, which would probably help a lot. And that is,\nbasically, make known", "id": "ZQu2B3GyI_k_26", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So the problem is that if\nyou give me a big data set and I start mining\nthis data set, I'm going to find tons and tons\nof interesting correlations in this data. And as soon as I get one\nthat has a good p value, my students and I go, fantastic. Time to publish. Now consider the\nfact that I'm not the only person in this role. So you know, David's group\nis doing the same thing. And John Guttag's and\nRegina Barzilay's and all of our colleagues at every\nother major university and hospital in\nthe United States. So there may be\nhundreds of people who are mining this data. And each of us has slightly\ndifferent ways of doing it. We select our cases differently. We preprocess the\ndata differently. We apply different learning\nalgorithms to them.", "id": "ZQu2B3GyI_k_27", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  are going to find interesting\nresults, interesting patterns. And of course, those are\nthe ones that get published. Because if you don't find\nan interesting result, you're not going to submit\nit to a journal and say, you know I looked for the\nfollowing fact phenomenon and I was unable to find it. Because the journal\nsays, well, that's not interesting to anybody. So Ioannidis is recommending\nthat, basically, every study that anybody\nundertakes should be registered. And if you don't get\na significant result, that should be known. And this would allow\nus to make at least some reasonable estimate of\nwhether the significant results that were gotten are just\nthe statistical outliers that happened to reach p equal 0.05\nor whatever your threshold is, or whether it's a real effect\nbecause not that many people", "id": "ZQu2B3GyI_k_28", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Yeah. AUDIENCE: [INAUDIBLE]\nwhy do you think this is? Is it because of the size\nof some core patients? Or bias in the assay? Or just purely\nrandomness in the study? PETER SZOLOVITS: It\ncould be any of those. It could be that your\nhospital has some biased data collection. And so you find an effect. My hospital doesn't,\nand so I don't find it. It could be that we just\nrandomly sub-sampled a different sample\nof the population. So it's very interesting. Last year I was\ninvited to a meeting by Jeff Drazen, who's the\nexecutive editor of the New England Journal. And he's thinking about-- has not decided--\nbut he's thinking about a policy for the\nNew England Journal, which is like the top medical journal,\nthat says that he will not publish any result unless\nit's been replicated", "id": "ZQu2B3GyI_k_29", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So that's interesting. And that's an attempt to fight\nback against this problem. It's a different solution than\nwhat Ioannidis is recommending. So this was a study\nby Enrico Carrara. And he's talking about\nwhat it means to replicate. And again, I'm not going\nto go through all this. But there's the notion\nthat replication might mean exact replication, i.e. You do exactly the same thing on\nexactly the same kind of data, but in a different data set. And then partial replication,\nconceptual replication, which says, you follow\nthe same procedures but in a different environment. And then quasi replication-- either partial or conceptual. And these have various\ncharacteristics that you can look at. It's an interesting framework.", "id": "ZQu2B3GyI_k_30", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  The first edition of this\nbook, Evaluation Methods in Biomedical Informatics,\nwas called Evaluation Methods in Medical Informatics\nby the same authors and was published\na long time ago. I can't remember. This one is relatively recent. And so they do a multi-hundred\npage, very detailed evaluation of exactly how one\nshould evaluate clinical systems like this. And it's very careful\nand very cautious, but it's also very conservative. So for example, one of the\nthings that they recommend is that the people\ndoing the evaluation should not be the people\nwho developed the technique, because there's innately bias. You know, I want my\ntechnique to succeed. And so they say, hand\nit off to somebody else who doesn't have\nthat same vested interest.", "id": "ZQu2B3GyI_k_31", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So Steve Pauker and\nI wrote a response to one of their early\npapers recommending this that said, well, that's\nso conservative that it sort of throws the baby\nout with the bathwater. Because if you make it so\ndifficult to do an evaluation, you'll never get\nanything past it. So we proposed instead a\nkind of staged evaluation that says, first of all, you\nshould do regression testing so that every time you use\nthese agile development methods, you should have the set of\ncases that your program has worked on before. You should\nautomatically rerun them and see which ones\nyou've made better and which ones\nyou've made worse. And that will give\nyou some insight into whether what you're\ndoing is reasonable. Then you might also\nbuild tools that look at automating ways of\nlooking for inconsistencies", "id": "ZQu2B3GyI_k_32", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Then you have retrospective\nreview, judged by clinicians. So you run a program\nthat you like over a whole bunch\nof existing data, like what you're doing with\nMimic or with Market Scan. And then you do\nit prospectively, but without actually\naffecting patients. So you do it in real time\nas the data is coming in, but you don't tell anybody\nwhat the program results in. You just ask them to\nevaluate in retrospect to see whether it was right. And you might say, well,\nwhat's the difference between collecting\nthe data in real time and collecting the\ndata retrospectively? Historically, the answer\nis there is a difference. So circumstances differ. The mechanisms that you have\nfor collecting the data differ. So this turns out to\nbe an important issue.", "id": "ZQu2B3GyI_k_33", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  where you're interested in\nevaluating both the answer that you get from the\nprogram, and ultimately the effect on health outcomes. So if I have a decision\nsupport system, the ultimate proof\nof the pudding is if I run that\ndecision support system. I give advice to\nclinicians, the clinicians change their behavior\nsometimes, and the patients get a better outcome. Then I'm convinced that\nthis is really useful. But you have to\nget there slowly, because you don't want to\ngive them worse outcomes. That's unethical and\nprobably illegal. And you want to compare\nthis to the performance of unaided doctors. So the Food and\nDrug Administration has been dealing with this\nissue for many, many years. I remember talking to\nthem in about 1976, when", "id": "ZQu2B3GyI_k_34", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  programs for diagnosis\nand therapy selection. And they said, well, how\nshould we regulate these? And my response at the\ntime was, God help us. Keep your hands off. Because if you regulate\nit, then you're going to slow down progress. And in any case, none of\nthese programs are being used. These programs are\nbeing developed as experimental programs\nin experimental settings. They're not coming\nanywhere close to being used on real patients. And so there is not\na regulatory issue. And about every five years, FDA\nhas revisited that question. And they have continued to make\nessentially the same decision, based on the rationale\nthat, for example, they don't regulate books. If I write a textbook\nthat explains something about medicine, the\nFDA is not going", "id": "ZQu2B3GyI_k_35", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And the reason is\nbecause the expectation is that the textbook is\nmaking recommendations, so to speak, to clinical\npractitioners who are responsible experts themselves. So the ultimate responsibility\nfor how they behave rests with them and\nnot with the textbook. And they said, we're going to\ntreat these computer programs as if they were dynamic\ntextbooks, rather than colleagues who are acting\nindependently and giving advice. Now as soon as you try\nto give that advice, not to a professional,\nbut to a patient, then you are immediately under the\nregulatory auspices of FDA. Because now there is no\nprofessional intermediate that can evaluate the\nquality of that advice. So what FDA has done,\njust in the past year, is they've said that\nwe're going to treat", "id": "ZQu2B3GyI_k_36", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And we're going to apply the\nsame regulatory requirements that we have for these\ndevices, except we don't really know how to do this. So there's a kind of\nexperiment going on right now where they're saying, OK,\nsubmit applications for review of these devices to us. We will review them. And we will use these criteria-- product quality, patient\nsafety, clinical responsibility, cybersecurity responsibility,\nand a so-called proactive culture in the organization\nthat's developing them-- in order to make a judgment\nof whether or not to let you proceed with marketing\none of these things. So if you look, there are\nin fact about 10 devices,", "id": "ZQu2B3GyI_k_37", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that have been\napproved so far by FDA. And almost all of them\nare imaging devices. They're things that do\nconvolutional networks over one thing or another. And so here are\njust a few examples. Imagen has OsteoDetect, which\nanalyzes two-dimensional X-ray images for signs of\ndistal radius fracture. So if you break your\nwrist, then this system will look at the X-ray\nand decide whether or not you've done that. Here's one from IDx, which\nlooks at the photographs of your retina and\ndecides whether you have diabetic retinopathy. And actually, they've\npublished a lot of papers that show that they\ncan also identify heart disease and stroke risk and\nvarious other things from those same photographs. So FDA has granted them\napproval to market this thing.", "id": "ZQu2B3GyI_k_38", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  CT scans for ER patients\nand is looking for blockages and major brain blood vessels. So this can obviously\nlead to a stroke. And this is an automated\ntechnique that does that. Here's another one. Arterys measures and tracks\ntumors or potential cancers in radiology images. So these are the ones\nthat have been approved. And then I just\nwanted to remind you that there's actually\nplenty of literature about this kind of stuff. So the book on the left\nactually comes out next week. I got to read a pre-print\nof it, by Eric Topol, who's one of these\ndoctors who writes", "id": "ZQu2B3GyI_k_39", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And he actually goes\nthrough tons and tons of examples of not\nonly the systems that have been approved\nby FDA, but also things that are in\nthe works that he's very optimistic that these\nwill again revolutionize the practice of medicine. Bob Wachter, who wrote the book\non the left a couple of years ago, is a little\nbit more cautious because he's chief of\nmedicine at UC San Francisco. And he wrote this\nbook in response to them almost killing\na kid by giving him a 39x overdose of a medication. They didn't quite succeed\nin killing the kid. So it turned out OK. But he was really\nconcerned about how this wonderful technology led\nto such a disastrous outcome. And so he spent a\nyear studying how these systems were being\nused, and writes a more cautionary tale.", "id": "ZQu2B3GyI_k_40", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  is a professor at the Brigham\nand Harvard Medical School. Please come and join me, and\nwe can have a conversation. ADAM WRIGHT: So my\nname is Adam Wright. I'm an associate professor of\nmedicine at Harvard Medical School. In that role, I lead\na research program and I teach the introduction to\nbiomedical informatics courses at the medical school. So if you're interested\nin the topics that Pete was\ntalking about today, you should definitely\nconsider cross-registering in VMI 701 or 702. The medical school\ncertainly always could use a few more\nenthusiastic and technically-minded machine\nlearning experts in our course. And then I have a\noperational job at Partners. Partners is the\nhealth system that includes Mass General\nHospital and the Brigham and some community hospitals. And I work on\nPartners eCare, which is our kind of cool\nbrand name for Epic.", "id": "ZQu2B3GyI_k_41", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And I help oversee the clinical\ndecision support there. So we have a decision\nsupport team. I'm the clinical lead for\nmonitoring and evaluation. And so I help make sure\nthat our decision support systems of the type that Pete's\ntalking about work correctly. So that's my job at the\nBrigham and at Partners. PETER SZOLOVITS: Cool. And I appreciate it very much. ADAM WRIGHT: Thanks. I appreciate the invitation. It's fun to be here. PETER SZOLOVITS: So Adam,\nthe first obvious question is what kind of\ndecision support systems have you guys\nactually put in place? ADAM WRIGHT: Absolutely. So we've had a long history\nat the Brigham and Partners of using decision support. Historically, we developed our\nown electronic health record, which was a little bit unusual. About three years\nago, we switched from our self-developed\nsystem to Epic, which is a very widely-used\ncommercial electronic health record. And to the point that\nyou gave, we really started with a lot of\nmedication-related decision support. So that's things like drug\ninteraction, alerting. So you prescribe two drugs that\nmight interact with each other.", "id": "ZQu2B3GyI_k_42", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  no machine learning or anything\ntoo complicated-- that says, we think this drug might\ninteract with this. We raise an alert to the\ndoctor, to the pharmacist. And they make a decision,\nusing their expertise as the learned\nintermediary, that they're going to continue with\nthat prescription. Let's have some dosing\nsupport, allergy checking, and things like that. So our first set\nof decision support really was around medications. And then we turned to\na broader set of things like preventative\ncare reminders, so identifying patients that\nare overdue for a mammogram or a pap smear or that\nmight benefit from a statin or something like that. Or a beta blocker, in the case\nof acute myocardial infarction. And we make suggestions\nto the doctor or to other members of the\ncare team to do those things. Again, those historically\nhave largely been rule-based. So some experts sat down and\nwrote Boolean if-then rules, using variables that are\nin a patient's chart. We have increasingly,\nthough, started trying to use some\npredictive models for things like readmission or whether a\npatient is at risk of falling down in the hospital. A big problem that\npatients often encounter is they're in the hospital,\nthey're kind of delirious.", "id": "ZQu2B3GyI_k_43", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  It's dark. They get up to go\nto the bathroom. They trip on their IV\ntubing, and then they fall and are injured. So we would like to prevent\nthat from happening. Because that's obviously\nkind of a bad thing to happen to you once\nyou're in the hospital. So we have some machine\nlearning-based tools for predicting patients\nthat are at risk for falls. And then there is a\nset of interventions like putting the bed rails up\nor putting an alarm that buzzes when if they get out of bed. Or in more extreme cases,\nhaving a sitter, like a person who actually sits in\nthe room with them and tries to keep\nthem from getting up or assists them to the bathroom. Or calls someone who can\nassist them to the bathroom. So we have increasingly\nstarted using those machine learning tools. Some of which we get\nfrom third parties, like from our electronic\nhealth record vendor, and some of which we\nsort of train ourselves on our own data. That's a newer pursuit for\nus, is this machine learning. PETER SZOLOVITS: So\nwhen you have something like a risk model,\nhow do you decide where to set the threshold? You know, if I'm at\n53% risk of falling,", "id": "ZQu2B3GyI_k_44", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  ADAM WRIGHT: It's\ncomplicated, right? I mean, I would like\nto say that what we do is a full kind of\nutility analysis, where we say, we pay a\nsitter this much per hour. And the risk of\nfalling is this much. And the cost of a fall-- most patients who\nfall aren't hurt. But some are. And so you would\ncalculate the cost-benefit of each of those\nthings and figure out where on the ROC curve you\nwant to place yourself. In practice, I think we\noften just play it by ear, in part because a\nlot of our things are intended to be suggestions. So our threshold for\nsaying to the doctor, hey, this patient is at\nelevated risk for fall, consider doing\nsomething, is pretty low. If the system were, say,\nautomatically ordering a sitter, we might\nset it higher. I would say that's\nan area of research. I would also say that one\nchallenge we have is we often set and forget\nthese kinds of systems. And so there is kind of\nfeature drift and patients change over time. We probably should do a better\njob of then looking back to see how well they're\nactually working", "id": "ZQu2B3GyI_k_45", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Really good question. PETER SZOLOVITS: But\nthese are, of course, very complicated decisions. I remember 50 years ago talking\nto some people in the Air Force about how much should they\ninvest in safety measures. And they had a utility\ntheoretic model that said, OK, how much\ndoes it cost to replace a pilot if you kill them? ADAM WRIGHT: Yikes. Yeah. PETER SZOLOVITS: And this\nwas not publicized a lot. ADAM WRIGHT: I mean,\nwe do calculate things like quality-adjusted\nlife-years and disability-adjusted life-years. So there is-- in all of medicine\nas people deploy resources, this calculus. And I think we tend to\nassign a really high weight to patient harm, because\npatient harm is-- if you think about the\noath the doctors swear, first do no harm. The worst thing we can do\nis harm you in the hospital. So I think we have a pretty\nstrong aversion to do that. But it's very hard to\nweigh these things.", "id": "ZQu2B3GyI_k_46", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  is that different doctors\nwould make different decisions. So if you put the same\npatient in front of 10 doctors and said, does this\npatient need a sitter? Maybe half would say yes\nand half would say no. So it's especially\nhard to know what to do with a decision\nsupport system if the humans can't\nagree on what you should do in that situation. PETER SZOLOVITS: So the\nother thing we talked about on the phone yesterday is I was\nconcerned-- a few years ago, I was visiting one of these\naugust Boston-area hospitals and asked to see an example of\nsomebody interacting with this Computerized Physician\nOrder Entry system. And the senior resident who\nwas taking me around went up to the computer\nand said, well, I think I remember\nhow to use this. And I said, wait a minute. This is something you're\nexpected to use daily. But in reality, what\nhappens is that it's not the senior doctors or even\nthe medium senior doctors.", "id": "ZQu2B3GyI_k_47", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  who actually use the systems. ADAM WRIGHT: This is true. PETER SZOLOVITS: And\nthe concern I had was that it takes a junior\nresident with a lot of guts to go up to the\nchief of your service and say, doctor x, even\nthough you asked me to order this drug for this\npatient, the computer is arguing back that you should\nuse this other one instead. ADAM WRIGHT: Yeah, it does. And in fact, I actually\nthought of this a little more after we chatted about it. We've heard from\nresidents that people have said to them,\nif you dare page me with an Epic suggestion in\nthe middle of the night, I'll never talk to you again. So just override\nall of those alerts. I think that one of\nthe challenges is-- and some culpability\non our part-- is that a lot of\nthese alerts we give have a PPV of like, 10 or 20%.", "id": "ZQu2B3GyI_k_48", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  We think it's really\nimportant, so we really raise these alerts a lot. But people experience this\nkind of alert fatigue, or what people call alarm fatigue. You see this in cockpits, too. But people get too\nmany alerts, and they start ignoring the alerts. They assume that they're wrong. They tell the resident\nnot to page them in the middle of the night, no\nmatter what the computer says. So I do think that we have\nsome responsibility to improve the accuracy of these alerts. I do think machine\nlearning could help us. We're actually just\nhaving a meeting about a pneumococcal\nvaccination alert. This is something that\nhelps people remember to prescribe this vaccination\nto help you not get pneumonia. And it takes four or five\nvariables into account. We started looking at\nthe cases where people would override the alert. And they were\nmostly appropriate. So the patient is in a really\nextreme state right now. Or conversely, the patient\nis close to the end of life. And they're not going to\nbenefit from this vaccination. If the patient has\na phobia of needles, if the patient has\nan insurance problem. And we think there's probably\nmore like 30 or 40 variables that you would need to\ntake into account to make", "id": "ZQu2B3GyI_k_49", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So the question is, when you\nhave that many variables, can a human develop and\nmaintain that logic? Or would we be better off\ntrying to use a machine learning system to do that? And would that\nreally work or not? PETER SZOLOVITS:\nSo how far are we from being able to use a machine\nlearning system to do that? ADAM WRIGHT: I think that the\nbiggest challenge, honestly, relates to the\navailability and accuracy of the data in our systems. So Epic, which is the\nEHR that we're using-- and Cerner and Allscripts and\nmost of the major systems-- have various ways to run\neven sophisticated machine learning models, either\ninside of the system or bolted onto the system and\nthen feeding model inferences back into the system. When I was giving that\nexample of the pneumococcal vaccination, one of\nthe major problems is that there's not always\na really good structured way in the system that we\nindicate that a patient is at the end of life and\nreceiving comfort measures only. Or that the patient is in\na really extreme state, that we're in the\nmiddle of a code blue", "id": "ZQu2B3GyI_k_50", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and stop giving these kind\nof friendly preventive care suggestions. So I would actually say\nthat the biggest barrier to really good\nmachine-learning-based decision support is just the lack of\ngood, reliably documented, coded usable features. I think that the second\nchallenge, obviously, is workflow. You said-- it's sometimes hard\nto know in the hospital who a patient's doctor is. The patient is admitted. And on the care team is an\nintern, a junior resident, and a fellow, an attending,\nseveral specialists, a couple of nurses. Who should get that message\nor who should get that page? I think workflow is second. This is where I think\nyou may have said, I have some optimism. I actually think that the\ntechnical ability of our EHR software to run these\nmodels is better than it was three\nor five years ago. And it's, actually, usually\nnot the barrier in the studies that we've done. PETER SZOLOVITS: So\nthere were attempts-- again, 20 years ago-- to create formal\nrules about who gets notified under\nwhat circumstances.", "id": "ZQu2B3GyI_k_51", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Center was going crazy, because\nwhen they implemented a new lab information system, it would\nalert on every abnormal lab. And this was crazy. But there were other\nhospitals that said, well, let's be a little more\nsophisticated about when it's necessary to alert. And then if somebody\ndoesn't respond to an alert within a very short\nperiod of time, then we escalate it to somebody\nhigher up or somebody else on the care team. And that seemed like a\nreasonable idea to me. But are there things\nlike that in place now? ADAM WRIGHT: There are. It works very differently in\nthe inpatient and the outpatient setting. At the inpatient setting,\nwe're writing very acute care to a patient. And so we have processes\nwhere people sign in and out of the care team. In fact, these prevalence\nof these automated messages is an incentive to do that well.", "id": "ZQu2B3GyI_k_52", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  otherwise I'm going to get\nall these pages all night about them. And the system will\nalways make sure that somebody is the\nresponding provider. It becomes a little thornier\nin the outpatient setting, because a lot of the academic\ndoctors at the Brigham only have clinic\nhalf a day a week. And so the question is, if an\nabnormal result comes back, should I send it to that doctor? Should I send it to the person\nthat's on call in that clinic? Should I send it to\nthe head of the clinic? There are also these edge\ncases that mess us up a lot. So a classic one is a\npatient is in the hospital. I've ordered some lab tests. They're looking well, so\nI discharge the patient. The test is still pending at the\ntime the patient is discharged. And now, who does that go to? Should it go to the patient's\nprimary care doctor? Do they have a\nprimary care doctor? Should it go to the person\nthat ordered the test? That person may be\non vacation now, if it's a test that takes\na few weeks to come back. So we still struggle with--\nwe call those TPADs-- tests pending at discharge. We still struggle with\nsome of those edge cases. But I think in the core,\nwe're pretty good at it.", "id": "ZQu2B3GyI_k_53", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  is an experience I've had and\nyou've probably had that-- for example, a few\nyears ago I was working with the people who\nrun the clinical labs at Mass General. And they run some ancient\nlaboratory information systems that, as you said, can add\nand subtract but not multiply or divide. ADAM WRIGHT: They can add and\nmultiply, but not subtract or divide. Yes. And it doesn't support\nnegative numbers. Only unsigned integers. PETER SZOLOVITS: So there are\nthese wonderful legacy systems around that really create\nhorrendous problems, because if you try\nto build anything-- I mean, even a risk\nprediction calculator-- it really helps to be able to\ndivide as well as multiply. So we've struggled\nin that project. And I'm sure you've had similar\nexperiences with how do we", "id": "ZQu2B3GyI_k_54", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of this squeaky old technology\nthat just doesn't support it? So what's the right\napproach to that? ADAM WRIGHT: There are\na lot of architectures and they all have pros and cons. I'm not sure if any one of\nthem is the right approach. I think we often do favor using\nthese creaky old technology or the new technology. So Epic has a built\nin rule engine. That laboratory you talked about\nhas a basic calculation engine with some significant\nlimitations to it. So where we can,\nwe often will try to build rules internally\nusing these systems. Those tend to have real-time\navailability of data, the best ability to sort of push\nalerts to the person right in their workflow and make\nthough those alerts actionable. In cases where we can't do\nthat-- like for example, a model that's too complex\nto execute in the system-- one thing that we've often\ndone is run that model against our data warehouse. So we have a data\nwarehouse that extracts", "id": "ZQu2B3GyI_k_55", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  every night at midnight. So if we don't need real-time\ndata, it's possible to run-- extract the data, run a\nmodel, and then actually write a risk score or a flag back\ninto the patient's record that can then be shown\nto the clinician, or used to drive an alert\nor something like that. That works really well, except\nthat a lot of things that happen-- particularly\nin an inpatient setting, like predicting sepsis-- depend on real-time data. Data that we need right away. And so we run into the challenge\nwhere that particular approach only works on a 24-hour\nkind of retrospective basis. We have also developed systems\nthat depend on messages. So there's this-- HL7\nis a standard format for exchanging data with an\nelectronic health record. There's various versions\nand profiles of HL7. But you can set up\nan infrastructure that sits outside of the EHR\nand gets messages in real time from the EHR. It makes inferences and sends\nmessages back into the EHR. Increasingly, EHRs\nalso do support kind", "id": "ZQu2B3GyI_k_56", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So that you can\nregister a hook and say, call my hook whenever\nthis thing happens. Or you can pull the EHR to get\ndata out and use another web service to write data back in. That's worked\nreally well for us. You can also ask the EHR to\nembed an app that you develop. So people here may\nhave heard-- or should hear at some point--\nabout SMART on FHIR, which is a open kind of API\nthat allows you to develop an application and\nembed that application into an electronic\nhealth record. We've increasingly been building\nsome of those applications. The downside right\nnow of the smart apps is that they're really\ngood for reading data out of the record and sort of\nvisualizing or displaying it. But they don't always\nhave a lot of capability to write data back into\nthe record or take actions. Most of the EHR vendors also\nhave a proprietary approach, like an app store. So Epic calls theirs\nthe App Orchard. And most of the EHRs\nhave something similar, where you can join\na developer program and build an application.", "id": "ZQu2B3GyI_k_57", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  They tend to be proprietary. So if you build\none Epic app, you have to then build a Cerner\napp and an Allscripts app and an eClinicalWorks\napp separately. There are often heavy fees\nfor joining those programs, although the EHR vendors-- Epic in particular-- have\nlowered their prices a lot. The federal\ngovernment, the Office of the National\nCoordinator of Health IT, just about a week and a half ago\nreleased some new regulations which really limit the rate\nat which vendors can charge application developers\nfor API access basically to almost\nnothing, except for incremental computation\ncosts or special support. So I think that may\nchange everything now that that regulation's\nbeen promulgated. So we'll see. PETER SZOLOVITS: So contrary\nto my pessimistic beginning, this actually is the thing\nthat makes me most optimistic. That even five years\nago, if you looked at many of these systems, they\nessentially locked you out.", "id": "ZQu2B3GyI_k_58", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  at the University\nof Pittsburgh, where they had one of the\nfirst centers that was doing heart-lung transplants. So their people had built\na special application for supporting heart-lung\ntransplant patients, in their own homemade electronic\nmedical records system. And then UPMC went to\nCerner at the time. And I remember I\nwas at some meeting where the doctors who ran this\nheart-lung transplant unit were talking to the\nCerner people and saying, how could we get something\nto support our special needs for our patients? And Cerner's answer was,\nwell, commercially it doesn't make sense for us to do this. Because at the time there\nwere like four hospitals in the country that did this.", "id": "ZQu2B3GyI_k_59", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So their offer was, well, you\npay us an extra $3 million and within three\nyears we will develop the appropriate\nsoftware for you. So that's just crazy, right? I mean, that's a\ntotally untenable way of going about things. And now that there are\nsystematic ways for you either to embed your own code\ninto one of these systems, or at least to have a\nwell-documented, reasonable way of feeding data out and\nthen feeding results back into the system, that\nmakes it possible to do special-purpose\napplications like this. Or experimental applications\nor all kinds of novel things. So that's great. ADAM WRIGHT: That's what\nwe're optimistic about. And I think it's worth adding\nthat there's two barriers you have to get through right. One is Epic has\nto sort of let you into their App Orchard,\nwhich is the barrier that", "id": "ZQu2B3GyI_k_60", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And then you need to find\na hospital or a health care provider that wants to\nuse your app, right. So you have to\nclear both of those, but I think it's\nincreasingly possible. You've got smart\npeople here at MIT, or at the hospitals that we\nhave in Boston always wanting to build these apps. And I would say five years ago\nwe would've told people, sorry, it's not possible. And today we're able,\nusually, to tell people that if there's\nclinical interest, the technical part\nwill fall into place. So that's exciting for us. PETER SZOLOVITS: Yeah ADAM WRIGHT: Yeah AUDIENCE: Question about that. ADAM WRIGHT: Absolutely AUDIENCE: Some of the\napplications that you guys develop in\nhouse, do you also put those on the Epic\nOrchard, or do you just sort of implement it one\ntime within your own system? ADAM WRIGHT: Yeah, there's\na lot of different ways that we share these\napplications, right. So a lot of us are researchers. So we will release\nan open source version of the application\nor write a paper and say, this is available. And we'll share it with you. The App Orchard is particularly\nfocused on applications that you want to sell.", "id": "ZQu2B3GyI_k_61", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to sell any applications. We've given a lot of\napplications away. Epic also has something called\nthe Community Library, which is like the AppOrchard, but it's\nfree instead of costing money. And so we released\na ton of stuff through the Community Library. To the point that I\nwas poking out before, one of the challenges is that\nif we build a Smart on FHIR app, we're able to sort of\nshare that publicly. And we can post that on the\nweb or put it on GitHub. And anybody can use it. Epic has a position that\ntheir APIs are proprietary. And they represent Epic's\nvaluable intellectual property or trade secrets. And so we're only allowed\nto share those apps through the Epic ecosystem. And so, we often now,\nwhen we get a grant-- most of my work is\nthrough grants-- we'll have an Epic site. And we'll share that through\nthe Community Library. And we'll have a Cerner site. And we'll share it through\nCerner's equivalent. But I think until the\ncapability of the open APIs, like Smart on FHIR,\nreaches the same level", "id": "ZQu2B3GyI_k_62", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  into having to build\ndifferent versions and distribute three-- each\nEHR under separate channels. Really, really good question. PETER SZOLOVITS: And\nso what's lacking in things like Smart on FHIR-- ADAM WRIGHT: Yeah. PETER SZOLOVITS: --that you\nget from the native interfaces? ADAM WRIGHT: So it's\nvery situational, right. So, for example, in some\nEHR implementations, the Smart on FHIR will give\nyou a list of the patient's current medications\nbut may not give you historical medications. Or it will tell you that\nthe medicine is ordered, but it won't tell you whether\nit's been administered. So one half of the battle\nis less complete data. The other one is that\nmost EHRs are not implementing, at\nthis point, the sort of write back capabilities, or\nthe actionable capabilities, that Smart on FHIR is\nsort of working on. And it's really some\nstandards for us. So if we want to build\nan application that shows how a patient fits on\na growth curve, that's fine. If we went to build\nan application that suggests ordering medicines,\nthat can be really challenging.", "id": "ZQu2B3GyI_k_63", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  have both read and\nwrite capabilities. So that's the other challenge. PETER SZOLOVITS:\nAnd do the vendors worry about, I guess\ntwo related things, one is sort of\ncognitive overload. Because if you build\n1,000 Smart on FHIR apps, and they all start firing\nfor these inpatients, you're going to be back\nin the same situation of over-alerting. And the other question is, are\nthey worried about liability? Since if you were\nusing their system to display recommendations, and\nthose recommendations turn out to be wrong and\nharm some patient, then somebody will\nreach out to them legally because they\nhave a lot of money. ADAM WRIGHT: Absolutely. They're worried\nabout both of those. Related particularly\nto the second one, they're also worried about just\nsort of corruption or integrity of the data, right. So somehow if I can write\na medication order directly to the database, and it\nmay bypass certain checks that would be done normally. And I could potentially enter\na wrong or dangerous order.", "id": "ZQu2B3GyI_k_64", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  is concerns about protection\nof data, sort of Cambridge Analytica style worries, right. So if I, as an Epic\npatient, authorize the Words With Friends app to\nsee my medical record, and then they post\nthat on the web, or monetize it in some\nsort of a tricky way, what liability, if any,\ndoes my health care provider organization, or my-- the EHR vendor, have for that? And the new regulations are\nextremely strict, right. They say that if a patient asks\nyou to, and authorizes an app to access their record, you\nmay not block that access, even if you consider that\napp to be a bad actor. So that's I think an area\nof liability that is just beginning to be sorted out. And it is, I think,\nsome cause for concern. But at the same time, you\ncould imagine a universe where, I think, there\nare conservative health organizations that would\nchoose to never authorize any application to avoid risk.", "id": "ZQu2B3GyI_k_65", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  PETER SZOLOVITS: Well--\nand to avoid leakage. ADAM WRIGHT: Absolutely. PETER SZOLOVITS: So I\nremember years ago there was a lot of reluctance, even\namong Boston area hospitals, to share data, because\nthey were worried that another\nhospital could cherry pick their most lucrative\npatients by figuring out something about them. So I'm sure that that hasn't\ngone away as a concern. ADAM WRIGHT: Absolutely, yeah. PETER SZOLOVITS: OK, we're going\nto try to remember to repeat the questions you're asking-- ADAM WRIGHT: Oh great, OK. PETER SZOLOVITS: --because\nof the recording. ADAM WRIGHT: Happy to. PETER SZOLOVITS: Yeah. AUDIENCE: So how does\na third party vendor deploy a machine learning\nmodel on your system? So is that done through Epic? Obviously, there's the\nApp Orchard kind of thing, but is there ways to go\naround that and go directly into partners and whatnot? And how does that work? ADAM WRIGHT: Yeah. So the question is how\ndoes a third party vendor deploy an application\nor a machine learning model or\nsomething like that? And so with Epic, there's\nalways a relationship", "id": "ZQu2B3GyI_k_66", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  provider organization. And so we could work\ntogether directly. So if you had an app that\nthe Brigham wanted to use, you could share that app\nwith us in a number of ways. So Epic supports this thing\ncalled Predictive Modeling Markup Language, or PMML. So if you train a model,\nyou can export a PMML model. And I can import it into\nEpic and run it natively. Or you can produce a web service\nthat I call out to and gives me an answer. We could work together directly. However, there are\nsome limitations in what I'm allowed to tell you\nor share with you about Epic's data model and what\nEpic perceives to be their intellectual property. And it is facilitated by\nyou joining this program. Because if you\njoin this program, you get access to documentation\nthat you would otherwise not have access to. You may get access to a test\nharness or a test system that lets you sort of\nvalidate your work. However, people who\njoin the program often think that means\nthat I can then just", "id": "ZQu2B3GyI_k_67", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  But with Epic, in\nparticular, you have to then make a deal with\nme to use it at the Brigham and make a deal with my\ncolleague to use at Stanford. Other EHR vendors have\ndeveloped a more sort of centralized model\nwhere you can actually release it and\nsell it, and I can pay for it directly through\nthe app store and integrate it. I think that last mile\npiece hasn't really been standardized yet. AUDIENCE: I guess\none of my questions there is, what\nhappens in the case that I don't want to\ntalk to Epic at all? And just I looked at\nyour data and just like Brigham and Women's stuff. And I build a really good model. You saw how it works, and\nwe just want to deploy it. ADAM WRIGHT: Epic would not\nstop us from doing that. The only real\nrestriction is that Epic would limit my ability to tell\nyou stuff about Epic's guts. And so you would need a\nrelatively sophisticated health care provider\norganization who could map between some kind of\nplatonic data, clinical data, model and Epic's\ninternal data model. But if you had that, you could. And at the Brigham, we have\nthis iHub Innovation Program.", "id": "ZQu2B3GyI_k_68", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  doing work like\nthat, some of whom are members of the\nEpic App Orchard and some who choose not to\nbe members of the Epic App Orchard. It's worth saying\nthat joining the App Orchard or these programs\nentails revenue sharing with Epic and some complexity. That may go way down with\nthese new regulations. But right now,\nsome organizations have chosen not to\npartner with the vendors and work directly\nwith the health care provider organizations. PETER SZOLOVITS: So on the\nquality side of that question, if you do develop an application\nand field it at the Brigham, will Stanford be\ninterested in taking it? Or are they going to be\nconcerned about the fact that somehow you've fit it\nto the patient population in Boston, and it won't be\nappropriate to their data? ADAM WRIGHT: Yeah, I think\nthat's a fundamental question, right, is to what extent do\nthese models generalize, right? Can you train a\nmodel at one place and transfer it\nto another place? We've generally seen\nthat many of them transfer pretty well, right. So if they really\nhave more to do with kind of core\nhuman physiology,", "id": "ZQu2B3GyI_k_69", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  If they're really bound up\nin a particular workflow, right, they assume that you're\ndoing this task, this task, this task in this order, they\ntend to transfer really, really poorly. So I would say that our\ngeneral approach has been to take a model\nthat somebody has, run it retrospectively\non our data warehouse, and see if it's accurate. And if it is, we might\ngo forward with it. If it's not, we would try\nto retrain it on our data, and then see how\nmuch improvement we get by retraining it. PETER SZOLOVITS: And\nso have you in fact imported such models\nfrom other places? ADAM WRIGHT: We have, yeah. Epic provides five\nor six models. And we've just started using\nsome of them at the Brigham or just kind of signed the\nlicense to begin using them. And I think Epic's\nguidance and our experience is that they work pretty\nwell out of the box. PETER SZOLOVITS: Great. AUDIENCE: So could you\nsay a little bit more about these rescores\nthat are being deployed, maybe they work. Maybe they don't. How can you really tell\nwhether they're working, even just beyond patient\nshift over time, just like how people\nreact to the scores. Like I know a lot of the\nbias in fairness works", "id": "ZQu2B3GyI_k_70", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  they'll trust it. And if it doesn't,\nthey ignore the score. So like how-- what\ndoes the process look like before you\ndeploy the score thing and then see whether\nit's working or not? ADAM WRIGHT: Yeah, absolutely. So the question is,\nwe get a risk score, or we deploy a new\nrisk score that says, patient has a risk of\nfalling, or patient has a risk of having sepsis\nor something like that. We tend to do several\nlevels of evaluation, right. So the first level is, when\nwe show the score, what do people do, right? If we-- typically we\ndon't just show a score, we make a recommendation. We say, based on\nthe score we think you should order a lactate\nto see if the patient is at risk of having sepsis. First we look to see if\npeople do what we say, right. So we think it's a good sign if\npeople follow the suggestions. But ultimately,\nwe view ourselves as sort of clinical\ntrialists, right. So we deploy this model with\nan intent to move something, to reduce the rate of sepsis, or\nto reduce the rate of mortality in sepsis. And so we would try to sort\nof measure, if nothing else, do a before and\nafter study, right, measure the rates before,\nimplement this intervention, and measure the rates after. In cases where we're less\nsure, or where we really care about the\nresults, we'll even", "id": "ZQu2B3GyI_k_71", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So we'll give half of the\nunits will get the alert, half the units\nwon't get the alert. And we'll compare the\neffect on a clinical outcome and see what the difference is. In our opinion, unless\nwe can show an effect on these clinical\nmeasures, we shouldn't be bothering people, right. Pete made this point\nthat what's the purpose of having-- if we\nhave 1,000 alerts, everyone will be overwhelmed. So we should only\nkeep alerts on if we can show that they're making\na real clinical difference. AUDIENCE: And are those sort\nof like just internal checks, are there papers of some\nof these deployments? ADAM WRIGHT: It's our-- it's our intent to\npublish everything, right. I mean, I think we're behind. But I'd say, we\npublish everything. We have some things\nthat we've finished that we haven't published yet. They're sort of the next\nthing to sort of come out. Yeah. AUDIENCE: I guess so\nearlier we were talking about how the models are just\nused to give recommendations to doctors. Do you have any metric, in\nterms of how often the model recommendation matches\nwith the doctor's decision? ADAM WRIGHT: Yeah, absolutely.", "id": "ZQu2B3GyI_k_72", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  ADAM WRIGHT: Oh yeah. Thanks, David. So the question is,\ndo we ever check to see how often the model\nrecommendation matches what the doctor does? And so there's sort of\ntwo ways we do that. We'll often retrospectively\ntest the model back. I think Pete shared\na paper from Cerner where they looked at\nthese sort of suggestions that they made to\norder lactates or to do other sort of sepsis work. And they looked to see whether\nthe recommendations that they made matched what the\ndoctors had actually done. And they showed that\nthey, in many cases, did. So that'll be the first\nthing that we do is, before we even turn the model\non, we'll run it in silent mode and see if the doctor\ndoes what we suggest. Now the doctor is not a\nperfect supervision, right, because the doctor may\nneglect to do something that would be good to do. So then when we turn\nit on, we actually look to see whether\nthe doctor takes the action that we suggested. And if we're doing it\nin this randomized mode, we would then look to see\nwhether the doctor takes the action we suggested more\noften in the case where we show the alert, than where we\ngenerate the alert but just logged it and don't--\ndon't show it. Yeah. Yes, sir? AUDIENCE: So you'd\nmentioned how there's", "id": "ZQu2B3GyI_k_73", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  ADAM WRIGHT: Yeah. AUDIENCE: --if it's a code\nblue, these alarms will-- ADAM WRIGHT: Right. AUDIENCE: And you said\nthat cockpits have-- pilots now-- ADAM WRIGHT: Yeah. AUDIENCE: --that have\nsimilar problems. My very limited\nunderstanding of aviation is that if you're flying,\nsay, below 10,000 feet, then almost all of the-- ADAM WRIGHT: Yeah. AUDIENCE: --alarms\nget turned off, and-- ADAM WRIGHT: Yeah. AUDIENCE: --I don't know if\nthere seems to be an airlock for that, for-- ADAM WRIGHT: Yeah. AUDIENCE: --hospitals yet. And is that just because\nthe technology workflow is not mature enough\nyet, only 10 years old? ADAM WRIGHT: Yeah. AUDIENCE: Or is that kind\nof the team's question about the incentives between\nif you build the tool and it doesn't flag this thing-- ADAM WRIGHT: Yeah. AUDIENCE: --the patient\ndies, then they could sued. And so they're just very-- ADAM WRIGHT: Yeah,\nno, we try, right? So since we often don't\nknow about the situations in a structured way at the EHR. And so most of our alerts are\nsuppressed in the operating room, right? So during an-- when a\npatient is on anesthesia, their physiology is\nbeing sort of manually controlled by a doctor. And so we often suppress the\nalerts in those situations. I guess I didn't say the\nquestion, but the question was,", "id": "ZQu2B3GyI_k_74", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  or how much can we? We didn't used to know that\na code blue was going on, because we used to do most of\nour code blue documentation on paper. We now use this code\nnarrator, right? So we can tell when\na code blue starts and when a code blue ends. A code blue is a cardiac arrest\nand resuscitation of a patient. And so we actually\ndo increasingly turn a lot of alerting\noff during a code blue. I get an email or\na page whenever a doctor overrides an alert\nand writes a cranky message. And they'll often say something\nlike, this patient is dying of a myocardial\ninfarction right now, and your bothering me about\nthis influenza vaccination. And then what I'll\ndo is I'll go back-- no, seriously, I\nhad that yesterday. And so what I'll do is I'll\ngo back and look in the record and say, what signs did\nI have this patient sort of in extremis? And in that particular\ncase, it was a patient who came into the ED\nand very little documentation had been started,\nand so there actually were very few signs that the\npatient was in the acute state. I think this, someday,\ncould be sorted by integrating monitor data and\ndevice data to figure that out. But at that point, we didn't\nhave a good, structured data", "id": "ZQu2B3GyI_k_75", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  said this patient\nis so ill that it's offensive to suggest an\ninfluenza vaccination right now. PETER SZOLOVITS: Now,\nthere are hospitals that have started experimenting\nwith things like acquiring data from the ambulance as\nthe patient is coming in so that the ED is already\nprimed with preliminary data. ADAM WRIGHT: Yeah. PETER SZOLOVITS: And in that\ncircumstance, you could tell. ADAM WRIGHT: So this is the\ninteroperability challenge, right? So we actually get\nthe run sheet, all of the ambulance data, to us. It comes in as a PDF that's\ntransmitted from the ambulance emergency management\nsystem to our EHR. And so it's not coming in in a\nway that we can read it well. But to your point,\nexactly, if we were better at interoperability-- I've also talked to\nhospitals who use things like video cameras\nand people's badges, and if there's 50 people\nhovering around a patient, that's a sign that\nsomething bad is happening. And so we might be able to\nuse something like that. But yeah, we'd like\nto be better at that.", "id": "ZQu2B3GyI_k_76", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  all of these problems? ADAM WRIGHT: This is a good\nphilosophical question. Come to BMI 701 and 702 and\nwe'll talk about the standards. HL7 version-- to his\nquestion-- version 2 was a very practical standard. Version 3 was a very deeply\nphilosophical standard-- PETER SZOLOVITS: Aspirational. ADAM WRIGHT: --aspirational,\nthat never quite caught on. And it did in pieces. I mean, FHIR is a\nsimplification of that. PETER SZOLOVITS: Yeah. ADAM WRIGHT: Yes, sir? AUDIENCE: So I think usually,\nthe machine learning models evaluates the\ndifficult [INAUDIBLE].. ADAM WRIGHT: Yes, sir. AUDIENCE: When it comes\nto a particular patient, is there a way to know\nhow reliable the model is? ADAM WRIGHT: Yeah, I mean,\nthere's calibration, right? So we can say this\nmodel works particularly well in these patients, or\nnot as well in these patients. There are some very\nsimple equations or models that we use,\nfor example, where we use a different model in\nAfrican-American patients versus non-African-American\npatients, because there's some data\nthat says this model is better calibrated in this subgroup\nof patients versus another.", "id": "ZQu2B3GyI_k_77", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that there's a\nsuggestion, an inference from a model-- this patient\nis at risk of a fall. And then there's this whole set\nof value judgments and beliefs and knowledge and understanding\nof a patient's circumstances that are very human. And I think that\nthat's largely why we deliver these suggestions\nto a doctor or to a nurse. And then that human\nuses that information plus their expertise\nand their relationship and their experience to make\na suggestion, rather than just having the computer adjust the\nknob on the ventilator itself. A question that\npeople always ask me, and that you should ask\nme, is, will we eventually not need that human? And I think I'm more\noptimistic than some people that there are cases where\nthe computer is good enough, or the human is\npoor enough, that it would be safe to have\na close to closed loop. However, I think those\ncases are not the norm. I think that there'll be more\ncases where human doctors are", "id": "ZQu2B3GyI_k_78", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  PETER SZOLOVITS: So\njust to add that there are tasks where patients\nare fungible, in the words that I used a few lectures ago. So for example, a\nlot of hospitals are developing models that\npredict whether a patient will show up for their optional\nsurgery, because then they can do a better job of\nover-scheduling the operating room in the same way that the\nairlines over over-sell seats. Because, statistically,\nyou could win doing that. Those are very safe predictions,\nbecause the worst thing that happens is you get delayed. But it's not going to\nhave a harmful outcome on an individual patient. ADAM WRIGHT: Yeah,\nand conversely, there are people that are working\non machine learning systems for dosing insulin or adjusting\npeople's ventilator settings, and those are high-- PETER SZOLOVITS: Those\nare the high risk. ADAM WRIGHT: --risk jobs. PETER SZOLOVITS: Yep. All right, last question\nbecause we have to wrap up.", "id": "ZQu2B3GyI_k_79", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  problems-- ADAM WRIGHT: Yes. AUDIENCE: --of some\nof these models. I'm, one, curious\nhow long [INAUDIBLE].. ADAM WRIGHT: Yeah. AUDIENCE: And I\nguess, two, once it's been determined that actually a\nsignificant issue has occurred, what are some of the decisions\nthat you made regarding tradeoffs of using the\nout-of-date model that looks at [INAUDIBLE] signal\nversus the cost of retraining? ADAM WRIGHT: Retraining? Yeah. Yeah, absolutely. So the question is the\nset-and-forget, right? We build the model. The model may become stale. Should we update the model? And how do we decide to do that? I mean, we're using-- it depends on what\nyou define as a model. We're using tables\nand rules that we've developed since the 1970s. I think we have a\npretty high desire to empirically revisit those. There's a problem\nin the practice called knowledge management or\nknowledge engineering, right? How do we remember which\nof our knowledge bases need to be checked\nagain or updated? And we'll often, just as a\nstandard, retrain a model", "id": "ZQu2B3GyI_k_80", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  because it's both\nharmful to patients if this stuff is\nout-of-date, and it also makes us look stupid, right? So if there's a new paper\nthat comes out and says, beta blockers are\nterrible poison, and we keep suggesting\nthem, then people no longer believe the suggestions\nthat we make, that said, we still make mistakes, right? I mean, things happen\nall of the time. A lot of my work has focused on\nmalfunctions in these systems. And so, as an example,\nempirically, the pharmacy might change the code or\nID number for a medicine, or a new medicine might\ncome on the market, and we have to make sure\nto continually update the knowledge base so that we're\nnot suggesting an old medicine or overlooking the fact\nthat the patient has already been prescribed a new medicine. And so we tried to do that\nprospectively or proactively. But then we also tried to\nlisten to feedback from users and fix things as we go. Cool. PETER SZOLOVITS: And just\none more comment on that. So some things are\ndone in real time. There was a system,\nmany years ago, at the Intermountain Health\nin Salt Lake City, where", "id": "ZQu2B3GyI_k_81", "title": "9. Translating Technology Into the Clinic", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of microbiology samples\nin the laboratory. And of course, that can\nchange on an hour-by-hour or day-to-day basis. And so they were updating\nthose systems that warned you about the possibility of that\nkind of infection in real time by taking feeds directly\nfrom the laboratory. ADAM WRIGHT: That's true. PETER SZOLOVITS: All\nright, thank you very much. ADAM WRIGHT: No,\nthank you, guys.", "id": "ZQu2B3GyI_k_82"}, {"text": "  PROFESSOR: So welcome, everyone. Today is the first\nof what will be a series of four guest lectures\nthroughout the semester. There will be two\nguest lectures, starting the week from\ntoday, and then there'll be another one towards\nthe end of the semester. And what Pete and\nI decided to do is to bring in people who\nknow a lot more than us about some area of expertise. In today's instance,\nit's going to be about cardiovascular\nmedicine, in particular about how to use imaging\nand machine learning on images in that context. And for today's\nlecture, we're very excited to have professor\nRahul Deo to speak. Rahul's name kept on showing\nup, as I did research", "id": "MoEaRpLNo9A_0", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  First, my group was\nstarting to get interested in echocardiography,\nand we said, oh, here's an interesting\npaper to read on it. We read it, and then\nwe read another paper on doing subtyping\nof ejection fraction which is a type of heart\nfailure, and we read it. I wasn't really paying attention\nto the names on the papers, and then suddenly,\nsomeone told me, there's this guy moving\nto Boston next month who's doing a lot of interesting\nwork and interesting machine learning. You should go meet him. And of course, I\nmeet him, and then I tell him about\nthese papers I read, and he said, oh, I wrote\nall of those papers. He was a senior author on them. So Rahul's been\naround for a while. He is already a\nsenior in his field. He started out doing his medical\nschool training at Cornell, in Cornell Medical\nSchool, in New York City, at the same time as\ndoing his PhD at Rockefeller University. And then he spent the\nfirst large chunk--", "id": "MoEaRpLNo9A_1", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  at Harvard Medical\nSchool-- he spent a large chunk of his career as\nfaculty at UCSF, in California. And just moved back this\npast year to take a position as the chief data\nscientist-- is that right-- for the One Brave\nIdea project which is a very large initiative\njoint between MIT and Brigham and Women's Hospital to study\ncardiovascular medicine. He'll tell you more maybe. And Rahul's research has\nreally gone the full spectrum, but the type of things\nyou'll hear about today is actually not what he's\nbeen doing most of his career, amazingly so. Most of his career,\nhe's been thinking more about genotype and\nhow to really bridge that genotype-phenotype branch,\nbut I asked him specifically to talk about imaging. So that's what he'll be focusing\non today in his lecture. And without further ado, thank\nyou, Rahul, for coming here. [APPLAUSE] RAHUL DEO: So I'm\nused to lecturing the clinical audiences,\nso you guys are by far", "id": "MoEaRpLNo9A_2", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So please spare me a\nlittle bit, but I actually want to encourage\ninterruptions, questions. This is a very\nopinionated lecture, so that if anybody has sort of\nany questions, reservations, please bring them\nup during lecture. Don't wait till the end. And in part, it's opinionated\nbecause I feel passionately that the stuff we're doing needs\nto make its way into practice. It's not by itself purely\nacademically interesting. We need to study the\nthings we're doing. We're already picking up\nwhat everybody else here is already doing. So it's OK from that\nstandpoint, but it really has to make its way. And that means that we have to\nhave some mature understanding of what makes its\nway into practice, where the resistance will be. So the lecture will be peppered\nthroughout with some opinions and comments in that, and\nhopefully, that will be useful. So just a quick\noutline, just going to introduce cardiac structure\nand function which is probably not part of the regular\nundergraduate and graduate training here at MIT.", "id": "MoEaRpLNo9A_3", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and how they use them. And all this is really\nto help guide the thought and the decision making\nabout how we would ever automate and bring this into-- how to bring machine learning,\nartificial intelligence, into actual clinical practice. Because I need to\ngive enough background so you realize what\nthe challenges are, and then the question probably\nevery has is where's the data? How would how would\none get access to some of this stuff to\nbe able to potentially do work in this area? And then, I'm going to venture a\nlittle bit into computer vision and just talk about\nsome of the topics that at least I've been\nthinking about that are relevant to what we're doing. And then talk about\nsome of this work around an automated pipeline for\nechocardiogram, not as by any means a gold standard\nbut really just as sort of an initial\nforay into trying to make a dent into this. And then thinking a little\nbit about what lessons-- David mentioned that you\ntalked about electrocardiogram last week or last class,\nand so a little bit of some of the ideas from there, and\nhow they would lend themselves to insights about future\ntypes of approaches", "id": "MoEaRpLNo9A_4", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And then my background is\nactually more in biology. So I'm going to come\nback and say, OK, enough with all this imaging\nstuff, what about the biology? How can we make\nsome insights there? OK. So every time people\ntry to get funding for coronary heart disease,\nthey try to talk up just how important it is. So this is still-- we have some battles with\nthe oncology people-- but this is still the leading\ncause of death in the world. And then people like I,\nyou're just emphasizing the developed world. There's lots of communicable\ndiseases that matter much more. So even if you look at those,\nand you look at the bottom here, this still, if this is all\ncauses of death age-adjusted, cardiovascular disease is\nstill number one amongst that. So certainly it remains\nimportant and increasingly so in some of the\ndeveloping world also. So it's important to think\na little bit about what the heart does,\nbecause this is going to guide at least the way that\ndiseases have been classified.", "id": "MoEaRpLNo9A_5", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and it delivers oxygenated\nblood throughout the circulatory system to all the\ntissues that need it-- the brain, the kidneys, the\nmuscles, and oxygen, of course, is required for ATP production. So it's a pretty\nimpressive organ. It pumps about five\nliters of blood a minute, and with exercise, that can go\nup five to seven-fold or so, with conditioned athletes,\nnot me, but other people can ramp that up substantially. And we have this need to keep\na very, very regular beat, so if you pause for\nabout three seconds, you are likely to get\nlightheaded or pass out. So you have to maintain this\nrhythmic beating of your heart, and you can compute\nwhat that would be, and somewhere around two billion\nbeats in a typical lifetime. So I'm going to show a\nlot of pictures and videos throughout this. So it's probably worthwhile just\nto take a pause a little bit and talk about what the\nanatomy of the heart is. So the heart sits like\nthis, so the pointy part is kind of sitting out\nto the side, like that.", "id": "MoEaRpLNo9A_6", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So the blood comes in something\ncalled the inferior vena cava or the superior vena cava,\nthat's draining from the brain. This is draining\nfrom the lower body, and then enters into a chamber\ncalled the right atrium. It moves through something\ncalled the tricuspid valve into what's called\nthe right ventricle. The right ventricle has\ngot some muscle to it. It pumps into the lungs. There, the blood\npicks up oxygen, so that's why it's\nshown as being red here. The oxygenated native blood\ncomes through the left atrium and then into the left\nventricle through something called the mitral valve. We'll show you some pictures\nof the mitral valve later on. And then the left\nventricle, which is the big workhorse\nof the heart, pumps blood through\nthe rest of the body, through a structure\nof the aorta. So in through the right\nheart, through the lungs, through the left heart,\nto the rest of the body. And then shown here in yellow\nis the conduction system. So you guys got a little bit\nof a conversation last class on the electrical system. So the sinoatrial node is\nup here in the right atrium,", "id": "MoEaRpLNo9A_7", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So the P wave on an EKG\nrepresents the conduction through there. You get through\nthe AV node, where there's a delay which\nis a PR interval, and then you get spreading\nthrough the ventricles which is the QRS complex, and then\nrepolarization is the T wave. So that's the electrical system,\nand of course, these things have to work\nintimately together. Every single basic kind\nof cardiac physiology will show this diagram called\nthe Wiggers diagram which really just shows the\ninterconnectedness of the electrical system. So there's the EKG up there. These are the heart sounds\nthat a provider would listen to with the stethoscope,\nand this is capturing the flow of sort\nof the changes in pressure in the heart and in the aorta. So heart fills during a period\nof time called diastole. The mitral valve closes. The ventricle contracts. The pressure increases. This is a period of\ntime called systole. Eventually, something called\nthe aortic valve pops open, and blood goes through\nthe rest of the body. The heart finally\nstarts to relax.", "id": "MoEaRpLNo9A_8", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Then, you fill again. So this happens again and again\nand again in a cyclical way, and you have this combination\nof electrical and mechanical properties. OK. So I have some pictures here. These are all MRIs. I'm going to talk about\nechocardiography which is these very ugly, grainy\nthings that I unfortunately have to work with. MRIs are beautiful\nbut very expensive. So there's a reason for that. So this is something called the\nlong axis view of the heart. So this is the thick walled\nleft ventricle there. This is the left\natrium there, and you can see this beautiful turbulent\nflow of blood in there, and it's flowing from the\natrium to the ventricle. This is another patient's. It's called the short axis view. There is the left ventricle\nand the right ventricle there. So we're kind of looking\nat it somewhat obliquely, and then this is another\nview called the physical. It's a little bit dull there. I'm sorry. We can brighten it a little bit. This is the what's called\nthe four chamber view. So you can see the left\nventricle and right ventricle here. So the reason for\nthese different views is, ultimately, that\npeople have measures", "id": "MoEaRpLNo9A_9", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  with these specific views. So you're going to see them\ncoming back again and again. OK. So the way that physicians like\nto organize disease definitions really around some of these\nsame kind of functions. So failures of the\nheart to pump properly causes a disease\ncalled heart failure, and this shows up in terms of\nbeing out of breath, having fluid buildup in the\nbelly and in the legs, and this is treated\nwith medications. Sometimes, you can have\nsome artificial devices to help the heart pump,\nand ultimately, you could even have a transplant,\ndepending on how severe it is. So that's the pump. Blood supply to the heart\nultimately can also be blocked, and that causes a disease\ncalled coronary artery disease. If blood is completely\nblocked, you can get something called a\nheart attack or myocardial infarction. That's chest pain, sometimes\nshortness of breath, and we open up those blocked\nvessels by angioplasty, stick a stent in there,\nor bypass them altogether. And then the flow of\nblood has to be one way.", "id": "MoEaRpLNo9A_10", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  is valvular disease, and so\nyou can have either two type valves, so that's\ncalled stenosis. Or you can have leaky valves. That's called regurgitation. That shows up as\nlight-headedness, shortness of breath, fainting, and then\nyou've got to fix those valves. And finally, there's\nabnormalities of rhythm. So something like\natrial fibrillation which is a quivering of the\natrium, so too slow heartbeats, which would look like cardiac,\ncan present as palpitations, fainting, or even sudden death. And you can stick a pacemaker in\nthere, defibrillator in there, or try to burn off\nthe arrhythmia. OK. So this is like the very\nphysiology-centric view, but the truth is that the\nheart has a whole lot of cells. So there's a lot more\nbiology there than simply just thinking about the pumping\nand the electrical function. Only 30% of the cells or so\nare these cardiomyocytes. So these are the cells that\nare involved in contraction. These are cells that are\nexcitable, but that's only 30% of the cells. There is endothelials\nin the cell. There's fibroblasts. There's a bunch of\nblood cells in there", "id": "MoEaRpLNo9A_11", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So you have lots\nof other things. So we're going to come\nback to here a little bit when talking about how should\nwe be thinking about disease? The historic way is\nto think about pumping and electrical activation,\nbut really, there's maybe a little bit\nmore complexity here that needs to be addressed. OK. So there's a lot of different-- so cardiology is\nvery imaging-centric, and as a result,\nit's very expensive. Because imaging costs\na lot of money to do, and so I have dollar\nsigns here reflecting the sorts of\ndifferent tests we do. So you saw the\ncheapest one last week, electrocardiogram,\nso one dollar sign, and that has lots of utility. For example, one could\ndiagnose an acute heart attack with that. Echocardiography, which\ninvolves sound waves, is ultimately more used\nfor quantifying structure and function, can pick up heart\nfailure, valvular disease, high blood pressure\nin the lungs. So that's another modality. MRI, which is just not used\nall that much in this country,", "id": "MoEaRpLNo9A_12", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  It does largely the same\nthings, and you can imagine, even though it's\nbeautiful, people have not had an easy\ntime and able to justify why it's any better than this\nslightly cheaper modality. And then you have angiography\nwhich can either be by CAT scan or by X-ray. And that visualizes the flow\nof blood through the heart and looks for blockages which\nare going to be stented, ballooned up and stented. And then you had these kind\nof non-invasive technologies, like PET and SPECT that\nuse radionucleotides, like technetium,\nrubidium, and they look for abnormalities\nin blood flow to detect whether\nor non-invasively there's some patch\nof the heart that isn't getting enough blood. If you get one of these,\nand it's abnormal, often, you go over there, and you\ntake a trip to the movies-- as my old teachers used to say-- and then you may find yourself\nwith an angioplasty or stent or bypass. So one of the sad\nthings about cardiology is we don't define our\ndiseases by biology. We define our\ndiseases often related to whether the anatomy\nof the physiology", "id": "MoEaRpLNo9A_13", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  or some of these numbers. OK. So we have to make\ndecisions, and we often use these very same\nthings too to be able to make some decisions. So we have to decide whether\nwe want to put a defibrillator, and to do so, you often need\nto get an echocardiogram to look at the pumping\nfunction of the heart. If you want to decide on whether\nsomebody needs angioplasty, you have to get an angiogram. If you want to decided to\nget a valve replacement, you need an echo. But some of these\nother ones actually don't involve any\nimaging, and this is sort of one of the\nchallenges that I'm going to talk about is\nthat all of the future-- you can imagine building\nbrand new risk models, new classification models. You're stuck with the\ndata that's out there, and the data that's\nout there is ultimately being collected because\nsomebody feels like it's worth paying for it already. So if you want to\nbuild a brand new risk model for who's going to\nhave a myocardial infarction, you're probably not going to\nhave any echocardiograms to be able to use for\nthat, because nobody is going to have paid\nfor that to be collected in the first place. So this is a problem. To be able to innovate, I've got\nto keep on coming back to that,", "id": "MoEaRpLNo9A_14", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  sizes that we face in\nsome of these things. And part of it is\nbecause if you just want to piggyback on\nwhat insurers are going to be willing to pay\nfor to get your data, you're going to\nbe stuck with only being able to work off\nthe stuff we already know something about. So much of my work\nhas been really trying to think about how\nwe can change that. OK, so just a little\nbit more, and then we can get into a\nlittle bit more meat. So sort of the\nuniversal standard for how imaging data is stored\nis something called DICOMs, or Digital Imaging and\nCommunications standard, and really, the end\nof the day, there is some compressed\ndata for the images. There's a DICOM header, which\nI'll show you in a moment. It's lots of nice\nPython libraries that are available to be\nable to work with this data, and there's a free\nviewer you could use too. OK. So where do I get\naccess to this? So this has actually\nbeen an incredible pain. So hospitals are set up\nto be clinical operations. They're not set\nup to make it easy for you to get gobs\nof data for being able to do machine learning. It's just not really there.", "id": "MoEaRpLNo9A_15", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that store this\ndata, but there's lots of reasons for why\npeople make that difficult. And one of them is\nbecause often images have these burned in pixels\nwith identifiable information. So you'll have a patient's\nname emblazoned in the image. You'll have date of birth. You'll have kind of\nother attributes. So you're stuck\nwith that, and not only is it a problem\nthat they're there, the vendors don't make it\neasy to be able to get rid of that information. So you actually have a\nproblem that they don't really make it easy to download in\nbulk or de-identify this. And part of the reason\nis because then it would make it easy for you\nto switch vendors and have somebody else take over. So they make it a\nlittle bit hard for you. Once it's in there, it's\nhard for you to get it out, and people are\nselling their data. That's certainly happening too. So there's a little\nbit of attempts to try to control things that\nway, and many of the labels you want are stored separately. So you want to know what the\ndiseases of these people. So you have the\nraw imaging data, but all the clinical\nstuff is somewhere else. So you have to\nsometimes link that, and so you need to\nget access there. And so just to give you a\nlittle bit of an idea of scale,", "id": "MoEaRpLNo9A_16", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  which is about 30\nmillion historically, and this is all related to cost. So positron emission tomography,\nyou can get about 8,000 or so, and we're one of the\nbusiest centers for that. Echocardiograms are in\nthe 300,000 to 500,000 range in archives. So that gets a little\nbit more interesting. OK. This is what a DICOM\nheader looks like. You have some sort\nof identifiers, and then you have some\ninformation there, attributes of the\nimages, patient name, date of birth, frame rate. These kind of things are there,\nand there's some variability. So it's never quite easy. OK. So these different modalities\nhave some different benefits to them which is\nwhy they're used for one disease or the other. And so one of the real headaches\nis that the heart moves. So the chest wall moves,\nbecause we breathe, and the heart moves too. So you have to\nimage something that has enough temporal frequency\nthat you're not overwhelmed by the basic movement\nof the heart itself, and so some of these\nthings aren't great.", "id": "MoEaRpLNo9A_17", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  which are radioactive\ncounts, over minutes. So that's certainly\na problem when it comes to something\nthat's moving like that, and if you want to\nhave high resolution. So typically, you have very\npoor spatial resolution for something that\nultimately doesn't deal well with the moving aspect. So coronary angiography has\nvery, very fast frame rates. So that's X-ray, and\nthat's sort of very fast. Echocardiography\ncan be quite fast. MRI and CT are\nnot quite as good, and so there's some\ndegradation of the image. As a result, people do\nsomething called gating, where they'll take the\nelectrocardiogram, the ECG, and try to line up\ndifferent portions of different heartbeats. And say, well, we'll take\nthis image from here, line it up with this one\nfrom there, this one-- I'm going to talk a little bit\nabout that, about registration, but ultimately, that's a problem\nthat people have to deal with. So it's a computer vision\nproblem of interest. OK. Preamble is almost done. OK. So why do we even\nimagine any of this stuff is going to be useful? So it turns out that the\npractice of interpreting", "id": "MoEaRpLNo9A_18", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So people like\nme, and people who have trained for\nway too long, find themselves getting little rulers\nand measuring various things. So for example, this is\na narrowing of an artery. So you could take a little\nbit of calipers and measure across that and\ncompare it to here and say, ah, this\nis 80% narrowed. You could measure the\narea of this chamber, the left ventricle, and you\ncan measure its area is, and you can see, ah,\nits peak area is this. It's minimum area is this. Therefore, it's contracting\na certain amount. So we do those things. We measure those things by hand. And the other thing we do is we\nactually diagnose things just by looking at them. So this is a disease called\ncardiac amyloid characterized by some thickening. I'll show you a little\nbit more about that and some sparkling here. So people do look and say,\nah, this is what this is. So there's kind of a\nclassification problem that comes either at the\nimage or video level. So we'll talk about whether\nthis is even worth doing. AUDIENCE: I have a question. RAHUL DEO: Yes. AUDIENCE: Is this with\nsoftware, or do you literally take a ruler and measure?", "id": "MoEaRpLNo9A_19", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  stretching something, and\nclicking another point. So it's a little better\nthan pulling the ruler out of your back pocket, but\nnot that much better. OK. So we're going to talk\nabout or three little areas, and again, this is not-- I got involved in this really\nin the last two years or so. It's nice of David to\nask me to speak here, but I think there\nare probably people in this room who have a lot\nmore experience in this space. But the areas that have\nbeen relevant to what we've been doing has been image\nclassification and then semantic segmentation. So image classification being\nassigning a label to an image, very great. Semantic segmentation, assigning\neach pixel to a class label, and we haven't done anything\naround the image registration, but there are some\ninteresting problems I've been thinking about there. And that's really mapping\ndifferent sets of images onto one coordinate system. OK. So seems obvious that\nimage classification would be something that you would\nimagine a physician does, and so maybe we can mimic that. Seems like a reasonable\nthing that happens. So lots of things that\nradiologists, people who interpret images, do\ninvolve terms of recognition,", "id": "MoEaRpLNo9A_20", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So it takes them a couple of\nminutes to often do things like detect if there's\ncancer, detect if somebody has pneumonia, detect if there's\nbreast cancer in a mammogram, tells there's\nfluid in the heart, and then even less than that,\none minute often, 30 seconds, they can very, very fast. So you can imagine\nthe wave of excitement around image classification\nwas really post-image net, so maybe about three years,\nfour years, or so ago. We're always a little\nslow in medicine, so a little bit\nbehind other fields. And the places that they\nwent were the places where there are huge\ndata sets already, and where there's simple\nrecognition tests. So chest X-rays and\nmammograms are both places that had a lot of\nattention, and other places have been slowed down by just\nhow hard it is to get data. So if you can't get a\nbig enough data set, then you're not going\nto be able to do much. OK. So David mentioned, you guys\nalready covered very nicely, and this is probably\nkind of old hat. But I would say that prior to\nconvolutional neural networks, nothing was happening in\nthe image classification", "id": "MoEaRpLNo9A_21", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  It was just not. People weren't even thinking\nthat it was even worth doing. Now, there's a lot\nof interest, and so I have many different companies\ncoming and asking for help with some of these things. And so it is now a\nvery attractive thing in terms of\nthinking, and I think people haven't thought\nout all that well how we're going to use that. So for example, if it takes\na radiologist a minute to two minutes to\nread something, how much benefit are you\ngoing to get to automate it? And the real\nproblem is you can't take that radiologist away. They're still there,\nbecause they're the ones who are on the hook. And they're going to\nget sued, and it's among the most sued\nprofession in medicine. So there's lots of people\nwho can read an X-ray. You don't need to have\nall that training. But if you're the one\nwho's going to be sued, it ends up being that\nthere really isn't any task shifting in medicine. There isn't that\nkind of, oh, I'm going to let such\nand such take on 99%, and just tell me when\nthere is a problem. It just doesn't happen, because\nthey ultimately don't feel comfortable passing that on. So that's something\nto think about.", "id": "MoEaRpLNo9A_22", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  easy for a very, very expensive\nand skilled person to do, and they refuse to give it up. OK. So that's a problem,\nbut you can imagine there is some scenarios-- and\nwe'll talk more about this-- as to where that could be. So let's say it's overnight. The radiologist is sleeping\ncomfortably at home, and you have a bunch\nof studies being done in the emergency room. And you want to figure out,\nOK, which one should we call them about? So you can imagine\nthere could be triage, because the status quo would\nbe, we'll take them one by one. Maybe you could imagine sifting\nthrough them quickly and then re-prioritizing them. They'll still be looked at. Every single one will\nstill be looked at. It's just the order may change. So that's an example,\nand you could imagine there could be\nseparate-- someone else could read at the same time. And we'll come back\nto this in terms of whether or not\nyou could have two streams and whether or not\nthat is a scenario that would make some sense. And maybe, in\nresource-poor settings, where we're not teaming\nwith the radiologist, maybe that makes sense too. So we'll come back to that too. OK. So here's another problem. So almost everything in\nmedicine requires some element", "id": "MoEaRpLNo9A_23", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and some of the reasons\nare very simple. So let's say you want to talk\nabout there being a tumor. So if you're going to ask\na surgeon to biopsy it, you better tell\nthem where it is. It's not enough to\njust say, this image has a tumor somewhere on it. So there is some element\nof that that you're going to need to be a little\nbit more detailed than simply making a classification\nwith a level one image, but I would say beyond that. Let's say, I'm going to try\nto get one of my patients to go for valve surgery. I'll sit with them,\nbring up their echo, sit side by side with them,\nand point them to where it is. Bring up a normal\none and compare, because I want them to be\ninvolved in the decision. I want them to feel like\nthey're not just trust-- and they have to trust me. At the end of the\nday, they don't even know that I'm showing-- I'll show them their name,\nbut ultimately, there is some element of trust. They're not able to do\nthis, but at the same time, there is this sense of\nshared decision making. You're trying to communicate to\nsomebody, whose life is really at risk here, that this is\nwhy we're doing this decision.", "id": "MoEaRpLNo9A_24", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  the more difficult it\nis to make that case. So medicine is this-- I found this review by Bin Yu\nfrom Berkeley, just came out, and it talks about this tension\nbetween predictive accuracy and descriptive accuracy. So this is of the typical thing\nwe think about that matters, and there's lots\nof people who've written about this thing. Medicine is tough in that it's\nvery demanding in this space here, and it's almost\ninflexible in this space here. So it's a tough nut\nto crack in terms of being able to\nmake some progress, and so we'll talk more about\nwhen that's likely to happen. OK. So this again may be something\nthat's very familiar to you. So we had this problem in\nterms of some of the disease detection models,\nand I didn't find this all that\nsatisfying in terms being able to\nsuccessfully localize. So just digging\nthrough the literature, it looks like this idea\nof being able to explain what part of the\nimage is driving a certain classification.", "id": "MoEaRpLNo9A_25", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Maybe it goes back before that. But ultimately,\nthere's two broad ways. You can imagine finding an\nexemplary image that maximally activates the classical work,\nor you can take a given image and say, what aspect of it is\ndriving the classification? And so in this paper here\ndid both those things. They either went\nthrough and optimized-- starting from an average\nof all the training data-- they optimized the intensities\nuntil they maximized the score for a given class. So that's what's shown here. And then another way to do\nit is in some sense you could take a derivative of\nthe score function relative to the intensities\nof all the pixels and come up with\nsomething like this. But you could imagine, if\nyou showed this to a patient, they wouldn't be very satisfied. So it's very difficult to make a\ncase that this is super useful, but it seems like this field\nhas progressed somewhat, and I haven't tried this out. This is a paper by Max\nWelling and company, out by a couple of\nyears, and maybe you guys are familiar with this. But this ultimately is a little\nbit of a different approach in the sense that\nthey take patches,", "id": "MoEaRpLNo9A_26", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and they compare the final\nscore, or class label, relative to what it-- so taking the intensity\nhere and replacing it by a conditional result\nsampling from the periphery. And just comparing\nthose two things and seeing whether or not\nyou either get activation, which is the red here. This is the way that they\ndid the conditional sampling, and then blue would be\nthe negative contributors. And there, you can\nimagine, there's a little bit more\ndistinction here, and then something a little\nbit more on the medical side is this is a brain MRI. And so depending\non this patch size, you get a different\ndegree of resolution to localizing some areas of\nthe image that are relevant. So this is something\nthat we're going to expect a lot of demands\nfrom the medical field in terms of being able to show this. And at least our\ninitial forays weren't very satisfying doing this\nwith what we were doing, but maybe these algorithms\nhave gotten better. OK. So next thing that matters. OK.", "id": "MoEaRpLNo9A_27", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So I did my cardiology\nfellowship in MGH, and I just traced circles. That's what I did. I just trace circles, and\nI stretched a ruler across, and then fed that in. At least the program\ncomputed the volumes for me, the areas and\nvolumes, but otherwise, you have to do this yourself. And so this is like\na task that's done, and sometimes you may have to-- here's an example of\nvolumes being computed by tracing these sorts of things\nand much radiology reports just involve doing that. So this seems like a\nvery obvious task we should be able to improve on. So medicine tends\nto be not the most creative in terms\nof trying a bunch of different architectures. So if you look at the papers,\nthey all jump on the U-net as being the\nfavorite architecture for semantic segmentation. So maybe familiar\nto people here, really, it just captures this\nencoding or contracting layer. Where you're downsampling,\nand then there's a symmetric upsampling\nthat takes place.", "id": "MoEaRpLNo9A_28", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  you take an image, and\nthen you can catonate it with this upsampled layer, and\nthis helps get a little bit more localization. So we used this for\nour paper, and we'll talk about this a little\nbit, and it's very popular within the medical literature. One of the things that\nwas quite annoying is that what you would find\nfor some of the images, you'd find, let's\nsay, a ventricle. You'd find this\nnicely segmented area, and then you'd find this\nlittle satellite ventricle that the image would just pick. The problem is that this\npixel-level classification tends to be a\nproblem, and a human would never make that mistake. But that tends to be something\nthat sounds like it is common in the-- this is a\ncommon tension is that this sort of focusing\non relatively limited scales ends up being problematic,\nwhen it comes to picking up the global architecture. And so there's lots\nof different solutions it looks like in the literature. I just highlighted some\nof these from a paper that was published from\nGoogle a little while ago. One of the things\nthat's captured is these ideas of\ndilated convolutions,", "id": "MoEaRpLNo9A_29", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  built on convolutions. And so ultimately, you have\na much bigger receptive field for this layer, though\nyou haven't really increased the\nnumber of parameters that you have to learn. So there is some. It seems like there's lots. This is not just\na problem for us but a problem for many\npeople in this field. So we need to be a little\nbit more adventurous in terms of trying some\nof these other methods. We did try a little bit of\nthat and didn't find a gains, but I think,\nultimately, there still needs to be a little\nbit more work there. OK. So the last thing I'm\ngoing to talk about before getting into my\nwork is really this idea of image registration. So I talked about how there are\nsometimes some techniques that have limitations, either in\nterms of spatial resolution or temporal resolution. So this is a PET scan here,\nthis sort of reddish glow here, and in the background, we\nhave a CAT scan of the heart. And so clearly, this is a\npoorly registered image, where you have the PET scan kind\nof floating out here, when it really should be lined up here. And so you have something\nthat's registered better there. I also mentioned this\nproblem but gating.", "id": "MoEaRpLNo9A_30", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  from different\ncardiac cycles, you're going to have align\nthem in some way. It seems like a very mature\nproblem in computer vision world. We haven't done\nanything in this space, but ultimately, it has\nbeen around for decades. If not, I would just at least\ntouch it, touch upon it. So this is sort of\nthe old school way, and then now people\nare starting to use conditional variational\nautoencoders to be able to learn\ngeometric transformations. This is the Siemens group out in\nPrinceton that has this paper. Again, nothing I'm\ngoing to focus on, just wanted to bring it up\nas being an area that remains of interest. OK. So I think we're doing\nOK, but you said 4:00. PROFESSOR: 3:55 RAHUL DEO: 3:55. OK. All right, and interrupt. Please, interrupt. OK? I'm hoping that I'm\nnot talking too fast. OK. As David said, this\nwas not my field, but increasingly,\nthere is some interest", "id": "MoEaRpLNo9A_31", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  because of my frustrations\nwith clinical medicine. So this is one of\nmy frustrations with clinical medicine. So cardiology has\nnot really changed, and one of the things\nit fails at miserably is picking up\nearly-onset disease. So here's the typical\nprofile, a little facetious. So people like me\nin our early 40s, start to already\nhave some problems with some of these numbers. So I like to joke that, since I\ncame back to the Harvard system from California, my blood\npressure has gone up 10 points which is\ntrue, unfortunately. So these changes\nalready start to happen, and nobody does\nanything about it. So you can go to your doctor,\nand you're also saying, no, I don't want to\nbe on any medicine. They're like, no, no, you\nshouldn't be on any medicine. So you kind hem and haw, and a\ndecade goes by, 15 years go by. And then finally,\nyou're like, OK, well, it looks like at\nleast my coworkers are on some medicines, or maybe\nI'll be willing to do that. And so they've got lots of\nstuff you can be treated,", "id": "MoEaRpLNo9A_32", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  this at the doctor level too. Yes. AUDIENCE: For the\noptical values, how much personal deviation\nis there for the values? RAHUL DEO: So the optimal\nvalue is fixed and is just like a reference value. And you can be off-- so blood pressure, let's say. So people consider optimal\nto be less than 120 over less than 80. People are in the 200s. So you'd be treated in\nthe 200s, but there'll be lots of people in\nthe 140s and the 150s, and there'll be a degree\nof kind of nihilism about that for some time. And my patients would be\nlike, oh, I got into the fight with the parking attendant. I just had a really bad phone-- there's like countless\nexcuses for why it is that one shouldn't\nstart a medication, and this can go on\nfor a long time. Yes. AUDIENCE: [INAUDIBLE]. How can you assess the risk\n[INAUDIBLE] for blood pressure? Is that, like,\nnoise [INAUDIBLE]??", "id": "MoEaRpLNo9A_33", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So OK. So that's a great point. So yeah. So the question is\nthat many of the things that we're seeing\nas risk factors have inherent variability to them. Blood sugar is another great\nexample of those things. If you could have a\nsingle-point estimate that arises in the setting of\na single clinic visit, how much do you trust that? So it's a couple of\nthings related to that. So one of them is\nthat people could be sent home with monitors, and\nthey can have 24-hour monitors. In Europe, that's much\nmore often done than here. And then, the thing is that\noften they'll say that, and then you go look at\nlike six consecutive visits, and they all have something\nelevate, but it's true. This is a noisy point\nestimate, and people have shown that averages\ntend to do better. But at the same time,\nif that's all you have-- and the bias is interesting. Because the bias comes\nfrom some degree of stress, but we have lots of\nstress in our life. I hopefully am not the\nmost stressful part", "id": "MoEaRpLNo9A_34", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  are-- and the problem\nwith that is it's a good reason for\nsomeone to talk you out of them starting\nthem on anything. And that's what\nends up happening, and so this can be a\nreally long period of time. OK. So this is the grim part. OK? So it turns out\nthat once symptoms develop for something like\nheart failure, decline is fast. So 50% mortality in five\nyears, after somebody gets hospitalized for their first\nheart failure admission, and often the symptoms\nare just around that time. So unfortunately,\nthese things tend to be irreversible changes\nthat happen in the background, and largely, you don't\nreally have any symptoms until late in the game. So we have this problem, where\nwe have this huge stretch. We know that there\nis risk factors, but we have this huge stretch,\nwhere nobody is doing anything about them. And then we have sort things\ngoing downhill relatively quickly after that. And unfortunately,\nI would make a case that probably\nresponsiveness is probably best did this phase over there. Expense is really\nall over there. So we really want to find--", "id": "MoEaRpLNo9A_35", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  I'm going to come back to\nthis again a little bit later on-- but really, we\nwant to have these-- if you're going to do something\nin this asymptomatic phase, it better be cheap. You're not going to be\ngetting MRIs every day or every year for people\nwho have no symptoms. The system would\nbankrupt if you had that. So we need these\nlow cost metrics that can tell us, at\nan individual level, not just if we had\n1,000 people like you, somebody would benefit. And this is what\nmy patients would say is that they would be\nso excited about their EKG or their echo being\ndone every year, because they want to know,\nhow does it look like compared to last year? They want some comparison\nat their level, not just some\npublic health report about this being a benefit\nto 100 people like you. And so it shouldn't\nbe both low cost, should be reflective\nat something an individual level,\nshould be relatively specific to the disease\nprocess, expressive in some way, and should get\nbetter with therapy. I think that's one\nof the things that's pretty important\nis if somebody does the things you ask\nthem to do, hopefully,", "id": "MoEaRpLNo9A_36", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And then that would\nbe motivating, and I think that's how\npeople get motivated is that they get responses. So I would make a case\nthat even simple things like an ultrasound--\nand I have one showed here--\nreally does capture some of these things,\nand not all those things, but they have some\nof those things. So you have, for example, that\nin the setting of high blood pressure, the left ventricular\nmass starts to thicken, and this is a quantitative,\ncontinuous measure. It just thickens over time,\nand the heart starts to change. The pumping function\ncan get worse over time. The left atrium, which is\nthis structure over here, this thin-walled structure\nis amazing in the sense that it's almost this barometer\nfor the pressure in the heart. Oh, that's a horrible reference. OK, but it tends to\nget kind of bigger and bigger in a very subtle\nway before any symptoms happen. So you have this, and\nthis is just one view. Right? So this is a simple\nview acquired from an ultrasound\nthat captures some of these things at\nan individual level. So this gets to\nsome of my thoughts around where we could imagine\nautomated interpretation benefiting. So if you want to think about\nwhere you're less likely.", "id": "MoEaRpLNo9A_37", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  or complex decisions,\nwhere you have a super skilled\nperson even collecting the data in the first place. They've gone through training. They're super experienced. You have a very expensive\npiece of hardware used to collect the data. You have an expert\ninterpreting it. This is done late in\nthe disease course. You have to make\nreally hard decisions, and you don't want\nto mess it up. So probably not\ngood places to try to stick in an automated\nsystem in there, but what would be\nattractive would be to try to enable studies that\nare not even being done at all. So move to the\nprimary care setting. Use low cost handhelds. So there's even\nnow companies that are starting to try to automate\nacquisition of the data by helping people\ncollect it and guide them to collecting the right views. Early in the disease course,\nno real symptoms here. Decision support just\naround whether you should start some meds or\nintensify them, low liability, low cost. So this is a place\nwhere we wanted", "id": "MoEaRpLNo9A_38", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  some kind of innovations\nin this space. OK. So this comes back\nto this slide of I talked about where you could\nimagine some of these things being low hanging fruit, but\nmaybe those aren't the ones that we should be focusing\non we should instead be focusing on enabling\nmore data at low cost, getting more out of the\ndata that we're collecting, and helping people even\nacquire it in the first place. So that's one category of\nthings, and that's the one I just highlighted in\nthe previous slide. You can imagine\nsomething running in the background at a\nhospital system level and just checking to see\nwhether there's anybody who was missed in some ways. And then triage I'm going to\ntalk about in the next slide. I'll come back to that,\nand then really-- and this is, again, one of\nthe reasons I got into this-- we want\nto do something that elevates practice beyond\njust simply repeating what we already do. And so this idea of\nquantitative tracking of intermediate states,\nsubclasses of disease, which is actually the real\nreason I got into this space is because I wanted to\nincrease scale of data to be able to do this, and\nthis is where you potentially would like to go. So the ECG example is\nan interesting one,", "id": "MoEaRpLNo9A_39", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  have been around\nfor 40 or 50 years, and they really got going\naround the early 2000s, when people realized-- there's a pattern\ncalled an ST elevation. I'm not sure if you\nguys talked about that. This is a marker of\ncomplete stoppage of blood flow to the heart. So muscle starts to die. And then the early 2000s, there\nwas a quality movement that said, as soon as\nanybody sees that, you should get to somebody\ndoing something about it within an\nhour and a half or so. And so the problem was that in\nthe old days and the old way to do this-- and even this was\naround the time I was a resident--\nyou would have to first call the cardiologist. Wake him up. They would come. You'd send them the image. They would look at it. Then, they would\ndecide whether or not this was the pattern\nthey were seeing, and then they would activate\nthe lab, the cath lab. They would come in, and you\nwere losing about an hour, hour and a half in this process. And so instead they decided\nthat automated systems could", "id": "MoEaRpLNo9A_40", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  or emergency room docs,\nso non-cardiologists, to be able to say,\nhey, look, this is what we think is going on. Let's bring the team in, and\nso people would get mobilized. People would come\nto the hospital. Nobody would do anything in\nterms of starting the case, until somebody confirmed it,\nbut already, the whole wheels were turning. And so you have\nthis triage system, where you're making a decision. You're not finalizing\nthe decision, but you're speeding things up. And so this is an\nexample where you could imagine it's\nimportant to try to offload this to something. So this is an\nexample, and there's going to be false positives. And people will laugh and mock\nthe emergency room doctors and mock the ambulance\ndrivers and say, ah, they don't know\nwhat they're doing. They don't have any experience. But ultimately,\npeople were dying, because they were waiting\nfor the cardiologist to be available to read the ECG. So you've got to think about\nthose in terms of places where there may\nbe cost for delay. OK. So coming back to echoes. OK. So why does an echo get studied? Because this is probably not\nsomething that is typical.", "id": "MoEaRpLNo9A_41", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  are about 70 different videos\ntypically in the studies that we do at the\ncenters that we're at. And they're taken\nover multiple cycles and multiple different\nviews, and often it takes somebody pretty skilled\nto acquire those views. And they take about\n45 minutes to an hour to gather that data,\nmultiple different views, and the stenographer\nis changing the depth to zoom in on given structures. And so you can understand\nthat there's already somebody who was\nalready very experienced in this process even collecting\nthe data which is a problem. Because you need to take\nthem out of the picture, because they're expensive to\nbe able to do those things. So we were doing at\nUCSF 12,000 to 50,000. Brigham was probably a little\nbusier at 30,000 to 35,000. Medicare back, in 2011, had\nseven million of these perform, and there's probably hundreds\nof millions of these archives, so lots of data. So we published\na paper last year trying to automate really all of\nthe main processes around this, and part of the reason to do all\nis it doesn't help you to have", "id": "MoEaRpLNo9A_42", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Because at the end\nof the day, if you have to have a\ncardiologist doing everything else\nand a stenographer doing everything else,\nwhat have you really saved by having one little step? So the goal here was to start\nfrom raw study, coming straight off the machine, and\ntry to do everything. And so that involves\nsorting through all these different views, coming\nup with empirical quality score with it, segmenting all the\nfive primary views that we use. Directly detecting\nsome diseases, and then computing\nall the standard mass and volume types of measurements\nthat come from this. So we wanted to do it all,\nand this was, I think, it wasn't strikingly original in\nthe algorithms that were used. But at the same time, it\nwas very bold for anybody in the community to try to\ntake this on, and of course, in general, all the backlash\nyou could imagine when, you try to do something like this. I still hear it, but\nthere's excitement. And certainly on\nthe industry side, there's really excitement\nin that this is feasible. So I was running biology\nlab, back in 2016 or so,", "id": "MoEaRpLNo9A_43", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  so my cousin's husband is the\nDean of Engineering at Penn, and I emailed him and said, do\nyou know anyone at Berkeley? I live near there. I have a very long commute,\nand I was like closer to there. Is anybody you know there? So he's like, yeah. I know Ruzena Bajcsy there. She used to be a Penn,\nand I know Alyosha Efros. And so he just emailed them\nand said, can you meet? [INAUDIBLE] And so I met some\nof them, and then I tried to find some people\nwho were willing to work. So I just spent a day a week\nthere for about two years, just hanging out,\nwriting, code and try to get this project\noff the ground. So we have a few\ndifferent institutions. Jeff Zhang was a senior\nundergraduate at the time. He's at Illinois right\nnow as a graduate student. It's interesting, because it's\nhard to get grad student level people excited over\nstuff that's applications of existing algorithms, but\nthey're happy to advise. So I ended up having to write\na lot of the code myself. And undergraduates\nare, of course, excited to do these\nkind of things, because it's better than\nhomework, and I can pay. But I think, ultimately,\nit's interesting to try", "id": "MoEaRpLNo9A_44", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  could be interesting from an\nalgorithmic standpoint too. So I'm trying to do\nmore of that these days. OK. So we aren't the first\nto even do something around classifying views. So somebody already\nhad publish something, but we wanted to be a little\nbit more nuanced than that. In that we wanted to\nbe able to distinguish, for example, whether this\nstructure, the left ventricle, is cut off. Because we don't want to\nmeasure it if it's cut off, and we don't want to measure\nthe atrium if it's completely cut off here. So we wanted to be able\nto have a classifier able to distinguish between\nsome of those things. It's not an easy task,\nand a lot of these labels were me riding the train in\nmy very long commute from East Bay, in California, to UCSF. And so I did a lot of labeling,\nand I did a lot of segmentation too. So I could fly a lot. And that's the\nother thing that's kind of interesting is\nthat you often need-- even to do the\ngrunt work-- you may need somebody fairly specialized\nto do it which is OK, but yeah, so that ended up being\nme for a lot of this. So I traced a lot\nof these images,", "id": "MoEaRpLNo9A_45", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  But you're not going to get a\ncomputer science undergraduate to trace art structures\nfor you, nor are you going to get them\nexcited about doing this. So we didn't end up\nhaving that much data, and I think we could probably\nget better than that. But we had the five main views,\nand we implemented a modified version of unit algorithm. We imposed a bit of\na penalty to keep this problem of, for example,\na little stray ventricle being out there. We imposed a penalty\nto say, well, if that's too far away\nfrom the center then, we're going to have the\nloss function take that into account. That helped somewhat, but so\nthat was our approach to-- this is a pretty\nsubstantial deal to be able to do all\nthese things that normally would be very tedious. And as a result, when we\nstart to analyze things, we can segment every single\nframe of every single video. The typical echo reader will\ntake two frames and trace them. That's it. That's all you get. So we can do everything over\nevery single cardiac cycle, because there's amazing\nvariability from beat to beat. And so it's silly\nto think that that should be the gold standard,\nbut that is the gold standard.", "id": "MoEaRpLNo9A_46", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So that's the other thing. So it turns out that it's\nalmost impossible to get access to echoes, so I wrote a\nkeystroke encoder that sat at the front end and just\nmimicked me entering in studies and downloading them. So that was the only\nway I could get. So I had about 30,000\nstudies built up over a year, but there's no way\nto do bulk download. And so again, you've got\nto do some grunt work to be willing to play this space. So we had a fair\nnumber of studies we could use in\nterms of where we had measurements and decent\nvalues in terms of that. I think it's\ninteresting in terms of thinking about how good one\ncan-- how close one can get. And one of the things\nwe found is that, when there were big deviations--\nthese are Bland-Altman plots-- almost always the\nmanual ones were wrong. AUDIENCE: Why is that? RAHUL DEO: Oh, OK. OK. So Bland-Altman plots, so people\ndon't like using correlations in the medical-- so Bland and Altman published\na paper in the Lancet about 30 years ago\ncomplaining that correlations and correlation coefficient are\nultimately not good metrics.", "id": "MoEaRpLNo9A_47", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and really you want to know,\nif this is the gold standard, you need to get that value. So it really is just\nlooking at differences between, let's say, the\nreference value and the, let's say, automated\nvalue, and then plotting that against\nthe mean of the two. So that's it. I did it as percentages here,\nbut ultimately, it's just that. It's that you're just\ntaking the mean of, let's, say the left\nventricular volume. You have a mean of the automated\nversus the manually measured one, and then you compare\nwhat the difference is of one minus the\nother, and so you'll be on one side or the other. So ideally, you would just be\nsitting perfectly on this line, and then you're\ngoing to look and see whether or not you're clustered\non one side or the other. So that's just\nthe typical thing. People try to avoid\ncorrelation coefficients, because they kind of consider\nthem to be not really telling you whether or not-- there really is a gold\nstandard, and there truly is a value here, and you\nwant to be near that value. And so that's the\nstandard for looking", "id": "MoEaRpLNo9A_48", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So we had about 8,000 things. The reviewers gave us a hard\ntime for the space up here, and there are not that\nmany studies up here, but ultimately, there are some. And when we manually looked\nat a bunch of them, always the manual ones were just wrong. Either there is a typo\nor something like that, so that was reassuring, but\nwe were sometimes very wrong. And you'd find that the\nplaces we'd be wrong would be these ridiculously\ncomplex congenital heart studies that we had never\nbeen given examples like that before. So that's a lesson to be learned\nis that, sometimes, you're going to be really off in\nthese sorts of approaches, and you have to\nthink a little bit. And what we ended\nup doing is having an interative cycle, where we\nwould identify those and feed them back and of\nkeep on doing that, but that still needs\nto be improved upon. OK. So function, again, there's, a\ncouple of measures a function. There's a company\nthat has something out there in this\nspace, got FDA approved for having an automated\nejection fraction.", "id": "MoEaRpLNo9A_49", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  overall, but yeah. I think that that's just\none of those things you're expected to be able to do. And then here's a\nproblem that we run into. So we're comparing to the\nstatus quo which, like I said, is one person tracing two\nimages and comparing them. That's it. So we're processing potentially\n200, 300 different frames per study and competing\nmedian, smoothing across. We're doing a whole\nlot more than that. So what do we do about that\nin terms of the gold standard? And if you just take into\nobserver variability, you're going to\nhave up to 8% to 9% in absolute compared to\n60% of the reference. So that's horrible. So what are you supposed to do? And I think so one\nthing people do is they take multiple readers\nand ask them to do that. But this is like,\nare you're going to get a bunch of\ncardiologists to do like 1,000 studies for you? It's very hard to imagine\nsomebody doing that. You could compare it\nto another modality. So we haven't done this yet,\nbut you could, for example,", "id": "MoEaRpLNo9A_50", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  you're more consistent\nwith another modality. And then this is\nindirect, but you can go to like\noutcomes in a trial and see whether or not\nyou do a better job. So there are things you can do. One of the things\nwe decided to do is look for correlations\nof structures within a study itself\nand say, well, the mass-- so we know that, for\nexample, thickened hearts lead to larger\nincreases of pressure and left atrial enlargement. So we can look for correlations\nbetween those things and see whether we\ndo a better job. I'd say, for, the most\npart we're about on par with everything that's there. So I don't think\nwe're any better. Sometimes we're better. Sometimes we're worse. And I think, for the most\npart, this was another way to try to get at this, because\nwe were stuck with this. How do you work\nwith a gold standard that ultimately I don't\nthink anybody really trusts as a gold standard? And this is a problem that\njust has to keep on coming up. This is just an\nexample of where you could facilitate this idea\nof low cost serial imaging and point of care. So these are patients who\nare getting chemotherapy,", "id": "MoEaRpLNo9A_51", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  it's like inception-- is an EGFR inhibitor that\ncauses cardiac toxicity, and so people are\ngetting screening echoes. So you could imagine,\nif you make it easier to acquire and\ninterpret that, all you want to care about is\nthe function and the size. So you can imagine\nautomating that. So we just did this\nas proof of concept that you could imagine\ndoing something like this. And for the last thing\nI want to talk about-- or sorry, the last\nthing in this space-- is that you could also imagine\ndirectly detecting disease. And so you have to say, well,\nwhy is that even worthwhile? Yes. AUDIENCE: I was curious. I guess it's going back\nto the idea of if you look at blended models\nbetween human groud truth and maybe a biological ground\ntruth, [INAUDIBLE] versus sort of what you could get\nfrom an MRI or something-- or maybe not necessarily an MRI,\nbut what you were saying based", "id": "MoEaRpLNo9A_52", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  kept separate? RAHUL DEO: Yeah. These are early days\nfor a lot of this, and I think, anytime you make\nanything more complicated, then the readers will\ngive you a hard time, but you can imagine that. And especially, you\nmay want to tune things to be able to be closer\nto something like that. So yeah, I think,\nunfortunately, people are pretty conservative in\nterms of how they interpret, but it does make some\nsense that there's probably something that-- Ideally, you want to be able to\nhave something that is useful, and useful may not be exactly\nthe same thing as mimicking what humans are doing. So no, I think it's a good idea. And I think that\nthis is going to be-- this next wave-- is going to\nbe thinking a little bit more about that in terms\nof like how do we improve on what's going on\nover there, rather than simply dragging it back to that? OK. So there are multiple\nrare diseases. I use to have a clinic\nthat would focus on these, and they tend to get\nmissed at centers that don't see them that often. So one place you\ncould imagine is you can focus on trying\nto pick those up, and you could imagine, this\ncould be just surveillance", "id": "MoEaRpLNo9A_53", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  It doesn't have to be kind\nof real time identification. So there's a few\ndiseases where it's very reasonable to\ndo these things, where it's very obvious. So this is a disease called\nhypertrophic cardiomyopathy. I used to see it in my clinic. So abnormally thickened hearts,\nleading cause of sudden death in young athletes. So Reggie Lewis, there's a bunch\nof people who've died suddenly from this condition. Unstable heart rhythm,\nsudden death, heart failure, it runs in families,\nand there are things you can do,\nif you identified it. And so it's actually a fairly\neasy task, in the sense that it tends to\nbe quite obvious. So we built the classification\nmodel around this, and we tried to understand\nwhat it was doing in part. And so we tried to do some\nof these kind of attention or saliency type things, and\nthey were very unsatisfying, in part because I think there's\nso many different features across the whole image. So you're just\ngetting this blob, but I think maybe we just\nweren't implementing it correctly. I'm not really sure, but you\nhave a left atrium gets bigger. The heart gets thicker. There's so many changes\nacross the image.", "id": "MoEaRpLNo9A_54", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So we did something\nsimple and just took the output of\nthe probabilities and compared it to\nsome simple things that we actually know\nabout these things and found that there was\nsome degree of correlation. But I would like to make\nthat a little bit better. Cardiac amyloid, a very\npopular disease for which there are now therapies. And so pharma is very interested\nin identifying these people, and they really get missed\nat a pretty high rate. So we built another\nmodel for this. Usually, we had about\n250 or 300 cases for each of these things and\nmaybe a few thousand controls. And then this one's\na little interesting. This is mitral valve prolapse. So this is what a\nprolapsing valve looks like. If you imagine the plane of the\nvalve here, it buckles back. So it does this,\nand that's abnormal, and this is a normal valve. So you notice, it\ndoesn't buckle back in. So it's a little interesting\nin that there's really only one part of the cardiac\ncycle that would really highlight this abnormality,\nat least that's the way that-- so the way that\nit's read clinically", "id": "MoEaRpLNo9A_55", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  where it's buckled back. They draw an\nimaginary line across, and they measure what the\ndisplacement is there, and so we built a\nreasonable model focusing. So we phased these\nimages and picked the part of the cardiac\ncycle, those relevant, all in an automated\nway and built a model around that and pretty good, in\nterms of being able to do that, in terms of being\nin detect that. Yes. AUDIENCE: And so is this model\non images at a certain time? Like can you just go back? Because obviously, you\nweren't doing videos. Right? RAHUL DEO: Well, so we\nwould take the whole video. We were segmenting it. We were phasing it, figuring\nout what the part of the-- when was the end\nsystole in that, and then using those as the--\nso using a stack of those to be able to classify. AUDIENCE: So how do you\nknow the time point? RAHUL DEO: Well,\nthat's I'm saying. So we we're using the\nvariation in the volumes. AUDIENCE: The\nsegmentation would allow you to know the time point. RAHUL DEO: Exactly, because so\na typical echo will have an ECG to use to gate, but\nthe handhelds don't. So we want to move away\nfrom the things that involve the fanciness and\nall the bells and whistles.", "id": "MoEaRpLNo9A_56", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to be able to tell\nthe cardiac cycle. So that's how we did it. Yes. AUDIENCE: So you\nmentioned handhelds. With the ultrasounds\n[INAUDIBLE],, are they different from these? RAHUL DEO: They\nlook pretty similar. We got some now, and\nthey look pretty similar in terms of the\nquality of the images, and you can acquire\nthe very same view. So I think we haven't shown that\nwe can do it off those, in part because there just isn't\nenough training data. But they look pretty\nnice, and I know at UCSF and at Brigham, all the\nfellows are using it. It looks pretty much the same in\nterms of the-- the transducers are similar, and image\nquality is very good. Resolution is very good. Frame rate probably doesn't\nget up as high necessarily, but for the most part, I don't\nthink it's that different. So that is the next phase. Yes. AUDIENCE: Could you comment on-- so you mentioned how each\nof these three examples could be used within a\nsurveillance algorithm. RAHUL DEO: Yeah. AUDIENCE: Could you\ncomment on where along this true positive,\nfalse positive trade-off", "id": "MoEaRpLNo9A_57", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  RAHUL DEO: Yeah. That's a good point. I think it would vary for\nevery single one of those, and you really want to have\nsome costs on what the-- so I would typically err on\nthe side of higher sensitivity and dump it on the\ncardiologists to be able to-- so I would work, but I think\nyou have to pick some-- let's say, you're\na product manager. AUDIENCE: Just choose one\nof these three, and maybe-- RAHUL DEO: OK. Yeah. So this is a pretty\nrare disease. So your priors are pretty low\nin terms of these individuals. And so I think you\nprobably would probably want to err somewhere\nalong this area here, and so just working\non what the-- so you probably will still\nbe a relatively high rate of false positives\neven that space. But I would argue that it would\ntake the treating cardiologist", "id": "MoEaRpLNo9A_58", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and if you picked up one\nof those patients, that would be a big win. So I think that the cost\nprobably wouldn't be that high, and you just have\nto make the case. So therapy for\namyloid, for example, this is a nice sharp\nup stroke there. There's new drugs\nout there that are sort of begging for\npatients, and they're having a real hard\ntime identifying them. So you could imagine\nagain, it's sort of a calculus based on\nwhat the benefits would be for that identification\nand what burden you're placing on the individuals to\nhave to over read something. And you could probably\ntune that depending on what the disease is and\nwho you're pitching it to. But you're right, you're\ngoing to crush people if like 1 in 100 ends up\ntaking a true positive then you're not going\nto get many fans. Yes. AUDIENCE: Could you comment\non whether, for example, [INAUDIBLE] basis,\nthe ones that you're able to predict very\nwell at that point you just chose what\ndistinguishes the ones that", "id": "MoEaRpLNo9A_59", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  RAHUL DEO: So\nthat's a good point, and I don't really\nknow in the sense that I haven't\nlooked that closely. But I'm going to guess, they're\nvery thick and very obvious in that sort of sense. So we have a ECG model that\nmay pick this up early. What you want is\nsomething to fix it up when it's treatable, not\nhaving something that's ridiculously exaggerated. So you may need multiple\nmodalities some of which are more sensitive than others\nthat can catch earlier stage disease to be able to do that. So there are interesting\nthings about this disease in particular. So cataracts sometimes\nhappen before-- so ideally, the way you do\nthis is-- and I'm actually consulting around\nsomething like this-- you ideally want a mixture\nof electronic health record, something from other findings--\nmirror findings, eye findings, plus maybe something\ncardiac plus and have something\nthat ideally catches the disease in the ideal\nmost treated state. And maybe echo's\nnot the best one,", "id": "MoEaRpLNo9A_60", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  We have a little bit of time. OK. So UCSF is filing-- I don't know. I don't think this is\nactually patentable, but they are filing\nfor a patent. I'm just filling the paperwork\nout today in terms of-- I don't know. But my code is all\nfreely available anyway, for academic, non-profit\nuse, and they're just trying to make it better. I think, ultimately, my\nview as an academic here is to try to show what's possible. And then, if you want to\nget a commercial product, then you need people to\nweigh in on the industry side and make something pretty and\nmake it usable and all that. But I think,\nultimately, I'm trying to just show, hey, if we could\ndo this in a scalable way and find out something\nnew, then you guys can catch up and\ndo something that ultimately can be deployed. And what's interesting is I have\na collaborator in New Zealand. There, they're\nare resource poor. So they have a huge\nbacklog of patients. They don't have\nenough stenographers, and they don't have\nenough cardiologists.", "id": "MoEaRpLNo9A_61", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  quick five-minute study\nand then have automation. And so they want our accuracy\nto be a little bit better, but I think they're\nready to roll out, if we're able to get something\nthat has probably more training data. Yes. Are you from New Zealand? AUDIENCE: No. I think you started talking\nabout the trade-off between accuracy and-- so in academia, I get the\nsense that they're always chasing perfect accuracy. RAHUL DEO: Yeah. AUDIENCE: But as\nyou said, you're not going to get rid of\ncardiologists in the diagnosis. So I have a\nphilosophical question of are you chasing\nthe wrong thing? Should we chase\nperfect accuracy? RAHUL DEO: Yeah. So the question is around\nwhat should our goals be? So should we be just chasing\nafter a level of accuracy that may be either very,\nvery difficult to attain?", "id": "MoEaRpLNo9A_62", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  no clinician involved,\nshould we instead be thinking about\nsomething that gets good enough to that next step? And I think that's\na really good point. And what's interesting is-- and also it's interesting\nfrom the industry side-- is the field starts\nwith the mimicking mode, because it's much harder\nto change practice. It's much easier to just pop\nsomething in and say, hey, I know you have to make\nthese measurements. Let me make them for you,\nand you could look at them and see if you agree. So that's what ECGs do. Right? So nobody these days is\nmeasuring the QR rests width. Nobody does that. That's just not done. If you've got a number that's\nabsurd, you'll change it. But for the most part, you're\nlike, it's close enough, but you almost have\nto start with that. To do something\nthat's transformative is very hard to do. So I think something\nthat involves-- and I talked to\nDavid about this. It's sort of like the\nman-machine interface is fascinating to think\nabout how do we together come up with something better?", "id": "MoEaRpLNo9A_63", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  requires buy-in in a way\nthat's different than just you do my work for me, but\nmore that we come together to do something better. And I think that's going\nto be interesting as to how to chip away at that problem. OK. So a couple of\nmusings, then I'm going to talk a little bit about One\nBrave Idea, if we have time, or I can stop and take\nquestions instead, because it's a little\nbit of a biology venture. OK. So I do think that we\nshould really look. People give me a hard time\naround echo, and I'm like, well, ECG's been\naround for a long time, and there's automation there. So let's think about\nhow it's used there, and then see whether or not-- it's not as outlandish\nas people think. So I think a lot of these\nroutine measurements are just going to be\ndone in an automated way. Already in our software, you\ncan put out a little picture and overlay the segmentation\non the original image and say how good it looks. So that's easy. So you can do that. And then this kind of idea\nof point of care automated diagnoses can make\nsome sense around", "id": "MoEaRpLNo9A_64", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So maybe you need a\nquick check of function. Maybe you want to\nknow if they have a lot of fluid around the\nheart, and you don't necessarily want to wait. So those will be\nthe places where there may be some\nkind of innovations around just getting\nsomething done quickly. And then you always\nhave somebody checking in the background,\nlayer on, a little the heart attack\nthing I showed you, and I think this problem\nin echo is there. And so if you need\nskilled people to be able to acquire the\ndata in the first place, you're stuck, because\nthey can read an echo. A really good stenography can\nread the whole study for you. So if you already have\nthat person involved in the pipeline,\nthen it's really hard to introduce a big advance. So you need to figure out\nhow to take a primary care doc off the street, put\na machine in their hand, and let them get the image\nand then automate all the interpretation for them. And so until you can task\nshift into that space, you're stuck with having still\ntoo high a level of skill. So there are these companies\nthat are in the space now, and there's a few\nthat are trying.", "id": "MoEaRpLNo9A_65", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to classify a view,\nyou could get it to-- this gets to this\nidea of registration a little bit-- you can recognize\nif you're off by 10 degrees, or if you need a translation. You could just train a\nmodel to be able to do that. So I think that's already\nhappening right now. So it's a question as to whether\nthat will get adopted or not, but I think that,\nultimately, if you want to get shifting towards\nsort of less skilled personnel, you need to do\nsomething in that space. OK. So this is where it\ngets a little bit harder is to think about how to make\nstuff and elevate medicine beyond what we're doing. And this gets back\nto this problem I mentioned is, at\nthe end of the day, you can't find\nnew uses for echo, unless the data is\nalready there for you to be able to show\nthat there's more value than there currently is, sort\nof this chicken and egg thing. So in some sense, what I\nhope to introduce in some way that we can get much\nbigger data sets, and they don't have to\nbe 100 video data sets. They can be three\nvideo data sets, but we want to be\nable to figure out", "id": "MoEaRpLNo9A_66", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So then you can sort\nof imagine learning many more complicated things. You want to track\npeople over time. You want to look at\ntreatment responses. So you've got to look at\nwhere the money is already and see who could do this. So pharma companies\nare interested, because they have\nthese phase II trials. They may only have three\nmonths or six months to show some benefit\nfor a drug, and they're really interested in\nseeing whether there's differences after a month,\ntwo months, three months, four months. So that may be a\nplace where you get-- and they're being frugal,\nbut they have money. So you could\nimagine, if you could introduce this pipeline in there\nand just have handheld, simple, quick to acquire, far\nmore frequency, and you show a treatment response, and\nthat's kind of transformative then. Because then, you\ncould imagine, that can get rolled out in\npractice after that. So you need somebody to\nbankroll this to start with, and then you could imagine,\nonce you have a use case, then you could imagine\nit getting much more. And this idea of\nsurveillance, you could imagine that would be\nvery doable, that you could just have something taking-- The problem is, you can even\nget the data in the archives", "id": "MoEaRpLNo9A_67", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  You could just have this\nsystem looking for amyloid, looking for whatever,\nand that would be a win too is to be able to imagine\ndoing something like that. It's not putting any pressure\non the clinical workflow. It's not making\nanybody look bad. I think, ultimately, it's\ntrying to just figure out if-- well, maybe somebody\nmay be looking bad if they miss\nsomething, but yeah. I think it is just trying\nto identify individuals. And so this is an area\nI think that's hard, and so this kind\nof idea, this is where I started a little\nbit, around this kind of idea of this disease\nsubclassification and risk models. And so that's like more\nsophisticated than anything we're doing. I think we're pretty crude\nat this kind of stuff, but one of the\nchallenges is people just aren't interested in new\ncategories or new risk models, if they don't have some way\nthat they can change practice. And that becomes more\ndifficult, because then you need to not only\nintroduce the model, you need to show\nhow incorporating that model in some way is\nable to either identify", "id": "MoEaRpLNo9A_68", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  It always comes\ndown to therapies at the end of the day. So can you tell me some subclass\nof people who will do better on this drug, which\nmeans that you have to have trial data that\nhas all those people with all that data. And unfortunately, because\nechoes are so expensive and places like the Brigham\ncharge like $3,000 per echo, then you only have\nlike 100 people who have an echo in a trial\nor 300 people have an echo. You have a 5,000 person trial,\nand 5% of them have an echo. So you need to change the way\nthat gets done, because you're massively underpowered to be\nable to detect anything that's sort of a subgroup\nwithin that kind of work. So yeah, unfortunately,\nthe research pace of things outpaces the change in\npractice in terms of the space, until we're able to enable\nmore data collection. So I can stop there. I was going to talk about\nblood cells in slides. PROFESSOR: We can\ntake some questions. RAHUL DEO: Yeah. Yeah. Yeah. OK. Why don't we do that. Yes.", "id": "MoEaRpLNo9A_69", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  I remember seeing some papers\nwhere people said, well, we know roughly what to the\nanatomy should look like, and so we can fill\nin missing details. In those days, the\nslices were run before, and so they would hallucinate\nwhat the structure looked like. RAHUL DEO: Yeah. AUDIENCE: And of course, that\nhas the benefit of giving you a better model, but\nit also does risk that it's hallucinated data. Have you guys tried doing\nthat with some of the-- RAHUL DEO: Yeah. That's a great point. So OK. So the question was\nso cardiac imaging has a very long history, and so\nthere was a period of time where there's these\nkind of active modelers around morphologies\nof the heart. And so people had these\nmodels around what the heart should look like\nfrom many, many, many studies. And they were using that,\nback at the time, when you had these relatively coarse\nmulti-slice scanners for a CT, they would reconstruct\nthe 3D image of the heart", "id": "MoEaRpLNo9A_70", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  the heart should look like. And there's, of course,\na benefit to that, but some risk in the\nsense that somebody may be very different in\nthe space that's missing. And so the question is\nwhether those kind of priors can be introduced\nin some way, and it hasn't been straightforward\nas to how to do that. Whenever you look at\nthese ridiculously poor segmentations, you're\nlike, this is idiotic. We should be able to\nintroduce some of that, and I've seen people, for\nexample, put an autoencoder. That's not exactly\ngetting at it, but it's actually\ngetting it somewhat with these coarser features. But no, I think\nin terms of using some degree of\ngeometric priors, I think I may have seen some\nliterature in that space. We haven't tried anything there. We don't have any data to\ndo that, unfortunately, and I suspect,\nyeah, I just don't know how difficult that is. AUDIENCE: You mentioned\nthat you don't want to see a small additional\natrium off at a distance.", "id": "MoEaRpLNo9A_71", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  RAHUL DEO: Yeah. No. I remember when I was\nstarting this space. I was like this is idiotic. Why can't we do this? Why don't we have some\nway of doing that? We couldn't find at that\ntime any architectures that were straightforward\nto be able to do that, but I'm sure there is\nsomething in that space. And we didn't also have the\ndata for those priors ourselves. There's a long history of\nthese de novo heart modelers that exist out there from\nOxford and the New Zealand group for that\nmatter who've been doing some of this kind\nof multi-scale modeling. It will be interesting\nto see whether or not there is anybody who pushes\nforward in that space, or is it just more data? I think that's\nalways that tension. AUDIENCE: Can I ask\nabout ultrasounds? RAHUL DEO: Yeah. AUDIENCE: You didn't\nshow us ultrasounds. Right? RAHUL DEO: Yeah, I did. AUDIENCE: Oh, you did? RAHUL DEO: Yeah. The echoes are ultrasounds. AUDIENCE: Oh, OK, but that's\nreally expensive ultrasound.", "id": "MoEaRpLNo9A_72", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Like there are\ncheaper ultrasounds that you could imagine\nthat you constantly do. Right? RAHUL DEO: Yeah. So there is a company\nthat just came out with the $2,000 handheld\nultrasound, the subscription model. Yeah. So I think that Philips\nhas a handheld device around the $8,000 marker, so\n$2,000 is getting quite cheap. So that's I think the\nspace for handheld devices. AUDIENCE: We're talking about\nresource-poor countries. RAHUL DEO: Yeah. AUDIENCE: In a developing\ncountry, where maybe they have very few doctors per\npopulation kind of thing. What kind of imaging\nmight be useful that we could then apply\ncomputer vision algorithms to? RAHUL DEO: I think ultrasound\nis that sweet spot. It has versatility, and\nits cost is about where-- and I'm sure those\ncompanies rented it out for much lower cost in\nthose kinds of places too. We're putting together-- or\nI put together-- actually, it may not have been funded. I'm not sure. But looking at\nsub-Saharan Africa and collaborating with\none of the Brigham doctors", "id": "MoEaRpLNo9A_73", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and looking to try to build some\nof these automated detection type of things in that space. So no, I think there is\ndefinite interest in that, and then there may be a much\nbigger win there then the stuff I'm proposing. But yeah, no, I think\nthat's a very good point, and that would be-- it's also, it's portable. You could have a\nphone-based thing. So it's actually very\nattractive from that standpoint. PROFESSOR: [INAUDIBLE] RAHUL DEO: All right. I feel like I'm changing the\ntopic substantially but not totally. OK. So this is that slide I showed,\nand I pitched it in a way to try to motivate you\nto think of ultrasound. But I'm not sure\nultrasound really achieves all these things, in\nthe sense I wouldn't call it the greatest biological tool\nto get at underlying disease pathways. Some of these things may\nbe late, like David said, or maybe not so reversible. So we've been given this One\nBrave Idea thing $85 million now to make some dent in\na specific disease, so", "id": "MoEaRpLNo9A_74", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  It's that arrogant\ntech thing, where you just dump a lot\nof money somewhere and think you're going\nto solve all problems. And happy to take\nit, but I think that there are some problems. So this is what I\nwanted to do, so I've wanted to do this for\nprobably the last five, six years, before I\neven started here, and this has motivated me\nin part for quite a while. And so here's our problems. OK. So we're studying heart disease,\nso coronary artery disease or coronary heart disease is\nthe arteries in the heart. You can't get at those. So you can't do any biology. You can't do the stuff\nthe cancer people-- do you can biopsy that. You can't do anything there. So you're stuck with\nthe thing that you want to get at is inaccessible. I talked about how a lot of\nthe imaging is expensive, but all those other omic\nstuff is really expensive too. So that's going to\nbe not so possible, and you're not going to be able\nto do serial $1,000 proteomics on people either. That's not happening\nanytime soon. And then everything I talked\nabout, we were woefully", "id": "MoEaRpLNo9A_75", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  especially if we\nwant to characterize underlying complex\nbiological processes. So we expect we're going to\nneed high dimensional data, and we're going to\nneed huge sample sizes. There's Vladimir\nVapnik over there. And then here's another problem. OK? So this stuff takes time. These diseases take time. So if I introduce a\nnew assay right now, how am I going to\nshow that any of this is going to be beneficial? Because this disease\ndevelops or 10 to 20 years. So I'm not going to talk\nabout the solution to that, well, a little bit. OK. So one of the issues with\na lot of the data that's out there is it's not\nparticularly expressive. It's a lot of that just\nthe same clinical stuff, the same imaging stuff. So all these big studies, these\nbillion dollar big studies, ultimately just have\nechoes and MRIs and maybe a little bit of\ngenetics, but they really don't have stuff\nthat is this low cost expressive biological\nstuff that we ideally want to be able to do. So this is really expensive\nand makes $85 million look like a joke, and\nit's not all that rich in terms of complexity. So we wanted to do\nsomething different,", "id": "MoEaRpLNo9A_76", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  We're focusing on\ncirculating cells, and so this is a compromise. And there's a\nreasonably good case to be made for\ntheir involvement. So there's lots\nof data to suggest that these are causal mediators\nof coronary artery disease or coronary heart disease. So you can find\nthem in the plaques. So patients who have\nautoimmune diseases certainly have accelerated\nforms after atherosclerosis. There are drugs. There's a drug\ncalled canakinumab that inhibits IL-1 one beta\nsecretion from macrophages, and this has mortality benefit\nin coronary artery disease. There are mutations in\nthe white blood cell population themselves that are\nassociated with early heart attack. So there's a lot there,\nand this has been going-- and there's plenty\nof mouse models that show that if\nyou make mutations only in the white\nblood cell compartment, that you will completely change\nthat the disease course itself. So there's a good\namount of data out there to suggest that there is an\ninformative kind of cell type there. It's accessible.", "id": "MoEaRpLNo9A_77", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  already there that could\nbe done with some of this, and they express many of\nthe genes that are involved. And there's a window on many\nof these biological processes. So we're focusing on computer\nvision approaches to this data. So we decided, if we\ncan't do the omic stuff, because it costs too\nmuch, we're going to take slides and\nhave tens of thousands of cells per individual. And then we can introduce\nfluorescent dyes that can focus on lots\nof different organelles. And then we can potentially\nexpand the phenotypic space by adding all kinds\nof perturbations that can be able to\nunmask attributes of people that may not even be\nrelatively there at baseline. And I think I've been empowered\nby the computer vision experience with the echo\nstuff, and I'm like, hey, I can do this. I can train these models. So we're in a position\nnow where we can-- this stuff costs a few\ndollars per person. It's cheap, and\nyou can just keep on expanding phenotypic space. You can bring in drugs. You can bring in\nwhatever you want here, and you're still in\nthat dollars type range.", "id": "MoEaRpLNo9A_78", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  just a couple of\nresearch assistants were hovering around clinics. And we can do\nthousands of patients a month, so tens of\nthousands of patients a year. So we can get into a deep\nlearning sample size here, and so we want\nthese primary assays to be low cost,\nreproducible, expressive, ideally responsive to therapy. So that's this space here,\nand there's lots of stuff that we have. We have all the medical record\ndata on all these people, and we can selectively\ndo somatic sequencing. We can do genome associations. We have all ECG data. We have selective\npositron emission data. So it's lots of\nadditional thought, and we want to be able\nto walk our cheap assay towards those things\nare more expensive but for which there's\nmuch more historical data. So that's what I do\nwith my life these days, and the time problem\nhas been solved. Because we found a collaborary\nMGH who has 3 1/2 million of these records in terms of\ncell counting and cytometer data going back for\nabout three years. So we should be able to get\nsome decent events in that time.", "id": "MoEaRpLNo9A_79", "title": "10. Application of Machine Learning to Cardiac Imaging", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  million records and decide\nwhether they have coronary heart disease, but sounds\nlike that's doable. We're fearless in this space. And then they also\nhave 13 million images, so hundreds of thousands\nof people worth of slides. So we can at the very\nleast, get decent weights for transfer learning\nfrom some of this data, and we're doing this for\nacute heart attack patients. So yeah, so this is what\nI'm doing, ultimately, and so it's this bridge between\nexisting imaging, existing conventional medical\ndata, and this low cost, expressive, serial-type\nof stuff that ultimately hoping to expand phenotypic\nspace and keep the cost down. I think all my lessons from\nworking with expensive imaging data has motivated me to build\nsomething around this space. So this is my it's\nmy baby right now. And so lots of things for\npeople to be involved in, if they want to, and these are\nsome of the funding sources. All right. Thank you.", "id": "MoEaRpLNo9A_80"}, {"text": "  PETER SZOLOVITS: OK. Today's topic is\ndifferential diagnosis. And so I'm just\nquoting Wikipedia here. Diagnosis is the identification\nof the nature and cause of a certain phenomenon. And differential diagnosis\nis the distinguishing of a particular\ndisease or condition from others that present\nsimilar clinical features. So doctors typically talk\nabout differential diagnosis when they're faced\nwith a patient and they make list of what\nare the things that might be wrong with this patient. And then they go\nthrough the process of trying to figure out\nwhich one it actually is. So that's what we're\ngoing to focus on today. Now, just to scare you,\nhere's a lovely model", "id": "VuKOW8d4KHw_0", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So this is from Guyton's\ntextbook of cardiology. And I'm not going to hold\nyou responsible for all of the details of this\nmodel, but it's interesting, because this is, at least\nas of maybe 20 years ago, the state of the art of how\npeople understood what happens in the circulatory system. And it has various\ncontrol inputs that determine things like\nhow your hormone levels change various aspects of the\ncardiovascular system and how the interactions\nbetween different components of the cardiovascular\nsystem affect each other. And so in principle, if I\ncould tune this model to me,", "id": "VuKOW8d4KHw_1", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that say if I increase my\nsystemic vascular resistance, then here's what's\ngoing to happen as the rest of the\nsystem adjusts. And if I get a blockage\nin a coronary artery, then here's what's going to\nhappen to my cardiac output and various other things. So this would be terrific. And if we had this\nkind of model for not just the cardiovascular system,\nbut the entire body, then we'd say, OK, we've solved medicine. Well, we don't have this kind\nof model for most systems. And also, there's\nthis minor problem that if I give you this\nmodel and say, \"How does this relate to a\nparticular patient?\", how would you figure that out? This has hundreds of\ndifferential equations that are being represented\nby this diagram.", "id": "VuKOW8d4KHw_2", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And so we were joking when we\nstarted working with this model that you'd really have to\nkill the patient in order to do enough measurements to\nbe able to tune this model to their particular physiology. And of course, that's probably\nnot a good practical approach. We're getting a little better\nby developing more non-invasive ways of measuring these things. But that's moving\nalong very slowly. And I don't expect that I\nor maybe even any of you will live long enough that\nsort of this approach to doing medical reasoning\nand medical diagnosis is actually going to happen. So what we're going\nto look at today is what simpler models are\nthere for diagnostic reasoning.", "id": "VuKOW8d4KHw_3", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  a bit of history on you, because\nI think it's interesting where a lot of these ideas came from. So the first idea was\nto build flowcharts. Oh, and by the way,\nthe signs and symptoms, I've forgotten if we've talked\nabout that in the class. So a sign is something\nthat a doctor sees, and a symptom is something\nthat the patient experiences. So a sign is objective. It's something that can\nbe told outside your body. A symptom is something\nthat you feel. So if you're feeling\ndizzy, then that's a symptom, because it's not\nobvious to somebody outside you that you're dizzy, or that you\nhave a pain, or such things. Normally, we talk about\nmanifestations or findings,", "id": "VuKOW8d4KHw_4", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that are determinable\nabout a patient. So we'll talk about\nflowcharts, models based on associations\nbetween diseases and these manifestations. Then there are some issues\nabout whether you're trying to diagnose a single\ndisease or a multiplicity of diseases, which makes the\nmodels much more complicated whether you're trying to\ndo probabilistic diagnosis or definitive or categorical. And then we'll talk about some\nutility theoretic methods. And I'll just mention some\nrule-based and pattern-matching kinds of approaches. So this is kind of cute. This is from 1973. And if you were a woman and\nwalked into the MIT Health Center and complained\nof potentially a urinary tract infection,\nthey would take out", "id": "VuKOW8d4KHw_5", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and they would check\na bunch of boxes. And if you hit a red box,\nthat represented a conclusion. And otherwise, it\ngave you suggestions about what further tests to do. And this was essentially\na triage instrument. It said, does this woman\nhave a problem that requires immediate attention? And so we should either\ncall an ambulance and take them to a hospital,\nor is it something where we can just tell them to\ncome back the next day and see a doctor,\nor is it in fact some self-limited thing where\nwe say, take two aspirin, and it'll go away. So that was the attempt here. Now, interestingly, if\nyou look at the history of this project between the\nBeth Israel Hospital and Lincoln Laboratories, it started\noff as a computer aid. So they were building\na computer system", "id": "VuKOW8d4KHw_6", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And then in-- but you can\nimagine, in the late 1960s, early 1970s, computers\nwere pretty clunky. PCs hadn't been invented yet. So this was like mainframe\nkinds of operations. It was very hard to use. And so they said, well, this\nis a small enough program that we can reduce it to\nabout 20 flow sheets-- 20 sheets like this, which\nthey proceeded to print up. And I was amused,\nbecause in the-- around 1980, I was working\nin my office one night. And I got this\nsplitting headache. And I went over to MIT medical. And sure enough,\nthe nurse pulled out one of these sheets\nfor headaches and went through it\nwith me and decided that a couple of\nTylenols should fix me. But it was interesting. So this was really\nin use for a while.", "id": "VuKOW8d4KHw_7", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of which there have\nbeen many, many, many in the medical world,\nis that they're very fragile. They're very specific. They don't take account\nof unusual cases. And there's a lot of effort\nin coming to consensus to build these things. And then they're not necessarily\nuseful for a long time. So MIT actually stopped\nusing them shortly after my headache experience. But if you go over\nto a hospital and you look on the bookshelf\nof a junior doctor, you will still find\nmanuals that look kind of like this\nthat say, how do we deal with tropical diseases? So you ask a bunch of\nquestions, and then depending on the branching\nlogic of the flowchart, it'll tell you whether\nthis is serious or not.", "id": "VuKOW8d4KHw_8", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  in Boston, you're\nnot going to see very many tropical diseases. And so you don't have\na base of experience on the basis of which\nyou can learn and become an expert at doing it. And so they use this as\na kind of cheat sheet. I mentioned that the association\nbetween diseases and symptoms is another important\nway of doing diagnosis. And I swear to you, there was\na paper in the 1960s, I think, that actually proposed this. So if any of you have hung\naround ancient libraries, libraries used to have\ncard catalogs that were physical pieces\nof paper, cardboard. And one of the things they\ndid with these was each card would be a book. And then around the edges\nwere a bunch of holes,", "id": "VuKOW8d4KHw_9", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  along various dimensions,\nlike its Dewey decimal number, or the top digits of\nits Library of Congress number or something, they would\npunch out holes in the borders. And this allowed\nyou to do a kind of easy sorting of these books. So if you've got\na bunch of cards together when people were\nreturning their books and you pulled a bunch of cards. And you wanted to find\nall the math books. So what you would do is you'd\nstick a needle through the hole that represented math books,\nand then you shake the pile, and all the math\nbooks would fall out because they had punched. So somebody seriously proposed\nthis as a diagnostic algorithm. And in fact, implemented it. And was trying to\neven make money on it. I think this was an attempt at\na commercial venture, where they", "id": "VuKOW8d4KHw_10", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  represented diseases. And the holes now\nrepresented not mathematics versus literature,\nbut they represented shortness of breath versus\npain in the left ankle versus whatever. And again, as people\ncame in and complained about some condition,\nyou'd stick a needle through that condition\nand you'd shake, and up would come the cards that\nhad that condition in common. So one of the obvious\nproblems with this approach is that if you had two\nthings wrong with you, then you would wind up with no cards\nvery quickly, because nothing would fall out of the pile. So this didn't go anywhere. But interestingly,\neven in the late 1980s, I remember being asked by the\nboard of directors of the New", "id": "VuKOW8d4KHw_11", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  where they had gotten a pitch\nfrom somebody who was proposing essentially exactly\nthis diagnostic model, except implemented in a computer\nnow and not in these library cards. And they wanted to know\nwhether this was something that they ought to get\nbehind and invest in. And I and a bunch\nof my colleagues assured them that this was\nprobably not a great idea and they should stay away\nfrom it, which they did. Well, a more sophisticated model\nis something like a Naive Bayes model that says if\nyou have a disease-- where is my cursor? If you have a disease, and you\nhave a bunch of manifestations that can be caused\nby the disease, we can make some\nsimplifying assumptions that say that you\nwill only ever have one disease at a\ntime, which means that the values of that node D\nform an exhaustive and mutually", "id": "VuKOW8d4KHw_12", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And we can assume that\nthe manifestations are conditionally independent\nobservables that depend only on the disease that you\nhave, but not on each other or not on any other factors. And if you make that\nassumption, then you can apply good old\nThomas Bayes's rule. This, by the way, is\nthe Reverend Bayes. Do you guys know his history? So he was a nonconformist\nminister in England. And he was not a\nmathematician, except I mean, he was an amateur mathematician. But he decided that\nhe wanted to prove to people that God existed. And so he developed\nBayesian reasoning in order to make this proof. And so his argument\nwas, well, suppose", "id": "VuKOW8d4KHw_13", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So you have 50/50\nodds that God exists. And then you say,\nlet's look at miracles. And let's ask, what's the\nlikelihood of this miracle having occurred if God exists\nversus if God doesn't exist? And so by racking up\na bunch of miracles, you can convince people more\nand more that God must exist, because otherwise\nall these miracles couldn't have happened. So he never publish this in his\nlifetime, but after his death one of his colleagues\nactually presented this as a paper at the Royal\nSociety in the UK. And so Bayes became\nfamous as the originator of this notion of how to do\nprobabilistic reasoning about at least fairly\nsimple situations, like in his case, the existence\nor nonexistence of God. Or in our case, the\ncause of some disease,", "id": "VuKOW8d4KHw_14", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And so you can draw these trees. And Bayes's rule is very simple. I'm sure you've all seen it. One thing that, again,\nmakes contact with medicine is that a lot of times,\nyou're not just interested in the impact of one\nobservable on your probability distribution, but you're\ninterested in the impact of a sequence of observations. And so one thing you\ncan do is you can say, well, here is my\ngeneral population. So let's say disease 2 has\n37% prevalence and disease 1 has 12%, et cetera. And now I make some observation. I apply Bayes's rule. And I revise my\nprobability distribution. So this is the equivalent of\nfinding a smaller population of patients who have\nall had whatever answer I got for symptom 1.", "id": "VuKOW8d4KHw_15", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And so this is the sequential\napplication of Bayes's rule. And of course, it does depend\non the conditional independence of all these symptoms. But in medicine,\npeople don't like to do math, even\narithmetic much. And they prefer doing addition\nrather than multiplication, because it's easier. And so what they've\ndone is they said, well, instead of\nrepresenting all this data in a probabilistic framework,\nlet's represent it as odds. And if you represent it\nas odds, then the odds of some disease given\na bunch of symptoms, given the independence\nassumption, is just the prior\nodds of the disease times the conditional\nodds, the likelihood ratio of each of the symptoms\nthat you've observed.", "id": "VuKOW8d4KHw_16", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And then because they like\nadding more than multiplying, they said, let's take\nthe log of both sides. And then you can just add them. And so if you remember when I\nwas talking about medical data, there are things\nlike the Glasgow Coma score, or the APACHE\nscore, or various measures of how badly or well a patient\nis doing that often involve adding up numbers corresponding\nto different conditions that they have. And what they're\ndoing is exactly this. They're applying\nsequentially Bayes's rule with these independence\nassumptions in the form of logs rather than\nmultiplications, log odds, and that's how they're doing it. Very quickly. Somebody in a previous lecture\nwas wondering about receiver", "id": "VuKOW8d4KHw_17", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And I just wanted to give you a\nlittle bit of insight on those. So if you do a test on two\npopulations of patients-- the red ones are sick patients. The blue ones are\nnot sick patients. You do some test. What you expect is that\nthe result of that test will be some continuous\nnumber, and it'll be distributed something\nlike the blue distribution for the well patients\nand something like the red distribution\nfor the ill patients. And typically, we\nchoose some threshold. And we say, well,\nif you choose this to be the threshold between\na prediction of sick or well, then what\nyou're going to get is that the part of the\nblue distribution that lies to the right is\nthe false positives and the part of the\nred distribution that lies to the left is\nthe false negatives. And often people will choose the\nlowest point at which these two", "id": "VuKOW8d4KHw_18", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  isn't necessarily the case. Now, if I give\nyou a better test, one like this, that's terrific,\nbecause there is essentially no overlap. Very small false negative\nand false positive rates. And as I said, you\ncan choose to put the threshold in\ndifferent places, depending on how you\nwant to trade off sensitivity and specificity. And we measure this by\nthis receiver operator characteristics curve,\nwhich has the general form that if you get a\ncurve like this, that means that there's an\nexact trade-off for sensitivity and specificity, which is the\ncase if you're flipping coins. So it's random. And of course, if you manage\nto hit the top corner up there, that means that there\nwould be no overlap whatsoever between\nthe two distributions,", "id": "VuKOW8d4KHw_19", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And so typically you get\nsomething in between. And so normally,\nif you do a study and your AUC, the area\nunder this receiver operator characteristics curve,\nis barely over a half, you're pretty\nclose to worthless, whereas if it's\nclose to 1, then you have a really good\nmethod for distinguishing these categories of patients. Next topic. What does it mean\nto be rational? I should have a\nphilosophy course here. AUDIENCE: Are you\ntalking about pi? PETER SZOLOVITS: Sorry. AUDIENCE: Are you\ntalking about pi? Pi is-- PETER SZOLOVITS:\nPi is irrational, but that's not what\nI'm talking about. Well, so there is this\nprinciple of rationality that says that what you want\nto do is to act in such a way", "id": "VuKOW8d4KHw_20", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So for example, if\nyou're a gambler and you have a choice of various\nways of betting in some poker game or something, then if\nyou were a perfect calculator of the odds of getting a\nqueen on your next draw, then you could make\nsome rational decision about whether to\nbet more or less, but you'd also have to take into\naccount things like, \"How could I convince my opponent that I am\nnot bluffing if I am bluffing?\" and \"How could I convince them\nthat I'm bluffing if I'm not bluffing?\" and so on. So there is a\ncomplicated model there. But nevertheless,\nthe idea is that you should behave in a\nway that will give you the best expected outcome. And so people joke that\nthis is Homo economicus, because economists make\nthe assumption that this", "id": "VuKOW8d4KHw_21", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And we now know that that's\nnot really how people behave. But it's a pretty common\nmodel of their behavior, because it's easy\nto compute, and it has some appropriate\ncharacteristics. So as I mentioned,\nevery action has a cost. And utility measures the\nvalue or the goodness of some outcome, which is the\namount of money you've won, or whether you live or die, or\nquality adjusted life years, or various other\nmeasures of utility-- how much it costs for\nyour hospitalization. So let me give you an example. This actually comes from a\ndecision analysis service at New England\nMedical Center Tufts Hospital in the late 1970s. So this was an elderly\nChinese gentleman whose foot had gangrene. So gangrene is an infection\nthat usually people", "id": "VuKOW8d4KHw_22", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And what he was\nfacing was a choice of whether to amputate\nhis foot or to try to treat him medically. To treat him medically\nmeans injecting antibiotics into his system and hoping that\nhis circulation is good enough to get them to the\ninfected areas. And so the choice becomes\na little more complicated, because if the medical treatment\nfails, then, of course, the patient may\ndie, a bad outcome. Or you may have to now\namputate the whole leg, because the gangrene has spread\nfrom his foot up the foot, and now you're\ncutting off his leg. So what should you do? And how should you\nreason about this? So Pauker's staff came up\nwith this decision tree. By the way, decision tree in\nthis literature means something different from decision\ntree in like C4.5.", "id": "VuKOW8d4KHw_23", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  or start with medical care. And if you amputate\nthe foot, let's say there is a 99% chance\nthat the patient will live. There's a 1% chance that maybe\nthe anesthesia will kill him. And if we treat\nhim medically, they estimated that there is a\n70% chance of full recovery, a 25% chance that he'd\nget worse, a 5% chance that he would die. If he got worse, you're now\nfaced with another decision, which is, do we\namputate the whole leg or continue pushing medicine? And again, there are various\noutcomes with various estimated probabilities. Now, the critical thing here\nthat this group was pushing was the idea that\nthese decisions shouldn't be based on what the\ndoctor thinks is good for you.", "id": "VuKOW8d4KHw_24", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And so they worked\nvery hard to try to elicit individualized\nutilities from patients. So for example, this guy said\nthat having your foot amputated was worth 850 points on a scale\nof 1,000 where being healthy was 1 and being dead was 0. Now, you could imagine\nthat that number would be very different\nfor different individuals. If you asked LeBron\nJames how bad would it be to have your\nfoot amputated, he might think that it's\nmuch worse than I would, because it would be a pain\nto have my foot amputated, but I could still do\nmost of the things that I do professionally,\nwhereas he probably couldn't as a star basketball player.", "id": "VuKOW8d4KHw_25", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Well, you say, OK,\nat every chance node I can calculate the expected\nvalue of what happens here. So here at it's 0.6\ntimes 995, 0.4 times 0. That gets me a value\nfor this decision. Do the same thing here. I compare the values here\nand choose the best one. That gives me a value\nfor this decision. And so I fold back\nthis decision tree. And my next slide should have-- yeah, so these are the\nnumbers that you get. And what you discover\nis that the utility of trying medical treatment\nis somewhat higher than the utility of\nimmediately amputating the foot if you believe these\nnumbers and those utilities, these probabilities\nand those utilities. Now, the difficulty is that\nthese numbers are fickle.", "id": "VuKOW8d4KHw_26", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And you say, for example,\nwhat if this gentleman valued his living\nwith an amputated foot at 900 rather than 850. And now you discover\nthat amputating the foot looks like a slightly better\ndecision than the other. So this is actually applied\nin clinical medicine. And there are now\nthousands of doctors who have been trained\nin these techniques and really try to work through\nthis with individual patients. Of course, it's used much more\non an epidemiological basis when people look at\nlarge populations. AUDIENCE: I have a question. PETER SZOLOVITS: Yeah. AUDIENCE: How are the\nprobabilities assessed? PETER SZOLOVITS:\nSo the service that did this study would\nread the literature, and they would\nlook in databases.", "id": "VuKOW8d4KHw_27", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  We can do a lot better today\nthan they could at that time, because we have a lot more\ndata that you can look in. But you could say,\nOK, for people-- men of this age who have\ngangrenous feet, what fraction of them have\nthe following experience? And that's how\nthese are estimated. Some of it feels like 5%. OK. So I just said this. And then the\nquestion of where do you get these utilities\nis a tricky one. So one way is to do the\nstandard gamble, which says, OK, Mr. Szolovits,\nwe're going to play this game.", "id": "VuKOW8d4KHw_28", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  with some continuous\nnumber between 0 and 1, and then I'm going to play the\ngame where either I chop off your foot, or I roll\nthis die and if it exceeds some threshold,\nthen I kill you. Nice game. So now if you find the point\nat which I'm indifferent, if I say, well, 0.8, that's\na 20% chance of dying. It seems like a lot. But maybe I'll go\nfor 0.9, right? Now you've said,\nOK, well, that means that you value\nliving without a foot at 0.9 of the value\nof being healthy.", "id": "VuKOW8d4KHw_29", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And this is typically done. Unfortunately, of course,\nit's difficult to ascertain the problem. And it's also not stable. So people have done experiments\nwhere they get somebody to give them this kind of\nnumber as a hypothetical, and then when that\nperson winds up actually faced with such a\ndecision, they no longer will abide with that number. So they've changed their mind\nwhen the situation is real. AUDIENCE: But it's nice, because\nthere are two feet, right? So you could run this\nexperiment and see. PETER SZOLOVITS: They\ndidn't actually do it. It was hypothetical. OK. Next program I want to\ntell you about, again, the technique for this was\ndeveloped as a PhD thesis here at MIT in 1967. So this is hot off the presses. But it's still used,\nthis type of idea.", "id": "VuKOW8d4KHw_30", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Journal of Medicine, which is\na high impact medical journal. I think this was\nactually the first sort of computational program\nthat journal had ever published as a medical journal. And it was addressed\nat the problem of the diagnosis of acute\noliguric renal failure. Oliguric means you're\nnot peeing enough. Renal is your kidney. So this is something's gone\nwrong with your kidney, and you're not\nproducing enough urine. Now, this is a good problem to\naddress with these techniques, because if something\nhappens to you suddenly, it's very likely that\nthere is one cause for it. If you are 85 years old\nand you have a little heart disease and a little\nkidney disease", "id": "VuKOW8d4KHw_31", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  there's no guarantee that there\nwas one thing that went wrong with you that caused all these. But if you were OK yesterday\nand then you stopped peeing, it's pretty likely that there's\none thing that's gone wrong. So it's a good\napplication of this model. So what they said is there\nare 14 potential causes. And these are exhaustive\nand mutually exclusive. There are 27 tests or\nquestions or observations that are relevant\nto the differential. These are cheap\ntests, so they didn't involve doing anything\neither expensive or dangerous to the patient. It was measuring\nsomething in the lab or asking questions\nof the patient. But they didn't want to\nhave to ask all of them, because that's pretty tedious. And so they were\ntrying to minimize the amount of\ninformation that they needed to gather\nin order to come up with an appropriate decision.", "id": "VuKOW8d4KHw_32", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that are dangerous\nand expensive, and then eight\ndifferent treatments that could be applied. And I'm only going to tell\nyou about the first part of this problem. This 1973 article shows you\nwhat the program looked like. It was a computer terminal\nwhere it gave you choices, and you would type in an answer. And so that was the state\nof the art at the time. But what I'm going to\ndo is, god willing, I'm going to demonstrate\na reconstruction that I made of this program. So these guys are the potential\ncauses of stopping to pee-- acute tubular necrosis,\nfunctional acute renal failure, urinary tract obstruction, acute\nglomerulonephritis, et cetera.", "id": "VuKOW8d4KHw_33", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Now, I have to warn\nyou, these numbers were, in fact, estimated by\npeople sticking their finger in the air and figuring out\nwhich way the wind was blowing, because in 1973, there were not\ngreat databases that you could turn to. And then these are\nthe questions that were available to be asked. And what you see in\nthe first column, at least if you're sitting\nclose to the screen, is the expected entropy of\nthe probability distribution if you answered this question. So this is basically saying, if\nI ask this question, how likely is each of the possible answers,\ngiven my disease distribution probabilities? And then for each\nof those answers, I do a Bayesian revision,\nthen I weight the entropy of that resulting distribution\nby the probability of getting", "id": "VuKOW8d4KHw_34", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And that gets me\nthe expected entropy for asking that question. And the idea is that the\nlower the expected entropy, the more valuable the question. Makes sense. So if we look, for example,\nthe most valuable question is, what was the blood pressure\nat the onset of oliguria? And I can click on this\nand say it was, let's say, moderately elevated. And what this little\ncolorful graph is showing you is that if you\nlook at the initial probability distribution, acute tubular\nnecrosis was about 25%, and has gone down to\na very small amount, whereas some of these\nothers have grown in importance considerably.", "id": "VuKOW8d4KHw_35", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  let's see. What is the degree-- is there proteinuria? Is there protein in the urine? And we say, no, there isn't. I think we say, no, there isn't. 0. And that revises the\nprobability distribution. And then it says the next most\nimportant thing is kidney size. And we say-- let's say\nthe kidney size is normal. So now all of a sudden\nfunctional acute renal failure, which, by the way, is one\nof these funny medical care categories that says\nit doesn't work well, doesn't explain to why\nit doesn't work well, but it's sort of\na generic thing. And sure enough. We can keep answering\nquestions about, are you producing\nless than 50 ccs of urine, which is a tiny\namount, or somewhere between 50", "id": "VuKOW8d4KHw_36", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Remember, this is for people\nwho are not producing enough. So normally you'd be over 400. So these are the only choices. So let's say it's moderate. And so you see the probability\ndistribution keeps changing. And what happened in\nthe original program is they had an\narbitrary threshold that said when the probability of one\nof these causes of the disease reaches 95%, then we\nswitch to a different mode, where now we're actually\nwilling to contemplate doing the expensive tests and\ndoing the expensive treatments. And we build the\ndecision tree, as we saw in the case of the\ngangrenous foot, that figures out which of those\nis the optimal approach. So the idea here was\nbecause building a decision tree with 27 potential questions\nbecomes enormously bushy,", "id": "VuKOW8d4KHw_37", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  information maximization\nor entropy reduction is a reasonable way of\nfocusing in on what's wrong with this patient. And then once we focused\nin pretty well, then we can begin to do\nmore detailed analysis on the remaining more\nconsequential and more costly tests that are available. Now, this program didn't\nwork terribly well, because the numbers\nwere badly estimated, and also because of the\nutility model that they had for the decision analytic\npart was particularly terrible. It didn't really reflect\nanything in the real world. They had an incremental utility\nmodel that said the patient either got better, or stayed\nthe same, or got worse.", "id": "VuKOW8d4KHw_38", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  but they didn't correspond\nto how much better he got or how much worse he got. And so it wasn't\nterribly useful. So nevertheless, in the 1990s,\nI was teaching a tutorial at a Medical\nInformatics conference, and there were a bunch of\ndoctors in the audience. And I showed them this program. And one of the doctors came\nup afterwards and said, wow, it thinks just the way I do. And I said, really? I don't think so. But clearly, it\nwas doing something that corresponded\nto the way that he thought about these cases. So I thought that\nwas a good thing. All right. Well, what happens\nif we can't assume that there's just a single\ndisease underlying the person's problems?", "id": "VuKOW8d4KHw_39", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  can build this kind\nof bipartite model that says we have\na list of diseases and we have a list\nof manifestations. And some subset of\nthe diseases can cause some subset\nof the symptoms, of the manifestations. And so the manifestations\ndepend only on the diseases that are\npresent, not on each other. And therefore, we have\nconditional independence. And this is a type of\nBayesian network, which can't be solved exactly\nbecause of the computational complexity. So a program I'll\nshow you in a minute had 400 or 500 diseases and\nthousands of manifestations. And the computational complexity\nof exact solution techniques for these networks tends\nto go exponentially with the number of undirected\ncycles in the network. And of course, there are\nplenty of undirected cycles in a network like that.", "id": "VuKOW8d4KHw_40", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  in the early 1970s\ncalled Dialog. And then they got sued, because\nsomebody owned that name. And then they\ncalled it Internist, and they got sued because\nsomebody owned that name. And then they\ncalled it QMR, which stands for Quick\nMedical Reference, and nobody owned that name. So around 1982, this program\nhad about 500 diseases, which they estimated\nrepresented about 70% to 75% of major diagnoses in\ninternal medicine, about 3,500 manifestations. And it took about 15 man\nyears of manual effort to sit there and read medical\ntextbooks and journal articles and look at records of\npatients in their hospital. The effort was led by\na computer scientist", "id": "VuKOW8d4KHw_41", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  at UPMC, the University of\nPittsburgh Medical Center, who was just a fanatic. And he got all the\nmedical school trainees to spend hours and hours\ncoming up with these databases. By 1997, they had\ncommercialized it through a company that had\nbought the rights to it. And they had-- that\ncompany had expanded it to about 750 diagnoses and\nabout 5,500 manifestations. So they made it\nconsiderably larger. Details are-- I've tried to put\nreferences on all the slides. So here's what data\nin QMR looks like. For each diagnosis,\nthere is a list of associated\nmanifestations with evoking strengths and frequencies. So I'll explain\nthat in a minute. On average, there are about\n75 manifestations per disease.", "id": "VuKOW8d4KHw_42", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  for each manifestation\nin addition to the data you see here, there is\nalso an important measure that says how critical is it to\nexplain this particular symptom or sign or lab value\nin the final diagnosis. So for example, if\nyou have a headache, that could be\nincidental and it's not that important to explain it. If you're bleeding from your\ngastrointestinal system, that's really\nimportant to explain. And you wouldn't\nexpect a diagnosis of that patient that\ndoesn't explain to you why they have that symptom. And then here is an example\nof alcoholic hepatitis. And the two numbers here are\na so-called evoking strength and a frequency. These are both on scales-- well, evoking strength\nis on a scale of 0 to 5,", "id": "VuKOW8d4KHw_43", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And I'll show you what\nthose are supposed to mean. And so, for example,\nwhat this says is that if you're\nanorexic, that should not make you think about alcoholic\nhepatitis as a disease. But you should expect\nthat if somebody has alcoholic hepatitis, they're\nvery likely to have anorexia. So that's the frequency number. This is the evoking\nstrength number. And you see that there\nis a variety of those. So much of that many,\nmany years of effort went into coming\nup with these lists and coming up with\nthose numbers. Here are the scales. So the evoking strength-- 0 means nonspecific. 5 means its pathognomonic. In other words, just\nseeing the symptom is enough to convince\nyou that the patient must have this disease.", "id": "VuKOW8d4KHw_44", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and 5 means that it occurs\nin essentially all cases with scaled values in between. And these are kind\nof like odds ratios. And they add them kind of as\nif they were log likelihood ratios. And so there's been\na big literature on trying to figure out exactly\nwhat these numbers mean, because there's no formal\ndefinition in terms of you count the number of this and\ndivide by the number of that, and that gives you\nthe right answer. These were sort of the\nimpressionistic kinds of numbers. So the logic in the system\nwas that you would come to it and give it a list of the\nmanifestations of a case. And to their credit, they went\nafter very complicated cases. So they took clinical\npathologic conference cases", "id": "VuKOW8d4KHw_45", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  These are cases selected to be\ndifficult enough that doctors are willing to read these. And they're typically presented\nat Grand Rounds at MGH by somebody who is often\nstumped by the case. So it's an opportunity to watch\npeople reason interactively about these things. And so you evoke\nthe diagnoses that have a high evoking strength\nfrom the giving manifestations. And then you do a\nscoring calculation based on those numbers. The details of this\nare probably all wrong, but that's the way\nthey went about it. And then you form a\ndifferential around the highest scoring diagnosis. Now, this is actually\nan interesting idea. It's a heuristic idea, but it's\none that worked pretty well. So suppose I have two diseases. D1 can cause\nmanifestations 1 through 4.", "id": "VuKOW8d4KHw_46", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So are these competing to\nexplain the same case or could they be complementary? Well, until we know what\nsymptoms the patient actually has, we don't know. But let's trace through this. So suppose I tell you that\nthe patient has manifestations 3 and 4. OK. Well, you would say,\nthere is no reason to think that the patient\nmay have both diseases, because either of them can\nexplain those manifestations, right? So you would consider\nthem to be competitors. What about if I add M1? So here, it's getting\na little dicier. Now you're more likely\nto think that it's D1. But if it's D1, that could\nexplain all the manifestations,", "id": "VuKOW8d4KHw_47", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  On the other hand,\nif I also add M6, now neither disease can\nexplain all the manifestations. And so it's more likely,\nsomewhat more likely, that there may be\ntwo diseases present. So what Internist had was\nthis interesting heuristic, which said that when you get\nthat complementary situation, you form a differential around\nthe top ranked hypothesis. In other words, you retain\nall those diseases that compete with that hypothesis. And that defines\na subproblem that looks like the acute\nrenal failure problem, because now you have\none set of factors that you're trying to\nexplain by one disease. And you set aside all of\nthe other manifestations", "id": "VuKOW8d4KHw_48", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  are potentially complementary. And you don't worry about\nthem for the moment. Just focus on this\ncluster of things that are competitors\nto explain some subset of the manifestations. And then there are different\nquestioning strategies. So depending on the scores\nwithin these things, if one of those diseases\nhas a very high score and the others have\nrelatively low scores, you would choose a pursue\nstrategy that says, OK, I'm interested\nin asking questions that will more\nlikely convince me of the correctness of\nthat leading hypothesis. So you look for the things\nthat it predicts strongly. If you have a very large\nlist in the differential, you might say, I'm\ngoing to try to reduce the size of the differential\nby looking for things that are likely in some of the\nless likely hypotheses", "id": "VuKOW8d4KHw_49", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So different strategies. And I'll come back to\nthat in a few minutes. So their test, of course,\nbased on their own evaluation was terrific. It did wonderfully well. The paper got published\nin The New England Journal of Medicine, which was an\nunbelievable breakthrough to have an AI program that\nthe editors of The New England Journal considered interesting. Now, unfortunately, it\ndidn't hold up very well. And so there was this\npaper by Eta Berner and her colleagues in\n1994 where they evaluated QMR and three other programs. DXplain is very similar\nin structure to QMR. Iliad and Meditel\nare Bayesian network, or almost naive\nBayesian types of models", "id": "VuKOW8d4KHw_50", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And they looked for\nresults, which is coverage. So what fraction of the real\ndiagnoses in these 105 cases that they chose to test on could\nany of these programs actually diagnose? So if the program didn't\nknow about a certain disease, then obviously it wasn't\ngoing to get it right. And then they said, OK, of\nthe program's diagnoses, what fraction were considered\ncorrect by the experts? What was the rank order\nof that correct diagnosis among the list of diagnoses\nthat the program gave? The experts were asked to list\nall the plausible diagnoses from these cases. What fraction of those showed\nup in the program's top 20? And then did the program have\nany value added by coming up with things that the experts\nhad not thought about,", "id": "VuKOW8d4KHw_51", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  them were reasonable\nexplanations for this case? So here are the results. And what you see is that the\ndiagnoses in these 105 test cases, 91% of them appeared\nin the DXplain program, but, for example, only 73%\nof them in the QMR program. So that means that\nright off the bat it's missing about a quarter\nof the possible cases. And then if you look\nat correct diagnosis, you're seeing numbers like\n0.69, 0.61, 0.71, et cetera. So these are-- it's like the\ndog who sings, but badly, right? It's remarkable that\nit can sing at all, but it's not something\nyou want to listen to.", "id": "VuKOW8d4KHw_52", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  is at like 12 or\n10 or 13 or so on. So it is in the top 20, but it's\nnot at the top of the top 20. So the results were\na bit disappointing. And depending on where\nyou put the cut off, you get the proportion of cases\nwhere a correct diagnosis is within the top end. And you see that at 20,\nyou're up at a little over 0.5 for most of these programs. And it gets better if you extend\nthe list to longer and longer. Of course, if you\nextended the list to 100, then you reach 100%, but it\nwouldn't be practically very useful. AUDIENCE: Why didn't\nthey somehow compare it to the human decision? PETER SZOLOVITS:\nWell, so first of all, they assumed that their\nexperts were perfect.", "id": "VuKOW8d4KHw_53", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So they were comparing\nit to a human in a way. AUDIENCE: Yeah. PETER SZOLOVITS: OK. So the bottom line is that\nalthough the sensitivity and specificity\nwere not impressive, the programs were\npotentially useful, because they had interactive\ndisplays of signs and symptoms associated with diseases. They could give you\nthe relative likelihood of various diagnoses. And they concluded\nthat they needed to study the effects of\nwhether a program like this actually helped a doctor\nperform medicine better. So just here's an example. I did a reconstruction\nof this program. This is the kind of\nexploration you could say. So if you click on\nangina pectoris,", "id": "VuKOW8d4KHw_54", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So you can browse\nthrough its database. You can type in an example\ncase, or select an example case. So this is one of those\nclinical pathological conference cases, and then\nthe manifestations that are present and\nabsent, and then you can get an\ninterpretation that says, OK, this is our differential. And these are the\ncomplementary hypotheses. And therefore these\nare the manifestations that we set aside, whereas\nthese are the ones explained by that set of diseases. And so you could watch how the\nprogram does its reasoning. Well, then a group at\nStanford came along when belief networks\nor Bayesian networks were created, and\nsaid, hey, why don't we treat this database as if\nit were a Bayesian network", "id": "VuKOW8d4KHw_55", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So they had to fill\nin a lot of details. They wound up using\nthe QMR database with a binary interpretation. So a disease was\npresent or absent. The manifestation was\npresent or absent. They used causal independence,\nor a leaky noisy-OR, which I think you've\nseen in other contexts. So this just says if there are\nmultiple independent causes of something, how likely is it\nto happen depending on which of those is present or not. And there is a simplified way\nof doing that calculation, which corresponds to sort\nof causal independence and is computationally\nreasonably fast to do. And then they also estimated\npriors on the various diagnoses", "id": "VuKOW8d4KHw_56", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  did not have prior data-- priors. They wound up not using\nthe evoking strengths, because they were doing a\npretty straight Bayesian model where all you need is the\npriors and the conditionals. They took the frequency as a\nkind of scaled conditional, and then built a\nsystem based on that. And I'll just show\nyou the results. So they took a bunch of\nScientific American medicine cases and said, what are the\nranks assigned to the reference diagnoses of these 23 cases? And you see that like\nin case number one, QMR ranked the correct\nsolution as number six, but their two methods,\nTB and iterative TB ranked it as number one.", "id": "VuKOW8d4KHw_57", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to see how well the program\nworks if you take away various of its clever features. But what you see is that\nit works reasonably well, except for a few cases. So case number 23, all variants\nof the program did badly. And then they excused\nthemselves and said, well, there's actually\na generalization of the disease that was in the\nScientific American medicine conclusion, which the\nprograms did find, and so that would have been\nnumber one across the board. So they can sort of make a\nkind of handwavy argument that it really got\nthat one right. And so these were pretty good. And so this validated the\nidea of using this model in that way. Now, today you can go out and\ngo to your favorite Google App", "id": "VuKOW8d4KHw_58", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and download tons and tons\nand tons of symptom checkers. So I wanted to give you a demo\nof one of these if it works. OK. So I was playing earlier\nwith having abdominal pain and headache. So let's start a new one. So type in how\nyou're feeling today. Should we have a cough, or runny\nnose, abdominal pain, fever, sore throat, headache, back\npain, fatigue, diarrhea, or phlegm? Phlegm? Phlegm is the winner. Phlegm is like coughing\nup crap in your throat. AUDIENCE: Oh, luckily,\nthey visualize it. PETER SZOLOVITS: Right.", "id": "VuKOW8d4KHw_59", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  When did it start? AUDIENCE: Last week. PETER SZOLOVITS: Last week? OK. I signed in as Paul,\nbecause I didn't want to be associated\nwith any of this data. So was the phlegm bloody\nor pus-like or watery or none of the above? AUDIENCE: None of the above. PETER SZOLOVITS:\nNone of the above. So what was it like? AUDIENCE: I don't know. Paul? PETER SZOLOVITS: Is it\nany of these colors? AUDIENCE: Green. PETER SZOLOVITS: I think\nI'll make it yellow. Next. Does it happen in the morning,\nmidday, evening, nighttime, or a specific time of year? AUDIENCE: Specific time of year. AUDIENCE: Yeah. Specific time of year. PETER SZOLOVITS:\nSpecific time of year.", "id": "VuKOW8d4KHw_60", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  AUDIENCE: Well, it's\ngenerally not worse. So that's physical activity. PETER SZOLOVITS:\nPhysical activity. How often is this a problem? I don't know. A couple times a week maybe. Did eating suspect food\ntrigger your phlegm? AUDIENCE: No. PETER SZOLOVITS: I don't know. I don't know what\na suspect food is. AUDIENCE: [INAUDIBLE] food. PETER SZOLOVITS: Yeah. This is going to\nkill most of my time. AUDIENCE: Is it getting better? PETER SZOLOVITS:\nIs it improving? Sure, it's improving. Can I think of another\nrelated symptom? No. I'm comparing your case\nto men aged 66 to 72. A number of similar\ncases gets more refined. Do I have shortness of breath?", "id": "VuKOW8d4KHw_61", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  That's good. All right. Do I have a runny nose? Yeah, sure. I have a runny nose. It's-- I don't know--\na watery, runny nose. AUDIENCE: Does it say you've\ngot to call [INAUDIBLE]?? PETER SZOLOVITS: Well,\nI'm going to stop, because it will just take-- it takes too long to go through\nthis, but you get the idea. So what this is\ndoing is actually running an algorithm\nthat is a cousin of the acute renal failure\nalgorithm that I showed you. So it's trying to optimize the\nquestions that it's asking, and it's trying to come up\nwith a diagnostic conclusion. Now, in order not\nto get in trouble with things like\nthe FDA, it winds up wimping out at the\nend, and it says, if you're feeling really\nbad, go see a doctor. But nevertheless, these kinds\nof things are now becoming real,", "id": "VuKOW8d4KHw_62", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  based on more and more data. Yeah. AUDIENCE: [INAUDIBLE] PETER SZOLOVITS: Well,\nI can't get to the end, because we're only at 36%. [INTERPOSING VOICES] Yeah. Here. All right. Somebody-- AUDIENCE: Oh, I think\nI need your finger. PETER SZOLOVITS: Oh. OK. Just don't drain\nmy bank account. So The British Medical\nJournal did a test of a bunch of symptom checkers,\nof 23 symptom checkers like this about four years ago. And they said, well, can it\non 45 standardized patient vignettes can it find at least\nthe right level of urgency to recommend whether you should\ngo to the emergency room, get other kinds of care, or\njust take care of yourself.", "id": "VuKOW8d4KHw_63", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  given by the program, it should\nbe in the top 20 of the list that it gives you. And if triage is\ngiven, then it should be the right level of urgency. The correct diagnosis was\nfirst in 34% of the cases. It was within the top\n20 in 58% of the cases. And the correct triage\nwas 57% accurate. But notice it was more accurate\nin the emergent cases, which is good, because those are the\nones where you really care. So we have-- OK. So based on what\nhe said about me, I have an upper respiratory\ninfection with 50% likelihood. And I can ask what to do next.", "id": "VuKOW8d4KHw_64", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Physicians often\nperform a physical exam, explore other treatment\noptions, and recovery for most cases like this is\na matter of days to weeks. And I can go back and\nsay, I might have the flu, or I might have\nallergic rhinitis. So that's actually reasonable. I don't know exactly\nwhat you put in about me. AUDIENCE: What is\nthe less than 50? PETER SZOLOVITS: What is what? AUDIENCE: The less than 50. [INTERPOSING VOICES] AUDIENCE: Patients have to\nbe the same demographics. PETER SZOLOVITS: Yeah. I don't know what the less\nthan 50 is supposed to mean. AUDIENCE: It started\nwith 200,000 or so. PETER SZOLOVITS:\nOh, so this is based on a small number of patients. So what happens, of course,\nis as you slice and dice a population, it gets\nsmaller and smaller. So that's what we're seeing.", "id": "VuKOW8d4KHw_65", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Thank you. OK. So two more topics I'm\ngoing to rush through. One is that-- as I mentioned in\none of the much earlier slides, every action has a cost. It at least takes time. And sometimes it induces\npotentially bad things to happen to a patient. And so people began studying\na long time ago what does it mean to be rational\nunder resource constraints rather than rational just in\nthis Home economicus model. And so Eric Horvitz, who's\nnow a big cheese guy, he's head of Microsoft\nResearch, but used to be just a lowly graduate\nstudent at Stanford when he started doing this work. He said, well,\nutility comes not only", "id": "VuKOW8d4KHw_66", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  but also from the\nreasoning process from the computational\nprocess itself. And so consider-- do\nyou guys watch MacGyver? This is way out of date. So if MacGyver is defusing\nsome bomb that's ticking down to zero and he runs out of\ntime, then his utilities take a very sharp\ndrop at that point. So that's what this work is\nreally about, saying, well, what can we do when we don't\nhave all the time in the world to do the computation as well\nas having to try to maximize utility to the patient? And Daniel Kahneman, who won\nthe Nobel Prize a few years ago in economics for this notion\nof bounded rationality that says that the way we\nwould like to be rational is not actually the\nway we behave, and he wrote this popular\nbook that I really like", "id": "VuKOW8d4KHw_67", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  says that if you're trying to\nfigure out which house to buy, you have a lot of time to do it,\nso you can deliberate and list all the advantages and\ndisadvantages and costs and so on of different houses and take\nyour time making a decision. If you see a car barreling\ntoward you as you are crossing in a crosswalk, you\ndon't stop and say, well, let me figure out the pluses and\nminuses of moving to the left or moving to the right, because\nby the time you figure it out, you're dead. And so he claims that\nhuman beings have evolved in a way where we have a kind of\ninstinctual very fast response, and that the deliberative\nprocess is only invoked relatively rarely. Now, he bemoans this\nfact, because he", "id": "VuKOW8d4KHw_68", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to be deliberative about based\non these sort of gut instincts. For example, our\ncurrent president. But never mind. So what Eric and his\ncolleagues were doing was trying really to look at\nhow this kind of meta level reasoning about how\nmuch reasoning and what kind of reasoning is worth doing\nplays into the decision making process. So the expected\nvalue of computation as a fundamental\ncomponent of reflection about alternative\ninference strategies. So for example, I\nmentioned that QMR had these alternative\nquestioning methods depending on the length\nof the differential that it was working on. So that's an example of a kind\nof meta level reasoning that says that it may be\nmore effective to do one kind of question asking\nstrategy than another.", "id": "VuKOW8d4KHw_69", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  about things like\njust-in-time algorithms, where if you run out of time\nto think more deliberately, you can just take the best\nanswer that's available to you now. And so taking the\nvalue of information, the value of computation, and\nthe value of experimentation into account in doing\nthis meta level reasoning is important to come up with\nthe most effective strategies. So he gives an example\nof a time pressure decision problem where you have\na patient, a 75-year-old woman in the ICU, and she develops\nsudden breathing difficulties. So what do you do? Well, it's a challenge, right? You could be very\ndeliberative, but the problem is that she may die because\nshe's not breathing well,", "id": "VuKOW8d4KHw_70", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  let's put her on a\nmechanical ventilator, because we know that\nthat will prevent her from dying in the\nshort term, but that may be the wrong decision,\nbecause that has bad side effects. She may get an infection, get\npneumonia, and die that way. And you certainly don't want\nto subject her to that risk if she didn't need\nto take that risk. So they designed an\narchitecture that says, well, this is the decision\nthat you're trying to make, which they're modeling\nby an influence diagram. So this is a Bayesian network\nwith the addition of decision nodes and value nodes. But you use Bayesian network\ntechniques to calculate optimal decisions here. And then this is kind of\nthe background knowledge of what we understand\nabout the relationships among different things in\nthe intensive care unit. And this is a representation\nof the meta reasoning", "id": "VuKOW8d4KHw_71", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Which reasoning\ntechnique should we use? And so on. And they built an\narchitecture that integrates these various approaches. And then in my last\n2 minutes, I just want to tell you about\nan interesting-- this is a modern view,\nnot historical. So this was a paper presented at\nthe last NeurIPS meeting, which said the kinds of problems\nthat we've been talking about, like the acute renal\nfailure problem or like any of these others,\nwe can reformulate this as a reinforcement\nlearning problem. So the idea is that if you\ntreat all activities, including putting somebody on a\nventilator or concluding", "id": "VuKOW8d4KHw_72", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of the other things\nthat we've contemplated, if you treat those\nall in a uniform way and say these are\nactions, we then model the universe as a\nMarkov decision process, where every time that you\ntake one of these actions, it changes the state\nof the patient, or the state of our\nknowledge about the patient. And then you do\nreinforcement learning to figure out what\nis the optimal policy to apply under all\npossible states in order to maximize\nthe expected outcome. So that's exactly the\napproach that they're taking. The state space is the set of\npositive and negative findings. The action space is\nto ask about a finding or conclude a diagnosis. The reward is the correct or\nincorrect single diagnosis.", "id": "VuKOW8d4KHw_73", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and you get your reward. It's finite horizon\nbecause they impose a limit on the number of questions. If you don't get an\nanswer by then, you lose. You get a minus one reward. There is a discount factor so\nthat the further away a reward is, the less value it has to you\nat any point, which encourages shorter question sequences. And they use a pretty standard Q\nlearning framework, or at least a modern Q learning framework\nusing a double deep neural network strategy. And then there are two\npieces of magic sauce that make this work better. And one of them\nis that they want to encourage asking\nquestions that are likely to have\npositive answers rather than negative answers. And the reason is\nbecause in their world, there are hundreds and\nhundreds of questions.", "id": "VuKOW8d4KHw_74", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  have most of those findings. And so you don't want to ask\na whole bunch of questions to which the answer is no, no,\nno, no, no, no, no, no, no, because that doesn't give\nyou very much guidance. You want to ask questions\nwhere the answer is yes, because that helps you clue\nin on what's really going on. So they actually\nhave a nice proof that they do this thing\nthey call reward shaping, which basically adds\nsome incremental reward for asking questions that\nwill have a positive answer. But they can prove that\nan optimal policy learned from that reward\nfunction is also optimal for the reward function\nthat would not include it. So that's kind of cool. And then the other\nthing they do is to try to identify a\nreduced space of findings by what they call\nfeature rebuilding.", "id": "VuKOW8d4KHw_75", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  where they're co-training. In this dual network\narchitecture, they're co-training\nthe policy model. It's, of course, the\nneural network model, this being that 2010s. And so they're generating a\nsequence, a deep layered set of neural networks that\ngenerate an output, which is the m questions and the n\nconclusions that can be made. And I think there's\na soft max over these to come up with the right policy\nfor any particular situation. But at the same time, they\nco-train it in order to predict a number of-- all of the manifestations from\nwhat they've observed before. So it's using-- it's learning\na probabilistic model that", "id": "VuKOW8d4KHw_76", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  in the following ways, here\nare the likely answers that you would give to the\nremaining manifestations. And the reason they\ncan do that, of course, is because they really\nare not independent. They're very often co-varying. And so they learn\nthat covariance, and therefore can predict which\nanswers are going to get yes answers, which questions are\ngoing to get yes answers. And therefore, they can bias\nthe learning toward doing that. So last slide. So this system is called REFUEL. It's been tested\non a simulated data set of 650 diseases\nand 375 symptoms. And what they show is that the\nred line is their algorithm. The yellow line uses only\nthis reward reshaping.", "id": "VuKOW8d4KHw_77", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  learning approach. And you can see\nthat they're doing much better after many\nfewer epochs of training in doing this. Now, take this with a grain of\nsalt. This is all fake data. So they didn't have real\ndata sets to test this on. They got statistics on\nwhat diseases are common and what symptoms are\ncommon in those diseases. And then they had\na generative model that generated this fake data. And then they learned from\nthat generative model. So of course it would be\nreally important to redo the study with real data,\nbut they've not done that. This was just published\na few months ago. So that's sort of where we\nare at the moment in diagnosis and in differential diagnosis. And I wanted to\nstart by introducing these ideas in a kind\nof historical framework.", "id": "VuKOW8d4KHw_78", "title": "11. Differential Diagnosis", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  as you can imagine, that have\nbeen written since the 1990s and '80s that I was\nshowing you that are essentially elaborations\non the same themes. And it's only in the\npast decade of the advent of these neural network\nmodels that people have changed strategy,\nso that instead of learning explicit\nprobabilities, for example, like you do in a Bayesian\nnetwork, you just say, well, this is simply\na prediction task. And so we'll predict\nthe way we predict everything else with neural\nnetwork models, which is we build a CNN, or an RNN,\nor some combination of things, or some attention\nmodel, or something. And we throw that at it. And it does typically\na slightly better job than any of the previous\nlearning methods that we've used\ntypically, but not always. OK.", "id": "VuKOW8d4KHw_79"}, {"text": "  PROFESSOR: All right,\neveryone, so we are very happy to have Andy Beck\nas our invited speaker today. Andy has a very\nunique background. He's trained both as a computer\nscientist and as a clinician. His specialty is in pathology. When he was a\nstudent at Stanford, his thesis was on how one could\nuse machine learning algorithms to really understand a\npathology data set, at the time, using more traditional\nregression-style approaches to understanding\nwhat the field is now called computational pathology. But his work was really at\nthe forefront of his field. Since then, he's\ncome to Boston, where he was an attending and faculty\nat Beth Israel Deaconess", "id": "PKCMH5KOcxQ_0", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  In the recent couple\nof years, he's been running a\ncompany called PathAI, which is, in my opinion, one\nof the most exciting companies of AI in medicine. And he is my favorite\ninvited speaker-- ANDY BECK: He says\nthat to everyone. PROFESSOR: --every time I get\nan opportunity to invite someone to speak. And I think you'll be\nreally interested in what he has to say. ANDY BECK: Great. Well, thank you so much. Thanks for having me. Yeah, I'm really excited\nto talk in this course. It is a super exciting time for\nmachine learning in pathology And if you have any\nquestions throughout, please feel free to ask. And so for some background\non what pathology is-- it's so like, if\nyou're a patient. You go to the\ndoctor, and AI could apply in any aspect of\nthis whole trajectory, and I'll kind of talk about\nspecifically in pathology. So you go to the doctor. They take a bunch\nof data from you. You talk to them. They get signs and symptoms. Typically, if they're\nat all concerned,", "id": "PKCMH5KOcxQ_1", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  not accessible just\nthrough taking blood work, say, like a cancer, which is\none of the biggest things, they'll send you to radiology\nwhere they want to-- the radiology is the best\nway for acquiring data to look for big\nstructural changes. So you can't see single\ncells in radiology. But you can see inside the body\nand see some large things that are changing to make\nevaluations for, like, you have a cough, like\nare you looking at lung cancer, or are you looking at pneumonia? And radiology only\ntakes you so far. And people are super excited\nabout applying AI to radiology, but I think one thing\nthey often forget is these images are not\nvery data-rich compared to the core data types. I mean, this is my\nbias from pathology, but radiology gets you\nsome part of the way, where you can sort of\ntriage normal stuff. And the radiologist will\nhave some impression of what they're looking at. And often, that's\nthe bottom line in the radiology report\nis impression-- concerning for cancer, or impression--\nlikely benign but not sure, or impression-- totally benign. And that will also guide\nsubsequent decisions. But if there's some concern that\nsomething serious is going on,", "id": "PKCMH5KOcxQ_2", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  which is a tissue biopsy. So pathology\nrequires tissue to do what I'm going to\ntalk about, which is surgical pathology that\nrequires tissue specimen. There's also blood-based things. But then this is the diagnosis\nwhere you're trying to say is this cancer? Is this not cancer? And that report by\nitself can really guide subsequent\ndecisions, which could be no further\ntreatment or a big surgery or a big decision about\nchemotherapy and radiotherapy. So this is one area\nwhere you really want to incorporate data\nin the most effective way to reduce errors, to\nincrease standardization, and to really inform\nthe best treatment decision for each patient\nbased on the characteristics of their disease. And the one thing\nabout pathology that's pretty interesting\nis it's super visual. And this is just a\nkind of random sampling of some of the types\nof different imagery that pathologists are\nlooking at every day. I think this is one thing that\ndraws people to this specialty is a saying in\nradiology, you're sort of looking at an impression of\nwhat might be happening based on sending different\ntypes of images", "id": "PKCMH5KOcxQ_3", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to estimate what's going on. Whereas here, you're actually\nstaining pieces of tissue and looking by eye at\nactual individual cells. You can look within cells. You can look at how populations\nof cells are being organized. And for many diseases,\nthis still represents sort of the core data type\nthat defines what's going on, and is this something with\na serious prognosis that requires, say, surgery? Or is this something\nthat's totally benign? All of these are different\naspects of benign processes. And so just the\nnormal human body creates all these\ndifferent patterns. And then there's a lot\nof patterns of disease. And these are all different\nsubtypes of disease that are all different morphologies. So there's sort of\nan incredible wealth of different visual imagery\nthat the pathologist has to incorporate\ninto their diagnosis. And then there's,\non top of that, things like special\nstains that can stain for specific organisms,\nfor infectious disease, or specific patterns\nof protein expression, for subtyping disease based\non expression of drug targets. And this even more sort of\nincreases the complexity of the work. So for many years,\nthere's really nothing new", "id": "PKCMH5KOcxQ_4", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  or computation to this field. It's actually a\nvery natural field, because it's sort\nof laboratory-based. It's all about data processing. You take this input,\nthings like images, and produces output,\nwhat a diagnosis is. So people have really been\ntrying this for 40 years or so now. This is one of the very first\nstudies that sort of just tried to see, could\nwe train a computer to identify the\nsize of cancer cells through a process they\ncalled morphometry, here on the bottom? And then could we just\nuse sort of measurements about the size of cancer\ncells in a very simple model to predict outcome? And in this study,\nthey have a learning set that they're learning\nfrom and then a test set. And they show that their\nsystem, as every paper that ever gets published shows, does\nbetter than the two competing approaches. Although even in this\nbest case scenario, there's significant degradation\nfrom learning to test. So one, it's super simple. It's using very simple\nmethods, and the data sets are tiny, 38 learning\ncases, 40 test cases. And this is published in The\nLancet, which is the leading", "id": "PKCMH5KOcxQ_5", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And then people got\nexcited about AI sort of building off\nof simple approaches. And back in 1990, it was thought\nartificial neural nets would be super useful for\nquantitative pathology for sort of obvious reasons. But at that time,\nthere was really no way of digitizing stuff\nat any sort of scale, and that problem's only\nrecently been solved. But sort of in 2000, people\nwere first thinking about once the slides\nare digital, then you could apply computational\nmethods effectively. But kind of nothing\nreally changed, and still, to a\nlarge degree, hasn't changed for the\npredominance of pathology, which I'll talk about. But as was mentioned\nearlier, I was part of one of the first studies\nto really take a more machine learning approach to this. And what we mean\nby machine learning versus prior\napproaches is the idea of using data-driven analysis\nto figure out the best features. And now you can do that in\nan even more explicit way with machine\nlearning, but there's sort of a progression\nfrom measuring one or two things in a very tedious way\non very small data sets to, I'd say, this way,\nwhere we're using", "id": "PKCMH5KOcxQ_6", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  learning to measure larger\nnumbers of features. And then using things\nlike those associations, those features with\npatient outcome to focus your analyses on\nthe most important ones. And the challenging\nmachine learning task here and really one of the\ncore tasks in pathology is image processing. So how do we train\ncomputers to sort of have the knowledge\nof what is being looked at that any pathologist\nwould want to have? And there's a few\nbasic things you'd want to train the\ncomputer to do, which is, for example,\nidentify where's the cancer? Where's the stroma? Where are the cancer cells? Where are the\nfibroblasts, et cetera? And then once you train a\nmachine learning based system to identify those\nthings, you can then extract lots of quantitative\nphenotypes out of the images. And this is all using\nhuman-engineered features to measure all the different\ncharacteristics of what's going on in an image. And machine learning\nis being used here to create those features. And then we use other\nregression-based methods to associate these features with\nthings like clinical outcome. And in this work, we\nshow that by taking a data-driven\napproach, sort of, you begin to focus on\nthings like what's happening in the tumor\nmicroenvironment,", "id": "PKCMH5KOcxQ_7", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And it sort of turned\nout, over the past decade, that understanding the way the\ntumor interacts with the tumor microenvironment is sort of one\nof the most important things to do in cancer with\nthings like fields like immunooncology\nbeing one of the biggest advances in the\ntherapy of cancer, where you're essentially\njust regulating how tumor cells interact\nwith the cells around them. And that sort of data\nis entirely inaccessible using traditional\npathology approaches and really required a\nmachine learning approach to extract a bunch of features\nand sort of let the data speak for itself in terms of\nwhich of those features is most important for survival. And in this study, we\nshowed that these things are associated with survival. I don't know if\nyou guys do a lot of Kaplan-Meier plots in here. PROFESSOR: They saw it once,\nbut taking us through it slowly is never a bad idea. ANDY BECK: Yeah, so these are-- I feel there's one\ntype of plot to know for most of biomedical research,\nand it's probably this one. And it's extremely simple. So it's really just an\nempirical distribution of how patients are\ndoing over time. So the x-axis is time.", "id": "PKCMH5KOcxQ_8", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  I wish I had a\npredictive one in here, but we can talk about\nwhat that would look like. But a prognostic model,\nany sort of prognostic test in any disease in\nmedicine is to try to create subgroups that show\ndifferent survival outcomes. And then by implication,\nthey may benefit from different therapies. They may not. That doesn't answer\nthat question, but it just tells\nyou if you want to make an estimate for\nhow a patient's going to be doing in five years,\nand you can sub-classify them into two groups, this is\na way to visualize it. You don't need two groups. You could do this\nwith even one group, but it's frequently used to show\ndifferences between two groups. So you'll see here, there's\na black line and a red line. And these are groups\nof patients where a model trained\nnot on these cases was trained to separate\nhigh-risk patients from low-risk patients. And the way we did that was\nwe did logistic regression on a different data set, sort\nof trying to classify patients alive at five years following\ndiagnosis versus patients deceased, five years diagnosis. We build a model. We fix the model. Then we apply it to this\ndata set of about 250 cases.", "id": "PKCMH5KOcxQ_9", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  two different groups of patients\nwhose survival distribution is significantly different? So what this p-value\nis telling you is the probability that\nthese two curves come from the same\nunderlying distribution or that there's no difference\nbetween these two curves across all of the time points. And what we see\nhere is there seems to be a difference between the\nblack line versus the red line, where, say, 10 years, the\nprobability of survival is about 80% in the low-risk\ngroup and more like 60% in the high-risk group. And overall, the\np-value's very small for there being a difference\nbetween those two curves. So that's sort of like\nwhat a successful type Kaplan-Meier plot would\nlook like if you're trying to create a model\nthat separates patients into groups with different\nsurvival distributions And then it's always important\nfor these types of things to try them on\nmultiple data sets. And here we show the same model\napplied to a different data set showed pretty similar overall\neffectiveness at stratifying patients into two groups. So why do you think doing\nthis might be useful?", "id": "PKCMH5KOcxQ_10", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Because there's actually,\nI think this type of curve is often confused with one that\nactually is extremely useful, which I would say-- yeah? PROFESSOR: Why don't you wait? ANDY BECK: Sure. PROFESSOR: Don't be shy. You can call them. ANDY BECK: All right. AUDIENCE: Probably\nyou can you use this to start off\nwhen the patient's of high-risk and\nprobably at five years, if the patient has high-risk,\nprobably do a follow-up. ANDY BECK: Right, exactly. Yeah, yeah. So that would be a great use. PROFESSOR: Can you repeat the\nquestion for the recording? ANDY BECK: So it was\nsaying like if you know someone's at a high\nrisk of having an event prior to five years, an event is\nwhen the curve goes down. So definitely, the red group\nis at 40, almost double or something the risk\nof the black group. So if you have\ncertain interventions you can do to help prevent\nthese things, such as giving", "id": "PKCMH5KOcxQ_11", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  for recurrence. Like if you can do a follow-up\nscan in a month versus six months, you could make that\ndecision in a data-driven way by knowing whether the\npatient's on the red curve or the black curve. So yeah, exactly right. It helps you to make therapeutic\ndecisions when there's a bunch of things you\ncan do, either give more aggressive treatment or\ndo more aggressive monitoring of disease, depending on\nis it aggressive disease or a non-aggressive disease. The other type of curve\nthat I think often gets confused with\nthese that's quite useful is one that directly\ntests that intervention. So essentially, you\ncould do a trial of the usefulness, the clinical\nutility of this algorithm, where on the one hand, you\nmake the prediction on everyone and don't do\nanything differently. And then the other one\nis you make a prediction on the patients,\nand you actually use it to make a decision, like\nmore frequent treatment or more frequent intervention. And then you could\ndo a curve, saying among the high-risk patients,\nwhere we actually acted on it, that's black.", "id": "PKCMH5KOcxQ_12", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And then, if you do the\nexperiment in the right way, you can make the inference\nthat you're actually preventing death by 50%\nif the intervention is causing black versus red. Here, we're not doing\nanything with causality. We're just sort of observing\nhow patients do differently over time. But frequently, you see these\nas the figure, the key figure for a randomized\ncontrol trial, where the only thing different\nbetween the groups of patients is the intervention. And that really lets you\nmake a powerful inference that changes what\ncare should be. This one, you're just like, OK,\nmaybe we should do something differently, but\nnot really sure, but it makes intuitive sense. But if you actually\nhave something from a randomized clinical\ntrial or something else that allows you to\ninfer causality, this is the most\nimportant figure. And you can actually infer\nhow many lives are being saved or things by doing something. But this one's not\nabout intervention. It's just about\nsort of observing how patients do over time. So that was some of the\nwork from eight years ago, and none of this has\nreally changed in practice.", "id": "PKCMH5KOcxQ_13", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  in the clinic. Research is a totally\ndifferent story. But still, 99% of\nclinic is using these old-fashioned\ntechnologies-- microscopes from technology breakthroughs\nin the mid-1800s, staining breakthroughs in the late 1800s. The H and E stain\nis the key stain. So aspects of pathology\nhaven't moved forward at all, and this has pretty\nsignificant consequences. And here's just\na couple of types of figures that really\nallow you to see the primary data for what\na problem interobserver variability really is\nin clinical practice. And this is just\nanother, I think, really nice, empirical\nway of viewing raw data, where there is a ground\ntruth consensus of experts, who sort of decided what all\nthese 70 or so cases were, through experts always\nknowing the right answer. And for all of these\n70, called them all the category\nof atypia, which here is indicated in yellow. And then they took\nall of these 70 cases that the experts that\nare atypia and sent them to hundreds of pathologists\nacross the country", "id": "PKCMH5KOcxQ_14", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of different diagnoses\nthey were receiving. And quite strikingly-- and\nthis was published in JAMA, a great journal, about\nfour years ago now-- they show this\nincredible distribution of different diagnoses\namong each case. So this is really\nwhy you might want a computational approach is\nthere should be the same color. This should just be one big\ncolor or maybe a few outliers, but for almost any case,\nthere's a significant proportion of people calling it\nnormal, which is yellow-- or sorry, tan, then\natypical, which is yellow, and then actually cancer,\nwhich is orange or red. PROFESSOR: What\ndoes atypical mean? ANDY BECK: Yeah, so atypical\nis this border area between totally normal and cancer,\nwhere the pathologist is saying it's-- which is actually the\nmost important diagnosis because totally\nnormal you do nothing. Cancer-- there's well-described\nprotocols for what to do. Atypia, they often overtreat. And that's sort of\nthe bias in medicine is always assume the worst when\nyou get a certain diagnosis back. So atypia has nuclear features\nof cancer but doesn't fully.", "id": "PKCMH5KOcxQ_15", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  or three of the five criteria. And it has to do\nwith sort of nuclei looking a little bigger and a\nlittle weirder than expected but not enough where the\npathologist feels comfortable calling it cancer. And that's part of\nthe reason that that shows almost a coin flip. Of the ones the experts\ncalled atypia, only 48% was agreed with\nin the community. The other interesting thing the\nstudy showed was intraobserver variability is just as big\nof an issue as interobserver. So a person disagrees with\nthemselves after an eight month washout period\npretty much as often as they disagree with others. So another reason why\ncomputational approaches would be valuable and why\nthis really is a problem. And this is in breast biopsies. The same research group\nshowed quite similar results. This was in British Medical\nJournal in skin biopsies, which is another super important\narea, where, again they have the same type of\nvisualization of data. They have five different classes\nof severity of skin lesions,", "id": "PKCMH5KOcxQ_16", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  sure many of us have on\nour skin to a melanoma, which is a serious, malignant\ncancer that needs to be treated as soon as possible. And here, the white\ncolor is totally benign. The darker blue\ncolor is melanoma. And again, they show lots of\ndiscordance, pretty much as bad as in the breast biopsies. And here again, the\nintraobserver variability with an eight-month washout\nperiod was about 33%. So people disagree\nwith themselves one out of three times. And then these aren't totally\noutlier cases or one research group. The College of\nAmerican Pathologists did a big summary of 116\nstudies and showed overall, an 18.3% median discrepancy\nrate across all the studies and a 6% major\ndiscrepancy rate, which would be a major\nclinical decision is the wrong one, like\nsurgery, no surgery, et cetera. And those sort of\nin the ballpark agree with the previously\npublished findings.", "id": "PKCMH5KOcxQ_17", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  but one reason to be very\noptimistic is the one area where AI is not-- not the one\narea, but maybe one of two or three areas where AI is\nnot total hype is vision. Vision really started working\nwell as, I don't if you've covered in this class but with\ndeep convolutional neural nets in 2012. And then all the\ngroups sort of just kept getting incrementally\nbetter year over year. And now this is an\nold graph from 2015, but there's been a huge\ndevelopment of methods even since 2015, where\nnow I think we really understand the strengths and the\nweaknesses of these approaches. And pathology sort of has\na lot of the strengths, which is super well-defined,\nvery focused questions. And I think there's lots of\nfailures whenever you try to do anything more general. But for the types of tasks where\nyou know exactly what you're looking for and you can\ngenerate the training data, these systems can\nwork really well. So that's a lot of what\nwe're focused on at PathAI is how do we extract\nthe most information out of pathology images\nreally doing two things. One is understanding\nwhat's inside the images", "id": "PKCMH5KOcxQ_18", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  try to infer patient\nlevel phenotypes and outcomes directly\nfrom the images. And we use both\ntraditional machine learning models\nfor certain things, like particularly\nmaking inference at the patient level, where\nn is often very small. But anything that's directly\noperating on the image is almost some variant always of\ndeep convolutional neural nets, which really are the state of\nthe art for image processing. And we sort of, a lot of what\nwe think about at PathAI, and I think what's really\nimportant in this area of ML for medicine is generating\nthe right data set and then using things\nlike deep learning to optimize all of the\nfeatures in a data-driven away, and then really\nthinking about how to use the outputs\nof these models intelligently and really\nvalidate them in a robust way, because there's many\nways to be fooled by artefacts and other things. So just some of the-- not to belabor the points, but\nwhy these approaches are really valuable in this\napplication is it allows you to exhaustively analyze slides.", "id": "PKCMH5KOcxQ_19", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  making so many errors is they're\njust kind of overwhelmed. I mean, there's two reasons. One is humans aren't good at\ninterpreting visual patterns. Actually, I think that's not\nthe real reason, because humans are pretty darn good at that. And there are difficult\nthings where we can disagree, but when people focus on small\nimages, frequently they agree. But these images are\nenormous, and humans just don't have enough time to\nstudy carefully every cell on every slide. Whereas, the computer,\nin a real way, can be forced to\nexhaustively analyze every cell on every slide, and\nthat's just a huge difference. It's quantitative. I mean, this is one thing\nthe computer is definitely better at. It can compute huge\nnumerators, huge denominators, and exactly compute proportions. Whereas, when a person\nis looking at a slide, they're really just eyeballing\nsome percentage based on a very small amount of data. It's super efficient. So you can analyze-- this whole process is\nmassively paralyzable, so you can almost\ndo a slide as fast as you want based on how much\nyou're willing to spend on it. And it allows you not only\ndo all of of these, sort of,", "id": "PKCMH5KOcxQ_20", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  but also discover a lot of new\ninsights from the data, which I think we did in\na very early way, back eight years ago,\nwhen we sort of had human-extracted features\ncorrelate those with outcome. But now you can really\nsupervise the whole process with machine learning\nof how you go from the components of an\nimage to patient outcomes and learn new biology that\nyou didn't know going in. And everyone's\nalways like, well, are you just going to\nreplace pathologists? And I really don't think this\nis, in any way, the future. In almost every field that's\nsort of like where automation is becoming very\ncommon, the demand for people who are experts\nin that area is increasing. And like airplane\npilots is one I was just learning about today. They just do a completely\ndifferent thing than they did 20 years\nago, and now it's all about mission control\nof this big system and understanding all the\nflight management systems and understanding all\nthe data they're getting. And I think the job has not\ngotten necessarily simpler, but they're much more\neffective, and they're doing much different types of work.", "id": "PKCMH5KOcxQ_21", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  going to move from\nsort of staring into a microscope with a\nliterally very myopic focus on very small\nthings to being more of a consultant with\nphysicians, integrating lots of different types\nof data, things that AI is really bad\nat, a lot of reasoning about specific instances,\nand then providing that guidance to physicians. So I think the job will\nlook a lot different, but we never really needed more\ndiagnosticians in the future than in the past. So one example, I think\nwe sent out a reading about this was this concept\nof breast cancer metastasis is a good use case\nof machine learning. And this is just\na patient example. So a primary mass is discovered. So one of the big\ndeterminants of the prognosis from a primary tumor is has\nit spread to the lymph nodes? Because that's one\nof the first areas that tumors metastasize to. And the way to diagnose whether\ntumors have metastasized to lymph nodes is\nto take a biopsy and then evaluate those\nfor the presence of cancer where it shouldn't be.", "id": "PKCMH5KOcxQ_22", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So the International Symposium\non Biomedical Imaging organized this challenge called\nthe Chameleon 16 Challenge, where they put together almost\n300 training slides and about 130 test slides. And they asked a bunch of teams\nto build machine learning based systems to automate the\nevaluation of the test slides, both to diagnose whether\nthe slide contained cancer or not, as well as to actually\nidentify where in the slides the cancer was located. And kind of the big machine\nlearning challenge here, why you can't just throw\nit into a off-the-shelf or on the web image\nclassification tool is the images are so\nlarge that it's just not feasible to throw\nthe whole image into any kind of neural net. Because they can be\nbetween 20,000 and 200,000 pixels on a side. So they have millions of pixels.", "id": "PKCMH5KOcxQ_23", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  we start with a\nlabeled data set, where there are these very\nlarge regions labeled either as normal or tumor. And then we build\nprocedures, which is actually a key component of getting\nmachine learning to work well, of sampling patches of images\nand putting those patches into the model. And this sampling\nprocedure is actually incredibly important\nfor controlling the behavior of the\nsystem, because you could sample in all different ways. You're never going to\nsample exhaustively just because there's far\ntoo many possible patches. So thinking about the right\nexamples to show the system has an enormous effect\non both the performance and the generalizability of\nthe systems you're building. And some of the, sort\nof, insights we learned was how best to do\nthe, sort of, sampling. But once you have these samples,\nit's all data driven-- sure. AUDIENCE: Can you talk more\nabout the sampling strategy schemes? ANDY BECK: Yeah, so\nfrom a high level, you want to go from\nrandom sampling, which", "id": "PKCMH5KOcxQ_24", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  based on knowing what\nthe computer needs to learn more about. And one thing we've done and-- so it's sort of like\nfiguring-- so the first step is sort of simple. You can randomly sample. But then the second\npart is a little harder to figure out what\nexamples do you want to enrich your\ntraining set for to make the system perform even better? And there's different things\nyou can optimize for, for that. So it's sort of like this\nwhole sampling actually being part of the\nmachine learning procedure is quite useful. And you're not just going\nto be sampling once. You could iterate on\nthis and keep providing different types of samples. So for example, if\nyou learn that it's missing certain types\nof errors, or it hasn't seen enough of certain-- there's many ways\nof getting at it. But if you know it hasn't\nseen enough types of examples in your training set, you\ncan over-sample for that. Or if you see you have\na confusion matrix and you see it's failing\non certain types,", "id": "PKCMH5KOcxQ_25", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and alter the sampling\nprocedure to enrich for that. You could even provide\noutputs to humans, who can point you to the areas\nwhere it's making mistakes. Because often you don't\nhave exhaustively labeled. In this case, we actually\ndid have exhaustively labeled slides. So it was somewhat easier. But you can see there's\neven a lot of heterogeneity within the different classes. So you might do some\nclever tricks to figure out what are the types of the red\nclass that it's getting wrong, and how am I going to fix that\nby providing it more examples? So I think, sort of, that's\none of the easier things to control. Rather than trying to\ntune other parameters within these super complicated\nnetworks, in our experience, just playing with the\ntraining, the sampling piece of the training,\nit should almost just be thought of as\nanother parameter to optimize for when you're\ndealing with a problem where you have humongous\nslides and you can't use all the training data. AUDIENCE: So decades ago,\nI met some pathologists who were looking at\ncervical cancer screening.", "id": "PKCMH5KOcxQ_26", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  in the degree of atypia. And so not at training time\nbut at testing time, what they were trying to do was to\nfollow that gradient in order to find the most atypical\npart of of the image. Is that still\nbelieved to be true? ANDY BECK: Yeah. That it's a continuum? Yeah, definitely. PROFESSOR: You mean within\na sample and in the slides. ANDY BECK: Yeah, I\nmean, you mean just like a continuum\nof aggressiveness. Yeah, I think it is a continuum. I mean, this is more\nof a binary task, but there's going\nto be continuums of grade within the cancer. I mean, that's another\nlevel of adding on. If we wanted to correlate\nthis with outcome, it would definitely be\nvaluable to do that. To not just say quantitate\nthe bulk of tumor but to estimate the malignancy\nof every individual nucleus,", "id": "PKCMH5KOcxQ_27", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So you can actually classify,\nnot just tumor region but you can classify\nindividual cells. And you can classify\nthem based on malignancy. And then you can\nget the, sort of, gradient within a population. In this study, it was just a\nregion-based, not a cell-based, but you can definitely do\nthat, and definitely, it's a spectrum. I mean, it's kind of\nlike the atypia idea. Everything in biology is\npretty much on a spectrum, like from normal to atypical\nto low-grade cancer, medium-grade cancer,\nhigh-grade cancer, and these sorts of\nmethods do allow you to really more\nprecisely estimate where you are on that continuum. And that's the basic approach. We get the big\nwhole site images. We figure out how\nto sample patches from the different regions\nto optimize performance of the model during\ntraining time. And then during\ntesting time, just we take a whole big\nwhole site image. We break it into millions\nof little patches. Send each patch individually. We don't actually--\nyou could potentially use spatial information\nabout how close they are to each other,\nwhich would make", "id": "PKCMH5KOcxQ_28", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  We don't do that. We just send them\nin individually and then visualize the\noutput as a heat map. And this, I think,\nisn't in the reference I sent so the one\nI sent showed how you were able to combine the\nestimates of the deep learning system with the human\npathologist's estimate to make the human pathologist's\nerror rate go down by 85% and get to less than 1%. And the interesting thing about\nhow these systems keep getting better over time and\npotentially they over-fit to the competition data set-- because I think we submitted,\nmaybe, three times, which isn't that many. But over the course of six\nmonths after the first closing of the competition, people kept\ncompeting and making systems better. And actually, the\nfully automated system on this data set achieved an\nerror rate of less than 1% by the final submission date,\nwhich was significantly better than both the pathologists\nin the competition, which is the error rate, I believe,\ncited in the initial archive paper.", "id": "PKCMH5KOcxQ_29", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and sent them out to\npathologists operating in clinical practice, where\nthey had really significantly higher error rates,\nmainly due to the fact, they were more constrained\nby time limitations in clinical practice\nthan in the competition. And most of the errors they\nare making are false negatives. Simply, they don't\nhave the time to focus on small regions of metastasis\namid these humongous giga pixel-size slides. AUDIENCE: In the paper, you\nsay you combined the machine learning options with\nthe pathologists, but you don't really say how. Is that it that they\nlook at the heat maps, or is it just sort of combined? ANDY BECK: Yeah, no,\nit's a great question. So today, we do it that way. And that's the way\nin clinical practice we're building it, that the\npathologists will look at both and then make a diagnosis\nbased on incorporating both. For the competition,\nit was very simple, and the organizers\nactually did it. They interpreted\nthem independently. So the pathologists just\nlooked at all the slides. Our system made a prediction. It was literally the\naverage of the probability", "id": "PKCMH5KOcxQ_30", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  That became the final\nscore, and then the AUC went to 99% from\nwhatever it was, 92% by combining\nthese two scores. AUDIENCE: I guess they\nmake uncorrelated errors. ANDY BECK: Exactly. They're pretty\nmuch uncorrelated, particularly because\nthe pathologists tend to have almost all\nfalse negatives, and the deep\nlearning system tends to be fooled by a few\nthings, like artefact. And they do make\nuncorrelated errors, and that's why there's a\nhuge bump in performance. So I kind of made a\nreference to this, but any of these\ncompetition data sets are relatively easy\nto get really good at. People have shown\nthat you can actually build models that just predict\na data set using deep learning. Like, deep learning\nis almost too good at finding certain patterns\nand can find artefact. So it's just a caveat\nto keep in mind. We're doing experiments on\nlots of real-world testing of methods like this\nacross many labs with many different standing\nprocedures and tissue preparation\nprocedures, et cetera,", "id": "PKCMH5KOcxQ_31", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  But that's why competition\nresults, even ImageNet always need to be taken\nwith a grain of salt. And then but we sort\nof think the value add of this is going to be huge. I mean, it's hard to tell\nbecause it's such a big image, but this is what a\npathologist today is looking at under a\nmicroscope, and it's very hard to see anything. And with a very simple\nvisualization, just of the output of the AI system as\nred where cancer looks like it is. It's clearly a sort of\ngreat map of the areas they need to be\nsure to focus on. And this is real data\nfrom this example, where this bright red area, in fact,\ncontains this tiny little rim of metastatic\nbreast cancer cells that would be very easy to miss\nwithout that assistant sort of just pointing you in\nthe right place to look at, because it's a tiny\nset of 20 cells amid a big sea of all\nthese normal lymphocytes. And here's another\none that, again, now you can see from low power. It's like a satellite\nimage or something, where you can focus immediately\non this little red area, that, again, is a tiny pocket\nof 10 cancer cells amid hundreds of thousands\nof normal cells that are now", "id": "PKCMH5KOcxQ_32", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So this is one application\nwe're working on, where the clinical\nuse case will be today, people are just\nsort of looking at images without the assistance\nof any machine learning. And they just have to kind\nof pick a number of patches to focus on with no guidance. So sometimes they focus\non the right patches, sometimes they don't, but\nclearly they don't have time to look at all of this\nat high magnification, because that would\ntake an entire day if you were trying to\nlook at 40X magnification at the whole image. So they sort of use\ntheir intuition to focus. And for that\nreason, they end up, as we've seen, making\nsignificant number of mistakes. It's not reproducible,\nbecause people focus on different\naspects of the image, and it's pretty slow. And they're faced with\nthis empty report. So they have to actually\nsummarize everything they've looked at in a report. Like, what's the diagnosis? What's the size? So let's say there's cancer\nhere and cancer here, they have to manually add the\ndistances of the cancer in those two regions. And then they have to put this\ninto a staging system that", "id": "PKCMH5KOcxQ_33", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and how big are they? And all of these things are\npretty much automatable. And this is the\nkind of thing we're building, where the system will\nhighlight where it sees cancer, tell the pathologist\nto focus there. And then based on the\ninput of the AI system and the input of the pathologist\ncan summarize all of that data, quantitative as\nwell as diagnostic as well as summary staging. Sort of if the pathologist\nthen takes this is their first\nversion of the report, they can edit it,\nconfirm it, sign it out. That data goes back\ninto the system, which can be used for more\ntraining data in the future and the case is signed out. So it's much faster, much more\naccurate, and standardized once this thing is fully\ndeveloped, which it isn't yet. So this is a great\napplication for AI, because you really do need-- you actually do\nhave a ton of data, so you need to do an\nexhaustive analysis that has a lot of value. It's a task where the local\nimage data in a patch, which is really what\nthis current generation of deep CNN's are really\ngood at, is enough.", "id": "PKCMH5KOcxQ_34", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Radiology actually\ncould be harder, because you often want to\nsummarize over larger areas. Here, you really often have\nthe salient information in patches that really are\nscalable in current ML systems. And then we can interpret\nthe output to the model. So it really isn't-- even though\nthe model itself is a black box, we can visualize the\noutput on top of the image, which gives us incredible\nadvantage in terms of interpretability of what\nthe models are doing well, what they're doing poorly on. And it's a specialty,\npathology, where sort of 80% is not good enough. We want to get as close\nto 100% as possible. And that's one sort of\ndiagnostic application. The last, or one of the last\nexamples I'm going to give has to do with precision\nimmunotherapy, where we're not only trying to identify\nwhat the diagnosis is but to actually subtype patients\nto predict the right treatment. And as I mentioned\nearlier, immunotherapy is a really important and\nexciting, relatively new area of cancer therapy,\nwhich was another one of the big advances in 2012. Around the same time that\ndeep learning came out,", "id": "PKCMH5KOcxQ_35", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that targeting a protein\nmostly on tumor cells but also on immune cells, the\nPD-1 or the PD-L1 protein, which the protein's\njob when it's on is to inhibit immune response. But in the setting of\ncancer, the inhibition of immune response is\nactually bad for the patient, because the immune system's\njob is to really try to fight off the cancer. So they realized a very\nsimple therapeutic strategy just having an antibody that\nbinds to this inhibitory signal can sort of unleash the\npatient's own immune system to really end up curing really\nserious advanced cancers. And that image on\nthe top right sort of speaks to that, where\nthis patient had a very large melanoma. And then they just got\nthis antibody to target, to sort of invigorate\ntheir immune system, and then the tumor\nreally shrunk. And one of the big biomarkers\nfor assessing which patients will benefit from\nthese therapies is the tumor cell or the\nimmune cell expressing this drug target PD-1 or PD-L1.", "id": "PKCMH5KOcxQ_36", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  which is the ligand\nfor the PD-1 receptor. So this is often the\nkey piece of data used to decide who\ngets these therapies. And it turns out, pathologists\nare pretty bad at scoring this, not surprisingly, because\nit's very difficult, and there's millions of\ncells potentially per case. And they show an\ninterobserver agreement of only 0.86 for scoring\non tumor cells, which isn't bad, but 0.2 for scoring\nit on immune cells, which is super important. So this is a drug target. We're trying to measure to\nsee which patients might get this life-saving therapy,\nbut the diagnostic we have is super hard to interpret. And some studies,\nfor this reason, have shown sort of mixed results\nabout how valuable it is. In some cases, it\nappears valuable. In other cases, it\nappears it's not. So we want to see would this\nbe a good example of where we can use machine learning? And for this type\nof application, this is really hard,\nand we want to be able to apply it across\nnot just one cancer but 20 different cancers. So we built a system at\nPathAI for generating lots", "id": "PKCMH5KOcxQ_37", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And that's something that a\ncompetition just won't get you. Like that competition\nexample had 300 slides. Once a year, they do it. But we want to be able to\nbuild these models every week or something. So now, we have something\n500 pathologists signed into our system that we can use\nto label lots of pathology data for us and to really build\nthese models quickly and really high quality. So now we have something\nlike over 2 and 1/2 million annotations in the system. And that allows us to\nbuild tissue region models. And this is immunohistochemistry\nin a cancer, where we've trained a\nmodel to identify all of the cancer epithelium\nin red, the cancer stroma in green. So now we know\nwhere the protein is being expressed, in the\nepithelium or in the stroma. And then we've also trained\ncellular classification. So now, for every single cell,\nwe classify it as a cell type. Is it a cancer cell or a\nfibroblast or a macrophage or a lymphocyte? And is it expressing\nthe protein, based on how brown it is? So while pathologists will\ntry to make some estimate across the whole slide, we can\nactually compute for every cell", "id": "PKCMH5KOcxQ_38", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  about which cells are\nexpressing this protein and which patients might be the\nbest candidates for therapy. And then the question is, can\nwe identify additional things beyond just PD-L1 protein\nexpression that's predictive of response to immunotherapy? And we've developed some\nmachine learning approaches for doing that. And part of it's doing\nthings like quantitating different cells and\nregions on H and E images, which\ncurrently aren't used at all in patient subtyping. But we can do analyses to\nextract new features here and to ask, even\nthough nothing's known about these images\nand immunotherapy response, can we discover\nnew features here? And this would be\nan example routinely of the types of features\nwe can quantify now using deep learning to extract\nthese features on any case. And this is sort of like\nevery sort of pathologic characteristic you\ncan sort of imagine. And then we correlate\nthese with drug response", "id": "PKCMH5KOcxQ_39", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  for identifying new aspects\nof pathology predictive of which patients\nwill respond best. And then we can combine\nthese features into models. This is sort of a\nridiculous example because they're so different. But this would be\none example where the output of the model, and\nthis is totally fake data but I think it's just\nto get to the point. Is here, the color\nindicates the treatment, where green would be\nthe immunotherapy, red would be the\ntraditional therapy, and the goal is to\nbuild a model to predict which patients actually\nbenefit from the therapy. So this may be an easy\nquestion, but what do you think, if\nthe model's working, what would the title of\nthe graph on the right be versus the graph on\nthe left if these are the ways of classifying\npatients with our model, and the classifications\nare going to be responder class or non-responder class? And the color\nindicates the drug. AUDIENCE: The drug works\nor it doesn't work. ANDY BECK: That's right but\nwhat's the output of the model?", "id": "PKCMH5KOcxQ_40", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  The interpretation of\nthese graphs is drug works, drug doesn't work. It's kind of a tricky\nquestion, right? But what is our model\ntrying to predict? AUDIENCE: Whether the person\nis going to die or not? It looks like\nlikelihood of death is just not as\nhigh on the right. ANDY BECK: I think\nthe overall likelihood is the same on the two\ngraphs, right versus left. You don't know how many\npatients are in each arm. But I think the\none piece on it-- so green is\nexperimental treatment. Red is conventional treatment. Maybe I already said that. So here, and it's sort of like\na read my mind type question, but here the output\nof the model would be responder to the drug would\nbe the right class of patients. And the left class\nof patients would be non-responder to the drug. So you're not actually saying\nanything about prognosis, but you're saying\nthat I'm predicting that if you're in the right\npopulation of patients, you will benefit\nfrom the blue drug. And then you actually see\nthat on this right population of patients, the blue\ndrug does really well. And then the red\ndrug are patients who we thought-- we predicted\nwould benefit from the drug,", "id": "PKCMH5KOcxQ_41", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  didn't give them the right drug. And in fact, they did\na whole lot worse. Whereas, the one on\nthe left, we're saying you don't benefit from\nthe drug, and they truly don't benefit from the drug. So this is the way of\nusing an output of a model to predict drug response\nand then visualizing whether it actually works. And it's kind of\nlike the example I talked about before, but\nhere's a real version of it. And you can learn this\ndirectly using machine learning to try to say, I want to\nfind patients who actually benefit the most from a drug. And then in terms of\nhow do we validate our models are correct? I mean, we have\ntwo different ways. One is do stuff like that. So we build a model that\nsays, respond to drug, don't respond to a drug. And then we plot the\nKaplan-Meier curves. If it's image analysis stuff, we\nask pathologists to hand label. Many cells, and we\ntake the consensus of pathologists as our ground\ntruth and go from there.", "id": "PKCMH5KOcxQ_42", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  it makes it sound like\nall the data comes from the pathology images. But in reality, people look at\nsingle nucleotide polymorphisms or gene sequences or all kinds\nof clinical data as well. So how do you get those? ANDY BECK: Yeah, I mean, the\nbeauty of the pathology data is it's always available. So that's why a lot\nof the stuff we do is focused on that, because\nevery clinical trial patient has treatment\ndata, outcome data, and pathology images. So it's like, we can really\ndo this at scale pretty fast. A lot of the other stuff is\nthings like gene expression, many people are collecting them. And it's important to\ncompare these to baselines or to integrate them. I mean, two things-- one is\ncompare to it as a baseline. What can we predict in terms of\nresponder, non-responder using just the pathology images versus\nusing just gene expression data versus combining them?", "id": "PKCMH5KOcxQ_43", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  space. Part of the input feature\nspace comes from the images. Part of it comes from\ngene expression data. Then you use machine\nlearning to focus on the most important\ncharacteristics and predict outcome. And the other is if you\nwant to sort of prioritize. Use pathology as a\nbaseline because it's available on everyone. But then an adjuvant test\nthat costs another $1,000 and might take another\ntwo weeks, how much does that add to the prediction? And that would be another way. So I think it is\nimportant, but a lot of our technology to\ndeveloping our platform is focused around how do\nwe most effectively use pathology and can certainly\nadd in gene expression date. I'm actually going to\ntalk about that next-- one way of doing it. Because it's a very natural\nsynergy, because they tell you very different things. So here's one example of\nintegrating, just kind of relative to that question,\ngene expression data with image data, where the\ncancer genome analysis, and this is all public. So they have pathology images,\nRNA data, clinical outcomes. They don't have the\ngreatest treatment data,", "id": "PKCMH5KOcxQ_44", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  for sort of ML in\ncancer, including pathology-type analyses. So this is a case of melanoma. We've trained a model to\nidentify cancer and stroma and all the different cells. And then we extract, as you saw,\nsort of hundreds of features. And then we can rank\nthe features here by their correlation\nwith survival. So now we're mapping\nfrom pathology images to outcome data and we find just\nin a totally data-driven way that there's some small set\nof 15 features or so highly associated with survival. The rest aren't. And the top ranking one\nis an immune cell feature, increased area of\nstroma plasma cells that are associated\nwith increased survival. And this was an analysis\nthat was really just linking the images with outcome. And then we can ask, well,\nwhat are the genes underlying this pathology? So pathology is telling you\nabout cells and tissues. RNAs are telling you about\nthe actual transcriptional landscape of what's\ngoing on underneath. And then we can rank all\nthe genes in the genome just by their correlation with\nthis quantitative phenotype", "id": "PKCMH5KOcxQ_45", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And here are all the genes,\nranked from 0 to 20,000. And again, we see a small\nset that we're thresholding at a correlation\nof 0.4, strongly associated with the pathologic\nphenotype we're measuring. And then we sort of\ndiscover these sets of genes that are known to be\nhighly enriched in immune cell genes. Sort of which is some\nform of validation that we're measuring what\nwe think we're measuring, but also this sets of genes are\npotentially new drug targets, new diagnostics, et\ncetera, that was uncovered by going from clinical\noutcomes to pathology data to the underlying RNA signature. And then kind of the beauty of\nthe approach we're working on is it's super scalable,\nand in theory, you could apply it to all of\nTCGA or other data sets and apply it across cancer\ntypes and do things like find-- automatically find artefacts\nin all of the slides and kind of do this\nin a broad way. And then sort of the most\ninteresting part, potentially,", "id": "PKCMH5KOcxQ_46", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and how they\ncorrelate with things like drug response or\nunderlying molecular profiles. And this is really the\nprocess we're working on, is how do we go from images to\nnew ways of measuring disease pathology? And kind of in summary, a lot\nof the technology development that I think is\nmost important today for getting ML to\nwork really well in the real world for\napplications in medicine is a lot about being super\nthoughtful about building the right training data set. And how do you do that in\na scalable way and even in a way that incorporates\nmachine learning? Which is kind of what\nI was talking about before-- intelligently\npicking patches. But that sort of concept\napplies everywhere. So I think there's almost\nmore room for innovation on the defining the\ntraining data set side than on the predictive\nmodeling side, and then putting\nthe two together is incredibly important. And for the kind of\nwork we're doing, there's already such great\nadvances in image processing. A lot of it's about\nengineering and scalability, as well as rigorous validation. And then how do we connect it\nwith underlying molecular data", "id": "PKCMH5KOcxQ_47", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Versus trying to solve a lot\nof the core vision tasks, which there's already just\nbeen incredible progress over the past couple of years. And in terms of in\nour world, things we think a lot about,\nnot just the technology and putting together\nour data sets but also, how do we work with regulators? How do we make\nstrong business cases for partners working\nwith to actually change what they're doing\nto incorporate some of these new approaches that\nwill really bring benefits to patients around quality and\naccuracy in their diagnosis? So in summary-- I know you have to\ngo in four minutes-- this has been a\nlongstanding problem. There's nothing new\nabout trying to apply AI to diagnostics or\nto vision tasks, but there are some really big\ndifferences in the past five years that, even\nin my short career, I've seen a sea\nchange in this field. One is availability\nof digital data-- it's now much cheaper to\ngenerate lots of images at scale. But even more\nimportant, I think, are the last two, which is\naccess to large-scale computing resources is a game-changer\nfor anyone with access", "id": "PKCMH5KOcxQ_48", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Just, we all have access\nto a sort of arbitrary compute today, and\n10 years ago, that was a huge limitation\nin this field. As well as these really\nmajor algorithmic advances, particularly deep\nCNN's revision. And, in general, AI\nworks extremely well when problems can be defined to\nget the right type of training data, access,\nlarge-scale computing, as well as implement things\nlike deep CNNs that work really well. And it sort of fails\neverywhere else, which is probably 98% of things. But if you can create a problem\nwhere the algorithms actually work, you can have lots\nof data to train on, they can succeed really well. And this sort of vision-based\nAI-powered pathology is broadly applicable across,\nreally, all image-based tasks and pathology. It does enable\nintegration with things like omics data--\ngenomics, transcriptonics, SNP data, et cetera. And in the near future, we\nthink this will be incorporated into clinical practice.", "id": "PKCMH5KOcxQ_49", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of research efforts. And I just want\nto end on a quote from 1987, where\nin the future, AI can be expected to become\nstaples of pathology practice. And I think we're much, much\ncloser than 30 years ago. And I want to thank\neveryone at PathAI, as well as Hunter, who\nreally helped put together a lot of these slides. And we do have lots\nof opportunities for machine learning\nengineers, software engineers, et cetera, at PathAI. So certainly reach out if you're\ninterested in learning more. And I'm happy to take any\nquestions, if we have time. So thank you. [APPLAUSE] AUDIENCE: Yes, I think generally\nvery aggressive events. I was wondering how close is\nthis to clinical practice? Is there FDA or-- ANDY BECK: Yeah, so I mean,\nactual clinical practice, probably 2020, like\nearly, mid-2020. But I mean, today, it's very\nactive in clinical research,", "id": "PKCMH5KOcxQ_50", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  involve patients, but it's in a\nmuch more well-defined setting. But the first\nclinical use cases, at least of the types\nof stuff we're building, will be, I think,\nabout a year from now. And I think it will\nstart small and then get progressively bigger. So I don't think it's\ngoing to be everything all at once transforms\nin the clinic, but I do think\nwe'll start seeing the first applications out. And they will go-- some of\nthem will go through the FDA, and there'll be some\nlaboratory-developed tests. Ours will go through the\nFDA, but labs themselves can actually validate\ntools themselves. And that's another path. AUDIENCE: Thanks. ANDY BECK: Sure. PROFESSOR: So have you been\nusing observational data sets? You gave one example where\nyou tried to use data from a randomized controlled\ntrial, or both trials, you used different\nrandomized control trials", "id": "PKCMH5KOcxQ_51", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  The next major segment\nof this course, starting in about two weeks,\nwill be about causal inference from observational data. I'm wondering if\nthat is something PathAI has gotten into yet? And if so, what has your\nfinding been so far? ANDY BECK: So we have focused\na lot on randomized controlled trial data and have\ndeveloped methods around that, which sort\nof simplifies the problem and allows us to do, I think,\npretty clever things around how to generate those types\nof graphs I was showing, where you truly can infer the\ntreatment is having an effect. And we've done far less. I'm super interested in that. I'd say the\nadvantages of RCTs are people are already investing\nhugely in building these very well-curated data sets\nthat include images, molecular data, when available,\ntreatment, and outcome. And it's just that's\nthere, because they've invested in the clinical trial. They've invested in\ngenerating that data set. To me, the big challenge\nin observational stuff, there's a few but I'd be\ninterested in what you guys are", "id": "PKCMH5KOcxQ_52", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  the data is not easy, right? The outcome data is not-- linking the pathology\nimages with the outcome data even is, actually,\nin my opinion, harder in observational\nway than in RCT. Because they're actually\ndoing it and paying for it and collecting it in RCTs. No one's really done\na very good job of-- TCGA would be a good place to\nplay around with because that is observational data. And we want to\nalso, we generally want to focus on\nactionable decisions. And RCT is sort of\nperfectly set up for that. Do I give drug X or not? So I think if you put\ntogether the right data set and somehow make the\nresults actionable, it could be really,\nreally useful, because there is a lot of data. But I think just\ncollecting the outcomes and linking them with images\nis actually quite hard. And ironically, I think it's\nharder for observational than for randomized control\ntrials, where they're already collecting it. I guess one example would\nbe the Nurses' Health Study or these big epidemiology\ncohorts, potentially. They are collecting that\ndata and organizing it.", "id": "PKCMH5KOcxQ_53", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Do you have anything\nwith pathology in mind for causal inference\nfrom observational data? PROFESSOR: Well, I\nthink, the example you gave, like Nurses' Health\nStudy or the Framingham study, where you're tracking\npatients across time. They're getting different\ninterventions across time. And because of the way the\nstudy was designed, in fact, there are even good outcomes\nfor patients across times. So that problem\nin the profession doesn't happen there. But then suppose you were\nto take it from a biobank and do pathologies? You're now getting the samples. Then, you can ask\nabout, well, what is the effect of different\ninterventions or treatment plans on outcomes? The challenge, of course,\ndrawing inferences there is that there\nwas bias in terms of who got what treatments. That's where the techniques\nthat we talk about in class would become very important. I just say, I appreciate the\nchallenges that you mentioned. ANDY BECK: I think it's\nincredibly powerful. I think the other issue I just\nthink about is that treatments change so quickly over time. So you don't want to be like\noverfitting to the past.", "id": "PKCMH5KOcxQ_54", "title": "12. Machine Learning for Pathology", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  the therapeutic decisions\ntoday are similar to what they were in the past. There are other areas, like\nimmunooncology, where there's just no history to learn from. So I think it depends on the-- PROFESSOR: All right, then with\nthat, let's thank Andy Beck. [APPLAUSE]", "id": "PKCMH5KOcxQ_55"}, {"text": "  ADAM YALA: OK, great. Well, thank you for\nthe great setup. So for this section, I'm gonna\ntalk about some of our work in interpreting\nmammograms for cancer. Specifically it's going to\ngo into cancer detection and triage mammograms. Next, we'll talk about\nour technical approach in breast cancer risk. And then finally close up in\nthe many, many different ways to mess up and the way\nthings can go wrong, and how does it [INAUDIBLE]\nclinical implementation. So let's kind of\nlook more closely at the numbers of the actual\nbreast cancer screening workflow. So as Connie already said,\nyou might see something like 1,000 patients. All them take mammograms. Of that 1,000, on\naverage maybe 100 they called back for\nadditional imaging. Of that 100, something\nlike 20 will get biopsied. And you end up with maybe\nfive or six diagnoses of breast cancer. So one very clear thing\nyou see about problems when you look at this\nfunnel is that way", "id": "2ZXYM1h9pgY_0", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So your actual\nincidence is very low. And so there's kind of a natural\nquestion that can come up. What can you do in\nterms of modeling if you have an even OK\ncancer detection model to raise the incidence\nof this population but automatically reading\na portion of the population is healthy. Does everybody just\nfollow that broad idea? OK. That's enough head nods. So the broad idea\nhere is you're going to train the cancer detection\nmodel to try to find cancer as well as we can. Given that, we're\ngoing to try to say, what's a threshold on\na development set such that we can kind of\nsay below the threshold no one has cancer. And if we use that\nat test times, simulating clinical\nimplementation, what would that look like? And can we actually do better\nby doing this kind of process? And the kind of broad plan of\nhow I'm gonna talk about this-- I'm gonna do this for\nthe next product as well. Of course, we're going\nto talk about the kind of dataset collection and\nhow we think about, like, you know, what is good data\nand how do we think about that. Next, the actual methodology and\ngo into the general challenges when you're modeling mammograms\nfor any computer mission tasks, specifically in cancer,\nand also, obviously, risk.", "id": "2ZXYM1h9pgY_1", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  of objectives there. So to kind of dive into it, we\ntook consecutive mammograms. I'll get back into this later. This is actually\nquite important. We took consecutive\nmammograms from 2009 to 2016. This started off with\nabout 280,000 cancers. And once we kind of filtered--\nso at least one year follow up, we ended up with\nthis final setting where we had 220,000\nmammograms for training and about 26,000 for\ndevelopment and testing. And the way we had it,\nit all comes to say, is this a positive\nmammogram or not? We didn't look at\nwhat cancers were caught by the radiologists. We'd say, you know, what\nwas cancer that was found in any means within a year? And where we looked to was\nthrough the radiology, EHR, and the Partners-- kind\nof five hospital registry. And there we were\ntrying to save cancer-- if anyway we can tell\na cancer occurred, let's mart it as such regardless\nof what others caught on MRI or some kind of later stage. And so the thing we're\ntrying to do here is just mimic the\nreal world of what are we trying to catch cancer. And finally, important\ndetails we always", "id": "2ZXYM1h9pgY_2", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  memorizing this specific\npatient didn't have cancer. And so we have some overlap\nthat's some bad bias to have. OK. That's pretty simple. Now let's go into the modeling. There's going to kind\nof follow two chunks. One chunk is going to be on\nthe kind of general challenges, and it's kind of shared between\nthe variety of projects. And next is going to be kind\nof more specific analysis for this project. So kind of a general\nquestion you might be asking, I have some image. I have some outcome. Obviously, this is just\nimage classification. How is it different\nfrom ImageNet? Well, it's quite similar. Most lessons are shared. But there are some\nkey differences. So I gave you two examples. One of them is a\nscene in my kitchen. Can anyone tell me\nwhat the object is? This is not a particularly\nhard question. AUDIENCE: [Intermingled\nvoices] Dog. Bear. ADAM YALA: Right. AUDIENCE: Dog. ADAM YALA: It is almost\nall of those things. So that is my dog, the best dog. OK. So can anyone tell\nme, now that you've had some training with Connie,\nif this mammogram indicates cancer?", "id": "2ZXYM1h9pgY_3", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And this is unfair for\na couple of reasons. Let's go into, like,\nwhy this is hard. It's unfair in part because\nyou don't have the training. But it's actually a much\nharder signal to learn. So first let's kind\nof delve into it. In this kind of task,\nthe image is really huge. So you have something like a\n3,200 by 2,600 pixel image. This is a single\nview of a breast. And in that, the actual\ncancer they're looking for might be 50 by 50 pixels. So intuitively your signal to\nnoise ratio is very different. Whereas an image that-- my dog is like the entire image. She's huge in real\nlife and in that photo. And the image itself\nis much smaller. So not only do you have\nmuch smaller images, but you're kind of, like, the\nrelative size of the object in there is much larger. To kind of further\ncompound the difficulty, the pattern you're looking\nfor inside the mammogram is really context-dependent. So if you saw that pattern\nsomewhere else in the breast, it doesn't indicate\nthe same thing. And so you really\ncare about where in this kind of global\ncontext this comes out. And if you kind of\ntake the mammogram at different times with\ndifferent compressions,", "id": "2ZXYM1h9pgY_4", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  of the image that's much\nmore difficult to model. Whereas that's a more or\nless context-independent dog. You see that kind of\nframe kind of anywhere, you know it's a dog. And so it's a much\neasier thing to learn in a traditional\ncomputer vision setting. And so the core challenge\nhere is that both the image is too big and too small. So if you're looking at just\nthe number of cancers we have, the cancer might be less\nthan 1% of the mammogram and about 0.7% of your\nimages have cancers, even in this data set,\nwhich is from 2000 to 2016 MGH, a massive imaging center,\nin total across all of that, you will still have\nless than 2,000 cancers. And this is super tiny\ncompared to regular object classification data sets. And this is looking at\nover a million images if you look at all the\nfour views of the exams. And at the same time,\nit's also too big. So even if I downsample\nthese images, I can only really fit three\nof them for a single GPU. And so this kind of limits the\nbatch size I can work with. And whereas the\nkind of comparable,", "id": "2ZXYM1h9pgY_5", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  I could fit batches of\n128, easily happy days and do all this\nparallelization stuff, and it's just much\neasier to play with. And finally, the actual data\nset itself is quite large. And so you have to do some-- there's nuisances to deal\nwith in terms of, like, just setting up your server\ninfrastructure to handle these massive data sets, also\nbe able to train efficiently. So you know, the\ncore challenge here across all of\nthese kind of tasks is, how do we make this\nmodel actually learn? The core problem is that\nour signal to noise ratio is quite low. So training ends up\nbeing quite unstable. And there's a kind of a\ncouple of simple levers you can play with. The first lever is often\ndeep learning initialization. Next, we're gonna talk about\nkind of the optimization or architecture choice\nand how this compares to what people often\ndo in the community, including in a recent\npaper from yesterday. And then finally, we're gonna\ntalk about something more explicit for the triage idea and\nhow we actually use this model once it's trained. OK. So before I go into how\nwe made these choices, I'm just going to say what\nwe chose to give you context before I dive in.", "id": "2ZXYM1h9pgY_6", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  We use a relatively large\nbatch size-ish of 24. And the way we do that\nis by taking 4 GPUs and just stepping\na couple of times before doing an optimizer step. So when you do a\ncouple rounds of back prop first to accumulate\nthose gradients before doing optimization. And you sample balanced\nbatches of training time. And for backbone architecture\nwe use ResNet-18. It's just kind of,\nlike, fairly standard. OK. But as I said before, one\nof the first key decisions is how do you think about\nyour initialization? So this is a figure of\nImageNet initialization versus random initialization. It's not any\nparticular experiment. I've done this across\nmany, many times. It's always like this. Where if you use\nimage initialization, your loss drops\nimmediately, both in train loss and development\nloss when you actually learn something. Whereas when you do\nrandom initialization, you kind of don't\nlearn anything. And your loss kind of\nbounds around the top for a very long time before\nit finds some region where it quickly starts learning. And then it will plateau\nagain for a long time before quickly start learning. And to kind of\ngive some context,", "id": "2ZXYM1h9pgY_7", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  16 hours. And so to wait long\nenough to even see if random initialization\ncould perform as well is beyond my level of patience. It just takes too long, and\nI have other experiments to be running. So this is more of an\nempirical observation that the image initialization\nlearns immediately. And there's some kind of\nquestions of why is this? Our theoretical understanding\nof this is not that strong. We have some intuitions of\nwhy this might be happening. We don't think it's anything\nabout this particular filter of this dog is really\ngreat for breast cancer. That's quite implausible. But if you look it into a lot\nof the earlier research in terms of the right kind of random\ninitialization for things like revenue networks,\na lot of focus was on does the\nactivation pattern not blow up as you go\nfurther down the line. One of the benefits of starting\nwith the pre-trained network is that a lot of\nthose kind of dynamics are already figured out\nfor a specific task. And so shifting from\nthat to other tasks has seemed to be not\nthat challenging. Another possible\narea of explanation is actually in a\nBatchNorm statistics.", "id": "2ZXYM1h9pgY_8", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And the way the BatchNorm\ninitialization is implemented across every deep learning\nlibrary that I know of, it computes\nindependently per GPU to minimize the kind of\ninter-GPU communication. And so it's also less able to\nkind of guess from scratch. But if you're starting with\nthe BatchNorm statistics to ImageNet and just\nslowly shifting it over, it might also result in\nsome stability benefits. But in general, or\nlike, a true deeper theoretical understanding, but\nas I said, it still eludes us. And it isn't something I can\ngive too much conclusions about, unfortunately. OK. So that's initialization. And if you don't get this\nright, kind of nothing works for a very long time. So if you're gonna start\na project in this space, try this. Next, another important\ndecision that if you don't do, it kind of breaks, is your\noptimization/architecture choice. So as I said before, kind of\na core problem in stability here is this idea that our\njust signal to noise ratio is really low. And so a very common\napproach throughout a lot of the prior work\nand things I actually have tried myself before\nis to say, OK, let's", "id": "2ZXYM1h9pgY_9", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  We can train at a\npatch level first. We're going to take just\nsubsets of a mammogram in this little\nbonding box, have it annotated for radiology\nfindings like benign masses or calcification and\nthings of that sort. We're going to\npre-train on that task to have this kind of\npixel level prediction. And then once we're\ndone with that, we're going to fine tune\nthat initialized model across the entire image. So you kind of have this\ntwo-stage training procedure. And actually, another paper\nthat came out just yesterday does the exact same approach\nwith some slightly different details. But one of the things\nwe wanted to investigate is if you just-- oh, And\nthe base architecture that's always used\nfor this, there is quite a few valid\noptions of things that just get\nreasonable performance and ImageNet, things like\nVGG, Wide ResNets and ResNets. In my experience, they all\nperformed fairly similarly. So it's kind of a\nspeed/benefit trade-off. And there's an advantage to\nusing fully convolutional architectures because if you\nhave fully connected layers that are assumed\nspecific dimensionality,", "id": "2ZXYM1h9pgY_10", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  They're just more\nconvenient to start with a full convolutional\narchitecture. There's going to be\nresolution invariant. Yes. AUDIENCE: In the last\nslide when you do patches-- ADAM YALA: Yes. AUDIENCE: How do you\nlabel every single patch? Are they just labeled\nwith a global label? Or do you have to\nactually look and catch, and figure out what's happened? ADAM YALA: So\nnormally what you do is you have positive\npatches labeled. And then you randomly\nsample other patches. So from your annotation--\nso, for example, a lot people do this on public data sets like\nthe Florida DSM dataset that has some entries,\nof like, here are benign masses, benign calcs,\nmalignant calcs, et cetera. What people do then is\ntake those annotations. They will randomly\nselect other patches and say, if it's not\nthere, it's negative. And I'm going to\ncall it healthy. And then they'll\nsay if this bonding box overlaps with patch\nby some marginal call, it's the same label. So do this heuristically. And other data sets that are\nproprietary also kind of play with a similar trick. In general, they don't actually\nlabel every single pixel accordingly. But there's relatively\nminor differences", "id": "2ZXYM1h9pgY_11", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  But the results are fairly\nsimilar, regardless. Yes. AUDIENCE: When you go from the\npatch level to the full image, if I understand correctly,\nthe architecture hasn't quite changed because it's just\nconvolution is over a larger-- ADAM YALA: Exactly. So the end thing right before we\ndo the prediction is normally-- ResNet, for example, does\na global average pool. Channel lies across\nthe entire feature map. And so they just-- for the patch level they take\nin an image that's 250 by 250, do the global average\npool across that to make the prediction. And when they just go up to\nthe full resolution image, now you're taking a global\naverage pool over a 3,000 by 2,000. AUDIENCE: And presumably there\nmight be some scaling issue that you might need to adjust. Do you do any of that? Or are you just-- ADAM YALA: So you feed it\nin at the full resolution the entire time. So you just-- do\nyou see what I mean? So you're taking a crop. So the resolution\nisn't changing. So the same filter map should\nbe able to kind of scale accordingly. But if you do things\nlike average pooling,", "id": "2ZXYM1h9pgY_12", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  any one thing that has\na very high activation will get averaged down lower. And so, for example,\nin our work, we use max pooling to\nkind of get around that. Any other questions? But if this looks\ncomplicated, have no worries because we actually\nthink it's totally unnecessary. And this is the next slide. So good for you. So as I said before,\nthis kind of, what are the problems\nthat signal to noise? So one obvious thing to kind\nof think about is, like, OK. Maybe doing SGD with\na batch size of three when the lesion is less than\n1% of the image is a bad idea. If I just take less\nnoisy gradients by increasing my batch size,\nwhich means use more GPUs, take more steps before\ndoing the weight update, we actually find that the\nneed to do this actually goes away completely. So these are experiments I did\nin the publicly available data set a while back while we\nwere figuring this out. If you take this kind of\n[INAUDIBLE] architecture and fine tune with a batch\nsize of 2, 4, 10, 16, and compare that to just\na one-stage training where you just do the\n[INAUDIBLE] beginning and initialized in ImageNet\nand as you use different batch", "id": "2ZXYM1h9pgY_13", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  on the development AUC. And so for all the\nexperiments that we do broadly we find that we actually get\nreasonably stable training by just using a batch\nsize of 20 and above. And this kind of comes down to\nif you use a batch size of one, it's just particularly unstable. In other details that we always\nsample the balanced batches. Cause otherwise you'd\nbe sampling like, 20 batches before you see\na single positive sample. You just don't learn anything. Cool. So that is like,\nif you do that, you don't do anything complicated. You don't do any fancy cropping\nor anything of that sort, or like, dealing with\nlike VGG annotations. We found that the actual using\nVGG annotation for this task is not actually helpful. OK. No questions? Yes. AUDIENCE: So with\nthe larger batch sizing you don't use\nthe magnified patches? ADAM YALA: We don't. We just take the whole\nimage from beginning. Pretend you-- like,\ncan you just see the annotation as\nwhole image, cancer with less than within a year. It's a much simpler setup. AUDIENCE: I don't get. That's the same thing\nI thought you said you couldn't do for memory reasons. ADAM YALA: Oh. So you just-- instead of--\nso normally when you do,", "id": "2ZXYM1h9pgY_14", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  the most common approach is\nyou do back prop and then step. Cause you do back\nprop several times, you're accumulating the\ngradients, at least in PyTorch. And then you can\ndo step afterwards. So instead of doing the\nwhole batch at one time, you just do it serially. So there you're just\ntrading time for space. The minimum, though, is you have\nto fit at least a single image per GPU. And in our case\nwe can fit three. But to make this actually scale,\nwe use four GPUs at a time. Yes. AUDIENCE: How much is\nthe trade-off with time? ADAM YALA: So if I'm gonna\ntake one batch size any bigger, I would only do it in\nincrements of let's say 12, because that's how much I\ncan fit within my set of GPUs at the same time. But to control the\nsize of the experiment you want to have the kind of the\nsame number of gradient updates per experiment. So if I want to use\na batch size of 48, so all my experiments, instead\nof taking about half a day, it takes about a day. And so there's kind of,\nlike, this natural trade-off as you go along. So one of the things I\nmentioned at the very end", "id": "2ZXYM1h9pgY_15", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  for something. And one of the annoying\nthings about that is that if I have five\ndiscriminator steps, oh my god. My my experiment-- I'll take\nthree days per experiment. And [INAUDIBLE]\nupdate of someone that's trying to\ndesign a better model becomes really slow\nwhen the experiments start taking this long. Yes. AUDIENCE: So you said\nthe annotations did not help with the training. Is that because\nthe actual cancer itself is not really different\nfrom the dense tissue, and the location of\nthat matters, and not the actual granularity of the-- what is the reason? ADAM YALA: So in general\nwhen something doesn't help, there's always kind of like\na possibility of two things. One thing is that the whole\nimage signal kind of subsumes that smaller scale signal. Or there is a\nbetter way to do it I haven't found that would help. And then this thing looks\nto us all very hard. As of now, so the\ntask we're [INAUDIBLE] on is whole image\nclassification. And so on that\ntask it's possible that the kind of\nsurrounding context--", "id": "2ZXYM1h9pgY_16", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  you kind of lose the\ncontext which it appears in. So it's possible that just by\nlooking at the whole context every time, it's as good-- you don't get any benefit from\nkind of the zooming boxes. However, we're not evaluating\non kind of an object detection type of evaluation metric. If you say how well we\nare catching the box. And if we were, we'd probably\nhave much better luck with using the VGG annotation. Because you might\nbe able to tell some of those\ndiscriminations by like, this looks like a breast\nthat's likely to develop cancer at all. And the ability of\nthe model to do that is part of why we\ncan do risk modeling. Which is going to be the kind\nof the last bit of the talk. Yes. AUDIENCE: So do you do\nthe object detection after you identify whether\nthere's cancer or not? ADAM YALA: So as of now we don't\ndo object detection in part because we're framing\nthe problem as triage. So there is quite a\nfew tool kits out there to draw more boxes\non the mammogram. But the insight\nis that if there's 1,000 things to look at,\nlooking at 2,000 things you drew more boxes per image. And it isn't\nnecessarily the problem we're trying to look at.", "id": "2ZXYM1h9pgY_17", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And it's something we might\nlook into later in the future. But it's not the\nfocus of this work. Yes. AUDIENCE: So Connie was saying\nthat the same pattern appearing in different parts of the breast\ncan mean different things. But when you're looking at\nthe entire image as once, I would worry\nintuitively about whether the convolutional\narchitecture is going to be able to pick\nthat up or whether-- because you were looking\nfor a very small cancer on a very large image. And then you were looking\nfor the significance of that very small cancer in\ndifferent parts of the image or in different\ncontexts of the image. And I'm just-- I mean, it's a pleasant\nsurprise that this works. ADAM YALA: So there is kind\nof like two pieces that can help explain that. So the first is that\nif you look at, like, the receptive fields of any\ngiven last receptive map", "id": "2ZXYM1h9pgY_18", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  summarizes through\nthese convolutions a fairly sizable\npart of the image. And so you are kind of, like,\neach pixel at the very end ends up being like something\nlike a 50 by 50 image. That's by five total dimensions. And so each part does summarize\nthis local context decently well. And when you do maximum\nat the very end, and you get some not perfect\nbut OK global summary, what is the context of this image? So something like, let's say,\nsome of the lower dimensions can summarize, like, is\nthis a dense breast or kind of some of the other\npattern information that might tell you what\nkind of breast this is. Whereas any one of\nthem can tell you this looks like a cancer\ngiven its local context. So do you have some\nlevel summarization, both because of the\nchannel-wise maxim of the end, and because each point through\nthe many, many convolutions of different strides gives you\nsome of that summary effect. OK, great. I'm going to jump forward. So we've talked about\nhow to make this learn. It's actually not\nthat tricky if we just", "id": "2ZXYM1h9pgY_19", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Now I'll talk about how to use\nthis model to actually deliver on this triage idea. So some of my choices again,\nImageNet initialization is going to make your\nlife a happier time. Use bigger batch sizes. And architecture\nchoice doesn't really matter if it's convolutional. And the overall setup that\nwe do through this work and across many\nother projects we're training independently\nper image. Now this is a harder task\nbecause you don't actually have the-- you're not taking any\nof the other view, you're not taking\nprior mammograms. But this is for kind of more\nharder reasons than that. We're going to get the\nprediction for the whole exam by taking the maximum\nacross the different images. So if I say this breast has\ncancer, the exam has cancer. So you should get it checked up. And at each\ndevelopment epoch we're going to evaluate the\nability of the model to do triage task, which\nI'll step into in a second. And we're going to\nkind of take the best model that can do triage. So you're always kind of\nlike, your true end metric is what you're measuring\nduring training. And you're going to\ndo model selection and kind of hyper\npatching based on that. And the way we're going\nto do triage and our goal", "id": "2ZXYM1h9pgY_20", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  without missing a single\ncancer that we always would have caught. So intuitively kind of\nby taking all the cancers that the radiologist\nwould have caught, what's the probability of cancer\nacross these images, and just take the minimum\nof those and call that the threshold. That's exactly what we do. And another detail\nthat's quite relevant often is if you want\nthese models to output a reasonable\nprobability like this is the probability of cancer,\nand you train on a 50/50 sample the batches, by default\nyour model thinks that the average\nincidence is 50%. So it's crazy\nconfidence all the time. So to calibrate that one\nreally simple trick is you do something called Platt's Method\nwhere you basically just fit like a two-parameter sigmoid\nor just scale and a shift to just-- on the development sets\nto make it actually fit the distribution. That way the average\nprobability you would expect to actually fit the incidence. And you don't get this kind\nof like crazy off-kilter probabilities. OK. So analysis. The objectives of what\nwe would try to do here", "id": "2ZXYM1h9pgY_21", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  One, does this thing work? Two, does this thing work\nacross all the people it's supposed to work for? So we did a subgroup analysis. First we looked at\nthe AUC in this model. So the ability to\ndiscriminate cancer is not. We did it across races. We have across MGH, age\ngroups, and density categories. And finally, how\ndoes this relate to radiologist's assessments? And if we actually\nuse this at test time on the test set, what\nwould have happened? Kind of a simulation before a\nfull clinical implementation. So overall AUC here was 82 with\nsome confident from 80 to 85. And we did our analysis by age. We found that the performance\nwas pretty similar across every age group. What's not shown here is\nthe confidence intervals. So for example-- but the\nkind of key core takeaway here is that there\nwas no noticeable gap in terms of by age group. We repeated this\nanalysis by race, and we saw the same trend again. The performance kind of\nranged generally around 82.", "id": "2ZXYM1h9pgY_22", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  the just confidence interval\nwas bigger accordingly due to smaller sample sizes,\ncause MGH is 80% white. We saw the exact same\ntrend by density. The outlier here is\nvery dense breasts. But there's only like\n100 of those on test set. So this confidence actually\ngoes from like, 60 to 90. So as far as we know for\nthe other three categories, it is very much tied\nto confidence interval and very similar,\nonce again, around 82. OK. So we have a decent idea\nthat this model seems at least with a\npublish of MGH actually serve the relevant\npopulations that exist as far as we know so far. The next question is, how\ndoes the model assessment relate to the\nradiologist's assessment? So to look at that we\nlooked at on the test, if you look at the\nradiologist's true positives, false positives, true\nnegatives, false negatives. Where do they fall within\nthe model distribution of percentile risk? And if there is\nbelow the threshold, we've got to color it in\nthis kind of cyan color. And if it's above\nthe threshold, we're", "id": "2ZXYM1h9pgY_23", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So this is kind of\ntriage, not triage. The first thing to notice--\nthis is the true positives-- is that there is like a\npretty kind of steep drop-off. And so there is only\none true positive fell below the threshold in\na test set of 26,000 exams. So none of this difference\nwas statistically significant. And the vast majority of them\nare kind of this top 10%. But you kind of see, like,\nthere's a clear trend here that they kind of get piled up\ntowards the higher percentages. Whereas if you look at the\nfalse positive assessments, this trend is much weaker. So you still see that\nthere is some correlation that there's going to more false\npositives the higher amounts, but much less stark. And this actually means\nthat a lot of radiologist's false positives we actually\nplace below the threshold. And so because these assessments\nare completely concordant and we're not just modeling\nwhat the radiologist would have said, we get an\nanticipated benefit of actually reducing the false\npositives significantly because of the weight of disagreeing. And finally, kind of\naiding that further,", "id": "2ZXYM1h9pgY_24", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  there is not that much\ntrending between where it falls within this. So it shows that they're kind of\npicking up on different things and they're-- where they\ndisagree gives them both areas to improve and ancillary\nbenefits because now we can reduce false positives. This directly leads into\nassimilating the impact. So one of the things we\ndid, we just said, OK. If people retrospective on\nthe test set as a simulation before which truly plug it\nin, if people didn't rebuild the triage threshold-- so\nwe can't catch any more cancer this way, but we can\nreduce false positives-- what would have happened? So at the top we have\nthe original performance. So this is looking at\n100% of mammograms, sensitivity was 98.6\nwith specificity of 93. And in the simulation\nthe sensitivity dropped not\nsignificantly to 90.1, but significantly improved\nto 93.7 while looking at 81% of the mammograms. So this is like promising\npreliminary data. But to reevaluate this and\ngo forward, our next step--", "id": "2ZXYM1h9pgY_25", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  I'm going to get to\nthat in a second. Our next step is we need to\ndo clinical implementation to really figure out-- because there's a\ncore assumption here is that people read\nit the same way. But if you have this higher\nincidence, what does that mean? Can you focus more on the\npeople that are more suspicious? And is the right way to do this\njust a single threshold to not read? Or have a double\nended with the seniors cause they're much more\nlikely to have cancer. And so there is quite a bit\nof exploration here to say, given we have these\ntools that give us some probability of\ncancer, that's not perfect, but gives us something. How well can we do that\nto improve care today? So as a quiz, can you tell\nwhich of these will be triaged? So this is no cherry-picking. I randomly picked\nfour mammograms that were below and\nabove the threshold. Can anyone guess which side-- left or right-- was triaged? This is not graded,\nChris, so you know. AUDIENCE: Raise your hand for-- ADAM YALA: Oh yeah. Raise your hand for the left. OK. Raise your hand for right. Here we go.", "id": "2ZXYM1h9pgY_26", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Well done. OK. And then next step,\nas I said before, is we need to kind of push to\nthe clinical implementation because that's where the\nrubber hits the road. We identify is there any\nbiases we didn't detect? And we need to say, can\nwe deliver this value? So the next project is on\nassessing breast cancer risk. So this is the same mammogram\nI showed you earlier. It was diagnosed with\nbreast cancer in 2014. It's actually my\nadvisor, Regina's. And you can see that in\n2013 you see it's there. In 2012 it looks\nmuch less prominence. And five years ago, really\nlooking at breast cancer risk. So if you can tell\nfrom an image that is going to be healthy\nfor a long time, you're really trying\nto model what's the likelihood of\nthis breast developing cancer in the future. Now modeling breast cancer\nrisk, as Connie earlier said, is not a new problem. It's been a quite researched\none in the community. And the more classical\napproach that we're gonna look at other\nkind of global health factors-- the person's\nage, their family history,", "id": "2ZXYM1h9pgY_27", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  of any other of these kind\nof facts we can sort of say are markers of\ntheir health to try to predict whether this person's\nat risk of developing breast cancer. People have thought that\nthe image contains something before. The way they've\nthought about this is through this kind of\nsubjective breast density marker. And the improvements\nseen across this are kind of marginal\nfrom 61 to 63. And as before,\nthe kind of sketch we're going to go through is\ndataset collection, modeling, and analysis. And dataset\ncollection we followed a very similar template. We saw from\nconsecutive mammograms from 2009 to 2012 we took\noutcomes from the EHR, once again, and the\nPartners Registry. We didn't do exclusions based on\nrace or anything of that sort, or implants. But we did exclude\nnegatives for followup. So if someone didn't have\ncancer in three years, but disappeared\nfrom the system, we didn't count them\nas negatives that we have some certainty in both\nthe modeling and the analysis. And as always, we split\npatients into train, dev, test. The modeling is very similar.", "id": "2ZXYM1h9pgY_28", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  except we experimented with a\nmodel that's only the image. And for the sake of analysis,\na model that's the image model I just talked to you\nbefore concatenated with those traditional risk\nfactors at the last layer and trained jointly. That make sense for everyone? So I'm going to call that\nImageOnly an Image+RF or hybrid. OK. Cool? Our kind of goals\nfor the analysis. As before, we want to\nsee does this model actually serve the\nwhole population? Is it going to be discriminative\nacross race, menopause status, the family history? And how does it relate to kind\nof classical portions of risk? And are we actually\ndoing any better? And so just diving\ndirectly into that, assuming there's no questions. Good. Just to kind of remind you,\nthis is the kind of the setting. One thing I forgot to mention--\nthat's why I had the slide here to remind me-- is that we excluded\ncancers from the first year from the test set. So there is truly a negative\nscreening population. That way we kind of\ndisentangle cancer detection from cancer risk. OK. Cool. So Tyrer-Cuzick is the kind of\nprior state-of-the-art model.", "id": "2ZXYM1h9pgY_29", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Their developer is\nsomeone named Sir Cuzick, who was knighted for this work. It's very commonly used. So that one had an AUC of 62. Our image-only model\nhad an AUC about 68. And hybrid one had an AUC of 70. So you know, what is\nthis kind of AUC thing gives you when you look\nusing a risk model. What it gives you is the\nability to build better high-risk and low-risk cohorts. So in terms of looking\nat high-risk cohorts, our best model place about\n30% of all the cancers in the population\nin the top 10%, and 3% of all the\ncancers in the bottom 10% compared to 18 and 5 to\nthe prior state of the art. And so what this\nenables you to do, if you're going to\nsay that this 10% should actually\nqualify for MRI, you can start fighting this\nproblem of majority of people that get\ncancer don't have MRI, and the majority of people\nthat get it don't need it. It's all about, is your\nrisk model actually place the right people\ninto the right buckets. Now we saw that this trend of\noutperforming the prior state of the art held across races. And one of the things that\nwas kind of astonishing", "id": "2ZXYM1h9pgY_30", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  makes sense because\nit was developed only using white women in the UK. It was worse than\nrandom [INAUDIBLE] for African-American women. And so this kind of\nemphasizes the importance of this kind of\nanalysis to make sure that the kind of\ndata that you have is reflective of the population\nthat you're trying to serve and actually doing the\nanalysis accordingly. So we saw that our model\nkind of held across races and as well across--\nwe see this trend from across\npre-postmenopausal and with and without family history. One thing we did in terms of\na more granular comparison of performance, if\nwe just look at kind of like the risk thirds for\nour model and the Tyrer-Cuzick model, what's the\ntrend that you see or the cases where kind\nof like which one is right that's kind of ambiguous. And what I should\nshow in these boxes is the cancer incidence\nprevalence in the population. So the darker the box,\nthe higher the incidence. And on the right-hand\nside are just random images from cases\nthat fit within those boxes. Does that make\nsense for everyone?", "id": "2ZXYM1h9pgY_31", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So a clear trend that you\nsee is that, for example, if TCv8 calls you a high\nrisk but we call it low, that is a lower incidence\nthan if we called it medium and they call it low. So kind of like you kind of\nsee this straight column-wise pattern showing that\ndiscrimination truly does follow the deep learning model\nand not the classical approach. And by looking at the\nrandom images that were selected in case\nwhere we disagree, it supports the notion\nthat it's not just that the column is just\nthe most dense, crazy, dense looking breast, that\nthere's something more subtle it's picking up that's actually\nindicative of breast cancer risk. Kind of a very\nsimilar analysis we looked at as if we look at just\nby a traditional breast density as labeled by the original\nradiologist on the development set or on the test\nset, we end up seeing the same trend where\nif someone is non-dense we call them high risk. They're much higher\nrisk than someone that is dense than\nwe call low risk. And as before, the\nkind of real next step here to make this truly valuable\nand truly useful is actually implementing a clinically\nseamless prospectively", "id": "2ZXYM1h9pgY_32", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  does this work and does it\ndeliver the kind of benefits that we care about. And viewing what is\nthe leverage of change once you know that\nsomeone is high risk? Perhaps MRI, perhaps\nmore frequent screening. And so this is the kind\nof gap between having a useful technology\non the paper side to an actual useful\ntechnology in real life. So I am moving on schedule. So now I'm gonna talk\nabout how to mess up. And it's actually\nquite interesting. There is like, so many ways. And I fall into them a few\ntimes myself, and it happens. And kind of\nfollowing the sketch, you can mess up in\ndataset collection. That's probably the\nmost common by far. You can mess up in modeling,\nwhich I'm doing right now. And it's very sad. And you can mess up in analysis,\nwhich is really preventable. So in dataset collection,\nenriched data sets are the kind of the most common\nthing you see in this space. You find in a public\ndata set that's most likely going to be like\n50-50 cancer, not cancer. And oftentimes these\ndatasets collect can have some sort of\nbias within the way it was collected.", "id": "2ZXYM1h9pgY_33", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  centers than you\nhave positive cases. Or they're collected\nfrom different years. And actually, this\nis something we ran into earlier in our own work. Once upon a time,\nConnie and I were in Shanghai for the opening\nof a cancer center there. And at that time we had all the\ncancers from the MGH dataset, about 2,000. But the mammograms were still\nbeing collected annually from 2012-- from 2009. So at that time, we only had,\nlike, half of the negatives by year, but all of the cancers. And all of a sudden\nI had to-- you know, I came from the slightly\nmore complicated model, as one often does. I looked at several\nimages at the same time. And my AUC went up to like, 95. And I had all this, like,\nbouncing off the wall. And then in-- you know, I\nhad some suspicion of like, wait a second. This is too high. This is too good. And we completely realized\nthat all these numbers were kind of a myth. But this level of-- kind of if you do these\nkind of case control things, you can oftentimes,\nunless you're very careful about the\nway it was constructed, you could easily run\ninto these issues.", "id": "2ZXYM1h9pgY_34", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And so having a clean\ndataset that truly follows the kind of spectrum\nwe expect to use it in-- i.e., a natural\ndistribution, collected through routine clinical\ncare is important to say will it behave as we\nactually want it to be used. In general, the only-- some of this you can think\nthrough in first principle. But it kind of\nstresses the importance of actually testing\nthis prospectively in external validation to try to\nsee does this work when I take away some of the\nbiases in my dataset, and being really\ncareful about that. The common approach\nof just controlling by age or by density\nis not enough when the model can catch\nreally fine-grained signals. How to mess up in modeling. So there's been adventures\nin this space as well. One of the things I've\nrecently discovered is that the actual\nmammography machine device that the\nmachine was captured on-- so you saw a\nbunch of mammograms probably from\ndifferent machines-- has an unexpected\nimpact on the model. So the actual probability\ndistribution-- the distribution of cancer\nprobabilities by the model is not independent\nof the device.", "id": "2ZXYM1h9pgY_35", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  We actually ran into\nthis while working on clinical implementation\nis like this kind of conditional adversarial\ntraining set up to try to rectify this issue. It's important. So this is much harder to\ncatch based on first principle. But it's important to think\nthrough as you kind of really start demoing out\nyour computations. This will kind of-- these\nissues pop up easily, and they're harder to avoid. And lastly, and I\nthink probably one that's probably\nthe most important is messing up in analysis. So it's quite common\nin the previous section in this field-- yes. AUDIENCE: With the\nadversarial up there, just to understand what you\ndo, do you that discriminate or predict the machine? And then you train against that? ADAM YALA: So my answer\nis going to be two parts. One, it doesn't work as\nwell as I want it to yet. So really who knows? But my best hunch\nin terms of what's been done before for other\nkind of work, specifically in radio signals, is they use\na conditional adversarial. So you're free to discriminate\nat both the label and the image presentation. You have to try to\npredict out the device", "id": "2ZXYM1h9pgY_36", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  contained within the\nlabel distribution. And that's been shown to be very\nhelpful for people trying to do [INAUDIBLE] detection\nbased off on Wi-Fi-- or not Wi-Fi-- but\nlike, radio waves. And the [INAUDIBLE]\nbut also, it seems to be the most common approach\nI've seen in literature. So it's something that\nI'm going to try soon. I haven't implemented it. It was just GPU time\nand kind of waiting to queue up the experiment. And the last part in\nterms of how to mess up is this kind of analysis. One thing that's\ncommon is people assume that's it kind of\nlike synthetic experiments or the same thing as\nclinical implementation. Like, people do reader\nstudies very often. And it's quite common\nto see that when you do reader studies that\nit doesn't actually-- like, you might find that\ncomputer detection does a huge difference\nin reader studies. And it's-- Connie actual showed\nit was harmful in real life. And it's important to kind\nof like, do these real world experiments that we can\nsay what is happening and just them the real\nbenefit that I expected. And a hopefully less\ncommon nowadays mistake is that oftentimes people\nexclude all inconvenient cases.", "id": "2ZXYM1h9pgY_37", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  that the cancer detection used a\nkind of patched-up architecture which would read more\nclosely into their details, they excluded all\nwomen with breasts that they considered too\nsmall by some threshold for like modeling convenience. But that might\ndisproportionately affect specifically Asian\nwomen in that population. And so they didn't do\na subgroup analysis for all the different\nraces, so it's hard to know what\nis happening there. If your population\nis mostly white, which it is at MGH, and\nis at a lot of the centers that these colleges\nhave developed, are reporting the\naverage that you see isn't enough to\nreally validate that. And so you can have things\nlike Tyrer-Cuzick model that are worse than random\nand especially harmful for African-American women. And so guarding\nagainst that is you can do a lot of that\nbased on first principle. But some of these things\nyou can only really find out by actively monitoring to say,\nis there any subpopulation that I didn't think about a\npriority that could be harmed? And finally, so I talked\nabout clinical deployments. We've actually done\nthis a couple times. And I'm going to switch\nover to Connie real soon. In general, what\nyou want to do is", "id": "2ZXYM1h9pgY_38", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  for the in-house IT\nteam to use your tool. We've gone through this with-- not like-- I don't--\ndepends on how you count. It's like once for density\nand then like three times at the same time. But I spent, like, many\nhours sitting there. And the broad way that we\nset it up so far is we just have a kind of\ndocker as container to manage a web app\nthat holds the model. This web app has kind of a\nbackup processing toolkit. So the kind of steps that\nall of our deployments follow and I look\nunder unified framework is the IT application\nwould get some images out of the PAC system. It will send it\nover to application. We're going to convert to the\nPNG in the way that we expect, because we kind of encapsulate\nthis functionality. Run for the models, send it\nback, and then write it back to the EHR. One of the things I ran into\nwas that they didn't actually know how to use things like\nHTTP because it's not actually normal within their\ninfrastructure. And so being cognizant that\nsome of these more, like, tech standard things\nlike just HTTP requests and responses and stuff is\nless standard within the inside", "id": "2ZXYM1h9pgY_39", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  how to actually do these\nthings in like C Sharp, or whatever language\nthey have, has been really what's enabled\nus to end block these things and actually plug it in. And that is it for my part. So I'm gonna hand\nit back-- oh, yes. AUDIENCE: So you're writing\nstuff in the IT application in C Sharp to do API requests? ADAM YALA: So\nthey're writing it. I just meet them to tell\nthem how to write it. But yes. So like, in general,\nlike, there's libraries. So like, the entire\nenvironment is in Windows. And Windows has a\nvery poor support for lots of things\nyou would expect it to have a good support for. So there was like,\nif you wanted to send HP requests for like\na multipart form and just put the\nimages in that form, apparently that has bugs in\nit in like, Windows whatever version they use today. And so that vanilla\nversion didn't work. Windows for Docker\nalso has bugs. And I had to set up this kind\nof locking function for them to like, automatically table\nlocks inside the container. And it just doesn't work\nin Windows for Docker. AUDIENCE: [INAUDIBLE] questions\nbecause he is short on time.", "id": "2ZXYM1h9pgY_40", "title": "13. Machine Learning for Mammography", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So we can get to\nthis at the end. I want to hand off to Connie.", "id": "2ZXYM1h9pgY_41"}, {"text": "  DAVID SONTAG: So today's lecture\nis going to be about causality. Who's heard about\ncausality before? Raise your hand. What's the number one thing\nthat you hear about when thinking about causality? Yeah? AUDIENCE: Correlation\ndoes not imply causation. DAVID SONTAG: Correlation\ndoes not imply causation. Anything else come to mind? That's what came to my mind. Anything else come to mind? So up until now in\nthe semester, we've been talking about purely\npredictive questions. And for purely\npredictive questions, one could argue that\ncorrelation is good enough. If we have some\nsigns in our data that are predictive of\nsome outcome of interest,", "id": "gRkUhg9Wb-I_0", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Whether it's\nupstream, downstream, the causal directionality is\nirrelevant for that purpose. Although even that\nisn't quite true, right, because Pete and I have been\nhinting throughout the semester that there are times\nwhen the data changes on you, for example, when you go\nfrom one institution to another or when you have non-stationary. And in those situations,\nhaving a deeper understanding about the data might\nallow one to build an additional robustness to\nthat type of data set shift. But there are other\nreasons as well why understanding something about\nyour underlying data generating processes can be\nreally important. It's because often,\nthe questions that we want to answer when\nit comes to health care are not predictive questions,\ntheir causal questions. And so what I'll do now is I'll\nwalk through a few examples of what I mean by this. Let's start out with what we saw\nin Lecture 4 and in Problem Set 2, where we looked\nat the question", "id": "gRkUhg9Wb-I_1", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  You used Truven\nMarketScan's data set to build a\nrisk stratification algorithm for\ndetecting who is going to be newly diagnosed\nwith diabetes one to three years from now. And if you think about\nhow one might then try to deploy that\nalgorithm, you might, for example, try to\nget patients into the clinic to get them diagnosed. But the next set of\nquestions are usually about the so what question. What are you going to do\nbased on that prediction? Once diagnosed, how\nwill you intervene? And at the end of the\nday, the interesting goal is not one of how do\nyou find them early, but how do you prevent them\nfrom developing diabetes? Or how do you prevent the\npatient from developing complications of diabetes? And those are questions\nabout causality. Now, when we built\na predictive model and we introspected\nat the weight, we might have noticed\nsome interesting things. For example, if you looked at\nthe highest negative weights,", "id": "gRkUhg9Wb-I_2", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  but is something that I did\nas part of my research study, you see that gastric\nbypass surgery has the biggest negative weight. Does that mean that if you give\nan obese person gastric bypass surgery, that will prevent\nthem from developing type 2 diabetes? That's an example of a causal\nquestion which is raised by this predictive model. But just by looking\nat the weight alone, as I'll\nshow you this week, you won't be able to\ncorrectly infer that there is a causal relationship. And so part of what\nwe will be doing is coming up with a mathematical\nlanguage for thinking about how does one\nanswer, is there a causal relationship here? Here's a second example. Right before spring break\nwe had a series of lectures about diagnosis,\nparticularly diagnosis from imaging data of\na variety of kinds,", "id": "gRkUhg9Wb-I_3", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And often, questions\nare of this sort. Here is a woman's breasts. She has breast cancer. Maybe you have an associated\npathology slide as well. And you want to know what is\nthe risk of this person dying in the next five years. So one can take a\ndeep learning model, learn to predict\nwhat one observes. So in the patient in your\ndata set, you have the input and you have, let's\nsay, survival time. And you might use that\nto predict something about how long it takes\nfrom diagnosis to death. And based on those predictions,\nyou might take actions. For example, if you predict\nthat a patient is not risky, then you might\nconclude that they don't need to get treatment. But that could be\nreally, really dangerous, and I'll just give\nyou one example", "id": "gRkUhg9Wb-I_4", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  These predictive models,\nif you're learning them in this way, the outcome,\nin this case let's say time to death, is\ngoing to be affected by what's happened in between. So, for example,\nthis patient might have been receiving\ntreatment, and because of them receiving treatment in\nbetween the time from diagnosis to death, it might have\nprolonged their life. And so for this patient\nin your data set, you might have observed that\nthey lived a very long time. But if you ignore what\nhappens in between and you simply learn to predict\ny from X, X being the input, then a new patient comes\nalong and you predicted that new patient is going\nto survive a long time, and it would be completely\nthe wrong conclusion to say that you don't need\nto treat that patient. Because, in fact, the only\nreason the patients like them in the training data\nlived a long time is because they were treated. And so when it comes to this\nfield of machine learning", "id": "gRkUhg9Wb-I_5", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  about these types of questions\nbecause an error in the way that we formalize our\nproblem could kill people because of mistakes like this. Now, other questions\nare ones about not how do we predict\noutcomes but how do we guide treatment decisions. So, for example, as\ndata from pathology gets richer and\nricher and richer, we might think that we\ncan now use computers to try to better predict\nwho is likely to benefit from a treatment than\nhumans could do alone. But the challenge\nwith using algorithms to do that is that people\nrespond differently to treatment, and the\ndata which is being used to guide treatment is\nbiased based on existing treatment guidelines. So, similarly, to the previous\nquestion, we could ask, what would happen if we trained\nto predict past treatment", "id": "gRkUhg9Wb-I_6", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  This would be the\nmost naive way to try to use data to guide\ntreatment decisions. So maybe you see David\ngets treatment A, John gets treatment B,\nJuana gets treatment A. And you might ask then,\nOK, a new patient comes in, what should this new\npatient be treated with? And if you've just\nlearned a model to predict from what you\nknow about the treatment that David is likely\nto get, then the best that you could hope\nto do is to do as well as existing clinical practice. So if we want to go beyond\ncurrent clinical practice, for example, to recognize\nthat there is heterogeneity in treatment response, then\nwe have to somehow change the question that we're asking. I'll give you one\nlast example, which is perhaps a more traditional\nquestion of, does X cause y? For example, does\nsmoking cause lung cancer is a major question of\nsocietal importance. Now, you might be familiar\nwith the traditional way", "id": "gRkUhg9Wb-I_7", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  would be to do a randomized\ncontrolled trial. Except this isn't\nexactly the type of setting where you could do\nrandomized controlled trials. How would you feel if you were\na smoker and someone came up to you and said, you have\nto stop smoking because I need to see what happens? Or how would you feel\nif you were a non-smoker and someone came\nup to you and said, you have to start smoking? That would be both not feasible\nand completely unethical. And so if we want to\ntry to answer questions like this from data,\nwe need to start thinking about\nhow can we design, using observational\ndata, ways of answering questions like this. And the challenge\nis that there's going to be bias in the data\nbecause of who decides to smoke and who decides not to smoke. So, for example,\nthe most naive way you might try to\nanswer this question would be to look at the\nconditional likelihood of getting lung\ncancer among smokers and getting lung cancer\namong non-smokers.", "id": "gRkUhg9Wb-I_8", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  can be very misleading\nbecause there might be confounding\nfactors, factors that would, for example, both\ncause people to be a smoker and cause them to\nreceive lung cancer, which would differentiate\nbetween these two numbers. And we'll have a\nvery concrete example of this in just a few minutes. So to properly answer\nall of these questions, one needs to be thinking\nin terms of causal graphs. So rather than the\ntraditional setup in machine learning where you just\nhave inputs and outputs, now we need to have triplets. Rather than having\ninputs and outputs, we need to be thinking\nof inputs, interventions,", "id": "gRkUhg9Wb-I_9", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So we now need be having\nthree quantities in mind. And we have to start\nthinking about, well, what is the causal relationship\nbetween these three? So for those of you who have\ntaken more graduate level machine learning\nclasses, you might be familiar with ideas\nsuch as Bayesian networks. And when I went to\nundergrad and grad school and I studied machine\nlearning, for the longest time I thought causal\ninference had to do with learning causal graphs. So this is what I thought\ncausal inference was about. You have data of the\nfollowing nature-- 1, 0, 0, 1, dot, dot, dot. So here, there are\nfour random variables. I'm showing the realizations\nof those four binary variables one per row, and you have\na data set like this. And I thought\ncausal inference had", "id": "gRkUhg9Wb-I_10", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  is the underlying\nBayesian network that created that data, is it\nX1 goes to X2 goes to X3 to X4? Or I'll say, this is X1,\nthat's X2, x3, and X4. Or maybe the causal graph\nis X1, to X2, to X3, to x4. And trying to distinguish\nbetween these different causal graphs from observational\ndata is one type of question that one can ask. And the one thing you learn\nin traditional machine learning treatments of\nthis is that sometimes you can't distinguish between\nthese causal graphs from the data you have. For example, suppose you just\nhad two random variables. Because any distribution could\nbe represented by probability of X1 times probability\nof X2 given X1, according to just rule of\nconditional probability,", "id": "gRkUhg9Wb-I_11", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  as the opposite, probability\nof X2 times probability of X1 given X2, which would\nlook like this, the statement that one would make\nis that if you just had data involving X1 and\nX2, you couldn't distinguish between these two causal graphs,\nX1 causes X2 or X2 causes X1. And usually another\ntreatment would say, OK, but if you have a third variable\nand you have a V structure or something like X1 goes\nto x2, X1 goes to X3, this you could distinguish from,\nlet's say, a chain structure. And then the final\nanswer to what is causal inference\nfrom this philosophy would be something like, OK, if\nyou're in a setting like this and you can't distinguish\nbetween X1 causes X2 or X2 causes X1, then you\ndo some interventions, like you intervene on X1 and you\nlook to see what happens to X2,", "id": "gRkUhg9Wb-I_12", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  None of this is what we're\ngoing to be talking about today. Today, we're going to be\ntalking about the simplest, simplest possible setting\nyou could imagine, that graph shown up there. You have three sets of\nrandom variables, X, which is perhaps\na vector, so it's high dimensional, a\nsingle random variable T, and a single\nrandom variable Y. And we know the\ncausal graph here. We're going to\nsuppose that we know the directionality, that we\nknow that X might cause T and X and T might cause Y. And\nthe only thing we don't know is the strength of the edges. All right. And so now let's try to\nthink through this in context", "id": "gRkUhg9Wb-I_13", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Yeah, question? AUDIENCE: Just to make sure-- so\nT does not affect X in any way? DAVID SONTAG: Correct,\nthat's the assumption we're going to make here. So let's try to\ninstantiate this. So we'll start\nwith this example. X might be what you know about\nthe patient at diagnosis. T, I'm going to assume for\nthe purposes of today's class, is a decision between two\ndifferent treatment plans. And I'm going to simplify\nthe state of the world. I'm going to say those\ntreatment plans only depend on what you know about\nthe patient at diagnosis. So at diagnosis, you\ndecide, I'm going to be giving them this\nsequence of treatments at this three-month interval\nor this other sequence of treatment at, maybe,\nthat four-month interval. And you make that decision\njust based on diagnosis and you don't change it based\non anything you observe. Then the causal graph\nof relevance there is,", "id": "gRkUhg9Wb-I_14", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  which I'm going to\nsay X is a vector because maybe it's\nbased on images, your whole electronic\nhealth record. There's a ton of data you have\non the patient at diagnosis. Based on that, you\nmake some decision about a treatment plan. I'm going to call that\nT. T could be binary, a choice between two treatments,\nit could be continuous, maybe you're deciding the\ndosage of the treatment, or it could be\nmaybe even a vector. For today's lecture,\nI'm going to suppose that T is just binary,\njust involves two choices. But most of what\nI'll tell you about will generalize to the setting\nwhere T is non-binary as well. But critically,\nI'm going to make the assumption for\ntoday's lecture that you're not observing\nnew things in between. So, for example, in this\nwhole week's lecture, the following scenario\nwill not happen. Based on diagnosis, you make a\ndecision about treatment plan.", "id": "gRkUhg9Wb-I_15", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Based on those new\nobservations, you realize that treatment\nplan isn't working and change to another\ntreatment plan, and so on. So that scenario goes\nby a different name, which is called\ndynamic treatment regimes or off-policy\nreinforcement learning, and that we'll learn\nabout next week. So for today's and\nThursday's lecture, we're going to suppose\nyou base on what you know about the patient at\nthis time, you make a decision, you execute the decision,\nand you look at some outcome. So X causes T, not\nthe other way around. And that's pretty clear\nbecause of our prior knowledge about this problem. It's not that the\ntreatment affects what their diagnosis was. And then there's the outcome\nY, and there, again, we suppose the outcome, what\nhappens to the patient, maybe survival time, for example, is\na function of what treatment they're getting and\naspects about that patient.", "id": "gRkUhg9Wb-I_16", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  We know it. But we don't know,\ndoes that treatment do anything to this patient? For whom does this\ntreatment help the most? And those are the types\nof questions we're going to try to answer today. Is the setting clear? OK. Now, these questions\nare not new questions. They've been studied\nfor decades in fields such as political science,\neconomics, statistics, biostatistics. And the reason why they're\nstudied in those other fields is because often you don't\nhave the ability to intervene, and one has to try to\nanswer these questions from observational data. For example, you might ask, what\nwill happen to the US economy", "id": "gRkUhg9Wb-I_17", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  When's the last time you\nheard of the Federal Reserve doing a randomized\ncontrolled trial? And even if they had done a\nrandomized controlled trial, for example, flipped a coin to\ndecide which way the interest rates would go, it wouldn't\nbe comparable had they done that experiment today to\nif they had done that experiment two years from now because\nthe state of the world has changed in those years. Let's talk about\npolitical science. I have close colleagues of mine\nat NYU who look at Twitter, and they want to\nask questions like, how can we influence\nelections, or how are elections influenced? So you might look at some\nunnamed actors, possibly people supported by the\nRussian government, who are posting to Twitter\nor their social media.", "id": "gRkUhg9Wb-I_18", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  did that actually\ninfluence the outcome of the previous\npresidential election? Again, in that\nscenario, it's one of, well, we have this\ndata, something happened in the\nworld, and we'd like to understand what was\nthe effect of that action, but we can't exactly go back\nand replay to do something else. So these are fundamental\nquestions that appear all across the\nsciences, and of course they're extremely relevant\nin health care, but yet, we don't teach\nthem in our introduction to machine learning classes. We don't teach them in our\nundergraduate computer science education. And I view this as a major\nhole in our education, which is why we're\nspending two weeks on it in this course, which\nis still not enough. But what has changed\nbetween these fields, and what is relevant\nin health care? Well, the traditional way\nin which these questions were asked in\nstatistics were ones where you took a huge\namount of domain knowledge to, first of all, make sure\nyou're setting up the problem", "id": "gRkUhg9Wb-I_19", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  But then to think through what\nare all of the factors that could influence the\ntreatment decisions called the confounding factors. And the traditional\napproach is one would write down 10,\n20 different things, and make sure that you do\nsome analysis, including the analysis I'll show you about\nin today and Thursday's lecture using those 10 or 20 variables. But where this field\nis going is one of now having high dimensional data. So I talked about how you\nmight have imaging data for X, you might have the whole entire\npatient's electronic health record data facts. And the traditional approaches\nthat the statistics community used to work on no longer\nwork in this high dimensional setting. And so, in fact, it's actually\na really interesting area for research, one that my\nlab is starting to work on and many other labs, where\nwe could ask, how can we bring machine learning\nalgorithms that are designed to work with high\ndimensional data to answer these types of\ncausal inference questions?", "id": "gRkUhg9Wb-I_20", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  from causal inference\nto machine learning, where we'll be\nable to use machine learning to answer one of those\ncausal inference questions. So the first thing we need\nis some language in order to formalize these notions. So I will work within what's\nknown as the Rubin-Neyman Causal Model, where\nwe talk about what are called potential outcomes. What would have happened under\nthis world or that world? We'll call Y 0,\nand often it will be denoted as Y underscore\n0, sometimes it'll be denoted as Y parentheses\n0, and sometimes it'll be denoted as Y given\nX comma do Y equals 0. And all three of these\nnotations are equivalent.", "id": "gRkUhg9Wb-I_21", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  have happened to this\nindividual if you gave them treatment to 0. And Y1 is the potential\noutcome of what would have happened to this\nindividual had you gave them treatment one. So you could think about Y1\nas being giving the blue pill and Y0 as being\ngiven the red pill. Now, once you can talk about\nthese states of the world, then one could start\nto ask questions of what's better, the red\npill or the blue pill? And one can formalize\nthat notion mathematically in terms of what's called the\nconditional average treatment effect, and this\nalso goes by the name of individual treatment effect. So it's going to take\nas input Xi, which I'm going to denote as\nthe data that you had at baseline for the individual. It's the covariance, the\nfeatures for the individual.", "id": "gRkUhg9Wb-I_22", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  we know about them, what's the\ndifference between giving them treatment one or giving\nthem treatment zero? So mathematically, that\ncorresponds to a difference in expectations. It's a difference in\nexpectation of Y1 from Y0. Now, the reason why I'm\ncalling this an expectation is because I'm not going to\nassume that Y1 and Y0 are deterministic\nbecause maybe there's some bad luck component. Like, maybe a medication usually\nworks for this type of person, but with a flip of a coin,\nsometimes it doesn't work. And so that's the\nrandomness that I'm referring to when I talk about\nprobability over Y1 given Xi. And so the CATE looks\nat the difference in those two expectations. And then one can now talk about\nwhat the average treatment effect is, which is the\ndifference between those two. So the average treatment effect\nis now the expectation of--", "id": "gRkUhg9Wb-I_23", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  of people, P of X.\nNow, we're going to go through this in four\ndifferent ways in the next 10 minutes, and then you're\ngoing to go over it five more ways doing your\nhomework assignment, and you'll go over it two more\nways on Friday in recitation. So if you don't get it\njust yet, stay with me, you'll get it by the\nend of this week. Now, in the data that you\nobserve for an individual, all you see is what happened\nunder one of the interventions. So, for example, if the i'th\nindividual in your data set received treatment Ti equals\n1, then what you observe, Yi is the potential outcome Y1. On the other hand, if the\nindividual in your data set received treatment\nTi equals 0, then what you observed\nfor that individual", "id": "gRkUhg9Wb-I_24", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So that's the observed\nfactual outcome. But one could also talk\nabout the counterfactual of what would have\nhappened to this person had the opposite treatment\nbeen done for them. Notice that I just swapped each\nTi for 1 minus Ti, and so on. Now, the key challenge in the\nfield is that in your data set, you only observe the\nfactual outcomes. And when you want to reason\nabout the counterfactual, that's where you have to impute\nthis unobserved counterfactual outcome. And that is known as\nthe fundamental problem of causal inference,\nthat we only observe one of the two outcomes\nfor any individual in the data set. So let's look at a\nvery simple example. Here, individuals\nare characterized by just one feature, their age. And these two curves\nthat I'm showing you are the potential\noutcomes of what", "id": "gRkUhg9Wb-I_25", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  if you gave them\ntreatment zero, which is the blue curve, versus\ntreatment one, which is the red curve. All right. So let's dig in a\nlittle bit deeper. For the blue curve,\nwe see people who received the control, what\nI'm calling treatment zero, their blood pressure\nwas pretty low for the individuals who\nwere low and for individuals whose age is high. But for middle age individuals,\ntheir blood pressure on receiving treatment zero\nis in the higher range. On the other hand,\nfor individuals who receive treatment\none, it's the red curve. So young people have much\nhigher, let's say, blood pressure under treatment one,\nand, similarly, much older people. So then one could\nask, well, what about the difference between\nthese two potential outcomes? That is to say the CATE, the\nConditional Average Treatment", "id": "gRkUhg9Wb-I_26", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  and the red curve\nfor that individual. So for someone with\na specific age, let's say a young person\nor a very old person, there's a very big difference\nbetween giving treatment zero or giving treatment one. Whereas for a\nmiddle aged person, there's very little difference. So, for example, if treatment\none was significantly cheaper than treatment zero,\nthen you might say, we'll give treatment one. Even though it's not quite\nas good as treatment zero, but it's so much cheaper and\nthe difference between them is so small, we'll\ngive the other one. But in order to make that\ntype of policy decision, one, of course,\nhas to understand that conditional\naverage treatment effect for that individual,\nand that's something that we're going to want\nto predict using data. Now, we don't always\nget the luxury of having personalized\ntreatment recommendations. Sometimes we have\nto give a policy.", "id": "gRkUhg9Wb-I_27", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  I took this example\nout of my slides, but I'll give it to you anyway. The federal government\nmight come out with a guideline saying that\nall men over the age of 50-- I'm making up that number-- need to get annual\nprostate cancer screening. That's an example of a\nvery broad policy decision. You might ask, well, what is\nthe effect of that policy now applied over the\nfull population on, let's say, decreasing deaths\ndue to prostate cancer? And that would be\nan example of asking about the average\ntreatment effect. So if you were to\naverage the red line, if you were to\naverage the blue line, you get those two dotted\nlines I show there. And if you look at the\ndifference between them, that is the average\ntreatment effect between giving the\nred intervention or giving the blue intervention. And if the average human\neffect is very positive, you might say that, on\naverage, this intervention", "id": "gRkUhg9Wb-I_28", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  If it's very negative, you\nmight say the opposite. Now, the challenge about\ndoing causal inference from observational data\nis that, of course, we don't observe those\nred and those blue curves, rather what we observe are\ndata points that might be distributed all over the place. Like, for example,\nin this example, the blue treatment happens\nto be given in the data more to young people, and\nthe red treatment happens to be given in the\ndata more to older people. And that can happen for\na variety of reasons. It can happen due to\naccess to medication. It can happen for\nsocioeconomic reasons. It could happen because existing\ntreatment guidelines say that old people should\nreceive treatment one and young people should\nreceive treatment zero. These are all reasons why\nin your data who receives what treatment could\nbe biased in some way. And that's exactly what this\nedge from X to T is modeling.", "id": "gRkUhg9Wb-I_29", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  might want to know, well, what\nwould have happened if they had gotten the other treatment? And that's asking about\nthe counterfactual. So these dotted circles\nare the counterfactuals for each of those observations. And by the way, you'll notice\nthat those dots are not on the curves, and the reason\nthey're not on the curve is because I'm\ntrying to point out that there could be some\nstochasticity in the outcome. So the dotted lines are the\nexpected potential outcomes and the circles are the\nrealizations of them. All right. Everyone take out a calculator\nor your computer or your phone, and I'll take out mine. This is not an opportunity to go\non Facebook, just to be clear. All you want is a calculator. My phone doesn't-- oh,\nOK, it has a calculator. Good. All right.", "id": "gRkUhg9Wb-I_30", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Here's a data set on\nthe left-hand side. Each row is an individual. We're observing the\nindividual's age, gender, whether they exercise\nregularly, which I'll say is a one or a zero,\nand what treatment they got, which is A or B. On\nthe far right-hand side are their observed sugar\nglucose sugar levels, let's say, at the end of the year. Now, what we'd like to\nhave, it looks like this. So we'd like to know what would\nhave happened to this person's sugar levels had they\nreceived medication A or had they received\nmedication B. But if you look at\nthe previous slide, we observed for each individual\nthat they got either A or B. And so we're only\ngoing to know one of these columns\nfor each individual.", "id": "gRkUhg9Wb-I_31", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  received treatment\nA, and so you'll see that I've taken\nthe observed sugar level for that individual,\nand since they received treatment A, that\nobserved level represents the potential outcome Ya, or Y0. And that's why I have a 6,\nwhich is bolded under Y0. And we don't know what\nwould have happened to that individual\nhad they received treatment B. So in this\ncase, some magical creature came to me and told me\ntheir sugar levels would have been 5.5, but we\ndon't actually know that. It wasn't in the data. Let's look at the\nnext line just to make sure we get what I'm saying. So the second\nindividual actually received treatment B. They're\nobserved sugar level is 6.5. OK. Let's do a little survey. That 6.5 number, should\nit be in this column?", "id": "gRkUhg9Wb-I_32", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Or should it be in this column? Raise your hand. All right. About half of you\ngot that right. Indeed, it goes to\nthe second column. And again, what we would like\nto know is the counterfactual. What would have been\ntheir sugar levels had they received medication A? Which we don't actually\nobserve in our data, but I'm going to\nhypothesize is-- suppose that someone\ntold me it was 7, then you would see that\nvalue filled in there. That's the unobserved\ncounterfactual. All right. First of all, is\nthe setup clear? All right. Now here's when you\nuse your calculators. So we're going to\nnow demonstrate the difference between\na naive estimator of your average treatment effect\nand the true average treatment effect. So what I want you\nto do right now is to compute, first,\nwhat is the average sugar level of the individuals who\ngot medication B. So for that,", "id": "gRkUhg9Wb-I_33", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So this is conditioning\non receiving medication B. And so this is equivalent\nto going back to this one and saying, we're only going to\ntake the rows where individuals receive medication\nB, and we're going to average their\nobserved sugar levels. And everyone should do that. What's the first number? 6.5 plus-- I'm getting 7.875.", "id": "gRkUhg9Wb-I_34", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  that they received\nmedication B. Is that what other people are getting? AUDIENCE: Yeah. DAVID SONTAG: OK. What about for\nthe second number? Average sugar, given A? I want you to compute it. And I'm going to ask\neveryone to say it out loud in literally one minute. And if you get it\nwrong, of course you're going to be embarrassed. I'm going to try myself. OK. On the count of\nthree, I want everyone to read out what\nthat third number is. One, two, three.", "id": "gRkUhg9Wb-I_35", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  DAVID SONTAG: All right. Good. We can all do arithmetic. All right. Good. So, again, we're just\nlooking at the red numbers here, just the red numbers. So we just computed\nthat difference, which is point what? AUDIENCE: 0.75. DAVID SONTAG: 0.75? Yeah, that looks about right. Good. All right. So that's a positive number. Now let's do\nsomething different. Now let's compute the actual\naverage treatment effect, which is we're now going to average\nevery number in this column, and we're going to average\nevery number in this column. So this is the\naverage sugar level under the potential outcome\nof had the individual received", "id": "gRkUhg9Wb-I_36", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  under the potential outcome\nthat the individual received treatment A. All right. Who's doing it? AUDIENCE: 0.75. DAVID SONTAG: 0.75 is what? AUDIENCE: The difference. DAVID SONTAG: How do you know? AUDIENCE: [INAUDIBLE] DAVID SONTAG: Wow, you're fast. OK. Let's see if you're right. I actually don't know. OK. The first one is 0.75. Good, we got that right. I intentionally didn't post\nthe slides to today's lecture. And the second\none is minus 0.75. All right. So now let's put us in the\nshoes of a policymaker. The policymaker has to\ndecide, is it a good idea to-- or let's say it's a\nhealth insurance company. A health insurance\ncompany is trying decide, should I reimburse for\ntreatment B or not? Or should I simply\nsay, no, I'm never going to reimburse for treatment\nbecause it doesn't work well? So if they had done the\nnaive estimator, that", "id": "gRkUhg9Wb-I_37", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  then it would look\nlike medication B is-- we want lower\nnumbers here, so it would look like medication B\nis worse than medication A. And if you properly\nestimated what the actual average\ntreatment effect is, you get the absolute\nopposite conclusion. You conclude that medication B\nis much better than medication A. It's just a simple\nexample to really illustrate the difference\nbetween conditioning and actually computing\nthat counterfactual. OK. So hopefully now you're\nstarting to get it. And again, you're going to have\nmany more opportunities to work through these things in your\nhomework assignment and so on. So by now you should be\nstarting to wonder, how the hell could I do anything in\nthis state of the world? Because you don't actually\nobserve those black numbers.", "id": "gRkUhg9Wb-I_38", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And clearly there\nis bias in what the values should\nbe because of what I've been saying all along. So what can we do? Well, the first thing\nwe have to realize is that typically, this is an\nimpossible problem to solve. So your instincts\naren't wrong, and we're going to have to make\na ton of assumptions in order to do anything here. So the first assumption\nis called SUTVA. I'm not even going\nto talk about it. You can read about\nthat in your readings. I'll tell you about\nthe two assumptions that are a little bit\neasier to describe. The first critical assumption\nis that there are no unobserved confounding factors. Mathematically\nwhat that's saying is that your potential\noutcomes, Y0 and Y1, are conditionally independent\nof the treatment decision given what you observe on\nthe individual, X. Now, this could\nbe a bit hard to-- and that's called ignorability. And this can be a bit\nhard to understand, so let me draw a picture.", "id": "gRkUhg9Wb-I_39", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And now I've drawn for you\na slightly different graph. Over here I said X goes\nto T, X and T go to Y. But now I don't have Y.\nInstead, I have Y0 and Y1, and I don't have any\nedge from T to them. And that's because\nnow I'm actually using the potential\noutcomes notation. Y0 is a potential\noutcome of what would have happened to this\nindividual had they received treatment 0, and\nY1 is what would have happened to this individual\nif they received treatment one. And because you already know\nwhat treatment the individual has received, it\ndoesn't make sense to talk about an edge\nfrom T to those values. That's why there's\nno edge there. So then you might wonder,\nhow could you possibly have a violation of this\nconditional independence assumption? Well, before I give\nyou that answer, let me put some names\nto these things. So we might think about X as\nbeing the age, gender, weight, diet, and so on\nof the individual. T might be a medication, like\nan anti-hypertensive medication to try to lower a\npatient's blood pressure. And these would be\nthe potential outcomes after those two medications.", "id": "gRkUhg9Wb-I_40", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  is if there is something else,\nsome hidden variable h, which is not observed\nand which affects both the decision\nof what treatment the individual in\nyour data set receives and the potential outcomes. Now it should be\nreally clear that this would be a violation of that\nconditional independence assumption. In this graph, Y0 and\nY1 are not conditionally independent of T\ngiven X. All right. So what are these\nhidden confounders? Well, they might be things,\nfor example, which really affect treatment decisions. So maybe there's a\ntreatment guideline saying that for\ndiabetic patients, they should receive\ntreatment zero, that that's the right thing to do. And so a violation\nof this would be if the fact that the\npatient's diabetic were not recorded in the\nelectronic health record. So you don't know--", "id": "gRkUhg9Wb-I_41", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  You don't know that,\nin fact, the reason the patient received treatment\nT was because of this h factor. And there's critically\nanother assumption, which is that h actually\naffects the outcome, which is why you have these\nedges from h to the Y's. If h were something\nwhich might have affected treatment decision but not the\nactual potential outcomes-- and that can happen, of course. Things like gender can often\naffect treatment decisions, but maybe, for some diseases,\nit might not affect outcomes. In that situation it wouldn't\nbe a confounding factor because it doesn't\nviolate this assumption. And, in fact, one would\nbe able to come up with consistent estimators\nof average treatment effect under that assumption. Where things go to hell is when\nyou have both of those edges. All right. So there can't be\nany of these h's. You have to observe\nall things that affect both treatment and outcomes. The second big\nassumption-- oh, yeah. Question? AUDIENCE: In practice, how\ngood of a model is this?", "id": "gRkUhg9Wb-I_42", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  AUDIENCE: Yeah. DAVID SONTAG: For hypertension? AUDIENCE: Sure. DAVID SONTAG: I have no idea. But I think what\nyou're really trying to get at here in asking your\nquestion, how good of a model is this, is, well, oh,\nmy god, how do I know if I've observed everything? Right? All right. And that's where\nyou need to start talking to domain experts. So this is my\nstarting place where I said, no, I'm not\ngoing to attempt to fit the causal graph. I'm going to assume I\nknow the causal graph and just try to\nestimate the effects. That's where this starts to\nbecome really irrelevant. Because if you notice, this\nis another causal graph, not the one I drew on the board. And so that's something\nwhere, really, talking with domain\nexperts would be relevant. So if you say, OK, I'm going\nto be studying hypertension and this is the data I've\nobserved on patients,", "id": "gRkUhg9Wb-I_43", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  doctor who often treats\npatients with hypertension, and you say, OK, what usually\naffects your treatment decisions? And you get a set\nof variables out, and then you check\nto make sure, am I observing all of those\nvariables, at least the variables that would\nalso affect outcomes? So, often, there's\ngoing to be a back and forth in that conversation\nto make sure that you've set up your problem correctly. And again, this\nis one area where you see a critical\ndifference between the way that we do causal\ninference from the way that we do machine learning. Machine learning, if there's\nsome unobserved variables, so what? I mean, maybe your predictive\naccuracy isn't quite as good as it could have\nbeen, but whatever. Here, your conclusions\ncould be completely wrong if you don't get those\nconfounding factors right. Now, in some of the\noptional readings for Thursday's lecture-- and we'll touch on it\nvery briefly on Thursday, but there's not much\ntime in this course-- I'll talk about ways and\nyou'll read about ways", "id": "gRkUhg9Wb-I_44", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  of these assumptions. And those go by the name\nof sensitivity analysis. So, for example, the type\nof question you might ask is, how would my\nconclusions have changed if there were a\nconfounding factor which was blah strong? And that's something that one\ncould try to answer from data, but it's really starting\nto get beyond the scope of this course. So I'll give you\nsome readings on it, but I won't be able to talk\nabout it in the lecture. Now, the second major\nassumption that one needs is what's known\nas common support. And by the way, pay\nclose attention here because at the end of today's\nlecture-- and if I forget, someone must remind me-- I'm going to ask you where did\nthese two assumptions come up in the proof that I'm\nabout to give you. The first one I'm going to give\nyou will be a dead giveaway. So I'm going to answer to you\nwhere ignorability comes up, but it's up to you\nto figure out where does common support show up.", "id": "gRkUhg9Wb-I_45", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Well, what common support\nsays is that there always must be some stochasticity\nin the treatment decisions. For example, if in\nyour data patients only receive treatment A and no\npatient receives treatment B, then you would never be able to\nfigure out the counterfactual, what would have happened if\npatients receive treatment B. But what happens if it's\nnot quite that universal but maybe there is\nclasses of people? Some individual is X, let's\nsay, people with blue hair. People with blue hair always\nreceive treatment zero and they never\nsee treatment one. Well, for those people,\nif for some reason something about them\nhaving blue hair was also going to affect\nhow they would respond to the treatment,\nthen you wouldn't be able to answer anything\nabout the counterfactual for those individuals. This goes by the name of what's\ncalled a propensity score.", "id": "gRkUhg9Wb-I_46", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  for each individual. And we're going to assume that\nthis propensity score is always bounded between 0 and 1. So it's between 1 minus\nepsilon and epsilon for some small epsilon. And violations of\nthat assumption are going to completely\ninvalidate all conclusions that we could draw\nfrom the data. All right. Now, in actual clinical\npractice, you might wonder, can this ever hold? Because there are\nclinical guidelines. Well, a couple of places where\nyou'll see this are as follows. First, often, there are settings\nwhere we haven't the faintest idea how to treat patients,\nlike second line diabetes treatments. You know that the first thing\nwe start with is metformin. But if metformin doesn't help\ncontrol the patient's glucose values, there are several\nsecond line diabetic treatments.", "id": "gRkUhg9Wb-I_47", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So a clinician might start\nwith treatments from one class. And if that's not working,\nyou try a different class, and so on. And it's a bit random\nwhich class you start with for any one patient. In other settings, there might\nbe good clinical guidelines, but there is randomness\nin other ways. For example, clinicians who\nare trained on the west coast might be trained that this is\nthe right way to do things, and clinicians who are\ntrained in the east coast might be trained that this is\nthe right way to do things. And so even if any one\nclinician's treatment decisions are deterministic\nin some way, you'll see some stochasticity\nnow across clinicians. It's a bit subtle how to\nuse that in your analysis, but trust me, it can be done. So if you want to\ndo causal inference from observational\ndata, you're going to have to first start to\nformalize things mathematically in terms of what is your X, what\nis your T, what is your Y. You", "id": "gRkUhg9Wb-I_48", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  satisfy these assumptions\nof ignorability and overlap? Some of these things you\ncan check in your data. Ignorability you can't\nexplicitly check in your data. But overlap, this thing,\nyou can test in your data. By the way, how? Any idea? Someone else who\nhasn't spoken today. So just think back to\nthe previous example. You have this table of these X's\nand treatment A or B and then sugar values. How would you test this? AUDIENCE: You could use\na frequentist approach and just count how\nmany things show up. And if there is zero, then you\ncould say that it's violated. DAVID SONTAG: Good. So you have this table. I'll just go back to that table. We have this table,\nand these are your X's.", "id": "gRkUhg9Wb-I_49", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  where it's a bit easier to see. Here, we're going to ignore\nthe outcome, the sugar levels because,\nremember, this only has to do with\nprobability of treatment given your covariance. The Y doesn't show\nup here at all. So this thing on\nthe right-hand side, the observed sugar levels, is\nirrelevant for this question. All we care about is\nwhat goes on over here. So we look at this. These are your X's, and\nthis is your treatment. And you can look to\nsee, OK, here you have one 75-year-old\nmale who does exercise frequently and received\ntreatment A. Is there any one else in the data set who\nis 75 years old and male, does exercise regularly\nbut received treatment B? Yes or no? No. Good. OK. So overlap is not satisfied\nhere, at least not empirically. Now, you might argue that I'm\nbeing a bit too coarse here.", "id": "gRkUhg9Wb-I_50", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  and received treatment B? Maybe that's close enough. So there starts to\nbecome subtleties in assessing these things\nwhen you have finite data. But it is something at\nthe fundamental level that you could start\nto assess using data. As opposed to ignorability,\nwhich you cannot test using data. All right. So you have to think about, are\nthese assumptions satisfied? And only once you start to think\nthrough those questions can you start to do your analysis. And so that now brings me to\nthe next part of this lecture, which is how do we actually--\nlet's just now believe David, believe that these\nassumptions hold. How do we do that\ncausal inference? Yeah? AUDIENCE: I just had a\nquestion on [INAUDIBLE].. If you know that some patients,\nfor instance, healthy patients, are not tracking to\nget any treatment, should we just remove\nthem, basically? DAVID SONTAG: So\nthe question is,", "id": "gRkUhg9Wb-I_51", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  For example, you know that\nhealthy individuals never receive any treatment. Should you remove them\nfrom your data set? Well, first of all, that has\nto do with how do you formalize the question because not\nreceiving a treatment is a treatment. So that might be your control\narm, just to be clear. Now, if you're asking about\nthe difference between two treatments-- two different\nclasses of treatment for a condition, then often one\ndefines the relevant inclusion criteria in order to have\nthese conditions hold. For example, we could try to\nredefine the set of individuals that we're asking about\nso that overlap does hold. But then in that\nsituation, you have to just make sure that your\npolicy is also modified. You say, OK, I conclude that\nthe average treatment effect is blah for this type of people. OK? OK.", "id": "gRkUhg9Wb-I_52", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  from data? Remember, average treatment\neffect, mathematically, is the expectation between\npotential outcome Y1 minus Y0. The key tool which we'll use\nin order to estimate that is what's known as the\nadjustment formula. This goes by many names in\nthe statistics community, such as the G-formula as well. Here, I'll give you\na derivation of it. We're first going to recognize\nthat this expectation is actually two\nexpectations in one. It's the expectation\nover individuals X and it's the expectation over\npotential outcomes Y given X. So I'm first just\ngoing to write it out in terms of those\ntwo expectations, and I'll write the expectations\nrelated to X on the outside. That goes by name of law\nof total expectation. This is trivial at this stage. And by the way, I'm just\nwriting out expectation of Y1.", "id": "gRkUhg9Wb-I_53", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  but it's going to be\nexactly analogous. Now, the next step is\nwhere we use ignorability. I told you I was going\nto give that one away. So remember, we said\nthat we're assuming that Y1 is conditionally\nindependent of the treatment T given X. What that\nmeans is probability of Y1 given X is equal to\nprobability of Y1 given X comma T equals\nwhatever-- in this case I'll just say T equals 1. This is implied by Y1 being\nconditionally independent of T given X. So I can just stick n\ncomma T equals 1 here, and that's explicitly because\nof ignorability holding.", "id": "gRkUhg9Wb-I_54", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  and here I've just done\nsome short notation. I'm just going to\nhide this expectation. And by the way, you could\ndo the same for Y0-- Y1, Y0. And now notice\nthat we can replace this average human effect\nwith now this expectation with respect to\nall individuals X of the expectation of Y1 given\nX comma T equals 1, and so on. And these are mostly\nquantities that we can now observe from our data. So, for example, we can\nlook at the individuals who received treatment one,\nand for those individuals we have realizations of Y1. We can look at individuals\nwho receive treatment zero, and for those individuals\nwe have realizations of Y0.", "id": "gRkUhg9Wb-I_55", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  to get estimates of the\ncorresponding expectations. So these we can easily\nestimate from our data. And so we've made progress. We can now estimate some\npart of this from our data. But notice, there\nare some things that we can't yet directly\nestimate from our data. In particular, we can't\nestimate expectation of Y0 given X comma T equals 1\nbecause we have no idea what would have happened to this\nindividual who actually got treatment one if they\nhad gotten treatment zero. So these we don't know. So these we don't know. Now, what is the trick\nI'm planning on you? How does it help\nthat we can do this? Well, the key point is\nthat these quantities that we can estimate from\ndata show up in that term. In particular, if you\nlook at the individuals X that you've sampled from the\nfull set of individuals P of X,", "id": "gRkUhg9Wb-I_56", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  we observed T equals 1, then we\ncan estimate expectation of Y1 given X comma T equals\n1, and similarly for Y0. But what we need to be able\nto do is to extrapolate. Because empirically, we only\nhave samples from P of X given T equals 1, P\nof X given T equals 0 for those two potential\noutcomes correspondingly. But we are going to also\nget samples of X such that for those individuals\nin your data set, you might have only\nobserved T equals 0. And to compute this formula,\nyou have to answer, for that X, what would it have been if\nthey got treatment equals one? So there are going to\nbe a set of individuals that we have to\nextrapolate for in order to use this adjustment\nformula for estimate. Yep? AUDIENCE: I thought because\ncommon support is true, we have some patients that\nreceived each treatment or a given type of X.", "id": "gRkUhg9Wb-I_57", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  But now-- so, yes, that's true. But that's a statement\nabout infinite data. And in reality, one\nonly has finite data. And so although common support\nhas to hold to some extent, you can't just build on\nthat to say that you always observe the counterfactual\nfor every individual, such as the pictures\nI showed you earlier. So I'm going to leave this slide\nup for just one more second to let it sink in and\nsee what it's saying. We started out from the goal of\ncomputing the average treatment effect, expected\nvalue of Y1 minus Y0. Using the adjustment\nformula, we've gotten to now an equivalent\nrepresentation, which is now an expectation with\nrespect to all individuals sampling from P of X\nof expected value of Y1", "id": "gRkUhg9Wb-I_58", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  given X comma T equals 0. For some of the individuals,\nyou can observe this, and for some of them,\nyou have to extrapolate. So from here, there are\nmany ways that one can go. Hold your question\nfor a little while. So types of causal\ninference methods that you will have\nheard of include things like\ncovariance adjustment, propensity score re-weighting,\ndoubly robust estimators, matching, and so on. And those are the tools of\nthe causal inference trade. And in this course,\nwe're only going to talk about the first two. And in today's\nlecture, we're only going to talk about the first\none, covariate adjustment. And on Thursday, we'll\ntalk about the second one. So covariate adjustment\nis a very natural way to try to do that extrapolation. It also goes by the name, by\nthe way, of response surface modeling. What we're going to\ndo is we're going to learn a function f, which\ntakes as an input X and T,", "id": "gRkUhg9Wb-I_59", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  should think about f as\nthis conditional probability distribution. It's predicting Y\ngiven X and T. So T is going to be an input\nto the machine learning algorithm, which is going\nto predict what would be the potential outcome Y for this\nindividual described by feature as X1 through Xd\nunder intervention T. So this is just from\nthe previous slide. And what we're going\nto do now are-- this is now where we get the\nreduction to machine learning-- is we're going to use empirical\nrisk minimization, or maybe some regularized empirical risk\nminimization, to fit a function f which approximates the\nexpected value of YT given", "id": "gRkUhg9Wb-I_60", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Got my X. And then once\nyou have that function, we're going to be able\nto use that to estimate the average treatment effect\nby just implementing now this formula here. So we're going to first take\nan expectation with respect to the individuals\nin the data set. So we're going to\napproximate that with an empirical expectation\nwhere we sum over the little n individuals in your data set. Then what we're\ngoing to do is we're going to estimate the first\nterm, which is f of Xi comma 1 because that is approximating\nthe expected value of Y1 given T comma X-- T equals 1 comma\nX. And we're going to approximate the second\nterm, which is just plugging now 0 for T instead of 1. And we're going to take the\ndifference between them, and that will be our estimator\nof the average treatment effect.", "id": "gRkUhg9Wb-I_61", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  One thing you might wonder\nis, in your data set, you actually did observe\nsomething for that individual, right. Notice how your raw data\ndoesn't show up in this at all. Because I've done\nmachine learning, and then I've thrown\naway the observed Y's, and I used this estimator. So what you could have done--\nan alternative formula, which, by the way, is also a\nconsistent estimator, would have been to\nuse the observed Y for whatever the factual\nis and the imputed Y for the counterfactual using f. That would have been\nthat would have also been a consistent estimator for\nthe average treatment effect. You could've done either. OK. Now, sometimes you're\nnot interested in just the average treatment\neffect, but you're actually", "id": "gRkUhg9Wb-I_62", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  in the population. Well, this also now\ngives you an opportunity to try to explore\nthat heterogeneity. So for each\nindividual Xi, you can look at just the\ndifference between what f predicts for\ntreatment one and what X predicts given treatment zero. And the difference\nbetween those is your estimate of your\nconditional average treatment effect. So, for example, if\nyou want to figure out for this individual, what\nis the optimal policy, you might look to see is\nCATE positive or negative, or is it greater than some\nthreshold, for example? So let's look at some pictures. Now what we're using is we're\nusing that function f in order to impute those counterfactuals. And now we have those\nobserved, and we can actually compute the CATE. And averaging over those,\nyou can estimate now the average treatment effect. Yep? AUDIENCE: How is f non-biased? DAVID SONTAG: Good. So where can this go wrong? So what do you mean\nby biased, first? I'll ask that. AUDIENCE: For\ninstance, as we've seen", "id": "gRkUhg9Wb-I_63", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  [INAUDIBLE] DAVID SONTAG: Oh, thank you so\nmuch for bringing that back up. So you're referring\nto one of the readings for the course\nfrom several weeks ago, where we talked about using\njust a pure machine learning algorithm to try to predict\noutcomes in a hospital setting. In particular, what\nhappens for patients who have pneumonia in\nthe emergency department? And if you all remember,\nthere was this asthma example, where patients with\nasthma were predicted to have better outcomes than\npatients without asthma. And you're calling that bias. But you remember, when\nI taught about this, I called it biased due\nto a particular thing. What's the language I used? I said bias due to\nintervention, maybe, is what I-- I can't remember\nexactly what I said.", "id": "gRkUhg9Wb-I_64", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  I don't know. Make it up. Now a textbook will be written\nwith bias by intervention. OK. So the problem\nthere is that they didn't formulize the\nprediction problem correctly. The question that\nthey should have asked is, for asthma patients-- what you really want to ask is a\nquestion of X and then T and Y, where T are the interventions\nthat are done for asthmatics. So the failure of that\npaper is that it ignored the causal inference question\nwhich was hidden in the data, and it just went to predict\nY given X marginalizing over T altogether. So T was never in\nthe predictive model. And said differently, they never\nasked counterfactual questions", "id": "gRkUhg9Wb-I_65", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And then they still used it\nto try to guide some treatment decisions. Like, for example, should\nyou send this person home, or should you keep them for\ncareful monitoring or so on? So this is exactly\nthe same example as I gave in the\nbeginning of the lecture, where I said if you just\nuse a risk stratification model to make some decisions,\nyou run the risk that you're making the wrong decisions\nbecause those predictions were biased by decisions\nin your data. So that doesn't happen here\nbecause we're explicitly accounting for T in\nall of our analysis. Yep? AUDIENCE: In the data sets\nthat we've used, like MIMIC, how much treatment\ninformation exists? DAVID SONTAG: So how much\ntreatment information is in MIMIC? A ton. In fact, one of the\nreadings for next week is going to be about trying to\nunderstand how one could manage sepsis, which is a condition\ncaused by infection, which is managed by, for example,\ngiving broad spectrum", "id": "gRkUhg9Wb-I_66", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  pressers and ventilators. And all of those\nare interventions, and all those interventions\nare recorded in the data so that one could then ask\ncounterfactual questions from the data, like\nwhat would have happened if this patient\nhad they received a different set\nof interventions? Would we have prolonged\ntheir life, for example? And so in an intensive care unit\nsetting, most of the questions that we want to ask about,\nnot all, but many of them are about dynamic treatments\nbecause it's not just a single treatment\nbut really about a service sequence of\ntreatments responding to the current\npatient condition. And so that's where we'll really\nstart to get into that material next week, not in\ntoday's lecture. Yep? AUDIENCE: How do you make sure\nthat your f function really learned from the relationship\nbetween T and the outcome? DAVID SONTAG: That's\na phenomenal question. Where were you\nthis whole course? Thank you for asking it. So I'll repeat it. How do you know that\nyour function f actually learned something about the\nrelationship between the input X and the treatment\nT and the outcome?", "id": "gRkUhg9Wb-I_67", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  is my reduction actually valid? So I've taken this\nproblem and I've reduced it to this machine\nlearning problem, where I take my data, and\nliterally I just learn a function f to\ntry to predict well the observations in the data. And how do we know that\nthat function f actually does a good job at\nestimating something like average treatment effect? In fact, it might not. And this is where\nthings start to get really tricky, particularly\nwith high dimensional data. Because it could happen, for\nexample, that your treatment decision is only one of a huge\nnumber of factors that affect the outcome Y. And it\ncould be that a much more important factor is hidden in\nX. And because you don't have", "id": "gRkUhg9Wb-I_68", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  algorithm, let's say, with L1\nor L2 regularization or maybe early stopping if you're\nusing deep neural network, your algorithm might never learn\nthe actual dependence on T. It might learn just to\nthrow away T and just use X to predict Y.\nAnd if that's the case, you will never be able to\ninfer these average treatment effects accurately. You'll have huge errors. And that gets back\nto one of the slides that I skipped, where I\nstarted out from this picture. This is the machine learning\npicture saying, OK, a reduction to machine learning is-- now you add an\nadditional feature, which is your\ntreatment decision, and you learn that\nblack box function f. But this is where machine\nlearning causal inference starts to differ because\nwe don't actually care about the quality\nof predicting Y. We can measure your\nroot mean squared error in predicting Y given your\nX's and T's, and that error", "id": "gRkUhg9Wb-I_69", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  But you can run into\nthese failure modes where it just completely\nignores T, for example. So T is special here. So really, the picture\nwe want to have in mind is that T is some\nparameter of interest. We want to learn a model f\nsuch that if we twiddle T, we can see how there is a\ndifferential effect on Y based on twiddling T.\nThat's what we truly care about when we're\nusing machine learning for causal inference. And so that's really\nthe gap, that's the gap in our\nunderstanding today. And it's really an\nactive area of research to figure out how do you change\nthe whole machine learning paradigm to recognize that when\nyou're using machine learning for causal inference,\nyou're actually interested in something\na little bit different. And by the way, that's a major\narea of my lab's research, and we just published\na series of papers trying to answer that question. Beyond the scope of\nthis course, but I'm happy to send you those\npapers if anyone's interested. So that type of question\nis extremely important.", "id": "gRkUhg9Wb-I_70", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  dimensional and where\nthings like regularization don't become important. But once your X becomes\nhigh dimensional and once you want to start to\nconsider more and more complex f's during your\nfitting, like you want to use deep neural\nnetworks, for example, these differences in goals\nbecome extremely important. So there are other ways\nin which things can fail. So I want to give you\nhere an example where-- shoot, I'm answering\nmy question. OK. No one saw that slide. Question-- where did\nthe overlap assumptions show up in our approach for\nestimating average treatment effect using\ncovariate adjustment?", "id": "gRkUhg9Wb-I_71", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Someone who hasn't\nspoken today, hopefully. You can be wrong, it's fine. Yeah, in the back? AUDIENCE: Is it the\nversion with the same age in receiving treatment\nB and treatment B? DAVID SONTAG: So maybe you have\nan individual with some age-- we're going to want\nto be able to look at the difference between what\nf predicts for that individual if they got treatment\nA versus treatment B, or one versus zero. And let me try to lead\nthis a little bit. And it might happen\nin your data set that for individuals\nlike them, you only ever", "id": "gRkUhg9Wb-I_72", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  like them who you\nobserve treatment zero. So what's this function\ngoing to output then when you input zero for\nthat second argument? Everyone say out loud. Garbage? Right? If in your data set you never\nobserved anyone even remotely similar to Xi who\nreceived treatment zero, then this function is basically\nundefined for that individual. I mean, yeah, your function\nwill output something because you fit it, but it's not\ngoing to be the right answer. And so that's where this\nassumption starts to show up. When one talks about the\nsample complexity of learning these functions f to do\ncovariate adjustment, and when one talks\nabout the consistency of these arguments--\nfor example, you'd like to be\nable to make claims that as the amount of\ndata grows to, let's", "id": "gRkUhg9Wb-I_73", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  the right estimate. So that's the type\nof proof which is often given in the\ncausal inference literature. Well, if you have overlap,\nthen as the amount of data goes to infinity, you\nwill observe someone, like the person who\nreceived treatment one, you'll observe someone who\nalso received treatment zero. It might have taken you a huge\namount of data to get there because treatment zero\nmight have been much less likely than treatment one. But because the probability\nof treatment zero is not zero, eventually you'll see\nsomeone like that. And so eventually\nyou'll get enough data in order to learn a function\nwhich can extrapolate correctly for that individual. And so that's where\noverlap comes in in giving that type of\nconsistency argument. Of course, in reality, you\nnever have infinite data. And so these questions\nabout trade-offs between the amount\nof data you have and the fact that\nyou never truly have empirical overlap with\na small amount of data,", "id": "gRkUhg9Wb-I_74", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  despite that is the\ncritical question that one needs to answer,\nbut is, by the way, not studied very well\nin the literature because people don't usually\nthink in terms of sample complexity in that field. That's where computer\nscientists can start really to contribute to this\nliterature and bringing things that we often think\nabout in machine learning to this new topic. So I've got a couple\nof minutes left. Are there any other\nquestions, or should I introduce some new\nmaterial in one minute? Yeah? AUDIENCE: So you said that\nthe average treatment effect estimator here is consistent. But does it matter if\nwe choose the wrong-- do we have to choose some\nfunctional form of the features to the effect? DAVID SONTAG: Great question. AUDIENCE: Is it consistent even\nif we choose a completely wrong function or formula? DAVID SONTAG: No. AUDIENCE: That's\na different thing? DAVID SONTAG: No, no. You're asking all\nthe right questions. Good job today, everyone. So, no.", "id": "gRkUhg9Wb-I_75", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  I assume two things. First, that you observe\nenough data such that you can have any chance\nof extrapolating correctly. But then implicit\nin that statement is that you're\nchoosing a function family which is\npowerful enough that it can extrapolate correctly. So if your true function is-- if you think back to this\nfigure I showed you here, if the true potential\noutcome functions are these quadratic functions\nand you're fitting them with a linear function,\nthen no matter how much data you\nhave you're always going to get wrong estimates\nbecause this argument really requires that you're considering\nmore and more complex non-linearity as your\namount of data grows. So now here's a visual\ndepiction of what can go wrong if you don't have overlap. So now I've taken out-- previously, I had one or two\nred points over here and one or two blue points over here,\nbut I've taken those out.", "id": "gRkUhg9Wb-I_76", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  and those red points. So all you have are\nthe points, and now one can learn as good functions,\nas you can imagine, to try to, let's say, minimize the mean\nsquared error of predicting these blue points and minimize\nthe mean squared error of predicting those red points. And what you might get\nout is something-- maybe you'll decide on\na linear function. That's as good as you\ncould do if all you have are those red points. And so even if you were\nwilling to consider more and more complex\nhypothesis classes, here, if you tried to consider\na more complex hypothesis class than this line, you'd\nprobably just over-fitting to the data you have. And so you decide\non that line, which, because you had\nno data over here, you don't even know that it's\nnot a good fit to the data. And then you notice\nthat you're getting completely wrong estimates. For example, if you asked about\nthe CATE for a young person, it would have the wrong sign\nover here because they flipped,", "id": "gRkUhg9Wb-I_77", "title": "14. Causal Inference, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So that's an example of how\none can start to get errors. And when we begin on\nThursday's lecture, we're going to pick up right\nwhere we left off today, and I'll talk about this issue\na little bit more in detail. I'll talk about how, if one were\nto learn a linear function, how one could actually,\nunder the assumption that the true potential\noutcomes are linear, how one could actually\ninterpret the coefficients of that linear function\nin a causal way under the very strong\nassumption that the two potential outcomes are linear.", "id": "gRkUhg9Wb-I_78"}, {"text": "  [SQUEAKING] [RUSTLING] [CLICKING] DAVID SONTAG: So\ntoday's lecture is going to continue on\nthe lecture that you saw on Tuesday, which\nwas introducing you to causal inference. So the causal inference\nsetting, which we're studying in this course,\nis a really simplistic one from a causal\ngraphs perspective. There are three sets of\nvariables of interest-- everything you know about an\nindividual or patient, which we're calling x over here;\nand intervention or action-- which for today's\nlecture, we're going to suppose that it's either 0\nor 1, so a binary intervention. You either take it or don't-- and an outcome y.", "id": "g5v-NvNoJQQ_0", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  the impact of the intervention\non the outcome challenging is that we have to\nmake that inference from observational data, where\nwe don't have the ability-- at least not in\nmedicine, we typically don't have the ability to\nmake active interventions. And the goal of what we will\nbe discussing in this course is about how to take\ndata that was collected from a practice of medicine\nwhere actions or interventions were taken, and then use\nthat to infer something about the causal effect. And obviously, there are also\nrandomized control trials where one intentionally\ndoes randomize, but the focus of\ntoday's lecture is going to be using observational\ndata, or ready collected data, to try to make\nthese conclusions. So we introduced the language of\npotential outcomes on Tuesday. Potential outcomes is the\nmathematical framework for trying to answer\nthese questions. Then with that definition\nof potential outcomes, we can define the conditional\naverage treatment effect,", "id": "g5v-NvNoJQQ_1", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  for the individual Xi. So you'll notice here\nthat I have patients, so treating the\npotential outcome as a random variable\nin case there might be some stochasticity. So sometimes, maybe if you were\nto give someone a treatment, it works, and\nsometimes it doesn't. So that's what the\nexpectation is accounting for. Any questions before I move on? So with respect\nto this definition of conditional average\ntreatment effect, then you could ask, well, what\nwould happen in aggregate for the population? And you can compute\nthat by taking the average of the conditional\naverage treatment effect over all of the individuals. So that's just this expectation\nwith respect to, now, p of x. Now, critically, this\ndistribution, p of x, you should think about as the\ndistribution of everyone", "id": "g5v-NvNoJQQ_2", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  So some of those individuals\nmight have received treatment 1 in the past. Some of them might have\nreceived treatment 0. But when we ask this question\nabout average treatment effect, we're asking, for both of\nthose populations, what would have been the effect-- what would have been the\ndifference about [INAUDIBLE] they received treatment 1 minus\nhad they received treatment 0? Now, I wanted to\ntake this opportunity to start thinking a little\nbit bigger picture about how causal inference can be\nimportant in a variety of societal\nquestions, and so I'd like to now spend just a\ncouple of minutes thinking with you about what some\ncausal questions might be that we urgently need to\nanswer about the COVID-19 pandemic. And as you try to think\nthrough these questions, I want you to have this\ncausal graph in mind. So there is the\ngeneral population.", "id": "g5v-NvNoJQQ_3", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  and the whole notion\nof causal inferences assessing the effective action\non some outcome of interest. So in trying to give\nthe answer to my-- various answers to\nmy questions of what are some causal inference\nquestions of relevance to the current\npandemic, I want you to try to frame your answers in\nterms of these Xs, Ts, and Ys. It's also, obviously,\nvery hard to answer using the types of techniques\nthat we will be discussing in this course, and partly\nbecause the techniques that I'm focusing on are very much\ndata driven techniques. That said, the general framework\nthat I've introduced on Tuesday for covariate adjustment\nof, come up with a model and use that model\nto make a prediction, and the assumptions that\nunderlie that in terms of, well, where's that model\ncoming from, if you're fitting the parameters from data, having\nto have common support in order", "id": "g5v-NvNoJQQ_4", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  Those underlying assumptions\nand the general premises will still hold,\nbut here, obviously, when it comes to something\nlike social distancing, they're complicated\nnetwork effects. And so whereas up\nuntil now, we've been making the assumption\nof what was called SUTVA-- it was a assumption that I\nprobably didn't even talk about in Tuesday's lecture. But intuitively, what\nthe SUTVA assumption says is that each of your\ntraining examples are independent of each other. And that might make sense\nwhen you think about, give a patient a\nmedication or not, but it certainly\ndoesn't make sense when you think about social\ndistancing type measures, where if some people\nsocial distance, but other people don't,\nit has obviously a very different impact on society. So one needs a different\nclass of models to try to think about\nthat, which have", "id": "g5v-NvNoJQQ_5", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  So those were all really\ngood answers to my question, and in some sense, now-- so there's the\nepidemiological type questions that we last spoke about. But the first few set\nof questions about, really, how does one treat\npatients who have COVID are the types of questions\nthat only now we can really start to answer\nnow, unfortunately, because we're starting to get\na lot of data in the United States and internationally. And so for example, my own\npersonal research group, we're starting to\nreally scale up our research on these\ntypes of questions. Now, one very simplified\nexample that I wanted to give of how a causal\ninference lens can be useful here is by trying to\nunderstand case fatality rates. So for example, in\nItaly, it was reported that 4.3% of individuals\nwho had this condition passed away,\nwhereas in China, it", "id": "g5v-NvNoJQQ_6", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  had this condition passed away. Now, you might ask, based\non just those two numbers, is something\ndifferent about China? For example, might\nit be that the way that COVID is being managed in\nChina is better than in Italy? You might also wonder if\nthe strain of the disease might be different\nbetween China and Italy? So perhaps there were some\nmutations since it left Wuhan. But if you dig a\nlittle bit deeper, you see that, if you plot case\nfatality rates by age group, you get this plot that\nI'm showing over here. And you see that if\nyou compare Italy, which is the orange, to\nChina, which is blue, now stratified by age range, you\nsee that for every single age range, the percentage\nof deaths is lower", "id": "g5v-NvNoJQQ_7", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  seem to be a contradiction\nwith what we saw-- with the aggregate\nnumbers, where we see that the case\nfatality rate in Italy is higher than in China. And so the reason why this can\nhappen has to do with the fact that the populations\nare very different. And by the way, this\nparadox goes by the name of Simpson's paradox. So if you dig a bit\ndeeper, you see then that, if you're\nto look at, well, what is the distribution of\nindividuals in China and Italy that have been\nreported to have COVID, you see that, in Italy,\nit's much more highly weighted towards\nthese older ages. And if you then combine that\nwith the total number of cases,", "id": "g5v-NvNoJQQ_8", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  so it now fully explains\nthese two numbers and the plot that you see. Now if we're to try to think\nabout this a bit more formally, we would try to\nformalize it in terms of following causal graph. And so here, we have the\nsame notions of X, T, and Y, where X is the age\nof an individual who has been diagnosed with COVID. T is now country, so we're going\nto think about the intervention here as transporting\nourselves from China to Italy, so thinking about changing\nthe environment altogether. And Y is the outcome on\nan individual level basis. And so the formal\nquestion that one might want to ask is about\na causal impact of changing the country on the outcome Y. Now, for this particular\ncausal question, this causal graph that I'm\ndrawing here is the wrong one, and in fact, the right\ncausal graph probably has an edge that\ngoes from T to X.", "id": "g5v-NvNoJQQ_9", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  is obviously a function\nof the country, not the other way around. But despite the\nfact that there is that difference\nin directionality, all of the techniques that\nwe've been teaching you in this course are still\napplicable for trying to ask a causal question about\nthe impact of intervening on a country, and that's\nreally because, in some sense, these two distributions,\nat an observational level, are equivalent. And if you want to dig a little\nbit deeper into this example-- and I want to stress this is\njust for educational purposes. Don't read anything\ninto these numbers-- I would go to this Colab\nnotebook after the course. So all of this was\njust a little bit of set up to help frame where\ncausal inference shows up", "id": "g5v-NvNoJQQ_10", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  and really very worried and\nstressed about ourselves personally recently. And I want to now shift\ngears to starting to get back to the course material,\nand in particular, I want to start today's\nmore theoretical parts of the lectures by returning\nto covariate adjustment, which we ended on and Tuesday. In covariate adjustment, one-- we'll use a machine learning\napproach to learn some model, which I'll call F. So you could\nimagine a black box machine learning algorithm, which\ntakes as input both X and T. So X are your covariates of\nthe individual that are going to receive the treatment, and\nT is that treatment decision, which for today's lecture, you\ncan just assume is binary 01, and uses those together now\nto predict the outcome Y. Now, what we showed on Tuesday\nwas that, under ignorability, where ignorability,\nremember, was the assumption of no\nhitting confounding,", "id": "g5v-NvNoJQQ_11", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  could be defined as\njust a difference-- could be could be computed\nas the expectation of Y1 now conditioned on\nT equals 1, so this is the piece that\nI've added in here, and minus the expectation of Y0\nnow conditioned on T equal 0. And it's that conditioning\nwhich is really important, because that's what enables you\nto estimate Y1 from data where treatment 1 was observed,\nwhereas you never get to observe Y1 in data when\ntreatment 0 was performed. So we have this formula, and\nafter fitting that model F, one could then use it\nto try to estimate CATE by just taking that\nlearned function, plugging in the number 1\nfor the treatment variable in order to get your\nestimate of this expectation,", "id": "g5v-NvNoJQQ_12", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  when you want to get your\nestimate of this expectation. Taking the difference\nbetween those then gives you your estimate\nof the conditional average treatment effect. So that's the approach, and what\nwe didn't talk about so much was the modeling choices of what\nshould your function class be. So this is going to turn\nout to be really important, and really, the punchline\nof the next several slides is going to be a major\ndifference in philosophy between machine\nlearning and statistics, and between prediction\nand causal inference. So let's now consider the\nfollowing simple model, where I'm going to assume that the\nground truth in the real world has that the potential outcome\nYT of X, where T, again is the treatment, is equal\nto some simple linear model", "id": "g5v-NvNoJQQ_13", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  T, the treatment T. So in\nthis very simple setting, I'm going to assume that we\njust have a single feature or covariate for the\nindividual, which is there age. I'm going to assume\nthat this model doesn't have any terms with an\ninteraction between X and T, so it's fully linear in X and T. So this is an assumption about\nthe true potential outcomes, and what we'll do over\nthe next couple of slides is think about what would happen\nif you now modeled Y of T, so modeling it with\nsome function F, where F was, let's say, a linear\nfunction versus a nonlinear function, if F took this\nform or a different form. And by the way,\nI'm going to assume that the noise here,\nepsilon t, can be arbitrary,", "id": "g5v-NvNoJQQ_14", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  So let's get started by\ntrying to estimate what the true CATE is, or\nConditional Average Treatment Effect, for this\npotential outcome model. Well, just by\ndefinition, the CATE is the expectation\nof Y1 minus Y0. We're going to\ntake this formula, and we're going to plug it\nin for the first term using T equals 1, and that's why\nyou get this term over here with gamma. And the gamma is because,\nagain, T is equal to 1. We're also going to\ntake this, and we're going to plug it in for,\nnow, this term over here, where T is equal to 0. And when T is equal to 0, then\nthe gamma term just disappears, and so you just get\nbeta X plus epsilon 0. So all I've done so far\nis plug in the Y1 and Y0 according to the assumed form,\nbut notice now that there's", "id": "g5v-NvNoJQQ_15", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  the beta X term over\nhere cancels out with a beta X term over here. And because epsilon 1 has a\n0 mean, and epsilon 0 also has a 0 mean. The only thing left\nis that gamma term, and expectation of a constant's\nobviously that constant. And so what we\nconclude from this is that the CATE value is gamma. Now, the average\ntreatment effect, which is the average of\nCATE over all individuals X, will then also be\ngamma, obviously. So we've done something\npretty interesting here. We've started from\nthe assumption that the true potential\noutcome model is linear, and what we concluded is that\nthe average treatment effect is precisely the coefficient\nof the treatment variable in this linear model.", "id": "g5v-NvNoJQQ_16", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  interested in is\ncausal inference, and suppose that we\nwere lucky enough to know that the true\nmodel were linear, and so we attempted to fit some\nfunction F, which had precisely the same form, we get some\nbeta hats and some gamma hats out from the learning\nalgorithm, all we need to do is look at that\ngamma hat in order to conclude something about\nthe average treatment effect. No need to do this\ncomplicated thing of plugging in to estimate CATEs. And again, the reason it's\nsuch a trivial conclusion is because of our\nassumption of linearity. Now, what that also\nmeans is that, if you have errors in\nlearning-- in particular, suppose, for example, that\nyou are estimating your gamma hat wrongly, then\nthat means you're also going to be getting\nwrong your estimates of your conditional and\naverage treatment effects.", "id": "g5v-NvNoJQQ_17", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  to see, that says, what\ndoes gamma represent in terms of the medication? Thank you for that question. So gamma is--\nliterally speaking, gamma tells you the conditional\naverage treatment effect, meaning if you were to give\nthe treatment versus not giving the treatment, how\nthat affects the outcome. Think about the outcome\nof interest being the patient's blood\npressure, there being potential confounding\nfactor of the patient's age, and T being one of two different\nblood pressure measurements. If gamma is positive,\nthen it means that treatment 1 is more-- treatment 1 increases the\npatient's blood pressure relative to treatment 0. And if gamma is\nnegative, it means that treatment 1 decreases\nthe patient's blood pressure relative to treatment 0.", "id": "g5v-NvNoJQQ_18", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  oh, sorry, there's another chat. Thank you, good. So in machine learning, I\ntypically tell my students, don't attempt to interpret\nyour coefficient. At least, don't\ninterpret them too much. Don't put too much\nweight into them, and that's because,\nwhen you're learning very high dimensional\nmodels, there can be a lot of redundancy\nbetween your features. But when you talk\nto statisticians, often they pay really\nclose attention to their coefficients,\nand they try to interpret those coefficients\noften with the causal lens. And when I first got\nstarted in this field, I couldn't understand why\nare they paying attention to those coefficients so much? Why are they coming up with\nthese causal hypotheses based on which coefficients\nare positive and which are the negative? And this is the answer. It really comes down\nto an interpretation of the prediction\nproblem in terms of the feature of\nrelevance being a treatment, that treatment\nbeing linear with respect", "id": "g5v-NvNoJQQ_19", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  looking at the coefficient\nof the treatment as telling you something\nabout the average treatment effect of that\nintervention or treatment. Moreover, that also tells us\nwhy it's often very important to look at confidence intervals,\nso one might want to know, we have some small data set, we\nget some estimate of gamma hat, but what if you had\na different data set? So what happens if you had a\nnew sample of 100 data points? How would your estimated\ngamma hat vary? And so you might be\ninterested, for example, in confidence intervals, like\na 95% confident interval that says that gamma hat is\nbetween, let's say, 1 and, let's say maybe, 0.5\nwith probability 0.95. That'll be an example\nof a confidence", "id": "g5v-NvNoJQQ_20", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  And such a confidence\ninterval then gives you confidence--\na confidence interval around the coefficients,\nthen gives you confidence intervals around\nthe average treatment effect via this analysis. So the second\nobservation is what happens if the true\nmodel isn't linear, but we hadn't realized\nthat as a modeler, and we had just assumed that,\nwell, the linear model's probably good enough? And maybe even, the linear model\ngets pretty good prediction performance? Well, let's look at the\nextreme example of this. Let's now assume that the\ntrue data generating process, instead of being just\nbeta X plus gamma T, we're going to add in now a new\nterm, delta times X squared. Now, this is the\nmost naive extension of the original\nlinear model that you could imagine, because I'm not\neven adding any interaction terms like 10 times XT.", "id": "g5v-NvNoJQQ_21", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  treatment and covariate. Treatment is still--\nthe potential outcome is still linear in treatment. We're just adding a\nsingle nonlinear term involving one of the features. Now, if you compute\nthe average treatment effect via the same\nanalysis we did before, you'll again find that\nour treatment effect is gamma. Let's suppose now\nthat we hadn't known that there was that delta\nX squared term in there, and we hypothesized that the\npotential outcome was given to you by this linear\nmodel involving X and T. And I'm going to use Y\nhat to denote that that's going to be the function family\nthat we're going to be fitting. So we now fit that\nbeta hat in gamma hat, and if you had infinite data\ndrawn from this true generating process, which is, again,\nunknown, what one can show is that the gamma hat\nthat you would estimate using any reasonable estimator,\nlike a least squared estimator, is actually equal to\ngamma, the true ATE value,", "id": "g5v-NvNoJQQ_22", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  And notice that this term does\nnot depend on beta or gamma. What this means is,\ndepending on delta, your gamma hat could be\nmade arbitrarily large or arbitrarily small. So for example, if\ndelta is very large, gamma hat might\nbecome positive when gamma might have been negative. And so your conclusions about\nthe average treatment effect could be completely wrong,\nand this should scare you. This is the thing which makes\nusing covariate adjustments so dangerous, which\nis that if you're making the wrong assumptions\nabout the true potential outcomes, you could get\nvery, very wrong conclusions. So because of\nthat, one typically wants to live in\na world where you don't have to make many\nassumptions about the form,", "id": "g5v-NvNoJQQ_23", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  So here, you see that there\nis this nonlinear term. Well, obviously, if you had\nused some nonlinear modeling algorithm, like a neural network\nor maybe a random forest, then it would have the potential\nto fix that nonlinear function, and then maybe we wouldn't\nget caught in this same trap. And there are a variety of\nmachine learning algorithms that have been applied to\ncausal inference, everything from random forests and\nBayesian additive regression trees to algorithms\nlike Gaussian processes and deep neural networks. I'll just briefly\nhighlight the last two. So Gaussian processes\nare very often used to model continuous\nvalued potential outcomes, and there are a couple of ways\nin which they can be done. So for example,\none class of models might treat Y1 and Y0 as two\nseparate Gaussian processes and fit those to the data. A different approach,\nshown on the right here, would be to treat T as\nan additional covariate,", "id": "g5v-NvNoJQQ_24", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  and fit a Gaussian process\nfor that joint model. When it comes to\nneural networks, neural networks had been used\nin causal inference going back about 20, 30 years, but really\nstarted catching on a few years ago with a paper that\nI wrote in my group as being one of\nthe earliest papers from this recent generation\nof using neural networks for causal inference. And one of the things that we\nfound to work very effectively is to use a joint model for\npredicting the causal effect, so we're going to be\nlearning a model that takes-- an F that takes, as input,\nX and T and has to predict", "id": "g5v-NvNoJQQ_25", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  going to allow us to share\nparameters across your T equals 1 and T equals 0 samples. But rather than\nfeeding in X and T in your first layer of\nyour neural network, we're only going to feed\nin X in the initial layer of the neural network,\nand we're going to learn a shared\nrepresentation, which is going to be used\nfor both predicting T equals 0 and T equals 1. And then for predicting\nwhen T is equal to 0, we use a different head\nfrom predicting T equals 1. So F0 is a function that\nconcatenates these shared layers with several\nnew layers used to predict for when\nT is equal to 0 and same analogously for 1. And we found that architecture\nworked substantially better than the naive\narchitectures when", "id": "g5v-NvNoJQQ_26", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  data sets. Now, the last thing\nI want to talk about for covariate\nadjustment, before I move on to a new\nset of techniques, is a method called\nmatching, that is intuitively very pleasing. It's a very-- would seem to\nbe a really natural approach to do causal inference,\nand at first glance, may look like it has nothing\nto do with covariate adjustment technique. What I'll do now is I'm\ngoing to first introduce you to the matching\ntechnique, and then I will show you that it\nactually is precisely identical to\ncovariate adjustment with a particular\nassumption of what the functional family for F is. So not Gaussian processes,\nnot deep neural networks, but it'll be something else. So before I get into\nthat, what is matching as a technique for\ncausal inference? Well, the key idea\nof matching is to use each\nindividual's twin to try", "id": "g5v-NvNoJQQ_27", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  have been? So I created these\nslides a few years ago when President\nObama was in office, and you might imagine this\nis the actual President Obama who did go to law school. And you might imagine who might\nhave been that other president? What President Obama\nhave been like had he not gone to law school, but let's\nsay, gone to business school? So if you can now imagine trying\nto find, in your data set, someone else who looks\njust like Barack Obama, but who, instead of\ngoing to law school, went to business school,\nand then you would then ask the following question. For example, would\nthis individual have gone on to\nbecome president had he gone to law school versus\nhad he gone to business school? If you find someone\nelse who's just like Barack Obama who went to\nbusiness school, look to see did that person become\npresident eventually, that would in essence give\nyou that counterfactual.", "id": "g5v-NvNoJQQ_28", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  because you would never get\nthe sample size to see that. So that's the general idea,\nand now, I'll show it to you in a picture. So here now, we have to\ncovariates or features-- a patient's age and their\nCharleson comorbidity index. This is some measure\nof how many-- what types of conditions\nor comorbidities the patient might have. Do they have diabetes, do they\nhave hypertension, and so on? And notably, what\nI'm not showing you here is the\noutcome Y. All I'm showing you are the\noriginal data points and what treatment\ndid they receive. So blue are the individuals who\nreceived the control treatment, or T equals 0, and red\nare the individuals who received treatment 1. So you can imagine trying\nto find nearest neighbors. For example, the\nnearest neighbor to this data point over here\nis this blue point over here,", "id": "g5v-NvNoJQQ_29", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  some Y1, for this individual,\nwe observed some Y0 for this individual. And if you wanted\nto know, well, what would have happened\nto this individual if they had received treatment\n0 instead of treatment 1, well, you could\njust look at what happened to this\nblue point and say, that's what would have\nhappened to this red point, because they're very\nclose to each other. Any questions\nabout what matching would do before I\ndefine it formally? Here, I'll-- yeah,\ngood, one question. What happens if the nearest\nneighbor is extremely far away? That's a great question. So you can imagine that you have\none red data point over here", "id": "g5v-NvNoJQQ_30", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  The matching approach\nwouldn't work very well. So this data point,\nthe nearest neighbor, is this blue point over here,\nwhich intuitively, is very far from this red point. And so if we were to estimate\nthis red point's counterfactual using that blue point,\nwe're likely to get a very bad estimate,\nand in fact, that is going to be one of the\nchallenges of matching based approaches. It's going to work really well\nin a high dimensional setting where you can imagine--\nsorry, in a large-- it's going to work very well in\na large sample setting, where you can hope that you're likely\nto observe a counterfactual for every individual. And it won't work well you\nhave very limited data, and of course, all\nthis is going to be subject to the assumption\nof common support. So one question's\nabout how does that translate into high dimensions? The short answer--\nnot very well. We'll get back to\nthat in a moment.", "id": "g5v-NvNoJQQ_31", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  Yes, and I will define, in\njust a moment, how and why. It won't be a strict matching. Are we trying to\nfind a counterfactual for each treated observation,\nor one for each control observation? I'll answer that\nin just a second. And finally, is it common\nfor medical data sets to find such matching pairs? I'm going to reinterpret\nthat question as saying, is this technique used\noften in medicine? And the answer\nis, yes, it's used all the time in clinical\nresearch despite the fact that bio statisticians,\nfor quite a few years now, have been trying to argue\nthat folks should not use this technique for\nreasons that you see shortly. So it's widely used. It's very intuitive, which\nis why I'm teaching it. And it's going to fit into\na very general framework, as you'll see in just a\nmoment, which I'll give you the natural solution\nfor the problems that I'm going to raise.", "id": "g5v-NvNoJQQ_32", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  to any remaining questions. So here, I'll define one way of\ndoing counterfactual inference using matching, and it's\ngoing to start, of course, by assuming that we\nhave some distance metric d between individuals. Then we're going to say,\nfor each individual i, let's let j of i be the\nother individual j, obviously different from i, who is closest\nto i, but critically, closest but has a different treatment. So where Ti is different\nfrom Tj, and again, I'm assuming binary,\nso Tj is either 0 or 1. With that definition\nthen, we're going to define our estimate of the\nconditional average treatment effect for an individual is\nwhatever their actual observed", "id": "g5v-NvNoJQQ_33", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  This, I'm going to give\nfor an individual that actually received treatment 1,\nso it's Y1, and the reason-- it's Yi minus the imputed\ncounterfactual corresponding to T is equal to 0. And the way we get that\ncomputed counterfactual is by trying to find\nthat nearest neighbor who received treatment 0\ninstead of treatment 1 and looking at their Y. Analogously, if T is\nequal to 0, then we're going to use the\nobserved Yi, now over here instead of over there\nbecause it corresponds to Y0. And where we need to impute Y1-- capital Y1, potential\noutcome Y1-- we're going to use the observed\noutcome from the nearest neighbor of individual i who\nreceived treatment 1 instead of 0. So this, mathematically, is what\nI mean by our matching based", "id": "g5v-NvNoJQQ_34", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  one of the questions which\nwas raised, which is, do you really need\nto have it matching, or could a data point be matched\nto multiple other data points? And indeed, here, you see the\nanswer to that last question is yes, because you could have\na setting where, for example, there are two red points here. And I can't draw\nblue, but I'll just use a square for what I\nwould have drawn as blue. And then everything\nelse very far away, and for both of\nthese red points, this blue point is\nthe closest neighbor. So both of the counterfactual\nestimates for these two points would be using the\nsame blue point, so that's the answer\nto that question. Now, I'm just going to\nrewrite this in a little bit more convenient form. So I'll take this\nformula, shown over here, and you can rewrite\nthat as Yi minus Yji, but you have to flip\nthe sign depending on whether Ti is\nequal to 1 or 0,", "id": "g5v-NvNoJQQ_35", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  If Ti is equal to 1,\nthen this evaluates to 1. If Ti is equal to 0, this\nevaluates to minus 1. Flips the sign. So now that we have\nthe definition of CATE, we can now easily estimate\nthe average treatment effect by just averaging these CATEs\nover all of the individuals in your data set. So this is now the\ndefinition of how to do one nearest\nneighbor matching. Any questions? So one question is, do we ever\nuse the metric d to weight how much we would, quote,\nunquote, \"trust\" the matching? That's a good question. So what Hannah's\nasking is, what happens if you have, for example,\nvery many nearest neighbors, or\nanalogously, what happens if you have some\nnearest neighbors that", "id": "g5v-NvNoJQQ_36", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  You might imagine trying to\nweight your nearest neighbors by the distance\nfrom the data point, and you could imagine\neven doing that-- you can even imagine coming\nup with an estimator, which might discount certain data\npoints if they don't have nearest neighbors\nnear them at all by the corresponding\nweighting factor. Yes, that's a good idea. Yes, you can come up with\na consistent estimator of the average treatment\neffect through such an idea. There are probably a few\nhundred papers written about it, and that's all I\nhave to say about it. So there's lots of variants\nof this, and they all end up having the same\ntheoretical justification that I'm about to give\nin the next slide. So one of the\nadvantages of matching is that you get some\ninterpretability. So if I was to ask\nyou, well, what's the reason why you tell\nme that this treatment is going to work for John? Well, someone can respond-- well, I used this\ntechnique, and I", "id": "g5v-NvNoJQQ_37", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  And Anna took this other\ntreatment from John, and this is what\nhappened for Anna. And that's why I\nconjecture that, for John, the difference between\nY1 and Y0 is as follows. And so then, that\ncan be criticized. So for example, a clinician\nwho has some domain expert, can look at Anna, look at John,\nand say, oh, wait a second, these two individuals are really\ndifferent from one another. Let's say the\ntreatment, for example, had to do with something\nwhich was gender specific. Comparing two individuals\nwhich are of different genders are obviously not going to\nbe comparable to one other, and so then the\ndomain expert would be able to reject that\nconclusion and say, nuh-uh, I don't trust\nany of these statistics. Go back to the drawing board. And so type of interpretability\nis very attractive. The second aspect of this,\nwhich is very attractive", "id": "g5v-NvNoJQQ_38", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  non-parametric in the same\nway that neural networks or random forest\nare non-parametric. So this does not rely\non any strong assumption about the parametric form\nof the potential outcomes. On the other hand,\nthis approach is very reliant on the\nunderlying metric. If your distance function\nis a poor distance function, then it's going to\ngive poor results. And moreover, it could\nbe very much misled by features that don't\naffect the outcome, which is not necessarily a\nproperty that we want. Now, here's that final slide\nthat makes the connection. Matching is equivalent\nto covariate adjustment. It's exactly the same. It's an instantiation\nof covariate adjustment with a particular\nfunctional family for F. So rather than assuming\nthat your function F, that black box, is a linear\nfunction or a neural network or a random forester or a\nBayesian regression tree,", "id": "g5v-NvNoJQQ_39", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  takes the form of a nearest\nneighbor classifier. In particular, we'll\nsay that Y hat of 1, the function for predicting\nthe potential outcome Y hat 1, is given to you by finding the\nnearest neighbor of the data point X according to the\ndata set of individuals that received treatment 1,\nand same thing for Y hat 0. And so that then allows\nus to actually prove some properties of matching. So for example, if\nyou remember from-- I think I mentioned\nin Tuesday's lecture that this covariate\nadjustment approach, under the assumptions of overlap\nand under the assumptions of no hidden confounding,\nand that your function", "id": "g5v-NvNoJQQ_40", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  can actually fit the\nunderlying model, then you're going to\nget correct estimates of your conditional\naverage treatment effect. Now, one can show that a nearest\nneighbor algorithm is not, generally, a\nconsistent algorithm. And what that means\nis that, if you have a small number\nof samples, you're going to be getting\nbiased estimate. Your function F might, in\ngeneral, be a biased estimate. Now, we can conclude\nfrom that, that if we were to use one nearest\nneighbor matching for inferring average treatment\neffect, that in general, it could give us\na biased estimate of the average treatment effect. However, in the limit\nof infinite data, one nearest neighbor\nalgorithms are guaranteed to be able to fit\nthe underlying function family.", "id": "g5v-NvNoJQQ_41", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  in the limit of a\nlarge amount of data, and thus, we can immediately\ndraw from that literature and causal inference--\nsorry, from that literature and machine learning to\nobtain theoretical results for matching for\ncausal inference. And so that's all I want\nto say about matching and its connection to\ncovariate adjustment. And really, the\npunchline is, think about matching just as another\ntype of covariate adjustment, one which uses a nearest\nneighbor function family, and thus should be compared\nto other approaches to covariate adjustments, such\nas, for example, using machine learning algorithms that are\ndesigned to be interpretable. So the last part\nof this lecture is going to be introducing a\nsecond approach for inferring average treatment effect that\nis known as the propensity score", "id": "g5v-NvNoJQQ_42", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  It's going to be a\ndifferent estimator from the covariate adjustment. So as I mentioned, it's going\nto be used for estimating average treatment effect. In problem set 4,\nyou're going to see how you can use the\nsame sorts of techniques I'll tell you about\nnow for also estimating conditional average\ntreatment effect, but that won't be obvious\njust from today's lecture. So the key intuition for\npropensity score method is to think back to what\nwould have happened if you had a randomized control trial. In a randomized control\ntrial, again, you get choice over what treatment\nto give each individual, so you might imagine\nflipping a coin. If it's heads, giving\nthem treatment 1. If it's tails, giving\nthem treatment 0. So given data from a\nrandomized control trial, then there's a really\nsimple estimator shown here for the average\ntreatment effect. You just sum up the values\nof Y for the individuals that receive treatment\n1, divided by n1, which is the number\nof individuals", "id": "g5v-NvNoJQQ_43", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  So this is the average\noutcome for all people who got treatment 1, and\nyou just subtract from that the average outcome\nfor all individuals who received treatment 0. And that can be easily shown\nto be an unbiased estimator of the average\ntreatment effect had your data come from a\nrandomized controlled trial. So the key idea of a\npropensity score method is to turn an observational\nstudy into something that looks like a randomized\ncontrol trial via re-weighting of the data points. So here's the picture I\nwant you to have in mind. Again, here, I am not\nshowing you outcomes. I'm just showing\nyou the features X-- that's what the\ndata points are-- and the treatments that\nwere given to them, the Ts. And the Ts, in this\ncase, are being denoted by the color of the\ndots, so red is T equals 1. Blue is T equals 0. And my apologies in advance\nfor anyone who's color blind. So the key challenge\nwhen working", "id": "g5v-NvNoJQQ_44", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  might be a bias in terms of who\nreceives treatment 0 versus who receives treatment 1. If this was a randomized\ncontrol trial, then you would expect to\nsee the reds and the blues all intermixed equally\nwith one another, but as you can see\nhere, in this data set, there are very many\nmore people who received-- very more young\npeople who received treatment 0 than received treatment 1. Said differently, if you look\nat the distribution over X conditioned on T\nequals 0 in the data, it's different from\nthe distribution over X conditioned on the\npeople who receive treatment 1. So what the propensity score\nmethod is going to do is it's going to recognize that\nthere is a difference between these two distributions, and\nit's going to re-weight data points so that, in aggregate, it\nlooks like, in any one region-- so for example, imagine\nlooking at this region-- that there's roughly\nthe same number of red and blue data points. Where if you think about blowing\nup this red data point-- here,", "id": "g5v-NvNoJQQ_45", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  about it being many,\nmany red data points of the corresponding weight. You look over here, see\nagain roughly the same amount of red and blue mass as well. So if we can find some way\nto increase or decrease the weight associated with\neach data point such that, now, it looks like the\ntwo distributions, those who received treatment 1 and\nthose who received treatment 0, look like they came\nfrom-- look like now they have the same\nweighted distribution, then we're going\nto be in business. So we're going to search\nfor those weights, w, that have that property. So to do that, we\nneed to introduce one new concept, which is\nknown as the propensity score. The propensity score is given\nto you by the probability that T equals 1\ngiven X. Here, again, we're going to use\nmachine learning. Whereas in covariate\nadjustment, we used machine learning to predict\nY conditioned on X comma T--", "id": "g5v-NvNoJQQ_46", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  here, we're going to be\nignoring Y altogether. We're just going\nto take X's input, and we're going to\nbe predicting T. So you can imagine using\nlogistic regression, given your covariates, to predict\nwhich treatment any given data point came from. Here, you're using the\nfull data set, of course, to make that\nprediction, so we're looking at both data\npoints where T equals 1 and T equals 0. T is your label for this. Then what we're\ngoing to do is given, that learned propensity score--\nso we take your data set. You, first, learn\nthe propensity score. Then we're going to re-weight\nthe data points according to the inverse of\nthe propensity score. And you might ask,\nthis looks familiar. This whole notion of\nre-weighting data points, this whole notion of trying\nto figure out which, quote, unquote, \"data set\" a\ndata point came from, the data set of individuals who\nreceive treatment 1 or the data set of individuals who\nreceive treatment 0-- that sounds really familiar. And it's because it's exactly\nwhat you saw in lecture 10,", "id": "g5v-NvNoJQQ_47", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  In fact, this whole\nentire method, as you'll develop\nin problem set 4, is a special case of learning\nunder data set shift. So here, now, is the\npropensity score algorithm. We take our data set, which\nhave samples of X, T, and Y where Y, of course, tells\nyou the potential outcome corresponding to\nthe treatment T. We're going to use any\nmachine learning method in order to estimate\nthis model that can give you a probability\nof treatment given X. Now, critically, we need\na probability for this. We're not trying to\ndo classification. We need an actual probability,\nand so if you remember back to previous lectures where\nwe spoke about calibration, about the ability to accurately\npredict probabilities, that is going to be\nreally important here. And so for example, if you were\nto use a deep neural network in order to estimate\nthe propensity scores,", "id": "g5v-NvNoJQQ_48", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  And so one would have to use\none of a number of new methods that have been\nrecently developed to make the outputs of\ndeep learning calibrated in order to use this\ntype of technique. So after finishing\nstep 1, now that you have a model that can allow\nyou to estimate the propensity score for every\ndata point X, we now can take those and estimate\nyour average treatment effect with the following formula. It's 1 over n of the sum\nover the data points, where the data points corresponding\nto the treatment 1 of Yi-- that part is\nidentical to before. But what you see now is\nwe're going to divide it by the propensity score,\nand so this denominator, that's the new piece here. That's the inverse of\nthe propensity score is precisely the weighting that\nwe were referring to earlier, and the same thing happens\nover here for Ti equals 0. Now, let's try to get some\nintuition about this formula,", "id": "g5v-NvNoJQQ_49", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  by looking at a special case. So the simplest special case\nthat we might be familiar with is that of a randomized\ncontrol trial, where because you're flipping a coin,\nand each data point either gets treatment 0 or treatment\n1, then the propensity score is precisely,\ndeterministically equal to 5. So let's take this now. No machine learning done here. Let's just plug it in to see if\nwe get back the formula that I showed you earlier\nfor the estimate of the average treatment effect\nin a randomized control trial. So we plug that in over there. This now becomes 0.5, and\nplug that in over here. This also becomes 0.5. And then what we're\ngoing to do is we're just going to take that 0.5. We're going to bring that out,\nand this is going to become a 2 over here, and\nsame, a 2 over here. And you get to the\nfollowing formula, which is-- if you were to compare to\nthe formula from a few slides", "id": "g5v-NvNoJQQ_50", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  ago over here, I had 1 over n1,\nand over here, I had 1 over n0. Now, these two are two different\nestimators for the same thing, and the reason why you can\nsay they're the same thing is that, in a randomized\ncontrol trial, the number of individuals\nthat receive treatment 1 is, on average, n over 2. Similarly, the number of\nindividuals receiving treatment 0 are, on average, n over 2. So if you were to-- that n over 2 cancels\nout with this 2 over n is what gets you a\ncorrect estimator. So this is a slightly\ndifferent estimator, but nearly identical to the\none that I showed you earlier, and by this argument, is\na consistent estimator of the average treatment effect\nin a randomized control trial. So any questions before I try\nto derive this formula for you?", "id": "g5v-NvNoJQQ_51", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  is the, quote,\nunquote, \"bias\" of how likely people are assigned\nto T equals 1 or T equals 0? Yes, that's exactly right. So if you were to imagine\ntaking an individual where this probability\nfor that individual is, let's say,\nvery close to 1, it means that there are very\nfew other people in the data set who receive treatment 1. They're a red data point in\na sea of blue data points.", "id": "g5v-NvNoJQQ_52", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  to be trying to remove that\nbias, and that's exactly right. Thank you for that question. Are there other questions? I really appreciate the\nquestions via the chat window, so thank you. So let's now try to\nderive this formula. Recall the definition of\naverage treatment effect, and for those who are\npaying very close attention, you might notice that I removed\nthe expectation over Y1. And for this derivation\nthat I'm going to give you, I'm going to suppose-- I'm going to assume that a\npotential outcomes are all deterministic because it\nmakes the math easier, but is without\nloss of generality. So the average\ntreatment effect is the expectation, with\nrespect to all individuals, of the potential outcome\nY1 minus the expectation", "id": "g5v-NvNoJQQ_53", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  So this term over here is going\nto be our estimate of that, and this term over here is\ngoing to be our estimate of this expectation. So naively, if you were to\njust take the observed data, it would allow you to compute-- if you, for example, just\naveraged the values of Y for the individual who\nreceived treatment 1, that would give you this\nexpectation that I'm showing on the bottom here. I want you to compare\nthat to the one that's actually needed in the\naverage treatment effect. Whereas over here, it's an\nexpectation with respect to individuals that received\ntreatment 1, up here, this was an expectation with\nrespect to all individuals. But the thing inside\nthe expectation is exactly identical,\nand that's the key point that we're going\nto work with, which is that we want an\nexpectation with respect to a different distribution than\nthe one that we actually have. And again, this\nshould ring bells, because this sounds\nvery, very familiar to the data set shift\nstory that we talked", "id": "g5v-NvNoJQQ_54", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  So I'm going to show you how\nto derive an estimator for just this first term, and the\nsecond term is obviously going to be identical. So let's start out\nwith the following. We know that p of X\ngiven T times p of T is equal to p of X\ntimes p of T given X. So what I've just done here\nis use two different formulas for a joint distribution,\nand then I've divided by p of T\ngiven X in order to get the formula that I\nshowed you a second ago. I'm not going to\nattempt to erase that. I'll leave it up there. So the next thing we're going\nto do is we're going to say, if we were to compute an\nexpectation with respect to p of X given T equals 1,\nand if we were to now take the value that we observe, Y1,\nwhich we can get observations", "id": "g5v-NvNoJQQ_55", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  and if we were to re-weight\nthis observation by this ratio, where remember,\nthis ratio showed up in the previous bullet point,\nthen what I'm going to show you in just a moment is that\nthis is equal to the quantity that we actually wanted. Well, why is that? Well, if you expand\nthis expectation, this expectation is an\nintegral with respect to p of X conditioned\non T equals 1 times the thing inside the brackets,\nand because we know that p of-- because we know from up here\nthat p of X conditioned on T equals 1 times p of T\nequals 1 divided by p of T", "id": "g5v-NvNoJQQ_56", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  this whole thing\nis just going to be equal to an integral of p of\nX times Y1, which is precisely the definition of\nexpectation that we want. So this was a very\nsimple derivation to show you that the\nre-weighting gets you what you need. Now, we can estimate this\nexpectation empirically as follows, the\nestimate that we're going to now sum\nover all data points that received treatment 1. We're going to take\nan average, so we're dividing by the\nnumber of data points that received treatment 1. For p of T equals\n1, we're just going to use the empirical estimate\nof how many individuals received treatment 1 in the\ndata set divided by the total number of\nindividuals in the data set. That's n1 divided by n. And for the denominator, p of\nT equals 1 conditioned on X, we just plug in, now, the\npropensity score, which we had previously estimated. And we're done. And so that, now,\nis our estimate for the first term in the\naverage treatment effect,", "id": "g5v-NvNoJQQ_57", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  And I've shown\nyou the full proof of why this is an unbiased\nestimator for average treatment effect. So I'm going to be concluding\nnow, in the next few minutes. First, I just wanted to\ncomment on what we just saw. So we saw a different way to\nestimate the average treatment effect, which only required\nestimating the propensity score. In particular, we never\nhad to use a model to predict Y in this\napproach for estimating the average treatment\neffect, and that's a good thing and a bad thing. It's a good thing\nbecause, if you had errors in\nestimating your model y, as I showed you in the very\nbeginning of today's lecture, that could have\na very big impact on your estimate of the\naverage treatment effect. And so that doesn't\nshow up here. On the other hand, this\nhas its own disadvantages. So for example, the propensity\nscore is going to be really,", "id": "g5v-NvNoJQQ_58", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  because when you\nhave lack of overlap, it means there's some data\npoints where the propensity score is very close to\n0 or very close to 1. And that really leads\nto very large variance in your estimators. And a very common\ntrick which is used to try to address\nthat concern is known as clipping, where you\nsimply clip the propensity scores so that they're always\nbounding away from 0 and 1. But that's really\njust a heuristic, and it can, of course, then\nlead to biased estimates of the average treatment effect. So there's a whole family of\ncausal inference algorithms that attempt to use ideas\nfrom both covariate adjustment and inverse\npropensity weighting. For example, there's\na method called doubly robust\nestimators, and we'll try to provide a citation for\nthose estimators in the Scribe notes. And these doubly\nrobust estimators are a different family of\nestimators that actually bring in both of these\ntechniques together, and they have a\nreally nice property, which is that if either\none of them fail, you still get valid estimates\nof average treatment effect.", "id": "g5v-NvNoJQQ_59", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  is that we've presented\ntwo different approaches for causal inference\nfrom observational data-- covariate adjustment and\npropensity score based methods. And both of these,\nI need to stress, are only going to\ngive you valid results under the assumptions\nwe outlined in the previous lecture-- for example, that your\ncausal graph is correct; critically, that there's no\nunobserved confounding; and second, that you have overlap\nbetween your two treatment classes. And third, if you're using\na non-parametric regression approach, overlap is\nextremely important, because without\noverlap, your model's undefined in regions of space. And thus, as a result,\nyou have no way of verifying if your\nextrapolations are correct,", "id": "g5v-NvNoJQQ_60", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  something we really like. And in propensity score methods,\noverlap is very important because if you don't have that,\nyou get inverse propensity scores that are either-- which are infinite and lead\nto extremely high variance estimators. So in the end of this\nslide, which are already posted online, I\ninclude some references that I strongly encourage\nfolks to follow up on. First references to two\nrecent workshops that have been held in the machine\nlearning community so that you can get a sense of what the\nlatest and greatest in terms of research in\ncausal inference are, two different books\non causal inference that you can download\nfor free from MIT, and finally, some\npapers that I think are really interesting,\nparticularly of interest, potentially, to course projects. So we are at time now. I will hang around for a\nfew minutes after lecture, as I would normally.", "id": "g5v-NvNoJQQ_61"}, {"text": "  something we really like. And in propensity score methods,\noverlap is very important because if you don't have that,\nyou get inverse propensity scores that are either-- which are infinite and lead\nto extremely high variance estimators. So in the end of this\nslide, which are already posted online, I\ninclude some references that I strongly encourage\nfolks to follow up on. First references to two\nrecent workshops that have been held in the machine\nlearning community so that you can get a sense of what the\nlatest and greatest in terms of research in\ncausal inference are, two different books\non causal inference that you can download\nfor free from MIT, and finally, some\npapers that I think are really interesting,\nparticularly of interest, potentially, to course projects. So we are at time now. I will hang around for a\nfew minutes after lecture, as I would normally.", "id": "g5v-NvNoJQQ_61", "title": "15. Causal Inference, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:56Z": "2020-10-22T19:36:56Z"}, {"text": "  PROFESSOR: Hi, everyone. We're getting started now. So this week's lecture\nis really picking up where last week's left off. You may remember we spent\nthe last week talking about cause inference. And I told you\nhow, for last week, we're going to focus\non a one-time setting. Well, as we know,\nlots of medicine has to do with multiple\nsequential decisions across time. And that'll be the focus\nof this whole week's worth of discussions. And as I thought about\nreally what should I teach in this\nlecture, I realized that the person who knew\nthe most about the topic was in fact a postdoctoral\nresearcher in my lab. Most about this topic\nin the general area of the medical field. FREDRIK D. JOHANSSON: Thanks. I'll take it. AUDIENCE: Global [INAUDIBLE]. FREDRIK D. JOHANSSON:\nIt's very fair.", "id": "YZ5pOgY5hEE_0", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and to give this as\nan invited lecture. And this is Fredrik Johansson. He'll be a professor in\nChalmers, in Sweden, starting in September. FREDRIK D. JOHANSSON:\nThank you so much, David. That's very generous. Yeah, so as David\nmentioned, last time we looked a lot at causal effects. And that's where we will\nstart on this discussion, too. So I'll just start\nwith this reminder, here-- we essentially introduced\nfour quantities last time, or the last two lectures,\nas far as I know. We had two potential outcomes,\nwhich represented the outcomes that we would see of\nsome treatment choice under the various choices. So, the two different choices-- 1 and 0. We had a set of covariates,\nx and a treatment, t. And we were interested\nin, essentially, what is the effect\nof this treatment, t, on the outcome, y,\ngiven the covariates, x. And the effect that we\nfocused on that time", "id": "YZ5pOgY5hEE_1", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  which is exactly the\ndifference between these potential outcomes-- a condition on the features. So the whole last\nweek was about trying to identify this quantity\nusing various methods. And the question that\ndidn't come up so much-- or one question that\ndidn't come up too much-- is how do we use this quantity? We might be\ninterested in it, just in terms of its\nabsolute magnitude. How large is the effect? But we might also be\ninterested in designing a policy for how to\ntreat our patients based on this quantity. So today, we will\nfocus on policies. And what I mean by\nthat, specifically, is something that\ntakes into account what we know about a patient and\nproduces a choice or an action as an output. Typically, we'll\nthink of policies as depending on medical\nhistory, perhaps which treatments they\nhave received previously, what state is the\npatient currently in.", "id": "YZ5pOgY5hEE_2", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that we produce last time-- the\nconditional average treatment effect. And one very natural\npolicy is to say, pi of x is equal to the\nindicator function representing if this\nCATE is positive. So if the effect is positive,\nwe treat the patient. If the effect is\nnegative, we don't. And of course, positive will\nbe relative to the usefulness of the outcome being high. But yeah, this is a very\nnatural policy to consider. However, we can also think about\nmuch more complicated policies that are not just\nbased on this number-- the quality of the outcome. We can think about\npolicies that take into account legislation or cost\nof medication or side effects. We're not going\nto do that today, but that's something\nthat you can keep in mind as we discuss these things. So David mentioned,\nwe should now move from the one-step\nsetting, where we have a single treatment\nacting at a single time and we only have to take\ninto account the state", "id": "YZ5pOgY5hEE_3", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And we will move from that\nto the sequential setting. And my first example of such a\nsetting is sepsis management. So, sepsis is a complication\nof an infection, which can have very disastrous consequences. It can lead to organ failure\nand ultimately death. And it's actually\none of the leading causes of deaths in the ICU. So it's of course important\nthat we can manage and treat this condition. When you start treating\nsepsis, the primary target-- the first things you\nshould think about fixing-- is the infection itself. If we don't treat\nthe infection, things are going to keep being bad. But even if we figure\nout the right antibiotic to treat the infection that is\nthe source of the septic shock or the septic\ninflammation, there are a lot of\ndifferent conditions that we need to manage. Because the infection\nitself can lead to fever, breathing difficulties, low\nblood pressure, high heart rate-- all these kinds of things\nthat are symptoms, but not", "id": "YZ5pOgY5hEE_4", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  But we still have to\nmanage them somehow so that the patient\nsurvives and is comfortable. So when I say sepsis\nmanagement, I'm talking about managing\nsuch quantities over time-- over a patient's\nstay in the hospital. So, last time-- again, just\nto really hammer this in-- we talked about potential\noutcomes and the choice of a single treatment. So we can think about this in\nthe septic setting as a patient coming in-- or a patient\nalready being in the hospital, presumably-- and is presenting with\nbreathing difficulties. So that means that their blood\noxygen will be low because they can't breathe on their own. And we might want to put them\non mechanical ventilation so that we can ensure that\nthey get sufficient oxygen. We can view this\nas a single choice. Should we put the patient on\nmechanical ventilation or not? But what we need to\ntake into account here is what will happen after\nwe make that choice. What will be the side effects\nof this choice going further?", "id": "YZ5pOgY5hEE_5", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and in good health\nthroughout their stay. So today, we will move towards\nsequential decision making. And in particular, what\nI alluded to just now is that decisions\nmade in sequence may have the property that\nchoices early on rule out certain choices later. And we'll see an example\nof that very soon. And in particular, we'll\nbe interested in coming up with a policy for making\ndecisions repeatedly that optimizes a given outcome-- something that we care about. It could be minimize\nthe risk of death. It could be a reward that says\nthat the vitals of a patients are in the right range. We might want to optimize that. But essentially,\nthink about it now as having this choice\nof administering a medication or an\nintervention at any time, t-- and having the best\npolicy for doing so.", "id": "YZ5pOgY5hEE_6", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  OK, so I mentioned already\none potential choice that we might want to\nmake in the management of a septic patient,\nwhich is to put them on mechanical ventilation\nbecause they can't breathe on their own. A side effect of\ndoing so is that they might suffer discomfort\nfrom being intubated. The procedure is not painless,\nit's not without discomfort. So something that you\nmight have to do-- putting them on\nmechanical ventilation-- is to sedate the patient. So this is an action\nthat is informed by the previous action,\nbecause if we didn't put the patient on\nmechanical ventilation, maybe we wouldn't consider\nthem for sedation. When we sedate a\npatient, we run the risk of lowering their\nblood pressure. So we might need to\nmanage that, too. So if their blood\npressure gets too low, maybe we need to\nadminister vasopressors, which artificially raise\nthe blood pressure, or fluids or anything else\nthat takes care of this issue.", "id": "YZ5pOgY5hEE_7", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of choices cascading, in\nterms of their consequences, as we roll forward in time. Ultimately, we will face the\nend of the patient's stay. And hopefully, we managed the\npatient in a successful way so that their response or\ntheir outcome is a good one. What I'm illustrating\nhere is that, for any one patient in our hospitals or\nin the health care system, we will only observe\none trajectory through these options. So I will show this type\nof illustration many times, but I hope that you can realize\nthe scope of the decision space here. Essentially, at any point, we\ncan choose a different action. And usually, the\nnumber of decisions that we make in an ICU\nsetting, for example, is much larger\nthan we could ever test in a randomized trial.", "id": "YZ5pOgY5hEE_8", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  as being different arms in a\nrandomized controlled trial that you want to compare the\neffects or the outcomes of. It's infeasible to run\nsuch a trial, typically. So one of the big\nreasons that we are talking about reinforcement\nlearning today and talking about learning\npolicies, rather than causal effects in the setup\nthat we did last week, is because the space of\npossible action trajectories is so large. Having said that, we now\nturn to trying to find, essentially, the policy that\npicks this orange path here-- that leads to a good outcome. And to reason\nabout such a thing, we need to also reason about\nwhat is a good outcome? What is good reward for\nour agent, as it proceeds through time and makes choices? Some policies that we\nproduce as machine learners", "id": "YZ5pOgY5hEE_9", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  We have to somehow restrict\nourself to something that's realistic. I won't focus very\nmuch on this today. It's something that will come\nup in the discussion tomorrow, hopefully. And also the notion of\nevaluating something for use in the\nhealth care system will also be talked\nabout tomorrow. AUDIENCE: Thursday. FREDRIK D. JOHANSSON:\nSorry, Thursday. Next time. OK, so I'll start by just\nbriefly mentioning some success stories. And these are not\nfrom the health care setting, as you can\nguess from the pictures. How many have seen\nsome of these pictures? OK, great-- almost everyone. Yeah, so these are from various\nvideo games-- almost all of them. Well, games anyhow. And these are good examples\nof when reinforcement learning works, essentially.", "id": "YZ5pOgY5hEE_10", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  because, essentially,\nit's very hard to argue that the\ncomputer or the program that eventually beat Lee Sedol. I think it's in this picture\nbut also, later, Go champions, essentially. In the AlphaGo picture\nin the top left, it's hard to argue that\nthey're not doing a good job, because they clearly\nbeat humans here. But one of the things\nI want you to keep in mind throughout\nthis talk is what is different between\nthese kinds of scenarios? And we'll come\nback to that later. And what is different\nto the health care setting, essentially? So I simply added\nanother example here, that's why I recognize it. So there was recently one\nthat's a little bit closer to my heart, which is AlphaStar. I play StarCraft. I like StarCraft, so it\nshould be on the slide. Anyway, let's move on. Broadly speaking,\nthese can be summarized in the following picture. What goes into those systems? There's a lot more nuance when\nit comes to something like Go.", "id": "YZ5pOgY5hEE_11", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  we will summarize\nthem with a slide. So essentially, one of\nthe three quantities that matters for a\nreinforcement learning is the state of the environment,\nthe state of the game, the state of the patient-- the state of the thing that we\nwant to optimize, essentially. So in this case, I've\nchosen Tic-tac-toe here. We have a state which\nrepresents the current positions of the circles and crosses. And given that state of the\ngame, my job as a player is to choose one of\nthe possible actions-- one of the free squares\nto put my cross in. So I'm the blue\nplayer here and I can consider these five choices\nfor where to put my next cross. And each of those will lead\nme to a new state of the game. If I put my cross\nover here, that means that I'm now in this box. And I have a new set of\nactions available to me for the next round, depending\non what the red player does.", "id": "YZ5pOgY5hEE_12", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and we have the next\nstate, essentially-- we have a trajectory or\na transition of states. And the last quantity that we\nneed is the notion of a reward. That's very important\nfor reinforcement learning, because that's what's\ndriving the learning itself. We strive to optimize the reward\nor the outcome of something. So if we look at the action\nto the farthest right here, essentially I left myself open\nto an attack by the red player here, because I didn't\nput my cross there. Which means that, probably,\nif the red player is decent, he will put his\ncircle here and I will incur a loss, essentially. So my reward will be negative,\nif we take positive to be good. And this is something that I\ncan learn from going forward. Essentially, what\nI want to avoid is ending up in\nthis state that's shown in the bottom right here. This is the basic\nidea of reinforcement learning for video games\nand for anything else. So if we take this board\nanalogy or this example and move to the\nhealth care setting, we can think of the state of\na patient as the game board", "id": "YZ5pOgY5hEE_13", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  We will always call\nthis St in this talk. The treatments that we prescribe\nor interventions will be At. And these are like the actions\nin the game, obviously. The outcomes of a patient--\ncould be mortality, could be managing vitals-- will be as the rewards in\nthe game, having lost or won. And then up at the end here,\nwhat could possibly go wrong. Well, as I alluded\nto before, health is not a game in the same sense\nthat a video game is a game. But they share a lot of\nmathematical structure. So that's why I make\nthe analogy here. These quantities\nhere-- S, A, and R-- will form something\ncalled a decision process. And that's what we'll\ntalk about next. This is the outline\nfor today and Thursday. I won't get to this\ntoday, but this is the talks we're considering. So a decision process\nis essentially the world that describes\nthe data that we access", "id": "YZ5pOgY5hEE_14", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Very often, if you've ever seen\nreinforcement learning taught, you have seen this picture\nin some form, usually. Sometimes there's a\nmouse and some cheese and there's other\nthings going on, but you know what\nI'm talking about. But there are the\nsame basic components. So there's the\nconcept of an agent-- let's think doctor for now-- that takes actions\nrepeatedly over time. So this t here indicates\nan index of time and we see that\nessentially increasing as we spin around\nthis wheel here. We move forward in time. So an agent takes an action\nand, at any time point, receives a reward\nfor that action. And that would be\nRt, as I said before. The environment is responsible\nfor giving that reward. So for example, if I'm\nthe doctor, I'm the agent, I make an action or an\nintervention to my patient, the patient will\nbe the environment.", "id": "YZ5pOgY5hEE_15", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  The state here is the\nstate of the patient, as I mentioned\nbefore, for example. But it might also be a state\nmore broadly than the patient, like the settings of the\nmachine that they're attached to or the availability of\ncertain drugs in the hospital or something like that. So we can think\na little bit more broadly around the patient, too. I said partially observed here,\nin that I might not actually know everything about the\npatient that's relevant to me. And we will come back a\nlittle bit later to that. So there are two different\nformalizations that are very close to each other, which\nis when you'd know everything about s and when you don't. We will, for the longest\npart of this talk, focus on the way I\nknow everything that is relevant about the environment. OK, to make this all\na bit more concrete, I'll return to the picture\nthat I showed you before, but now put it in context\nof the paper that you read. Was that the compulsory one? The mechanical ventilation?", "id": "YZ5pOgY5hEE_16", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So in this case, they had an\ninteresting reward structure, essentially. The thing that they\nwere trying to optimize was the reward related to\nthe vitals of the patient. But also whether they were\nkept on mechanical ventilation or not. And the idea of this\npaper is that you don't want to keep a\npatient unnecessarily on mechanical ventilation,\nbecause it has the side effects that we talked about before. So at any point in\ntime, essentially, we can think about taking\na patient on or off-- and also dealing with\nthe sedatives that are prescribed to them. So in this example, the\nstate that they considered in this application included\nthe demographic information of the patient, which doesn't\nreally change over time. Their physiological\nmeasurements, ventilator settings,\nconsciousness level, the dosages of the\nsedatives they use, which could be an\naction, I suppose--", "id": "YZ5pOgY5hEE_17", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And these are the values that\nwe have to keep track of, moving forward in time. The actions concretely included\nwhether to intubate or extubate the patient, as well\nas the administer and dosing the sedatives. So this is, again, an example\nof a so-called decision process. And essentially, the\nprocess is the distribution of these quantities that I've\nbeen talking about over time. So we have the states, the\nactions, and the rewards. They all traverse or they\nall evolve over time. And the loss of how that\nhappens is the decision process. I mentioned before\nthat we will be talking about policies today. And typically,\nthere's a distinction between what is called a\nbehavior policy and a target policy-- or there are\ndifferent words for this. Essentially, the\nthing that we observe is usually called\na behavior policy.", "id": "YZ5pOgY5hEE_18", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  happening there at\nthe moment, that will be the behavior policy. And I will denote that mu. So that is what we have to\nlearn from, essentially. So decision processes so\nfar are incredibly general. I haven't said anything about\nwhat this distribution is like, but the absolutely\ndominant restriction that people make when they\nstudy system processes is to look at Markov\ndecision processes. And these have a specific\nconditional independent structure that I will illustrate\nin the next slide-- well, I'll just define it\nmathematically here. It says, essentially,\nthat all of the quantities that we care about-- the states. I guess that should say state. Rewards and the actions only\ndepend on the most recent state in action. If we observe an action taken\nby a doctor in the hospital, for example-- to make a mark of\nassumption, we'd say that this\ndoctor did not look", "id": "YZ5pOgY5hEE_19", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  in time or any other\ninformation than what is in the state variable\nthat we observe at that time. That is the assumption\nthat we make. Yeah? AUDIENCE: Is that an assumption\nyou can make for a health care? Because in the end, you don't\nhave access to the real state, but only about what's measured\nabout the state in health care. FREDRIK D. JOHANSSON:\nIt's a very good question. So the nice thing in terms of\ninferring causal quantities is that we only\nneed the things that were used to make the\ndecision in the first place. So the doctor can only act\non such information, too. Unless we don't\nrecord everything that the doctor knows--\nwhich is also the case. So that is something that we\nhave to worry about for sure. Another way to lose\ninformation, as I mentioned, that is relevant for\nthis is if we look to-- What's the opposite of far? AUDIENCE: Near. FREDRIK D. JOHANSSON: Too near\nback in time, essentially.", "id": "YZ5pOgY5hEE_20", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And when I say St\nhere, it doesn't have to be the instantaneous\nsnapshot of a patient. We can also Include\nhistory there. Again, we'll come back\nto that a little later. OK, so the Markov assumption\nessentially looks like this. Or this is how I will\nillustrate, anyway. We have a sequence of states\nhere that evolve over time. I'm allowing myself\nto put some dots here, because I don't want\nto draw forever. But essentially, you could think\nof this pattern repeating-- where the previous state\ngoes into the next state, the action goes\ninto the next state, and the action and state\ngoes in through the reward. This is the world that we\nwill live in for this lecture. Something that's not allowed\nunder the mark of assumption is an edge like this, which says\nthat an action at an early time influences an action\nat a later time. And specifically, it can't\ndo so without passing", "id": "YZ5pOgY5hEE_21", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  It very well can\nhave an influence on At by this trajectory\nhere, but not directly. That that's the Markov\nassumption in this case. So you can see that if I\nwere to draw the graph of all the different\nmeasurements that we see during a state,\nessentially there are a lot of errors that I could\nhave had in this picture that I don't have. So it may seem that the\nMarkov assumption is a very strong one, but one\nway to ensure that the Markov assumption is more likely\nis to include more things in your state, including\nsummaries of the history, et cetera, that I\nmentioned before. An even stronger restriction\nof decision processes is to assume that\nthe states over time are themselves independent. So this goes by\ndifferent names--", "id": "YZ5pOgY5hEE_22", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  But the bandits part of that\nitself is not so relevant here. So let's not go into\nthat name too much. But essentially, what\nwe can say is here, the state at a later time point\nis not influenced directly by the state at a\nprevious time point, nor the action of the\nprevious time point. So if you remember\nwhat you did last week, this looks like\nbasically T repetitions of the very simple\ngraph that we had for estimating\npotential outcomes. And that is indeed\nmathematically equivalent. If we assume that\nthis S here represents the state of a patient\nand all patients are drawn from some sum\nprocess, essentially. So that S0, 1, et cetera,\nup to St are all i.i.d. draws of the same distribution. Then we have,\nessentially, a model for t different patients\nwith a single time step or single action,\ninstead of them", "id": "YZ5pOgY5hEE_23", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So we can see that by going\nbackwards through my slides, this is essentially\nwhat we had last week. And we just have\nto add more arrows to get to whatever we have\nthis week, which indicates that last week was a\nspecial case of this-- just as David said before. It also hints at the\nreinforcement learning problem being more complicated than\nthe potential outcomes problem. And we'll see more\nexamples of that later. But, like with causal\neffect estimation that we did last week, we're\ninterested in the influences of just a few\nvariables, essentially. So last time we studied the\neffect of a single treatment choice. And in this case, we\nwill study the influence of these various actions\nthat we take along the way. That will be the goal. And it could be either\nthrough an immediate effect on the immediate reward or\nit can be through the impact that an action has on the\nstate trajectory itself.", "id": "YZ5pOgY5hEE_24", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  We have these Ss and As and Rs. And I haven't told you so\nmuch about the goal that we're trying to solve or the problem\nthat we're trying to solve. Most RL-- or reinforcement\nand learning-- is aimed at optimizing\nthe value of a policy or finding a policy\nthat has a good return, a good sum of rewards. There are many names for\nthis, but essentially a policy that does well. The notion of well that we\nwill be using in this lecture is that of a return. So the return at a time step\nt, following the policy, pi, that I had before, is the\nsum of the future rewards that we see if we were to\nact according to that policy. So essentially, I stop now. I ask, OK, if I keep on\ndoing the same as I've", "id": "YZ5pOgY5hEE_25", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  maybe that was a good policy. I don't know. And keep going until the end\nof time, how well will I do? What is the sum of those\nrewards that I get, essentially? That's the return. The value is the\nexpectation of such things. So if I'm not the\nonly person, but there is the whole population\nof us, the expectation over that population is\nthe value of the policy. So if we take patients as a\nbetter analogy than my life, maybe, the expectation\nis over patients. If we fact on every patient in\nour population the same way-- according to the same\npolicy, that is-- what is the expected\nreturn over those patients? So as an example, I drew\na few trajectories again, because I like drawing. And we can think about three\ndifferent patients here. They start in different states. And they will have different\naction trajectories as a result. So we're treating them\nwith the same policy. Let's call it pi. But because they're\nin different states, they will have different\nactions at the same times.", "id": "YZ5pOgY5hEE_26", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Here, we take a 0\naction, we go down. That's what that means here. The specifics of this\nis not so important. But what I want you\nto pay attention to is that, after each\naction, we get a reward. And at the end, we can sum\nthose up and that's our return. So each patient has one value\nfor their own trajectory. And the value of the policy\nis then the average value of such trajectories. So that is what we're\ntrying to optimize. We have now a notion of good\nand we want to find a pi such that V pi up there is good. That's the goal. So I think it's time for\na bit of an example here. I want you to play\nalong in a second. You're going to\nsolve this problem. It's not a hard one.", "id": "YZ5pOgY5hEE_27", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  I think you'll be fine. But this is now yet another\nexample of a world to be in. This is the robot in a room. And I've stolen this\nslide from David, who stole it from Peter Bodik. Yeah, so credits to him. The rules of this world\nsays the following-- if you tell the robot, who is\ntraversing this set of tiles here-- if you tell the robot\nto go up, there's a chance he doesn't go up,\nbut goes somewhere else. So we have the stochastic\ntransitions, essentially. If I say up, he\ngoes up with point a probability and somewhere else\nwith uniform probability, say. So 0.8 up and then 0.2-- this is the only\npossible direction to go in if you start here. So 0.2 in that way. There's a chance you move in\nthe wrong direction is what I'm trying to illustrate here. There's no chance\nthat they're going in the opposite direction. So if I say right here,\nit can't go that way.", "id": "YZ5pOgY5hEE_28", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  in the green box up there,\nminus 1 in the box here. And these are also\nterminal states. So I haven't told\nyou what that is, but it's essentially a state\nin which the game ends. So once you get to either plus\n1 or minus 1, the game is over. For each step that\nthe robot takes, it incurs 0.04 negative reward. So that says,\nessentially, that if you keep going for a long time,\nyour reward would be bad. The value of the\npolicy will be bad. So you want to be efficient. So basically, you\ncan figure out-- you want to get to the green\nthing, that's one part of it. But you also want\nto do it quickly. So what I want you to do now\nis to essentially figure out what is the best\npolicy, in terms of in which way should\nthe arrows point in each of these different boxes?", "id": "YZ5pOgY5hEE_29", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  pointing in some direction. We know the different\ntransitions will be stochastic, so you might need to\ntake that into account. But essentially,\nfigure out how do I have a policy that gives me\nthe biggest expected reward? And I'll ask you in a\nfew minutes if one of you is brave enough to put it on the\nboard or something like that. AUDIENCE: We start the\ndiscount over time? FREDRIK D. JOHANSSON:\nThere's no discount. AUDIENCE: Can we\ntalk to our neighbor? FREDRIK D. JOHANSSON: Yes. It's encouraged. [INTERPOSING VOICES] FREDRIK D. JOHANSSON:\nSo I had a question. What is the action space? And essentially, the action\nspace is always up, down, left, or right, depending\non if there's a wall or not. So you can't go right\nhere, for example. AUDIENCE: You can't\ngo left either. FREDRIK D. JOHANSSON: You\ncan't go left, exactly. Good point. So each box at the\nend, when you're done, should contain an arrow\npointing in some direction.", "id": "YZ5pOgY5hEE_30", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  has solved this problem now. Who thinks they have solved it? Great. Would you like to\nshare your solution? AUDIENCE: Yeah, so I think\nit's going to go up first. FREDRIK D. JOHANSSON: I'm going\nto try and replicate this. Ooh, sorry about that. OK, you're saying up here? AUDIENCE: Yeah. The basic idea is you want to\nreduce the chance that you're ever adjacent to the red box. So just do everything you\ncan to stay far from it. Yeah, so attempt\nto go up and then once you eventually get there,\nyou just have to go right. FREDRIK D. JOHANSSON: OK. And then? AUDIENCE: [INAUDIBLE]. FREDRIK D. JOHANSSON: OK. So what about these ones? This is also part of\nthe policy, by the way. AUDIENCE: I hadn't\nthought about this. FREDRIK D. JOHANSSON: OK. AUDIENCE: But those,\nyou [INAUDIBLE],, right? FREDRIK D. JOHANSSON: No.", "id": "YZ5pOgY5hEE_31", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  FREDRIK D. JOHANSSON:\nSo discount usually means something else. We'll get to that later. But that is a reward for\njust taking any step. If you move into a space\nthat is not terminal, you incur that negative reward. AUDIENCE: So if you\nkeep bouncing around for a really long time, you\nincur a long negative reward. FREDRIK D. JOHANSSON:\nIf we had this, there's some chance I'd\nnever get out of all this. And very little chance\nof that working out. But it's a very bad\npolicy, because you keep moving back and forth. All right, we had\nan arm somewhere. What should I do here? AUDIENCE: You could take a vote. FREDRIK D. JOHANSSON: OK. Who thinks right? Really? Who thinks left? OK, interesting. I don't actually remember. Let's see. Go ahead. AUDIENCE: I was just\nsaying, that's an easy one. FREDRIK D. JOHANSSON:\nYeah, so this is the part that we already determined. If we had deterministic\ntransitions, this would be great,\nbecause we don't have to think about the other ones. This is what Peter\nput on the slide.", "id": "YZ5pOgY5hEE_32", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  It depends, actually,\nheavily on the minus 0.04. So if you increase\nthat by a little bit, you might want to\ngo that way instead. Or if you decrease-- I don't remember. Decrease, exactly. And if you increase it, you\nmight get something else. It might actually be\ngood to terminate. So those details\nmatter a little bit. But I think you've\ngot the general idea. And especially I like\nthat you commented that you want to stay\naway from the red one, because if you look at\nthese different paths. You go up there and there-- they have the same\nnumber of states, but there's less chance\nyou end up in the red box if you take the upper route. Great. So we have an\nexample of a policy and we have an example\nof a decision process. And things are\nworking out so far. But how do we do this? As far as the class goes, this\nwas a blackbox experiment. I don't know anything about\nhow you figured that out. So reinforcement\nlearning is about that--", "id": "YZ5pOgY5hEE_33", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  up with a policy in a rigorous\nway, hopefully-- ideally. So that would be\nthe next topic here. Up until this point, are there\nany questions that you've been dying to ask, but haven't? AUDIENCE: I'm curious how\nmuch behavioral biases could play into the first\nMarkov assumption? So for example, if\nyou're a clinician who's been working for\n30 years and you're just really used to giving\na certain treatment. An action that you\ngave in the past-- that habit might influence\nan action in the future. And if that is a\nworry, how one might think about addressing it. FREDRIK D. JOHANSSON:\nInteresting. I guess it depends a little\nbit on how it manifests, in that if it also influenced\nyour most recent action, maybe you have an observation of\nthat already in some sense. It's a very broad question. What effect will that have? Did you have something\nspecific in mind? AUDIENCE: I guess I\nwas just wondering if it violated that assumption,\nthat an action of the past", "id": "YZ5pOgY5hEE_34", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  FREDRIK D. JOHANSSON:\nInteresting. So I guess my response there is\nthat the action didn't really depend on the choice\nof action before, because the policy\nremained the same. You could have a bias towards\nan action without that being dependent on what\nyou gave as action before, if you know what I mean. Say my probability of\ngiving action one is 1, then it doesn't matter\nthat I give it in the past. My policy is still the same. So, not necessarily. It could have\nother consequences. We might have reason to come\nback to that question later. Yup. AUDIENCE: Just\npractically, I would think that a doctor would\nwant to be consistent. And so you wouldn't,\nfor example, want to put somebody\non a ventilator and then immediately take them\noff and then immediately put them back on again. So that would be\nan example where the past action influences\nwhat you're going to do. FREDRIK D. JOHANSSON:\nCompletely, yeah. I think that's a great example. And what you would hope is that\nthe state variable in that case", "id": "YZ5pOgY5hEE_35", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  That's what your job is then. So that's why state can\nbe somewhat misleading as a term-- at least\nfor me, I'm not American or English-speaking. But yeah, I think of it as\ntoo instantaneous sometimes. So we'll move into\nreinforcement learning now. And what I had you do\non the last slide-- well, I don't know which\nmethod you use, but most likely the middle one. There are three very\ncommon paradigms for reinforcement learning. And they are essentially divided\nby what they focus on modeling. Unsurprisingly,\nmodel-based RL focused on-- well, it has some sort\nof model in it, at least. What you mean by\nmodel in this case is a model of the transitions. So what state will I end up in,\ngiven the action in the state I'm in at the moment?", "id": "YZ5pOgY5hEE_36", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  for the environment\nor of the environment. There are several examples\nof model-based RL. One of them is\nG-computation, which comes out of the statistic\nliterature, if you like. And MDPs are essentially-- that's a Markov\ndecision process, which is essentially trying to\nestimate the whole distribution that we talked about before. There are various ups\nand downsides of this. We won't have time to go into\nall of these paradigms today. We will actually focus only\non value-based RL today. Yeah, you can ask me\noffline if you are interested in model-based RL. The rightmost one here\nis policy-based RL, where you essentially\nfocus only on modeling the policy that was used in\nthe data that you observed. And the policy that you want\nto essentially arrive at. So you're optimizing\na policy and you are estimating a policy\nthat was used in the past.", "id": "YZ5pOgY5hEE_37", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and focuses on only\nestimating the return-- that was the G. Or the\nreward function as a function of your actions and states. And it's interesting\nto me that you can pick any of the variables-- A, S, and R-- and model those. And you can arrive at something\nreasonable in reinforcement learning. This one is particularly\ninteresting, because it doesn't\ntry to understand how do you arrive at\na certain return based on the actions in the states? It's just optimize\nthe policy directly. And it has some obvious-- well, not obvious, but it has\nsome downsides, not doing that. OK, anyway, we're going to\nfocus on value-based RL. And the very dominant\ninstantiation of value-based RL is Q-learning. I'm sure you've heard of it. It is what drove the success\nstories that I showed before, the goal in the\nStarCraft and everything. G-estimation is another example\nof this, which, again, has come", "id": "YZ5pOgY5hEE_38", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  But we'll focus on\nQ-learning today. So Q-learning is an example\nof dynamic programming, in some sense. That's how it's\nusually explained. And I just wanted to check--\nhow many have heard the phrase dynamic programming before? OK, great. So I won't go into details of\ndynamic programming in general. But the general idea\nis one of recursion. In this case, you know\nsomething about what is a good terminal state. And then you want to\nfigure out how to get there and how to get to the state\nbefore that and the state before that and so on. That is the recursion\nthat we're talking about. The end state that is the\nbest here is fairly obvious-- that is the plus 1 here. The only way to get there\nis by stopping here first, because you can't move from here\nsince it's a terminal state. Your only bet is that one. And then we can ask what is\nthe best way to get to 3, 1? How do we get to the state\nbefore the best state?", "id": "YZ5pOgY5hEE_39", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And one way from here. And as we got from\nthe audience before, this is a slightly worse\nway to get there then from there, because here we\nhave a possibility of ending up in minus 1. So then we recurse\nfurther and essentially, we end up with something\nlike this that says-- or what I tried to illustrate\nhere is that the green boxes-- I'm sorry for any colorblind\nmembers of the audience, because this was a\npoor choice of mine. Anyway, this bottom\nside here is mostly red and this is mostly green. And you can follow the green\ncolor here, essentially, to get to the best end state. And what I used here\nto color this in is this idea of knowing\nhow good a state is, depending on how good the\nstate after that state is. So I knew that plus 1 is a\ngood end state over there. And that led me to recurse\nbackwards, essentially.", "id": "YZ5pOgY5hEE_40", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  know that that state\nover there is a good one? When we have it\nvisualized in front of us, it's very easy to see. And it's very easy because we\nknow that plus 1 is a terminal state here. It ends there, so those\nare the only states we need to consider in this case. But more in general,\nhow do we learn what is the value of a state? That will be the\npurpose of Q-learning. If we have an idea of\nwhat is a good state, we can always do that recursion\nthat I explained very briefly. You find a state that\nhas the high value and you figure out\nhow to get there. So we're going to have to\ndefine now what I mean by value. I've used that word a\nfew different times. I say recall here, but I\ndon't know if I actually had it on a slide before. So let's just say this is\nthe definition of value", "id": "YZ5pOgY5hEE_41", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  I think I had it on a\nslide before, actually. This is the expected return. Remember, this G\nhere was the sum of rewards going into the\nfuture, starting at time, t. And the value,\nthen, of this state is the expectation\nof such returns. Before, I said that\nthe value of an policy was the expectation\nof returns, period. And the value of a\nstate and the policy is the value of that return\nstarting in a certain state. We can stratify this\nfurther if we like and say that the value of\na state action pair is the expected return,\nstarting in a certain state and taking an action, a. And after that,\nfollowing the policy, pi. This would be the\nso-called Q value of a state-action pair-- s, a.", "id": "YZ5pOgY5hEE_42", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So Q-learning attempts to\nestimate the Q function-- the expected return starting in\na state, s, and taking action, a-- from data. The Q-learning is\nalso associated with a deterministic policy. So the policy and the\nQ function go together in this specific way. If we have a Q\nfunction, Q, that tries to estimate the value of a\npolicy, pi, the pi itself is the arg max according to that\nQ. It sounds a little recursive, but hopefully it will be OK. Maybe it's more obvious\nif we look here. So Q, I said before, was\nthe value of starting an s, taking action, a, and\nthen following policy, pi. This is defined by the\ndecision process itself. The best pi, the best policy, is\nthe one that has the highest Q.", "id": "YZ5pOgY5hEE_43", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Well, that is not what\nwe call Q-star, that is what we call little q-star. Q-star, the best estimate of\nthis, is obviously the thing itself. So if you can find\na good function that assigns a value to\na state-action pair, the best such\nfunction you can get is the one that is\nequal to little q-star. I hope that wasn't\ntoo confusing. I'll show on the next slide\nwhy that might be reasonable. So Q-learning is based\non a general idea from dynamic programming,\nwhich is the so-called Bellman question. There we go. This is an instantiation of\nBellman optimality, which says that the best\nstate-action value function has the\nproperty that it is equal to the immediate reward\nof taking action, a, and state,", "id": "YZ5pOgY5hEE_44", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  value for the next state. So we're going to stare\nat this for a bit, because there's a\nbit here to digest. Remember, q-star assigns a\nvalue to any state action pair. So we have q-star here,\nwe have q-star here. This thing here is supposed\nto represent the value going forward in time after I've\nmade this choice, action, a, and state, s. If I have a good idea of how\ngood it is to take action, a, instead of s, it should\nboth incorporate the immediate reward that I get-- that's RT-- and how good that choice\nwas going forward. So think about mechanical\nventilation, as I said before. If we put a patient on\nmechanical ventilation, we have to do a bunch of\nother things after that. If none of those other things\nlead to a good outcome, this part will be low. Even if the immediate\nreturn is good. So for the optimal q-star,\nthis quantity holds.", "id": "YZ5pOgY5hEE_45", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So the question is how\ndo we find this thing? How do we find q-star? Because q-star is not only\nthe thing that gives you the optimal policy-- it also satisfied this equality. This is not true for\nevery Q function, but it's true for\nthe optimal one. Questions? If you haven't seen this before,\nit might be a little tough to digest. Is the notation clear? Essentially, here\nyou have the state that you are arriving\nat the next time. A prime is the parameter of this\nhere, or the argument to this. You're taking the best\npossible q-star value and then state that you arrive at after. Yeah, go ahead. AUDIENCE: Can you instantiate an\nexample you have on the board? FREDRIK D. JOHANSSON: Yes. Actually, I might do a\nfull example of Q-learning in a second. Yes, I will. I'll get to that example then.", "id": "YZ5pOgY5hEE_46", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  It might take some time,\nbut it could be useful. So where are we? So what I showed you before--\nthe Bellman inequality. We know that this holds\nfor the optimal thing. And if there is a quality\nthat is true at an optimum, one general idea in optimization\nis this so-called fixed point iteration that you can\ndo to arrive there. And that's essentially what\nwe will do to get to a good Q. So a nice thing\nabout Q-learning is that if your states and action\nspaces are small and discrete, you can represent the\nQ function as a table. So all you have to\nkeep track of is, how good is the certain\naction in a certain state? Or all actions in\nall states, rather? So that's what we did here. This is a table. I've described to you the policy\nhere, but what we'll do next is to describe the\nvalue of each action. So you can think of a value of\ntaking the right one, bottom,", "id": "YZ5pOgY5hEE_47", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Those will be the values\nthat we need to consider. And so what Q-learning can\ndo with discrete states is to essentially start\nfrom somewhere, start from some idea of\nwhat Q is-- could be random, could be 0. And then repeat the following\nfixed-point iteration, where you update your\nformer idea of what Q should be, with\nits current value plus essentially a mixture of\nthe immediate reward for taking action, At, in that state,\nand the future reward, as judged by your current\nestimate of the Q function. So we'll do that\nnow in practice. Yeah. AUDIENCE: Throughout\nthis, where are we getting the transition\nprobabilities or the behavior of the game? FREDRIK D. JOHANSSON: So\nthey're not used here, actually. A value-based RL-- I\ndidn't say that explicitly, but they don't rely on knowing\nthe transition probabilities. What you might ask is where\ndo we get the S and the As and the Rs from? And we'll get to that. How do we estimate these?", "id": "YZ5pOgY5hEE_48", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Good question, though. I'm going to throw a\nvery messy slide at you. Here you go. A lot of numbers. So what I've done now here\nis a more exhaustive version of what I put on the board. For each little triangle\nhere represents the Q value for the state-action pair. So this triangle is,\nagain, for the action right if you're in this state. So what I've put on\nthe first slide here is the immediate\nreward of each action. So we know that any step\nwill cost us minus 0.04. So that's why there's\na lot of those here. These white boxes here\nare not possible actions. Up here, you have a\n0.96, because it's 1, which is the immediate\nreward of going right here, minus 0.04. These two are minus 1.04\nfor the same reason--", "id": "YZ5pOgY5hEE_49", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  OK, so that's the first step\nand the second step done. We initialize Qs to be 0. And then we picked these two\nparameters of the problem, alpha and gamma, to be 1. And then we did the first\niteration of Q-learning, where we set the Q to\nbe the old version of Q, which was 0, plus alpha\ntimes this thing here. So Q was 0, that means\nthat this is also 0. So the only thing we need to\nlook at is this thing here. This also is 0, because\nthe Qs for all states were 0, so the only thing\nwe end up with is R. And that's what populated\nthis table here. Next timestep-- I'm\ndoing Q-learning now in a way where I update all\nthe state-action pairs at once. How can I do that? Well, it depends on the question\nI got there, essentially. What data do I observe? Or how do I get to know the\nrewards of the S&A pairs? We'll come back to that.", "id": "YZ5pOgY5hEE_50", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So it's the previous Q\nvalue, which was minus 0.04 for a lot of things, then plus\nthe immediate reward, which was this RT. And I have to keep going. So the dominant thing\nfor the table this time was that the best Q value\nfor almost all of these boxes was minus 0.04. So essentially I will\nadd the immediate reward plus that almost everywhere. What is interesting, though,\nis that here, the best Q value was 0.96. And it will remain so. That means that the best Q\nvalue for the adjacent states-- we look at this max\nhere and get 0.96 out. And then add the\nimmediate reward. Getting to here gives\nme 0.96 minus 0.04 for the immediate reward. And now we can figure out\nwhat will happen next.", "id": "YZ5pOgY5hEE_51", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  from the plus 1. I don't think we should\ngo through all of this, but you get a\nsense, essentially, how information is moved\nfrom the plus 1 and away. And I'm sure that's\nhow you solved it yourself, in your head. But this makes it clear\nwhy you can do that, even if you don't know where\nthe terminal states are or where the value of the\nstate-action pairs are. AUDIENCE: Doesn't\nthis calculation assume that if you want to\nmove in a certain direction, you will move in that direction? FREDRIK D. JOHANSSON: Yes. Sorry. Thanks for reminding me. That should have been\nin the slide, yes. Thank you. I'm going to skip\nthe rest of this. I hope you forgive me. We can talk more about it later. Thanks for reminding\nme, Pete, there, that one of the things\nI exploited here", "id": "YZ5pOgY5hEE_52", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Another thing that I\nrelied very heavily on here is that I can represent\nthis Q function as a table. I drew all these boxes and\nI filled the numbers in. That's easy enough. But what if I have thousands\nof states and thousands of actions? That's a large table. And not only is it a large\ntable for me to keep in memory-- it's also very bad\nfor me statistically. If I want to observe anything\nabout a state-action pair, I have to do that\naction in that state. And if you think about treating\npatients in a hospital, you're not going to try\neverything in every state, usually. You're also not going to have\ninfinite numbers of patients. So how do you figure out\nwhat is the immediate reward of taking a certain\naction in a certain state? And this is where a function\napproximation comes in. Essentially, if you can't\nrepresent your data set table, either for statistical\nreasons or for memory reasons, let's say, you might want to\napproximate the Q function", "id": "YZ5pOgY5hEE_53", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And this is exactly\nwhat we can do. So we can draw now an analogy\nto what we did last week. I'm going to come back\nto this, but essentially instead of doing this\nfixed-point iteration that we did before, we will try and\nlook for a function Q theta that is equal to R plus gamma max Q. Remember before, we had\nthe Bellman inequality? We said that q-star S,\nA is equal to R S, A, let's say, plus gamma max A\nprime q star S prime A prime, where S prime is the state\nwe get to after taking action", "id": "YZ5pOgY5hEE_54", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  is to take this equality and\nmake it instead a loss function on the violation\nof this equality. So by minimizing\nthis quantity, I will find something\nthat has approximately the Bellman equality that\nwe talked about before. This is the idea of\nfitted Q-learning, where you substitute the\ntabular representation with the function\napproximations, essentially. So just to make this\na bit more concrete, we can think about\nthe case where we have only a single step. There's only a single\naction to make, which means that there is no\nfuture part of this equation here. This part goes away,\nbecause there's only one stage in our trajectory. So we have only the\nimmediate reward. We have only the Q function. Now, this is exactly a\nregression equation in the way that you've seen it when\nestimating potential outcomes. RT here represents\nthe outcome of doing", "id": "YZ5pOgY5hEE_55", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  be our estimate of this RT. Again, I've said this before--\nif we have a single time point in our process,\nthe problem reduces to estimating\npotential outcomes, just the way we\nsaw it last time. We have curves that\ncorrespond outcomes under different actions. And we can do\nregression adjustment, trying to find an F such\nthat this quantity is small so that we can model each\ndifferent potential outcomes. And that's exactly what happens\nwith the fitted Q iteration if you have a single\ntimestep, too. So to make it even\nmore concrete, we can say that there's some\ntarget value, G hat, which represents the immediate reward\nand the future rewards that is", "id": "YZ5pOgY5hEE_56", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And we're fitting some\nfunction to that value. So the question\nwe got before was how do I know the\ntransition matrix? How do I get any information\nabout this thing? I say here on the\nslide that, OK, we have some target that's\nR plus future Q values. We have some prediction\nand we have an expectation of our transitions here. But how do I\nevaluate this thing? The transitions I have to\nget from somewhere, right? And another way to say\nthat is what are the inputs and the outputs\nof our regression? Because when we estimate\npotential outcomes, we have a very\nclear idea of this. We know that y, the outcome\nitself, is a target. And the input is\nthe covariates, x. But here, we have a moving\ntarget, because this Q hat,", "id": "YZ5pOgY5hEE_57", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  This is something that\nwe estimate as well. So usually what happens is that\nwe alternate between updating this target, Q, and Q theta. So essentially, we copy Q\ntheta to become our new Q hat and we iterate this somehow. But I still haven't told you how\nto evaluate this expectation. So usually in RL, there are a\nfew different ways to do this. And either depending on where\nyou coming from, essentially, these are varyingly viable. So if we look back\nat this thing here, it relies on having\ntuples of transitions-- the state, the action,\nthe next state, and the reward that I got. So I have to somehow\nobserve those. And I can obtain\nthem in various ways. A very common one when\nit comes to learning to play video\ngames, for example, is that you do something\ncalled on-policy exploration.", "id": "YZ5pOgY5hEE_58", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that you're\ncurrently optimizing. You just play the game\naccording to the policies that you have at the moment. And the analogy in\nhealth care would be that you have some idea\nof how to treat patients and you just do that\nand see what happens. That could be\nproblematic, especially if you've got that policy-- if you randomly initialized\nit or if you got it for some somewhere very suboptimal. A different thing that\nwe're more, perhaps, comfortable with in health\ncare, in a restricted setting, is the idea of a\nrandomized trial, where, instead of trying out some\npolicy that you're currently learning, you decide\non a population where it's OK to flip\na coin, essentially, between different\nactions that you have. The difference between\nthe sequential setting and the one-step\nsetting is that now we have to randomize a\nsequence of actions, which is a little bit unlike\nthe clinical trials that you have seen\nbefore, I think. The last one, which is\nthe most studied one when it comes to\npractice, I would say,", "id": "YZ5pOgY5hEE_59", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  is off-policy evaluation\nor learning, in which case you observe health care\nrecords, for example. You observe registries. You observe some data from\nthe health care system where patients have\nalready been treated and you try to\nextract a good policy based on that information. So that means that you see\nthese transitions between state and action and the next\nstate and the reward. You see that based on\nwhat happened in the past and you have to figure\nout a pattern there that helps you come up with a\ngood action or a good policy. So we'll focus on\nthat one for now. The last part of this talk\nwill be about, essentially, what we have to be\ncareful with when we learn with off-policy data. Any questions up\nuntil this point? Yeah. AUDIENCE: So if\n[INAUDIBLE] getting there for the [INAUDIBLE],, are\nthere any requirements that", "id": "YZ5pOgY5hEE_60", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  we had [INAUDIBLE]\nand cause inference? FREDRIK D. JOHANSSON:\nYeah, I'll get to that on the next set of slides. Thank you. Any other questions about\nthe Q-learning part? A colleague of mine,\nRahul, he said-- or maybe he just paraphrased\nit from someone else. But essentially,\nyou have to see RL 10 times before you get it,\nor something to that effect. I had the same experience. So hopefully you have\nquestions for me after. AUDIENCE: Human\nreinforcement learning. FREDRIK D. JOHANSSON: Exactly. But I think what you should\ntake from the last two sections, if not how to do\nQ-learning in detail, because I glossed\nover a lot of things. You should take with you the\nidea of dynamic programming and figuring out,\nhow can I learn about what's good early on in my\nprocess from what's good late? And the idea of moving\ntowards a good state and not just arriving\nthere immediately. And there are many ways\nto think about that.", "id": "YZ5pOgY5hEE_61", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And again, the set-up here is\nthat we receive trajectories of patient states, actions,\nand rewards from some source. We don't know what these sources\nnecessarily-- well, we probably know what the source is. But we don't know how these\nactions were performed, i.e., we don't know what the\npolicy was that generated these trajectories. And this is the\nsame set-up as when you estimated causal effects\nlast week, to a large extent. We say that the actions\nare drawn, again, according to some behavior\npolicy unknown to us. But we want to\nfigure out what is the value of a new policy, pi. So when I showed\nyou very early on-- I wish I had that slide again. But essentially, a bunch of\npatient trajectories and some return. Patient trajectories,\nsome return. The average of those,\nthat's called a value. If we have trajectories\naccording to a certain policy, that is the value\nof that policy-- the average of these things. But when we have trajectories\naccording to one policy", "id": "YZ5pOgY5hEE_62", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that's the same problem as the\ncovariate adjustment problem that you had last\nweek, essentially. Or the confounding\nproblem, essentially. The trajectories\nthat we draw are biased according to the\npolicy of the clinician that created them. And we want to figure out the\nvalue of a different policy. So it's the same\nas the confounding problem from the last time. And because it is the same as\nthe confounding from last time, we know that this is at\nleast as hard as doing that. We have confounding-- I already\nalluded to variance issues. And you mentioned overlap\nor positivity as well. And in fact, we need to make\nthe same assumptions but even stronger assumptions\nfor this to be possible. These are sufficient conditions. So, under very\ncertain circumstances, you don't need them. I should say, these are\nfairly general assumptions that are still strict-- that's how I should put it. So last time, we\nlooked at something called strong ignorability.", "id": "YZ5pOgY5hEE_63", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Can you see in the back? Is that OK? OK, great. So strong ignorability said\nthat the potential outcomes-- Y0 and Y1-- are conditionally\nindependent of the treatment, t, given the set of variables,\nx, or the variable, x. And that's saying that\nit doesn't matter if we know what treatment was given. We can figure out\njust based on x what would happen under\neither treatment arm, where we should treat this patient,\nwith t equals 0, t equals 1. We had an idea of-- or an assumption\nof-- overlap, which says that any treatment\ncould be observed in any state or any context, x. That's what that means. And that is only\nto ensure that we can estimate at least a\nconditional average treatment effect at x. And if we want to estimate\nthe average treatment effect in a population,\nwe would need to have that for every\nx in that population.", "id": "YZ5pOgY5hEE_64", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  is that we need even\nstronger assumptions. There's some notation I\nhaven't introduced here and I apologize for that. But there's a bar here\nover these Ss and As-- I don't know if you can see it. That usually indicates\nin this literature that you're looking at the\nsequence, up to the index here. So all the states up\nuntil t have observed and all the actions\nup until t minus 1. So in order for the best\npolicy to be identifiable-- or the value of a positive\nto be identifiable-- we need this strong condition. So the return of a\npolicy is independent of the current action,\ngiven everything that happened in the past. This is weaker than\nthe Markov assumption, to be clear, because there, we\nsaid that anything that happens in the future is\nconditionally independent,", "id": "YZ5pOgY5hEE_65", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So this is weaker,\nbecause we now just need to observe\nsomething in the history. We need to observe all\nconfounders in the history, in this instance. We don't need to\nsummarize them in S. And we'll get back to\nthis in the next slide. Positivity is the real\ndifficult one, though, because what we're saying\nis that at any point in the trajectory, any action\nshould be possible in order for us to estimate the value\nof any possible policy. And we know that that's not\ngoing to be true in practice. We're not going to consider\nevery possible action at every possible point in\nthe health care setting. There's just no way. So what that tells\nus is that we can't estimate the value of\nevery possible policy. We can only estimate\nthe value of policies that are consistent with\nthe support that we do have. If we never see\naction 4 at time 3, there's no way we can learn\nabout a policy that does that--", "id": "YZ5pOgY5hEE_66", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  That's what I'm trying to say. So in some sense,\nthis is stronger, just because of how\nsequential settings work. It's more about the application\ndomain than anything, I would say. In the next set of\nslides, we'll focus on sequential randomization\nor sequential ignorability, as it's sometimes called. And tomorrow, we'll\ntalk a little bit about the statistics\ninvolved in or resulting from the positivity\nassumption and things like importance\nweighting, et cetera. Did I say tomorrow? I meant Thursday. So last recap on the\npotential outcome story. This is a slide-- I'm not sure if he\nshowed this one, but it's one that we\nused in a lot of talks. And it, again, just serves\nto illustrate the idea of a one-timestep decision. So we have here, Anna. A patient comes in. She has high blood sugar\nand some other properties. And we're debating whether to\ngive her medication A or B.", "id": "YZ5pOgY5hEE_67", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  what would be her blood sugar\nunder these different choices a few months down the line? So I'm just using this\nhere to introduce you to the patient, Anna. And we're going to talk\nabout Anna a little bit more. So treating Anna once, we can\nrepresent as this causal graph that you've seen a\nlot of times now. We had some treatment,\nA, we had some state, S, and some outcome, R. We want to\nfigure out the effect of this A on the outcome, R. Ignorability in\nthis case just says that the potential outcomes\nunder each action, A, is conditionally\nindependent of A, given S. And so we know that\nignorability and overlap is sufficient conditions for\nidentification of this effect. But what happens now if\nwe add another time point? OK, so in this case, if I\nhave no extra arrows here-- I just have completely\nindependent time points-- ignorability\nclearly still holds.", "id": "YZ5pOgY5hEE_68", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to R, et cetera. So ignorability is still fine. If Anna's health status in the\nfuture depends on the actions that I take now, here,\nthen the situation is a little bit different. So this is now not in the\ncompletely independent actions that I make, but\nthe actions here influence the state\nin the future. So we've seen this. This is a Markov decision\nprocess, as you've seen before. This is very likely in practice. Also, if Anna, for\nexample, is diabetic, as we saw in the example\nthat I mentioned, it's likely that\nshe will remain so. This previous state will\ninfluence the future state. These things seem very\nreasonable, right? But now I'm trying to\nargue about the sequential", "id": "YZ5pOgY5hEE_69", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  How can we break that? How can we break\nignorability when it comes to the sequential, say? If you have an action here-- so the outcome at a later point\ndepends on an earlier choice. That might certainly\nbe the case, because we could have a\ndelayed effect of something. So if we measure,\nsay, a lab value which could be in the\nright range or not, it could very well\ndepend on medication we gave a long time ago. And it's also likely\nthat the reward could depend on a state which\nis much earlier, depending on what we include in\nthat state variable. We already have an example,\nI think, from the audience on that. So actually, ignorability should\nhave a big red cross over it, because it doesn't hold there. And it's luckily\non the next slide. Because there are\neven more errors that we can have, conceivably,\nin the medical setting. The example that\nwe got from Pete before was, essentially,\nthat if we've", "id": "YZ5pOgY5hEE_70", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Or if we knew that\nsomething worked previously, we might want to do it again. So if we had a good\nreward here, we might want to do the\nsame thing twice. And this arrow here\nsays that if we know that a patient had\na symptom earlier on, we might want to base\nour actions on it later. We've known that the patient\nhad an allergic reaction at some point, for example. We might not want to use that\nmedication at a later time. AUDIENCE: But you can always\nput everything in a state. FREDRIK D. JOHANSSON: Exactly. So this depends on what\nyou put in the state. This is an example\nwhere I should introduce these arrows to show\nthat, if I haven't got that information here, then\nI introduce this dependence. So if I don't have\nthe information about allergic reaction or\nsome symptom before in here, then I have to do\nsomething else. So exactly that is the point.", "id": "YZ5pOgY5hEE_71", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  if I can compress all\nof these four variables into some variable age\nstandard for the history, then I have ignorability,\nwith respect to that history, H. This is your solution and\nit introduces a new problem, because history is usually\na really large thing. We know that history grows\nwith time, obviously. But usually we don't\nobserve patients for the same number\nof time points. So how do we represent\nthat for a program? How do we represent that\nto a learning algorithm? That's something we\nhave to deal with. You can pad history\nwith 0s, et cetera, but if you keep every\ntimestep and repeat every variable in\nevery timestep, you get a very large object. That might introduce\nstatistical problems, because now you have\nmuch more variance if you have new variables, et cetera. So one thing that\npeople do is that they look some amount of\ntime backwards-- so instead of just looking\nat one timestep back,", "id": "YZ5pOgY5hEE_72", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And your state essentially\ngrows by a factor, k. And another alternative\nis to try and learn a summary function. Learn some function that\nis relevant for predicting the outcome that takes all\nof the history into account, but has a smaller representation\nthan just t times the variables that you have. But this is something that\nneeds to happen, usually. Most health care\ndata, in practice-- you have to make\nchoices about this. I just want to stress that\nthat's something you really can't avoid. The last point I want to make\nis that unobserved confounding is also a problem that\nis not avoidable just due to summarizing history. We can introduce\nnew confounding. That is a problem, if we\ndon't summarize history well. But we can also have\nunobserved confounders, just like we can in\nthe one-step setting. One example is if we have\nan unobserved confounded", "id": "YZ5pOgY5hEE_73", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  It impacts both the\naction at time 1 and the reward at time 1. But of course, now we're\nin the sequential setting. The confounding structure\ncould be much more complicated. We could have a confounder\nthat influences an early action and a late reward. So it might be a\nlittle harder for us to characterize what is the\nset of potential confounders? So I just wanted\nto point that out and to reinforce that\nthis is only harder than the one-step setting. So we're wrapping up now. I just want to end on\na point about the games that we looked at before. One of the big reasons\nthat these algorithms were so successful in\nplaying games was that we have full observability\nin these settings. We know everything from\nthe game board itself-- when it comes to Go, at least. We can debate that when it\ncomes to the video games. But in Go, we have complete\nobservability of the board.", "id": "YZ5pOgY5hEE_74", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  is there at any time point. Not only can we observe\nit through the history, but in the case of Go, you don't\neven need to look at history. We certainly have Markov\ndynamics with respect to the board itself. You don't ever have to remember\nwhat was a move earlier on, unless you want to read into\nyour opponent, I suppose. But that's a game\ntheoretic notion we're not going\nto get into here. But more importantly, we\ncan explore the dynamics of these systems\nalmost limitlessly, just by simulation\nand self-play. And that's true regardless if\nyou have full observability or not-- like in\nStarCraft, you might not have full observability. But you can try your\nthings out endlessly. And contrast that with\nhaving, I don't know, 700 patients with rheumatoid\narthritis or something like that. Those are the samples you have. You're not going\nto get new ones. So that is an amazing\nobstacle for us", "id": "YZ5pOgY5hEE_75", "title": "16. Reinforcement Learning, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  The current\nalgorithms are really inefficient with the\ndata that they use. And that's why this limitless\nexploration or simulation has been so important\nfor these games. And that's also\nwhy the games are the success stories of this. A last point is that\ntypically for these settings that I put here, we have\nno noise, essentially. We get perfect observations\nof actions and states and outcomes and\neverything like that. And that's really true in\nany real-world application. All right. I'm going to wrap up. Tomorrow-- nope,\nThursday, David is going to talk about\nmore explicitly if we want to do this properly\nin health care, what's going to happen? We're going to have a great\ndiscussion, I'm sure, as well. So don't mind the slide. It's Thursday. All right. Thanks a lot.", "id": "YZ5pOgY5hEE_76"}, {"text": "  DAVID SONTAG: A\nthree-part lecture today, and I'm still continuing on\nthe theme of reinforcement learning. Part one, I'm going\nto be speaking, and I'll be following up\non last week's discussion about causal inference\nand Tuesday's discussion on reinforcement learning. And I'll be going into sort\nof one more subtlety that arises there and\nwhere we can develop some nice mathematical\nmethods to help with. And then I'm going\nto turn over the show to Barbra, who I'll formally\nintroduce when the time comes. And she's going to both\ntalk about some of her work on developing and evaluating\ndynamic treatment regimes, and then she will\nlead a discussion on the sepsis paper,\nwhich was required", "id": "zdotUAxiPGM_0", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So those are the three\nparts of today's lecture. So I want you to return\nback, put yourself back in the mindset of\nTuesday's lecture where we talked about\nreinforcement learning. Now, remember that the goal\nof reinforcement learning was to optimize some reward. Specifically, our goal is\nto find some policy, which I can note as pi\nstar, which is the arg max over all possible\npolicies pi of v of pi, where just to\nremind you, v of pi is the value of the policy pi. Formally, it's defined as\nthe expectation of the sum of the rewards across time.", "id": "zdotUAxiPGM_1", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  an expectation with like\nthe pi is because there's stochasticity both in the\nenvironment, and possibly pi is going to be a\nstochastic policy. And this is summing\nover the time steps, because this is not just a\nsingle time step problem. But we're going to be\nconsidering interventions across time of the reward\nat each point in time. And that reward function could\neither be at each point in time or you might imagine that\nthis is 0 for all time steps, except for the last time step. So the first question I\nwant us to think about is, well, what are\nthe implications of this as a learning paradigm? If we look what's going on\nover here, hidden in my story is also an expectation\nover x, the patient, for example, or\nthe initial state. And so this\nintuitively is saying, let's try to find a policy\nthat has high expected reward, average [INAUDIBLE]\nover all patients.", "id": "zdotUAxiPGM_2", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  the right goal. Can anyone think\nabout a setting where that might not be desirable? Yeah. AUDIENCE: What if the reward\nis the patient living or dying? You don't want it\nto have high ratings like saving two\npatients and [INAUDIBLE] and expect the same [INAUDIBLE]. DAVID SONTAG: So what\nhappens if this reward is something mission critical\nlike a patient dying? You really want to try to\navoid that from happening as much as possible. Of course, there\nare other criteria that we might be\ninterested in as well. And both in Frederick's lecture\non Tuesday and in the readings, we talked about how there might\nbe other aspects about making sure that a patient is not\njust alive but also healthy, which might play into\nyour reward functions. And there might be rewards\nassociated with those. And if you were to\njust, for example, put a positive or\nnegative infinity", "id": "zdotUAxiPGM_3", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  right, because if you did that,\nunfortunately in this world, we're not always going to be\nable to keep patients alive. And so you're going to get\ninto an infeasible optimization problem. So minus infinity\nis not an option. We're going to have to\nput some number to it in this type of approach. But then you're going to start\ntrading off between patients. In some cases, you might\nhave a very high reward for-- there are two\ndifferent solutions that you might\nimagine, one solution where the reward is somewhat\nbalanced across patients and another situation\nwhere you have really small values of\nreward for some patients and a few patients with very\nlarge values and rewards. And both of them could be\nthe same average, obviously. But both are not\nnecessarily equally useful. We might want to say\nthat we prefer to avoid that worst-case situation. So one could imagine\nother ways of formulating this optimization\nproblem, like maybe you", "id": "zdotUAxiPGM_4", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of the average-case reward. Or maybe you want\nto say something about different quartiles. I just wanted to point that\nout, because really that's the starting place for a lot of\nthe work that we're doing here. So now I want us\nto think through, OK, returning back to this goal,\nwe've done our policy iteration or we've done our Q\nlearning, that is, and we get a policy out. And we might now\nwant to know what is the value of that policy? So what is our estimate\nof that quantity? Well, to get that,\none could just try to read it off\nfrom the results of Q learning by just\ncomputing that the pi-- what I'm calling v pi\nhat-- the estimate is just equal to now a\nmaximum over actions a of your Q function\nevaluated at whatever your initial state is and the\noptimal choice of action a.", "id": "zdotUAxiPGM_5", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  might be to ask, well,\nwhat is the expected reward of this policy? And if you remember,\nthe Q learning algorithm is, in essence, a dynamic\nprogramming algorithm working its way from the\nsort of large values of time up to the present. And it is indeed actually\ncomputing this expected value that you're interested in. So you could just read\nit off from the Q values at the very end. But I want to point\nout that here there's an implicit policy built in. So I'm going to compare this\nin just a second to what happens under the causal\ninference scenario. So just a single time\nstep in potential outcomes framework that we're used to. Notice that the value of this\npolicy, the reason why it's a function of pi is\nbecause the value is a function of every\nsubsequent action that you're taking as well. And so now let's\njust compare that", "id": "zdotUAxiPGM_6", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  framework. So there, our starting place-- so now I'm going to turn\nour attention for just one moment from reinforcement\nlearning now back to just causal inference. In reinforcement learning,\nwe talked about policies. How do we find\npolicies to do well in terms of some expected\nreward of this policy? But yet when we were talking\nabout causal inference, we only used words like\naverage treatment effect or conditional average\ntreatment effect, where for example, to estimate\nthe conditional average treatment effect,\nwhat we said is we're going to first learn, if we\nuse a covariate adjustment approach, we learn\nsome function f of x comma t, which\nis intended to be an approximation of the expected\nvalue of your outcome y given", "id": "zdotUAxiPGM_7", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  I'll say y of t. There. So that notation. So the goal of\ncovariate adjustment was to estimate this quantity. And we could use that then\nto try to construct a policy. For example, you could think\nabout the policy pi of x, which simply looks to see is-- we'll say it's 1 if CATE or\nyour estimate of CATE for x is positive and 0 otherwise. Just remind you, the way that\nwe got the estimate of CATE", "id": "zdotUAxiPGM_8", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  at f of x comma 1\nminus f of x comma 0. So if we have a policy-- so now we're going to start\nthinking about policies in the context of\ncausal inference, just like we were doing\nin reinforcement learning. And I want us to think through\nwhat would the analogous value of the policy be? How good is that policy? It could be another\npolicy, but right now I'm assuming I'm just going\nto focus on this policy that I show up here. Well, one approach\nto try to evaluate how good that policy is, is\nexactly analogous to what we", "id": "zdotUAxiPGM_9", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  In essence, what\nwe're going to say is we evaluate the\nquality of the policy by summing over your\nempirical data of pi of xi. So this is going to be 1 if the\npolicy says to give treatment 1 to individual xi. In that case, we say that\nthe value is f of x comma 1. Or if you gave the second-- if the policy would\ngive treatment 0, the value of the policy on\nthat individual is 1 minus pi of x times f of x comma 0. So I'm going to call this\nsort of an empirical estimate", "id": "zdotUAxiPGM_10", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And it's exactly analogous\nto the estimate of v of pie that you would get from a\nreinforcement learning context. But now we're talking\nabout policies explicitly. So let's try to dig\ndown a little bit deeper and think about what\nthis is actually saying. Imagine the story where you\njust have a single covariate x. We'll think about x as being,\nlet's say, the patient's age. And unfortunately there's\njust one color here. But I'll do my best with that. And imagine that the\npotential outcome y0 as a function of\nthe patient's age x", "id": "zdotUAxiPGM_11", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Now imagine that the\nother potential outcome y1 looked like that. So I'll call this the\ny1 potential outcome. Suppose now that the policy\nthat we're defining is this. So we're going to\ngive treatment one if the condition of our\ntreatment effect is positive and 0 otherwise. I want everyone to draw what\nthe value of that policy is on a piece of paper. It's going to be-- I'm sorry-- I want everyone\nto write on a piece of paper what the value of the policy\nwould be for each individual. So it's going to\nbe a function of x. And now I want it to be-- I'm looking for y of pi of x.", "id": "zdotUAxiPGM_12", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And feel free to talk\nto your neighbor. In fact, I encourage you\nto talk to your neighbor. [SIDE CONVERSATION] Just to try to connect\nthis a little bit better to what I have up here, I'm\ngoing to assume that f-- this is f of x1,\nand this is f of x0. All right. Any guesses? What does this plot look like? Someone who hasn't spoken in\nthe last one week and a half, if possible. Yeah? AUDIENCE: Does it take like\nthe max of the functions", "id": "zdotUAxiPGM_13", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  until they intersect\nand then y1 afterward? DAVID SONTAG: So it would\nbe something like this until the intersection point. AUDIENCE: Yeah. DAVID SONTAG: And then\nlike that afterwards. Yeah. That's exactly\nwhat I'm going for. And let's try to\nthink through why is that the value of the policy? Well, here the CATE,\nwhich is looking at a difference between\nthese two lines as negative-- so for every x up to\nthis crossing point, the policy that we've\ndefined over there is going to perform action-- wait. Am I drawing this correctly? Maybe it's actually\nthe opposite, right? This should be doing action one. Here. OK. So here the CATE is negative.", "id": "zdotUAxiPGM_14", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And so the value of the\npolicy is actually this one. [INTERPOSING VOICES] DAVID SONTAG: Oh. Wait. Oh, good. [INAUDIBLE] Because this is the\ngraph I have in my notes. Oh, good. OK. I was getting worried. OK. So it's this action, all the\nway up until you get over here. And then over here, now the\nCATE suddenly becomes positive. And so the action chosen is 1. And so the value of\nthat policy is y1. So one could write this a\nlittle bit differently for-- in the case of just\ntwo policies, and now I'm going to write this in a\nway that it's really clear. In the case of just\ntwo actions, one could write this\nequivalently as an average", "id": "zdotUAxiPGM_15", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and f of x comma 1. And this simplification turning\nthis formula into this formula is making the\nassumption that the pi that we're being evaluated\non is precisely this pi. So this simplification\nis only for that pi. For another policy, which is not\nlooking at CATE or for example, which might threshold\nCATE at a gamma, it wouldn't quite be this. It would be something else. But I've gone a\nstep further here. So what I've shown\nyou right here is not the average value but\nsort of individual values. I have shown you\nthe max function. But what this is\nactually looking at is the expected reward, which\nis now averaging across all x.", "id": "zdotUAxiPGM_16", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and the average reward\nof that policy, what we should be looking\nat is the average of these two functions, which is\nwe'll say something like that. And that value is\nthe expected reward. Now, this all goes to show\nthat the expected reward of this policy is not a\nquantity that we've considered in the previous\nlectures, at least not in the previous lectures\nin causal inference. This is not the same as\nthe average treatment effect, for example. So I've just given you\none way to think through, number one, what is\nthe policy that you might want to derive when\nyou're doing causal inference? And number two, what\nis one way to estimate the value of that\npolicy, which goes", "id": "zdotUAxiPGM_17", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  via covariate adjustment? But we might wonder,\njust like when we talked about in\ncausal inference where I said there are two\napproaches or more than two, but we focused on two,\nusing covariate adjustment and doing inverse\npropensity score weighting, you might wonder is\nthere another approach to this problem all together? Is there an approach\nwhich wouldn't have had to go\nthrough estimating the potential outcomes? And that's what\nI'll spend the rest of this third of the lecture\nfocused talking about. And so to help you\npage this back in, remember that we derived\nin last Thursday's lecture an estimator for the average\ntreatment effect, which was 1 over n times the\nsum over data points that got treatment 1 of yi, the\nobserved outcome for that data", "id": "zdotUAxiPGM_18", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  which I'm just going\nto write as ei. So ei is equal to\nthe probability of observing t equals\n1 given the data point xi minus a sum over data\npoint i such that ti equals 0 of yi divided by 1 minus ei. And by the way, there was\na lot of confusion in class why do I have a 1 over\nn here, a 1 over n here, but right now I just\ntook it out all together, and not 1 over the\nnumber of positive points and 1 over the number\nof 0 data points.", "id": "zdotUAxiPGM_19", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and I posted new slides\nonline after class. So if you're curious about\nthat, go to those slides and look at the derivation. So in a very\nanalogous way now, I'm going to give you\na new estimator for this same quantity\nthat I had over here, the expected reward of a policy. Notice that this estimator here,\nit made sense for any policy. It didn't have to be the\npolicy which looked at, is CATE just greater\nthan 0 or not? This held for any policy. The simplification\nI gave was only in this particular setting. I'm going to give you\nnow another estimator for the average value\nof a policy, which doesn't go through estimating\npotential outcomes at all. Analogous to this is\njust going to make use of the propensity scores. And I'll call it R hat.", "id": "zdotUAxiPGM_20", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  for inverse propensity weighted. And it's a function of\npi, and it's given to you by the following formula-- 1 over n sum over the data\npoints of an indicator function for if the\ntreatment, which was actually given to the i-th\npatient, is equal to what the policy would have done\nbefore the i-th patient. And by the way, here\nI'm assuming that pi is a deterministic function. So the policy says\nfor this patient, you should do this treatment. So we're going to\nlook at just the data points for which the\nobserved treatment is consistent with what\nthe policy would have done for that patient. And this indicator\nfunction is 0 otherwise. And we're going to divide it by\nthe probability of ti given xi.", "id": "zdotUAxiPGM_21", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So this formula will hold for\nnonbinary treatments as well. And that's one of the\nreally nice things about thinking about\npolicies, which is whereas when talking about\naverage treatment effect, average treatment effect\nsort of makes sense in the comparative sense,\ncomparing one to another. But when we talk about\nhow good is a policy, it's not a comparative\nstatement at all. The policy does\nsomething for everyone. You could ask, well, what is the\naverage value of the outcomes that you get for those\nactions that we're taking for those individuals? So that's why I'm writing a\nslightly more general fashion already here. Times yi obviously. So this is now a new estimator. I'm not going to derive\nit for you in class, but the derivation is\nvery similar to what we did last week when we tried\nto drive the average treatment effect. And the critical point is we're\ndividing by that propensity score, just like\nwe did over there.", "id": "zdotUAxiPGM_22", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  you had infinite\ndata, should give you exactly the same\nestimate as this. But here, you're not estimating\npotential outcomes at all. So you never have to try to\nimpute the counterfactuals. Here, all it relies\non knowing is that you have the\npropensity scores for each of the data\npoints in your training set or in a data set. So for example,\nthis opens the door to tons of new\nexciting directions. Imagine that you had a very\nlarge observational data set. And you learned\na policy from it. For example, you might have\ndone covariate adjustment and then said, OK, based\non covariate adjustment, this is my new policy. So you might have gotten\nit via that approach.", "id": "zdotUAxiPGM_23", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Well, suppose that you then\nrun a randomized control trial. And then you run a\nrandomized control trial, you have 100 people, maybe 200\npeople, and so not that many. So not nearly enough\npeople to have actually estimated\nyour policy alone. You might have needed thousands\nor millions of individuals to estimate your policy. Now you're only going to\nhave a couple individuals that you could actually afford\nto do a randomized control trial on. For those people,\nbecause you're flipping a coin for which treatment\nthey're going to get, suppose that were\nin a binary setting where the only two\ntreatments, then this value is always 1/2 1/2. And what I'm giving\nyou here is going to be an unbiased estimate\nof how good that policy is, which one can now estimate using\nthat randomized control trial. Now, this also might\nlead you to think", "id": "zdotUAxiPGM_24", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  the policy through-- rather than obtaining a policy\nthrough the lens of optimizing CATE, of figuring\nhow to estimate CATE, maybe we could have\nskipped that all together. For example, suppose that we had\nthat randomized control trial data. Now imagine that rather\nthan 100 individuals, you had a really large\nrandomized control trial with 10,000 individuals in it. This now opens the door\nto thinking about directly maximizing or minimizing,\ndepending whether you want this to be large or small,\npi with respect to this quantity, which\ncompletely bypasses the goal of estimating the\ncondition of average treatment effect. And you'll notice how\nthis looks exactly like a classification problem.", "id": "zdotUAxiPGM_25", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And the only difference\nis that you're weighting each of\nthe data points by this inverse propensity. So one can reduce the\nproblem of actually finding an optimal policy here to that\nof a weighted classification problem, in the case of a\ndiscrete set of treatments. There are two big caveats\nto that line of thinking. The first major\ncaveat is that you have to know these\npropensity scores. And so if you have data coming\nfrom randomized control trial, you will know this\npropensity scores or if you have, for\nexample, some control over the data\ngeneration process. For example, if you\nare an ad company and you get to choose which\nad to show to your customers,", "id": "zdotUAxiPGM_26", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  you might know what that policy\nwas that was showing things. In that case, you might exactly\nknow the propensity scores. In health care, other than\nin randomized control trials, we typically don't\nknow this value. So we either have to have a\nlarge enough randomized control trial that we won't over-fit\nby trying to directly minimize this or we have to work within\nan observational data setting. But we have to estimate the\npropensity scores directly. So you would then have\na two-step procedure, where first you estimate these\npropensity scores, for example, by doing logistic regression. And then you attempt\nto maximize or minimize this quantity in order to\nfind the optimal policy. And that has a\nlot of challenges, because this quantity\nshown in the very bottom here could be really\nsmall or really large in an observational data set\ndue to these issues of having very small overlap\nbetween your treatments.", "id": "zdotUAxiPGM_27", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that the variant of this\nestimator is very, very large. And so when one wants to\nuse an approach like this, similar to when one wants to\nuse an average treatment effect estimator, and when you're\nestimating these propensities, often you might\nneed to do things like clipping of the\npropensity scores in order to prevent the\nvariants from being too large. That then, however, leads to\na biased estimate typically. I wanted to give you a\ncouple of references here. So one is Swaminathan\nand Joachims, J-O-A-C-H-I-M-S ACML 2015. In that paper, they\ntackle this question. They focus on the setting\nwhere the propensity scores are", "id": "zdotUAxiPGM_28", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And they recognize\nthat you might decide that you prefer something\nlike a biased estimator because of the fact that these\npropensity scores could be really small. And so they use some\ngeneralization results from the machine learning\ntheory community in order to try to control the\nvariants of the estimator as a function of these\npropensity scores. And they then learn,\ndirectly minimize the policy which is what they\ncall counterfactual regret minimization, in\norder to allow one to generalize as\nbest as possible from the small amount of data\nyou might have available. A second reference that\nI want to give just to point you into this\nliterature, if you're interested, is by Nathan\nKallus and his student, I believe Angela Zhou,\nfrom NeurIPS 2018. And that was a paper which was\none of the optional readings for last Thursday's class.", "id": "zdotUAxiPGM_29", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  like this, from\nthis perspective. And they say that,\noh, now that we're working in this\nframework, one could think about what happens\nif you have actually unobserved confounding. So there, you might not actually\nknow the true propensity scores, because there are\nunobserved confounders that you don't observe. And that you can think about\ntrying to bound how wrong your estimator can be as\na function of how much you don't know this quantity. And they show that\nwhen you try to-- if you think about having\nsome backup strategy, like if your goal is to find\na new policy which performs as best as possible with\nrespect to an old policy, then it gives you a\nreally elegant framework for trying to think about a\nrobust optimization of this, even taking into consideration\nthe fact that there might be unobserved confounding. And that works also\nin this framework. So I'm nearly done now.", "id": "zdotUAxiPGM_30", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  can we do the same thing\nfor policies learned by reinforcement learning? So now that we've sort\nof built up this language that's returned\nto the RL setting. And there one can\nshow that you can get a similar estimate\nfor the value of a policy by summing over your\nobserved sequences, summing over the time steps\nof that sequence of the reward observed at that time step times\na ratio of probabilities, which is going from the\nfirst time step up to time little t\nof the probability that you would actually take\nthe observed action t prime, given that you are in the\nobserved state t prime, divided", "id": "zdotUAxiPGM_31", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  this is the analogy\nof the propensity score, the probability under\nthe data generating process-- of seeing action a given that\nyou are in state t prime. So if, as we\ndiscussed there, you had a deterministic\npolicy, then this pi, it would just be\na delta function. And so this would\njust be looking at-- this estimator would\nonly be looking at sequences where the precise\nsequence of actions taken are identical to the\nprecise sequence of actions that the policy\nwould have taken. And the difference here\nis that now instead of having a single\npropensity score, one has a product of these\npropensity scores corresponding to the propensity of\nobserving that action given", "id": "zdotUAxiPGM_32", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And so this is nice,\nbecause this gives you one way to do what's called\noff-policy evaluation. And this is an\nestimator, which is completely analogous\nto the estimator that we got from Q learning. So if all assumptions\nwere correct, and you had a lot of\ndata, then those two should give you precisely\nthe same answer. But here, like in the\ncausal inference setting, we are not making the\nassumption that we can do covariate adjustment well. Or said differently,\nwe're not assuming that we can fit the Q function well. And this is now,\njust like there, based on the assumption\nthat we have the ability to really accurately know what\nthe propensity scores are. So it now gives you an\nalternative approach to do evaluation. And you could\nthink about looking at the robustness\nof your estimates", "id": "zdotUAxiPGM_33", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And this is the most\nnaive of the estimators. There are many ways to try\nto make this better, such as by doing w robust estimators. And if you want to\nlearn more, I recommend reading this paper by Thomas\nand Emma Brunskill in ICML 2016. And with that, I want Barbra\nto come up and get set up. And we're going to transition\nto the next part of the lecture. Yes. AUDIENCE: Why do we sum\nover t and take the project across all t? DAVID SONTAG: One easy\nway to think about this is suppose that you only had a\nreward of the last time step. If you only had a reward\nof the last time step, then you wouldn't\nhave this sum over t, because the rewards in the\nearlier steps would be 0. You would just have that\nproduct going from 0 up to capital T of last time step. The reason why you have\nit up to at each time step", "id": "zdotUAxiPGM_34", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  the likelihood of seeing that\nreward at that point in time. One could rewrite\nthis in other ways. I want to hold other\nquestions, because this part of the lecture is going to\nbe much more interesting than my part of the lecture. And with that, I want\nintroduce Barbra. Barbra, I first met her\nwhen she invited me to give a talk in her class last year. She's an instructor at\nHarvard Medical School-- or School of Public Health. She recently finished\nher PhD in 2018. And her PhD looked\nat many questions related to the themes of\nthe last couple of weeks. Since that time, in addition\ncontinuing her research, she's been really leading the\nway in creating data science curriculum over at Harvard. So please take it away. BARBRA DICKERMAN:\nThank you so much for the introduction, David. I'm very happy to be here\nto share some of my work", "id": "zdotUAxiPGM_35", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  which you've been talking about\nover the past few lectures. So my goals for\ntoday, I'm just going to breeze over defining\ndynamic treatment strategies, as you're already\nfamiliar with it. But I would like\nto touch on when we need a special class of\nmethods called g-methods. And then we'll talk about\ntwo different applications, different analyses, that\nhave focused on evaluating dynamic treatment strategies. So the first will\nbe an application of the parametric\ng-formula, which is a powerful g-method\nto cancer research. And so the goal\nhere is to give you my causal inference\nperspective on how we think about this task of\nsequential decision making and then with\nwhatever time remains, we'll be discussing a recent\npublication on the AI clinician to talk through the\nreinforcement learning perspective. So I think it'll be a really\ninteresting discussion, where", "id": "zdotUAxiPGM_36", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  about the relative strengths\nand limitations as well. And please stop me if\nyou have any questions. So you already know this. When it comes to\ntreatment strategies, there's three main types. There's point\ninterventions happening at a single point in time. There's sustained interventions\nhappening over time. When it comes to clinical\ncare, this is often what we're most interested in. Within that, there\nare static strategies, which are constant over time. And then there's\ndynamic strategies, which we're going to focus on. And these differ in that\nthe intervention over time depends on evolving\ncharacteristics. So for example, initiate\ntreatment at baseline and continue it over follow\nup until a contraindication occurs, at which point\nyou may stop treatment and decide with your\ndoctor whether you're going to switch to an\nalternate treatment. You would still be\nadhering to that strategy, even though you quit. The comparison here being do\nnot initiate treatment over follow up, likewise unless\nan indication occurs,", "id": "zdotUAxiPGM_37", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  be adhering to the strategy. So we're focusing on\nthese because they're the most clinically relevant. And so clinicians encounter\nthese every day in practice. So when they're making\na recommendation to their patient about a\nprevention intervention, they're going to be\ntaking into consideration the patient's evolving\ncomorbidities. Or when they're deciding\nthe next screening interval, they'll consider the previous\nresult from the last screening test when deciding that. Likewise for treatment, deciding\nwhether to keep the patient on treatment or not. Is the patient\nhaving any changes in symptoms or lab values\nthat may reflect toxicity? So one thing to note\nis that while many of the strategies that you\nmay see in clinical guidelines and in clinical practice\nare dynamic strategies, these may not be the\noptimal strategies. So maybe what we're\nrecommending and doing is not optimal for patients. However, the optimal\nstrategies will", "id": "zdotUAxiPGM_38", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  will be adapting to\nindividuals' unique and evolving characteristics. So that's why we\ncare about them. So what's the problem? So one problem\ndeals with something called treatment\nconfounder feedback, which you may have spoken\nabout in this class. So conventional statistical\nmethods cannot appropriately compare dynamic treatment\nstrategies in the presence of treatment\nconfounder feedback. So this is when time\nvarying confounders are affected by previous treatment. So if we kind of ground\nthis in a concrete example with this causal\ndiagram, let's say we're interested in estimating\nthe effect of some intervention A, vasopressors or it could be\nIV fluids, on some outcome Y, which we'll call survival here. We know that vasopressors\naffect blood pressure, and blood pressure will\naffect subsequent decisions", "id": "zdotUAxiPGM_39", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  We also know that\nhypotension-- so again, blood pressure, L1,\naffects survival, based on our clinical knowledge. And then in this DAG, we\nalso have the node U, which represents disease severity. So these could be potentially\nunmeasured markers of disease severity that are\naffecting your blood pressure and also affecting your\nprobability of survival. So if we're interested\nin estimating the effect of a sustained\ntreatment strategy, then we want to know something\nabout the total effect of treatment at all time points. We can see that L1 here is a\nconfounder for the effect of A1 on Y so we have to do\nsomething to adjust for that. And if we were to apply a\nconventional statistical method, we would essentially\nbe conditioning on a collider and inducing a selection bias. So an open path from\nA0 to L1 to U to Y.", "id": "zdotUAxiPGM_40", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  If we look in our\ndata set, we may see an association\nbetween A and Y. But that association is not\nbecause there's necessarily an effect of A on Y.\nIt might not be causal. It may be due to this\nselection bias that we created. So this is the problem. And so in these cases, we\nneed a special type of method that can handle these settings. And so a class of methods\nthat was designed specifically to handle this is g-methods. And so these are sometimes\nreferred to as causal methods. They've been developed by\nJamie Robins and colleagues and collaborators since 1986. And they include the\nparametric g-formula, g-estimation of\nstructural nested models, and inverse\nprobability weighting of marginal structural models. So in my research,\nwhat I do is I combine g-methods with\nlarge longitudinal databases", "id": "zdotUAxiPGM_41", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So I'm particularly interested\nin bringing these methods to cancer research,\nbecause they haven't been applied much there. So a lot of my\nresearch questions are focused on answering\nquestions like, how and when can we intervene to\nbest prevent, detect, and treat cancer? And so I'd like to share\none example with you, which focused on evaluating\nthe effect of adhering to guideline-based\nphysical activity interventions on survival\namong men with prostate cancer. So the motivation\nfor this study, there's a large clinical\norganization, ASCO, the American Society\nof Clinical Oncology, that had actually called\nfor randomized trials to generate these estimates\nfor several cancers. The thing with\nprostate cancer is it's a very slowly\nprogressing disease. So the feasibility of doing\na trial to evaluate this is very limited.", "id": "zdotUAxiPGM_42", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So given that, given the absence\nof this randomized evidence, we did the next\nbest thing that we could do to generate\nthis estimate, which was combine high-quality\nobservational data with advanced EPI methods, in\nthis case parametric g-formula. And so we leveraged data\nfrom the Health Professionals Follow-up Study, which is a\nwell-characterized prospective cohort study. So in these cases, there's\na three-step process that we take to extract the\nmost meaningful and actionable insights from\nobservational data. So the first thing\nthat we do is we specify the protocol\nof the target trial that we would have liked to\nconduct had it been feasible. The second thing we\ndo is we make sure that we measure enough\ncovariates to approximately adjust for confounding\nand achieve conditional exchangeability. And then the third\nthing we do is", "id": "zdotUAxiPGM_43", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  treatment strategies\nunder this assumption of conditional exchangeability. And so in this case,\neligible men for this study had been diagnosed with\nnon-metastatic prostate cancer. And at baseline,\nthey were free of cardiovascular and\nneurologic conditions that may limit physical ability. For the treatment\nstrategies, men were to initiate one of\nsix physical activity strategies at diagnosis and\ncontinue it over followup until the development\nof a condition limiting physical activity. So this is what made\nthe strategies dynamic. The intervention\nover time depended on these evolving conditions. And so just to note,\nwe pre-specified these strategies that\nwe were evaluating as well as the conditions. Men were followed\nuntil diagnosis, until death, and to followup\n10 years after diagnosis or administrative\nend to followup,", "id": "zdotUAxiPGM_44", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Our outcome of interest\nwas all cause mortality within 10 years. And we were interested in\nestimating the per protocol effect of not just\ninitiating these strategies but adhering to\nthem over followup. And again, we applied\nthe parametric g-formula. So I think you've already\nheard about the g-formula in a previous lecture, possibly\nin a slightly different way. So I won't spend too\nmuch time on this. So the g-formula, essentially\nthe way I think about it is a generalization\nof standardization to time varying exposures\nand confounders. So it's basically\na weighted average of risks, where you can\nthink of the weights being the probability density\nfunctions of the time varying confounders, which we estimate\nusing parametric regression models. And we approximate\nthe weighted average using Monte Carlo simulation. So practically\nhow do we do this? So the first thing we do is\nwe fit parametric regression models for all of the\nvariables that we're", "id": "zdotUAxiPGM_45", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So for treatment confounders\nand death at each followup time. The next thing we do is\nMonte Carlo simulation where essentially\nwhat we want to do is simulate the\noutcome distribution under each treatment strategy\nthat we're interested in. And then we bootstrap\nthe confidence intervals. So I'd like to show you\nkind of in a schematic what this looks like,\nbecause it might be a little bit easier to see. So again, the idea\nis we're going to make copies of our data\nset, where in each copy everyone is adhering\nto the strategy that we're focusing\non in that copy. So how do we construct each of\nthese copies of the data set? We have to build them\neach from the ground up, starting with time 0. So the values of all of the time\nvarying covariates at time 0 are sampled from their\nempirical distribution. So these are actually observed\nvalues of the covariates.", "id": "zdotUAxiPGM_46", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  We use the parametric\nregression models that I mentioned that\nwe fit in step 1. Then what we do is we force\nthe level of the intervention variable to be whatever was\nspecified by that intervention strategy. And then we estimate\nthe risk of the outcome at each time period\ngiven these variables, again using the\nparametric regression model for the outcome now. And so we repeat this\nover all time periods to estimate a cumulative risk\nunder that strategy, which is taken as the average of\nthe subject-specific risks. So this is what I'm doing. This is kind of\nunder the hood what's going on with this method. DAVID SONTAG: So\nmaybe we should try to put that in language of\nwhat we saw in the class. And let me know if I'm\ngetting this wrong. So you first estimate the\nmarkup decision process,", "id": "zdotUAxiPGM_47", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  distribution. So you know that probability\nof this sort of next sequence of observations, given the\nprevious sequence and action and previous actions,\nand then with that, then you could then intervene\nand simulate the forms. Because that was,\nif you remember Frederick gave you\nthree different buckets of approaches. Then he focused\non the middle one. This is the left-most bucket. The right? AUDIENCE: Yes. DAVID SONTAG: So we\ndidn't talk about it. AUDIENCE: No, [INAUDIBLE]\nmodel based on relevance. BARBRA DICKERMAN: Yeah. Yes. DAVID SONTAG: But\nit's very sensible. AUDIENCE: Yeah. But it seems very hard. BARBRA DICKERMAN: What's that? AUDIENCE: Sorry. Oh, it seems very hard to\nmodel this [INAUDIBLE].. BARBRA DICKERMAN: Yeah. So that is a challenge. That is the hardest\npart about this. And it's relying on a\nlot of assumptions, yeah. So the primary\nresults that kind of", "id": "zdotUAxiPGM_48", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So this is the estimated\nrisk of all cause mortality under several physical\nactivity interventions. So I'm not going to focus\ntoo much on the results. I want to focus on two main\ntakeaways from this slide. One thing to emphasize\nis we pre-specified the weekly duration\nof physical activity. Or you can think of this like\nthe dose of the intervention. We pre-specified that. And this was based on\ncurrent guidelines. So the third row\nof each band, we did look at some dose or\nlevel beyond the guidelines to see if there might be\nadditional survival benefits. But these were\nall pre-specified. We also pre-specified all of\nthe time varying covariates that made these\nstrategies dynamic. So I mentioned that\nmen were excused from following the\nrecommended physical activity levels if they developed one\nof these listed conditions, metastasis, MI,\nstroke, et cetera. We pre-specified all of those.", "id": "zdotUAxiPGM_49", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  on a different time\nvarying covariate may have led to a\nmore optimal strategy. There was a lot that\nremained unexplored. So we did a lot of\nsensitivity analyses as part of this project. I'd like to focus, though,\non the sensitivity analyses that we did for potential\nunmeasured confounding by chronic disease that\nmay be severe enough to affect both physical\nactivity and survival. And so the g-formula is\nactually providing a natural way to at least partly\naddress this by estimating the risk of these physical\nactivity interventions that are at each time\npoint t only applied to men who are healthy enough\nto maintain a physical activity level at that time. And so again in\nthe main analysis, we excused men from following\nthe recommended levels if they developed one of\nthese serious conditions.", "id": "zdotUAxiPGM_50", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  expanded this list\nof serious conditions to also include the conditions\nthat are shown in blue text. And so this attenuated\nour estimates but didn't change\nour conclusions. One thing to point out is that\nthe validity of this approach rests on the assumption\nthat at each time t we had available data\nneeded to identify which men were healthy\nat that time enough to do the physical activity. Yeah. AUDIENCE: Sorry,\njust to double-check, does excuse mean\nthat you remove them? BARBRA DICKERMAN:\nGreat question. So because the strategy\nwas pre-specified to say that if you develop one\nof these conditions, you may essentially do whatever\nlevel of physical activity you're able to do. So importantly-- I'm glad\nyou brought this up-- we did not censor\nmen at that time. They were still followed,\nbecause they were still adhering to the\nstrategy as defined.", "id": "zdotUAxiPGM_51", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And so given that we don't\nknow whether the data contain at each time t the\ninformation necessary to know, are these men healthy enough\nat that time, we therefore conducted a few alternate\nanalyses in which we lagged physical activity and\ncovariate data by two years. And we also used a\nnegative outcome control to explore potential unmeasured\nconfounding by clinical disease or disease severity. So what's the\nrationale behind this? So in the DAGs below for\nthe original analysis, we have physical activity\nA. We have survival Y. And this may be confounded\nby disease severity U. So when we see an association\nbetween A and Y in our data, we want to make sure\nthat it's causal, that it's because\nof the blue arrow, and not because of\nthis confounding bias, the red arrow. So how can we\npotentially provide evidence for whether that\nred pathway is there?", "id": "zdotUAxiPGM_52", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  as an alternate outcome,\ninstead of survival, that we assumed was not directly\naffected by physical activity, but that we thought would\nbe similarly confounded by disease severity. And so when we\nrepeated the analysis with a negative\noutcome control, we found that physical activity\nhad a nearly null effect on questionnaire nonresponse,\nas we would expect, which provides some support\nthat in our original analysis, the effect of physical\nactivity on death was not confounded through\nthe pathways explored through the negative control. So one thing to highlight\nhere is the sensitivity analyses were driven by our\nsubject matter knowledge. And there's nothing in the\ndata that kind of drove this. And so just to\nrecap this portion. So g-methods are a\nuseful tool, because they let us validly\nestimate the effect", "id": "zdotUAxiPGM_53", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and estimate adjusted absolute\nrisks, which are clinically meaningful to us, and\nappropriately adjusted survival curves, even in the presence\nof treatment confounder feedback, which occurs\noften in clinical questions. And of course, this is under\nour typical identifiability assumptions. So this makes it a\npowerful approach to estimate the effects\nof currently recommended or proposed strategies\nthat therefore we can specify and write out\nprecisely as we did here. However, these\npre-specified strategies may not be the\noptimal strategies. So again, when I was\ndoing this analysis, I was thinking there are so\nmany different weekly durations of physical activity that\nwe're not looking at. There are so many different\ntime-varying covariates where we could have different\ndependencies on those for these strategies over time. And maybe those would have\nled to better survival", "id": "zdotUAxiPGM_54"}, {"text": "  and estimate adjusted absolute\nrisks, which are clinically meaningful to us, and\nappropriately adjusted survival curves, even in the presence\nof treatment confounder feedback, which occurs\noften in clinical questions. And of course, this is under\nour typical identifiability assumptions. So this makes it a\npowerful approach to estimate the effects\nof currently recommended or proposed strategies\nthat therefore we can specify and write out\nprecisely as we did here. However, these\npre-specified strategies may not be the\noptimal strategies. So again, when I was\ndoing this analysis, I was thinking there are so\nmany different weekly durations of physical activity that\nwe're not looking at. There are so many different\ntime-varying covariates where we could have different\ndependencies on those for these strategies over time. And maybe those would have\nled to better survival", "id": "zdotUAxiPGM_54", "title": "17. Reinforcement Learning, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  DAVID SONTAG: So we're done with\nour segment on causal inference and reinforcement learning. And for the next week,\ntoday and Tuesday's lecture, we'll be talking about\ndisease progression modeling and disease subtyping. This is, from my perspective,\na really exciting field. It's one which has really a\nrichness of literature going back to somewhat\nsimple approaches from a couple of decades ago up\nto some really state of the art methods, including\none which is in one of your readings\nfor today's lecture. And I could spent a few weeks\njust talking about this topic. But instead, since we have a lot\nto cover in this course, what I'll do today is give\nyou a high-level overview of one approach to try to\nthink through these questions. The methods in today's lecture\nwill be somewhat simple. They're meant to illustrate\nhow simple methods can", "id": "yYWyLZrdRRI_0", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And they're meant\nto illustrate, also, how one could learn\nsomething really significant about\nclinical outcomes and about predicting\nthese progression from these simple methods. And then in Tuesday's lecture,\nI'll ramp it up quite a bit. And I'll talk about several\nmore elaborate approaches towards this problem,\nwhich tackle some more substantial problems that\nwe'll really elucidate at the end of today's lecture. So there's three\ntypes of questions that we hope to answer when\nstudying disease progression modeling. At a high level, I\nwant you to think about this type of\npicture and have this in the back of your\nhead throughout today and Tuesday's lecture. What you're seeing here is\na single patient's disease trajectory across time. On the x-axis is time. On the y-axis is some\nmeasure of disease burden. So for example, you could\nthink about that y-axis as summarizing the amount of\nsymptoms that a patient is reporting or the amount of pain\nmedication that they're taking,", "id": "yYWyLZrdRRI_1", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And initially, that disease\nburden might be somewhat low, and maybe even the patient's\nin an undiagnosed disease state at that time. As the symptoms get worse\nand worse, at some point the patient might be diagnosed. And that's what I'm\nillustrating by this gray curve. This is the point in\ntime which the patient is diagnosed with their disease. At the time of diagnosis, a\nvariety of things might happen. The patient might\nbegin treatment. And that treatment\nmight, for example, start to influence\nthe disease burden. And you might see a drop in\ndisease burden initially. This is a cancer. Unfortunately, often we'll\nsee recurrences of the cancer. And that might manifest\nby a uphill peak again, where it is burden grows. And once you start\nsecond-line treatment, that might succeed in\nlowering it again and so on. And this might be a cycle that\nrepeats over and over again. For other diseases for which\nhave no cure, for example,", "id": "yYWyLZrdRRI_2", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and we'll talk about\nsome of those-- you might see, even on a\nday-by-day basis, fluctuations. Or you might see nothing\nhappening for a while. And then, for example,\nin autoimmune diseases, you'll see these flare-ups where\nthe disease burden grows a lot, then comes down again. It's really inexplicable\nwhy that happens. So the types of questions that\nwe'd like to really understand here are, first, where is\nthe patient in their disease trajectory? So a patient comes in today. And they might be\ndiagnosed today because of symptoms somehow\ncrossing some threshold and them coming into\nthe doctor's office. But they could be\nsort of anywhere in this disease trajectory\nat the time of diagnosis. And a key question is, can we\nstage patients to understand, for example, things\nlike, how long are they likely to live based on what's\ncurrently going on with them?", "id": "yYWyLZrdRRI_3", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So if you have a patient\nwith kidney disease, you might want to\nknow something about, when will this patient kidney\ndisease need a transplant? Another question is, how will\ntreatment effect that disease progression? That I'm sort of\nhinting at here, when I'm showing these\nvalleys that we conjecture to be affected by treatment. But one often wants to ask\ncounterfactual questions like, what would happen to this\npatient's disease progression if you did one treatment\ntherapy versus another treatment therapy? So the example that I'm\nmentioning here in this slide is a rare blood cancer\nnamed multiple myeloma. It's rare. And so you often\nwon't find data sets with that many patients in them. So for example,\nthis data set which I'm listening in the very bottom\nhere from the Multiple Myeloma Research Foundation\nCoMMpass study has roughly 1,000 patients. And it's a publicly\navailable data set. Any of you can\ndownload it today. And you could study questions\nlike this about disease", "id": "yYWyLZrdRRI_4", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Because you can look at\nlaboratory tests across time. You could look at when\nsymptoms start to rise. You have information about what\ntreatments a patient is on. And you have\noutcomes, like death. So for multiple myeloma,\ntoday's standard for how one would attempt\nto stage a patient looks a little bit like this. Here I'm showing you two\ndifferent staging systems. On the left is a\nDurie-Salmon Staging System, which is a bit older. On the right is what's called\nthe Revised International Staging System. A patient walks into\ntheir oncologist's office newly diagnosed with\nmultiple myeloma. And after doing a\nseries of blood tests, looking at quantities such as\ntheir hemoglobin rates, amount of calcium in the\nblood, also doing, let's say, a biopsy of\nthe patient's bone marrow to measure amounts of different\nkinds of immunoglobulins, doing gene expression\nassays to understand", "id": "yYWyLZrdRRI_5", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that data will then feed into\na staging system like this. So in the Durie-Salmon\nStaging System, a patient who is in\nstage one is found to have a very low\nM-component production rate. So that's what I'm\nshowing over here. And that really corresponds to\nthe amount of disease activity as measured by their\nimmunoglobulins. And since this is\na blood cancer, that's a very good\nmarker of what's going on with the patient. So at sort of this\nmiddle stage, which is called neither stage\none nor stage three, is characterized\nby, in this case-- well, I'm not going\nto talk with that. If you go to stage\nthree for here, you see that the M-component\nlevels are much higher. If you look at X-ray studies\nof the patient's bones, you'll see that there are\nlytic bone lesions, which are caused by the disease\nand really represent an advanced status\nof the disease.", "id": "yYWyLZrdRRI_6", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  the amount of\nlight-chain production, you see that it has much\nlarger values as well. Now, this is an\nolder staging system. In the middle, now I'm showing\nyou a newer staging system, which is both\ndramatically simpler and involves some\nnewer components. So for example, in stage one, it\nlooks at just four quantities. First it looks at\nthe patient's albumin and beta-2 microglobulin levels. Those are biomarkers that can be\neasily measured from the blood. And it says no\nhigh-risk cytogenetics. So now we're starting to\nbring in genetic quantities in terms of quantifying\nrisk levels. Stage three is characterized\nby significantly higher beta-2 microglobulin\nlevels, translocations corresponding to particular\nhigh-risk types of genetics. This will not be the focus\nof the next two lectures, but Pete is going to\ngo much more detail in two genetic aspects\nof precision medicine in a week and a half now. And in this way, each\none of these stages", "id": "yYWyLZrdRRI_7", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of how far along the patient\nis and is really strongly used to guide treatment therapy. So for example, patient\nis in stage one, an oncologist might\ndecide we're not going to treat this patient today. So a different type of question,\nwhereas you could think about this as being one\nof characterizing on a patient-specific level-- one patient walks in. We want to stage that\nspecific patient. And we're going to look\nat some long-term outcomes and look at the\ncorrelation between stage and long-term outcomes. A very different question is\na descriptive-type question. Can we say what will the typical\ntrajectory of this disease look like? So for example, we'll talk\nabout Parkinson's disease for the next couple of minutes. Parkinson's disease is a\nprogressive nervous system disorder.", "id": "yYWyLZrdRRI_8", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Parkinson's affects over 1 in\n100 people, age 60 and above. And like multiple myeloma, there\nis also disease registries that are publicly available and\nthat you could use to study Parkinson's. Now, various\nresearchers have used those data sets in the past. And they've created\nsomething that looks a little bit like\nthis to try to characterize, at now a population\nlevel, what it means for a patient to\nprogress through their disease. So on the x-axis,\nagain, I have time now. On the y-axis, again,\nit denotes some level of disease disability. But what we're\nshowing here now are symptoms that might arise at\ndifferent parts of the disease stage. So very early in\nParkinson's, you might have some sleep behavior\ndisorders, some depression, maybe constipation, anxiety. As the disease gets\nfurther and further along, you'll see symptoms such as mild\ncognitive impairment, increased", "id": "yYWyLZrdRRI_9", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  As the disease goes further on,\nyou'll see things like dementia and an increasing amount\nof psychotic symptoms. And information like\nthis can be extremely valuable for a patient who is\nnewly diagnosed with a disease. They might want to make\nlife decisions like, should they buy this home? Should they stick with\ntheir current job? Can they have a baby? And all of these\nquestions might really be impact-- the answer\nto those questions might be really impacted by\nwhat this patient could expect their life to be like over\nthe next couple of years, over the next 10 years\nor the next 20 years. And so if one could\ncharacterize really well what the disease trajectory\nmight look like, it will be incredibly valuable\nfor guiding those life decisions. But the challenge is that-- this is for Parkinson's. And Parkinson's is\nreasonably well understood. There are a large\nnumber of diseases that are much more rare,\nwhere any one clinician might see a very small number of\npatients in their clinic.", "id": "yYWyLZrdRRI_10", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  are seen in a very noisy fashion\nfor a small number of patients, how to bring that together to\na coherent picture like this is actually very,\nvery challenging. And that's where some\nof the techniques we'll be talking about in\nTuesday's lecture, which talks about how do\nwe infer disease stages, how do we automatically\nalign patients across time, and how do we use very\nnoisy data to do that, will be particularly valuable. But I want to emphasize\none last point regarding this descriptive question. This is not about prediction. This is about understanding,\nwhereas the previous slide was about prognosis,\nwhich is very much a prediction-like question. Now, a different type of\nunderstanding question is that of disease subtyping. Here, again, you might be\ninterested in identifying, for a single patient, are they\nlikely to progress quickly", "id": "yYWyLZrdRRI_11", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Are they likely to progress\nslowly through their disease? Are they likely to\nrespond to treatment? Are they not likely to\nrespond to treatment? But we'd like to be able to\ncharacterize that heterogeneity across the whole\npopulation and summarize it into a small number of subtypes. And you might think about\nthis as redefining disease altogether. So today, we might say patients\nwho have a particular blood abnormality, we will say are\nmultiple myeloma patients. But as we learn more\nand more about cancer, we increasingly understand that,\nin fact, every patient's cancer is very unique. And so over time, we're going\nto be subdividing diseases, and in other cases combining\nthings that we thought were different diseases,\ninto new disease categories. And in doing so it will allow us\nto better take care of patients by, first of all, coming\nup with guidelines that are specific to each\nof these disease subtypes. And it will allow us to\nmake better predictions", "id": "yYWyLZrdRRI_12", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So we can say a patient\nlike this, in subtype A, is likely to have the\nfollowing disease progression. A patient like\nthis, in subtype B, is likely to have a\ndifferent disease progression or be a responder\nor a non-responder. So here's an example of\nsuch a characterization. This is still sticking with\nthe Parkinson's example. This is a paper from a\nneuropsychiatry journal. And it uses a\nclustering-like algorithm, and we'll see many more examples\nof that in today's lecture, to characterize patients\ninto, to group patients into, four different clusters. So let me walk you\nthrough this figure so you see how to interpret it. Parkinson's patients\ncan be measured in terms of a few different axes. You could look at their\nmotor progression. So that is shown here\nin the innermost circle. And you see that\npatients in Cluster 2 seem to have intermediate-level\nmotor progression. Patients in Cluster 1 have very\nfast motor progression, means", "id": "yYWyLZrdRRI_13", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  very quickly over time. One could also look at\nthe response of patients to one of the drugs,\nsuch as levodopa that's used to treat patients. Patients in Cluster\n1 are characterized by having a very poor\nresponse to that drug. Patients in Cluster\n3 are characterized as having intermediate,\npatients in Cluster 2 as having good\nresponse to that drug. Similarly one could look\nat baseline motor symptoms. So at the time the\npatient is diagnosed or comes into the clinic\nfor the first time to manage their\ndisease, you can look at what types of motor-like\nsymptoms do they have. And again, you see different\nheterogeneous aspects to these different clusters. So this is one means-- this is\na very concrete way, of what I mean by trying to\nsubtype patients. So we'll begin our journey\nthrough disease progression modeling by starting out\nwith that first question of prognosis.", "id": "yYWyLZrdRRI_14", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  is really a supervised\nmachine-learning problem. So we can think about prognosis\nfrom the following perspective. Patient walks in at time zero. And you want to know\nsomething about what will that patient's disease\nstatus be like over time. So for example, you\ncould ask, at six months, what is their disease status? And for this patient, it might\nbe, let's say, 6 out of 10. And where these\nnumbers are coming from will become clear\nin a few minutes. 12 months down the line,\ntheir disease status might be 7 out of 10. 18 months, it might\nbe 9 out of 10.", "id": "yYWyLZrdRRI_15", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to tackle for the first\nhalf of today's lecture is this question of,\nhow do we take the data, what I'll call the x vector,\navailable for the patient at baseline and\npredict what will be these values at\ndifferent time points? So you could think about that as\nactually drawing out this curve that I showed you earlier. So what we want to do is\ntake the initial information we have about the\npatient and say, oh, the patient's disease status, or\ntheir disease burden, over time is going to look a\nlittle bit like this. And for a different\npatient, based on their initial covariance,\nyou might say that their disease burden might look like that. So we want to be able to\npredict these curves in this-- for this presentation,\nthere are going to actually be sort of\ndiscrete time points. We want to be able\nto predict that curve from the baseline information\nwe have available. And that will give\nus some idea of how this patient's going to\nprogress through their disease. So in this case\nstudy, we're going", "id": "yYWyLZrdRRI_16", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Here I'm showing you two\nbrains, a healthy brain and a diseased brain, to really\nemphasize how the brain suffers under Alzheimer's disease. We're going to characterize\nthe patient's disease status by a score. And one example of such\na score is shown here. It's called the Mini\nMental State Examination, summarized by the acronym MMSE. And it's going to\nlook as follows. For each of a number of\ndifferent cognitive questions, a test is going to\nbe performed, which-- for example, in the middle,\nwhat it says is registration. The examiner might name three\nobjects like apple, table, penny, and then ask the patient\nto repeat those three objects. All of us should be able to\nremember a sequence of three things so that when we\nfinish the sequence, you should be able\nto remember what the first thing in\nthe sequence was.", "id": "yYWyLZrdRRI_17", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  But as patients get\nincreasingly worse in their Alzheimer's\ndisease, that task becomes very challenging. And so you might give 1.4\ncorrect for each correct. And so if the patient gets all\nthree, if they repeat all three of them, then they\nget three points. If they can't remember\nany of them, zero points. Then you might continue. You might ask something else\nlike subtract 7 from 100, then repeat some\nresults, so some sort of mathematical question. Then you might return back to\nthat original three objects you asked about originally. Now it's been, let's\nsay, a minute later. And you say, what\nwere those three objects I mentioned earlier? And this is trying to get\nat a little bit longer-term memory and so on. And one will then\nadd up the number of points associated with\neach of these responses and get a total score. Here it's out of 30 points. If you divide by 3, you get\nthe story I give you here. So these are the\nscores that I'm talking about for Alzheimer's disease. They're often characterized\nby scores to questionnaires.", "id": "yYWyLZrdRRI_18", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  the disease status\nmight, for example, be inferred automatically\nfrom brain imaging. If you had a smartphone device,\nwhich patients are carrying around with them, and which\nis looking at mobile activity, you might be able to\nautomatically infer their current disease\nstatus from that smartphone. You might be able to infer it\nfrom their typing patterns. You might be able to infer it\nfrom their email or Facebook habits. And so I'm just\ntrying to point out, there are a lot\nof different ways to try to get this number\nof how the patient might be doing at any one point in time. Each of those an\ninteresting question. For now, we're just going\nto assume it's known. So retrospectively,\nyou've gathered this data for patients, which is now\nlongitudinal in nature. You have some\nbaseline information. And you know how\nthe patient is doing over different\nsix-month intervals. And we'd then like to be able\nto predict to those things. Now, if this were--", "id": "yYWyLZrdRRI_19", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  how could we predict\nthese different things? So what are some approaches\nthat you might try? Why don't you talk to your\nneighbor for a second, and then I'll call on a random person. [SIDE CONVERSATION] OK. That's enough. My question was\nsufficiently under-defined that if you talk\nlonger, who knows what you'll be talking about. Over here, the two of you-- the person with the computer. Yeah. How would you\ntackle this problem? AUDIENCE: Me? OK. DAVID SONTAG: No, no, no. Over here, yeah. Yeah, you. AUDIENCE: I would\njust take, I guess, previous data, and then--", "id": "yYWyLZrdRRI_20", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of disease progression over\nthat time span, and then treated [INAUDIBLE]. DAVID SONTAG: But\njust to understand, would you learn five\ndifferent models? So our goal is to get these-- here I'm showing\nyou three, but it might be five different numbers\nat different time points. Would you learn one\nmodel to predict what it would be at\nsix months, another to predict what\nwould be a 12 months? Would you learn a single model? Other ideas? Somewhere over in\nthis part of the room. Yeah. You. AUDIENCE: [INAUDIBLE] DAVID SONTAG: Yeah. Sure. AUDIENCE: [INAUDIBLE] DAVID SONTAG: So use a\nmulti-task learning approach,", "id": "yYWyLZrdRRI_21", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  What was the other thing? AUDIENCE: So you can learn to\nuse these datas in six months and also use that as your\nbaseline [INAUDIBLE].. DAVID SONTAG: Oh, that's\na really interesting idea. OK. So the suggestion was-- so there are two different\nsuggestions, actually. The first suggestion was do a\nmulti-task learning approach, where you attempt\nto learn-- instead of five different and sort\nof independent models, try to learn them\njointly together. And in a second,\nwe'll talk about why it might make sense to do that. The different thought was, well,\nis this really the question you want to solve? For example, you\nmight imagine settings where you have the\npatient not at time zero but actually at six months. And you might want\nto know what's going to happen to\nthem in the future. And so you shouldn't just\nuse the baseline information.", "id": "yYWyLZrdRRI_22", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  you have available for time. And a different way of\nthinking through that is you could imagine learning\na Markov model, where you learn something\nabout the joint distribution of the disease stage over time. And then you could, for\nexample, even if you only had baseline\ninformation available, you could attempt\nto marginalize over the intermediate values\nthat are unobserved to infer what the later values might be. Now, that Markov model approach,\nalthough we will talk about it extensively in the\nnext week or so, it's actually not a very good\napproach for this problem. And the reason why is because\nit increases the complexity. So when you are\nlearn-- in essence if you wanted to predict\nwhat's going on at 18 months, and if, as an intermediate\nstep to predict what goes on at 18 months, you\nhave to predict what's going to go\non at 12 months, and then the likelihood of\ntransitioning from 12 months to 18 months, then\nyou might incur error in trying to predict what's\ngoing on at 12 months. And that error is then\ngoing to propagate", "id": "yYWyLZrdRRI_23", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to 18 months. And that propagation of\nerror, particularly when you don't have\nmuch data, is going to really hurt the [INAUDIBLE]\nof your machine learning algorithm. So the method I'll be\ntalking about today is, in fact, going\nto be what I view as the simplest possible\napproach to this problem. And it's going to be\ndirect prediction approach. So we're directly\ngoing to predict each of the different\ntime points independently. But we will tie\ntogether the parameters of the model, as was suggested,\nusing a multi-task learning approach. And the reason why\nwe're going to want to use a multi-task\nlearning approach is because of data sparsity. So imagine the\nfollowing situation. Imagine that we had just\nbinary indicators here. So let's say patient is\nOK, or they're not OK. So the data might\nlook like this-- 0, 0, 1. Then the data set\nyou might have might look a little bit like this.", "id": "yYWyLZrdRRI_24", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And one row is one patient. Different columns are\ndifferent time points. So the first patient, as I\nshowed you before, is 0, 0, 1. Second patient\nmight be 0, 0, 1, 0. Third patient might\nbe 1, 1, 1, 1. Next patient might\nbe 0, 1, 1, 1. So if you look at the\nfirst time point here, you'll notice that you have\na really imbalanced data set. There's only a single 1\nin that first time point. If you look at the second\ntime point, there are two. It's more of a\nbalanced data set. And then in the\nthird time point, again, you're sort of back\ninto that imbalanced setting. What that means is\nthat if you were to try to learn from just\none of these time points by itself, particularly in the\nsetting where you don't have", "id": "yYWyLZrdRRI_25", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and in outcome label is\ngoing to really hurt you. It's going to be\nvery hard to learn any interesting signal just\nfrom that time point alone. The second problem is that\nthe label is also very noisy. So not only might you\nhave lots of imbalance, but there might be noise in\nthe actual characterizations. Like for this patient,\nmaybe with some probability, you would calculate 1, 1, 1, 1. With some other probability,\nyou would observe 0, 1, 1, 1. And it might correspond\nto some threshold in that score I showed you earlier. And just by chance, a\npatient, on some day, passes the threshold. On the next day, they might\nnot pass that threshold. So there might be a lot of\nnoise in the particular labels at any one time point. And you wouldn't want that noise\nto really dramatically affect your learning algorithm\nbased on some, let's say, prior\nbelief that we might have that there might be\nsome amount of smoothness in this process across time. And the final problem is that\nthere might be censoring.", "id": "yYWyLZrdRRI_26", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  For much later time\npoints, we might have many fewer observations. And so if you were to\njust use those later time points to learn your\npredictive model, you just might not\nhave enough data. So those are all\ndifferent challenges that we're going to\nattempt to solve using a multi-task learning approach. Now, to put some\nnumbers to these things, we have these four\ndifferent time points. We're going to have 648 patients\nat the six-month time interval. And at the four-year\ntime interval, there will only be 87 patients\ndue to patients dropping out of the study. So the key idea here will be,\nrather than learning these five independent models,\nwe're going to try to jointly learn the parameters\ncorresponding to those models. And the intuitions\nthat we're going to try to incorporate\nin doing so are that there might\nbe some features that", "id": "yYWyLZrdRRI_27", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And so I'm using the example of\nbiomarkers here as a feature. Think of that like a\nlaboratory test result, for example, or an answer to\na question that's available baseline. And so one approach\nto learning is to say, OK, let's\nregularize the learning of these different\nmodels to encourage them to choose a common\nset of predictive features or biomarkers. But we also want to allow\nsome amount of flexibility. For example, we might want to\nsay that, well, at any one time point, there might be\ncouple of new biomarkers that are relevant for\npredicting that time point. And there might be some small\namounts of changes across time. So what I'll do right now\nis I'll introduce to you the simplest way to think\nthrough multi-task learning, which-- I will focus specifically\non a linear model setting. And then I'll show you\nhow we can slightly modify this simple approach to\ncapture those criteria that I have over there.", "id": "yYWyLZrdRRI_28", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And let's talk about regression. Because here, in the example\nI showed you earlier, we were trying to\npick the score that's a continuous value number. We want to try to predict it. And we might care about\nminimizing some loss function. So if you were to try to\nminimize a squared loss, imagine a scenario where you\nhad two different prediction problems. So this might be time\npoint 0, and this might be time point 12, for\nsix months and 12 months. You can start by summing\nover the patients, looking at your\nmean squared error at predicting what I'll say\nis the six-month outcome label by some linear\nfunction, which, I'm going to have it as subscript\n6 to denote that this is a linear model for\npredicting the six-month time point value, dot-producted\nwith your baseline features.", "id": "yYWyLZrdRRI_29", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  this one is going be the same. But now you'll be\npredicting the y12 label. And we're going to\nhave a different weight vector for predicting that. Notice that x is the same. Because I'm assuming\nin everything I'm telling you here that\nwe're going to be predicting from baseline data alone. Now, a typical approach and try\nto regularize in this setting might be, let's say, to\ndo L2 regularization. So you might say,\nI'm going to add onto this some lambda times\nthe weight vector 6 squared. Maybe-- same thing over here. So the way that I set this\nup for you so far, right now, is two different independent\nprediction problems. The next step is to\ntalk about how we could try to tie these together.", "id": "yYWyLZrdRRI_30", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  studied multi-task\nlearning in class? So for those of you\nwho did, don't answer. For everyone else,\nwhat are some ways that you might try to\ntie these two prediction problems together? Yeah. AUDIENCE: Maybe you could share\ncertain weight parameters, so if you've got a\ncommon set of biomarkers. DAVID SONTAG: So maybe you could\nshare some weight parameters. Well, I mean, the simplest way\nto tie them together is just to say, we're going to-- so you might say,\nlet's first of all add these two objective\nfunctions together. And now we're\ngoing to minimize-- instead of minimizing just-- now we're going to minimize over\nthe two weight vectors jointly. So now we have a single\noptimization problem. All I've done is I've\nnow-- we're optimizing.", "id": "yYWyLZrdRRI_31", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  where I'm summing this\nobjective with this objective. We're minimizing it with respect\nto now two different weight vectors. And the simplest thing to\ndo what you just described might be to say, let's\nlet W6 equal to W12. So you might just add in\nthis equality constraint saying that these two weight\nvectors should be identical. What would be wrong with that? Someone else, what\nwould be wrong with-- and I know that wasn't\nprecisely your suggestion. So don't worry. AUDIENCE: I have a question. DAVID SONTAG: Yeah. What's your question? AUDIENCE: Is x-- are\nthose also different? DAVID SONTAG: Sorry. Yeah. I'm missing some\nsubscripts, right. So I'll put this in superscript. And I'll put subscript\ni, subscript i. And it doesn't matter for the\npurpose of this presentation whether these are\nthe same individuals or different individuals\nacross these two problems.", "id": "yYWyLZrdRRI_32", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So you might imagine\nthat there are n individuals in the data set. And we're summing\nover the same n people for both of\nthese sums, just looking at different\noutcomes for each of them. This is the six-month outcome. This is the 12-month outcome. Is that clear? All right. So the simplest thing to do\nwould be just to not-- now that we have a joint\noptimization problem, we could constrain the two\nweight vectors to be identical. But of course, this is\na bit of an overkill. This is like saying that\nyou're going to just learn a single prediction problem,\nwhere you sort of ignore the difference between\nsix months and 12 months and just try to predict-- you put those under\nthere and just predict them both together. So you had another\nsuggestion, it sounded like. AUDIENCE: Oh, no. You had just asked\nwhy that was not it. DAVID SONTAG: Oh, OK. And I answered that. Sorry. What could we do differently? Yeah, you. AUDIENCE: You could\nmaybe try to minimize the difference between the two. So I'm not saying that\nthey need to be the same. But the chances that they're\ngoing to be super, super", "id": "yYWyLZrdRRI_33", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  DAVID SONTAG: That's a\nvery interesting idea. So we don't want\nthem to be the same. But I might want them to be\napproximately the same, right? AUDIENCE: Yeah. DAVID SONTAG: And what's\none way to try to measure how different these two are? AUDIENCE: Subtract them. DAVID SONTAG: Subtract\nthem, and then do what? So these are vectors. So you-- AUDIENCE: Absolute value. DAVID SONTAG: So it's not\nabsolute value of a vector. What can you do to turn a\nvector into a single number? AUDIENCE: Take the\nnorm [INAUDIBLE].. DAVID SONTAG: Take a norm of it. Yeah. I think what you meant. So we might take the norm of it. What norm should we take? AUDIENCE: L2? DAVID SONTAG: Maybe the L2 norm. OK. And we might say we want that. So if we said that this was\nequal to 0, then, of course, that's saying that they\nhave to be the same. But we could say that\nthis is, let's say, bounded by some epsilon. And epsilon now is a\nparameter we get to choose. And that would then\nsay, oh, OK, we've now tied together these\ntwo optimization problems.", "id": "yYWyLZrdRRI_34", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that far from each other. Yep? AUDIENCE: You represent\neach weight vector as-- have it just be duplicated\nand force the first place to be the same and the\nsecond ones to be different. DAVID SONTAG: You're suggesting\na slightly different way to parameterize this\nby saying that W12 is equal to W6 plus\nsome delta function, some delta difference. Is that you're suggesting? AUDIENCE: No, that you have\nyour-- say it's n-dimensional, like each vector\nis n-dimensional. But now it's going\nto be 2n-dimensional. And you force the\nfirst n dimensions to be the same on\nthe weight vector. And then the others, you-- DAVID SONTAG: Now, that's\na really interesting idea. I'll return to that\npoint in just a second. Thanks. Before I return to\nthat point, I just want to point out this isn't the\nmost immediate think optimize. Because this is now a\nconstrained optimization problem. What's our favorite algorithm\nfor convex optimization in machine learning, and\nnon-convex optimization? Everyone say it out loud.", "id": "yYWyLZrdRRI_35", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  DAVID SONTAG: TAs are\nnot supposed to answer. AUDIENCE: Just muttering. DAVID SONTAG:\nNeither are faculty. But I think I\nheard enough of you say stochastic gradient descent. Yes. Good. That's what I was expecting. And well, you could do\nprojected gradient descent. But it's much easier to\njust get rid of this. And so what we're going\nto do is we're just going to put this into\nthe objective function. And one way to do that--\nso one motivation would be to say we're going\nto take the Lagrangian of this inequality. And then that'll bring\nthis into the objective. But you know what? Screw that motivation. Let's just erase this. And I'll just say\nplus something else. So I'll call that lambda 1,\nsome other hyper-parameter, times now W12 minus W6 squared. Now let's look to\nsee what happens. If we were to push this\nlambda 2 to infinity,", "id": "yYWyLZrdRRI_36", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So if lambda 2 is\npushed to infinity, what is the solution of\nW12 with respect to W6? Everyone say it out loud. AUDIENCE: 0. DAVID SONTAG: I said\n\"with respect to.\" So there, 1 minus other is 0. Yes. Good. All right. So it would be forcing\nthem that they be the same. And of course, if\nlambda 2 is smaller, then it's saying we're going\nto allow some flexibility. They don't have to be the same. But we're going to\npenalize their difference by the squared difference\nin their norms. So this is good. And so you raised a really\ninteresting question, which I'll talk about now,\nwhich is, well, maybe you don't want to enforce all of\nthe dimensions to be the same. Maybe that's too much. So one thing one could\nimagine doing is saying, we're going to only enforce\nthis constraint for-- [INAUDIBLE] we're only\ngoing to put this penalty in for, let's say, dimensions--", "id": "yYWyLZrdRRI_37", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  I think I'll use this notation. Let's see if you guys like this. Let's see if this notation\nmakes sense for you. What I'm saying is I'm\ngoing to take the-- d is the dimension. I'm going to take the first half\nof the dimensions to the end. I'm going to take that vector\nand I'll penalize that. So it's ignoring the first\nhalf of the dimensions. And so what that's\nsaying is, well, we're going to share parameters for\nsome of this weight vector. But we're not going\nto worry about-- we're going to let them be\ncompletely dependent of each other for the rest. That's an example of\nwhat you're suggesting. So this is all great and dandy\nfor the case of just two time points. But what do we do if then\nwe have five time points? Yeah? AUDIENCE: There's some\npercentage of shared entries", "id": "yYWyLZrdRRI_38", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So instead of saying these\nhave to be in common, you say, treat all\nof them [INAUDIBLE].. DAVID SONTAG: I think you\nhave the right intuition. But I don't really know\nhow to formalize that just from your verbal description. What would be the simplest\nthing you might think of? I gave you an example of\nhow to do, in some sense, pairwise similarity. Could you just easily\nextend that if you have more than two things? You have idea? Nope? AUDIENCE: [INAUDIBLE] DAVID SONTAG: Yeah. AUDIENCE: And then I'd\nget y1's similar to y2, and y2 [INAUDIBLE] y3. And so I might just-- DAVID SONTAG: So\nyou might say w1 is similar to w2.\nw2 is similar to w3. w3 is similar to w4 and so on. Yeah. I like that idea. I'm going to generalize\nthat just a little bit. So I'm going to start\nthinking now about graphs. And we're going to now define a\nvery simple abstraction to talk about multi-task learning.", "id": "yYWyLZrdRRI_39", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and an edge between\ntasks, between nodes, if those two tasks, we want\nto encourage their weights to be similar to another. So what are our tasks here? W6, W12. So in what you're\nsuggesting, you would have the following graph. W6 goes to W12 goes to W24\ngoes to W36 goes to W48. Now, the way that we're\ngoing to transform a graph into an\noptimization problem is going to be as follows. I'm going to now suppose\nthat I'm going to let-- I'm going to define a graph\non V comma E. V, in this case, is going to be the set\n6, 12, 24, and so on.", "id": "yYWyLZrdRRI_40", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And E is going to refer\nto a particular two tasks. So for example, the task of\nsix, predicting at six months, and the task of\npredicting at 12 months. Then what we'll do is we'll\nsay that the new optimization problem is going to\nbe a sum over all of the tasks of the loss\nfunction for that task. So I'm going to ignore what is. I'm just going to simply write-- over there, I have two\ndifferent loss functions for two different tasks. I'm just going to\nadd those together. I'm just going to leave\nthat in this abstract form. And then I'm going to now sum\nover the edges s comma t in E in this graph that I've just\ndefined of Ws minus Wt squared.", "id": "yYWyLZrdRRI_41", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  there were only two\ntasks, W6 and W12. And we had an edge between them. And we penalized it\nexactly in that way. But in the general\ncase, one could imagine many different solutions. For example, you could\nimagine a solution where you have a complete graph. So you may have\nfour time points. And you might penalize\nevery pair of them to be similar to one another. Or, as was just\nsuggested, you might think that there might be\nsome ordering of the tasks. And you might say\nthat you want that-- instead of a complete\ngraph, you're going to just have a\nchain graph, where, with respect to\nthat ordering, you want every pair of\nthem along the ordering to be close to each other.", "id": "yYWyLZrdRRI_42", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  the most reasonable thing to\ndo in a setting of disease progression modeling. Because, in fact, we\nhave some smoothness type prior in our head\nabout these values. The values should be\nsimilar to one another when they're very\nclose time points. I just want to mention\none other thing, which is that from an\noptimization perspective, if this is what you\nhad wanted to do, there is a much cleaner\nway of doing it. And that's to\nintroduce a dummy node. I wish I had more colors. So one could instead\nintroduce a new weight vector. I'll call it W. I'll just\ncall it W with no subscript. And I'm going to say that\nevery other task is going to be connected to it in that star. So here we've\nintroduced a dummy task. And we're connecting\nevery other task to it. And then, now you'd\nhave a linear number", "id": "yYWyLZrdRRI_43", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  But yet you are not\nmaking any assumption that there exists some ordering\nbetween them in the task. Yep? AUDIENCE: Do you-- DAVID SONTAG: And W is never\nused for prediction ever. It's used during optimization. AUDIENCE: Why do you need a\nW0 instead of just doing it based on like W1? DAVID SONTAG: Well, if\nyou do it based on W1, then it's basically saying\nthat W1 is special in some way. And so everything sort\nof pulled towards it, whereas it's not clear that\nthat's actually the right thing to do. So you'll get different answers. And I'd leave that as\nan exercise for you to try to derive. So this is the\ngeneral idea for how one could do multi-task\nlearning using linear models. And I'll also leave it\nas an exercise for you to think through how you\ncould take the same idea and now apply it to, for\nexample, deep neural networks. And you can believe\nme that these ideas do generalize in the ways that\nyou would expect them to do.", "id": "yYWyLZrdRRI_44", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And so whenever you\nare tasked with-- when you tackle\nproblems like this, and you're in settings\nwhere a linear model might do well, before you believe that\nsomeone's results using a very complicated approach is\ninteresting, you should ask, well, what about the\nsimplest possible multi-task learning approach? So we already\ntalked about one way to try to make\nthe regularization a bit more interesting. For example, we could attempt\nto regularize only some of the features' values\nto be similar to another. In this paper, which was\ntackling this disease progression modeling problem\nfor Alzheimer's, they developed a slightly more\ncomplicated approach, but not too much\nmore complicated, which they call the convex\nfused sparse group lasso.", "id": "yYWyLZrdRRI_45", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  where you're going to\nnow learn a matrix W. And that matrix W is\nprecisely the same notion. You have a different\nweight vector per task. You just stack them\nall up into a matrix. L of W, that's just what I\nmean by the sum of the loss functions. That's the same thing. The first term in the\noptimization problem, lambda 1 times the L1 norm\nof W, is simply saying-- it's exactly like\nthe sparsity penalty that we typically see when\nwe're doing regression. So it's simply saying\nthat we're going to encourage the weights\nacross all of the tasks to be as small as possible. And because it's\nan L1 penalty, it adds the effect of actually\ntrying to encourage sparsity. So it's going to push things\nto zero wherever possible. The second term in this\noptimization problem, this lambda 2 RW squared,\nis also a sparsely penalty.", "id": "yYWyLZrdRRI_46", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  This R matrix, in this\nexample, is shown by this. And this is just one way to\nimplement precisely this idea that I had on the board here. So what this R matrix is\ngoing to say it is it's going to say for-- it's going to have one-- you can have as many\nrows as you have edges. And you're going to have-- for\nthe corresponding task which is S, you have a 1. For the corresponding task\nwhich is T, you have a minus 1. And then if you multiply this\nR matrix by W transpose, what you get is precisely these\ntypes of pair-wise comparisons out, the only difference being\nthat here, instead of using a L2 norm, they penalized\nusing an L1 norm.", "id": "yYWyLZrdRRI_47", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  It's simply an implementation\nof precisely this idea. And that final term is\njust a group lasso penalty. It's nothing really\ninteresting happening there. I just want to comment-- I had forgotten to mention this. The loss term is going to\nbe precisely a squared loss. This F refers to\na Frobenius norm, because we've just\nstacked together all of the different\ntasks into one. And the only interesting\nthing that's happening here is this S, which we're doing\nan element-wise multiplication. What that S is is simply\na masking function. It's saying, if we don't observe\na value at some time point, like, for example, if either\nthis is unknown or censored, then we're just\ngoing to zero it out. So there will not be any loss\nfor that particular element. So that S is just\nthe mask which allows", "id": "yYWyLZrdRRI_48", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  data. So this is the approach used\nin that KDD paper from 2012. And returning now to\nthe Alzheimer's example, they used a pretty simple\nfeature set with 370 features. The first set of features\nwere derived from MRI scans of the patient's brain. In this case, they just derived\nsome pre-established features that characterize the amount\nof white matter and so on. That includes some\ngenetic information, a bunch of cognitive scores. So MMSE was one example\nof an input to this model, at baseline is critical. So there are a number\nof different types of cognitive scores that\nwere collected at baseline, and each one of those makes\nup some feature, and then a number of laboratory\ntests, which I'm just noting as random numbers here. But they have some significance. Now, one of the most interesting\nthings about the results is if you compare the\npredictive performance", "id": "yYWyLZrdRRI_49", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  approach. So here we're showing\ntwo different measures of performance. The first one is some\nnormalized mean squared error. And we want that to\nbe as low as possible. And the second one is\nR, as in R squared. And you want that to\nbe as high as possible. So one would be\nperfect prediction. On this first column here,\nit's showing the results of just using independent\nregressors-- so if instead of tying them together with that\nR matrix, you had R equal to 0, for example. And then in each of\nthe subsequent columns, it shows now learning with\nthis objective function, where we are pumping up increasingly\nhigh this lambda 2 coefficient. So it's going to be asking\nfor more and more similarity across the tasks.", "id": "yYWyLZrdRRI_50", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  you start to get\nimprovements between this multi-task\nlearning approach and the independent regressors. So the average R\nsquared, for example, goes from 0.69 up to 0.77. And you notice how we have 95%\nconfidence intervals here as well. And it seems to be significant. As you pump that\nlambda value larger, although I won't comment about\nthe statistical significance between these columns,\nwe do see a trend, which is that performance gets\nincreasingly better as you encourage them to be\ncloser and closer together. So I don't think I want\nto mention anything else about this result.\nIs there a question? AUDIENCE: Is this\nlike a holdout set? DAVID SONTAG: Ah, thank you. Yes. So this is on a holdout set. Thank you. And that also reminded\nme of one other thing I wanted to mention, which is\ncritical to this story, which is that you see these results\nbecause there's not much data.", "id": "yYWyLZrdRRI_51", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  you would see no difference\nbetween these columns. Or, in fact, if you\nhad a really data set, these results would be worse. As you pump lambda higher,\nthe results will get worse. Because allowing flexibility\namong the different tasks is actually a\nbetter thing if you have enough data for each task. So this is particularly valuable\nin the data-poor regime. When it goes to try to\nanalyze the results in terms of looking at the\nfeature importances as a function of\ntime, so one row here corresponds to the weight\nvector for that time point's predictor. And so here we're just looking\nat four of the time points, four of the five time points. And the columns correspond\nto different features that were used in the predictions. And the colors correspond to\nhow important that feature is to the prediction. You could imagine\nthat being something like the norm of the\ncorresponding weight", "id": "yYWyLZrdRRI_52", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  What you see are some\ninteresting things. First, there are some\nfeatures, such as these, where they're important at\nall different time points. That might be expected. But then there also\nmight be some features that are really important\nfor predicting what's going to happen right\naway but are really not important to predicting\nlonger-term outcomes. And you start to see\nthings like that over here, where you see that, for\nexample, these features are not at all important for predicting\nin the 36th time point but were useful for the\nearlier time points. So from here, now\nwe're going to start changing gears a little bit. What I just gave\nyou is an example of a supervised approach. Is there a question? AUDIENCE: Yes. If a faculty member\nmay ask this question. DAVID SONTAG: Yes. I'll permit it today. AUDIENCE: Thank you. So it's really two questions.", "id": "yYWyLZrdRRI_53", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  better than the\nfully coupled model. Because it seems more\nintuitively plausible to-- DAVID SONTAG: And indeed,\nit's the linear model which is used in this paper. AUDIENCE: Ah, OK. DAVID SONTAG: Yes. Because you noticed how that\nR was sort of diagonal in-- AUDIENCE: So it's-- OK. The other observation is that,\nin particular in Alzheimer's, given our current state\nof inability to treat it, it never gets better. And yet that's not\nconstrained in the model. And I wonder if it\nwould help to know that. DAVID SONTAG: I think that's\na really interesting point. So what Pete's\nsuggesting is that you could think about this as-- you could think about putting\nan additional constraint in,", "id": "yYWyLZrdRRI_54", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  say, yi6 is typically less\nthan yi12, which is typically less than yi24 and so on. And if we were able to do\nperfect prediction, meaning if it were the case\nthat your predicted y's are equal to your\ntrue y's, then you should also have that W6 dot xi\nis less than W12 dot xi, which", "id": "yYWyLZrdRRI_55", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And so one could imagine\nnow introducing these as new constraints in\nyour learning problem. In some sense, what\nit's saying is, well, we may not\ncare that much if we get some errors in\nthe predictions, but we want to make\nsure that at least we're able to sort the\npatients correctly, a given patient correctly. So we want to ensure at\nleast some monotonicity in these values. And one could easily\ntry to translate these types of constraints\ninto a modification to your learning algorithm. For example, if you\ntook any pair of these-- let's say, I'll take\nthese two together. One could introduce\nsomething like a hinge loss,", "id": "yYWyLZrdRRI_56", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  you're going to add a new\nobjective function, which says something\nlike, you're going to penalize the max\nof 0 and 1 minus-- and I'm going to\nscrew up this order. But it will be\nsomething like W-- so I'll derive it correctly. So this would be W12 minus\nW24 dot product with xi, we want to be less than 0. And so you could look\nat how far from 0 is it. So you could look at W12-- do, do, do. You might imagine\na loss function which says, OK, if it's greater\nthan 0, then you have problem. And we might penalize it at,\nlet's say, a linear penalty", "id": "yYWyLZrdRRI_57", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And if it's less than 0,\nyou don't penalties at all. So you say something like\nthis, max of W12 minus W24 dot product xi. And you might add\nsomething like this to your learning objective. That would try to encourage--\nthat would penalize violations of this constraint using a\nhinge loss-type loss function. So that would be\none approach to try to put such constraints into\nyour learning objective. A very different\napproach would be to think about it as a\nstructured prediction problem, where instead of\ntrying to say that you're going to be predicting a\ngiven time point by itself, you want to predict the\nvector of time points. And there's a whole\nfield of what's called structured prediction,\nwhich would allow one to formalize objective\nfunctions that might encourage, for example, smoothness\nin predictions across time that one could\ntake advantage of. But I'm not going to go more\ninto that for reasons of time.", "id": "yYWyLZrdRRI_58", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Because I want to make sure I\nget through this last piece. So what we've\ntalked about so far is a supervised\nlearning approach to trying to predict what's\ngoing to happen to a patient given what you know at baseline. But I'm now going to talk\nabout a very different style of thought, which is using\nan unsupervised learning approach to this. And there are going\nto be two goals of doing unsupervised learning\nfor tackling this problem. The first goal is\nthat of discovery, which I mentioned at the very\nbeginning of today's lecture. We might not just be\ninterested in prediction. We might also be interested\nin understanding something, getting some new insights\nabout the disease, like discovering\nthat there might be some subtypes of the disease. And those subtypes might\nbe useful, for example, to help design new\nclinical trials. Like maybe you want to\nsay, OK, we conjecture", "id": "yYWyLZrdRRI_59", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to treatment. So we're only going to\nrun the clinical trial for patients in this subtype,\nnot in the other one. It might be useful, also,\nto try to better understand the disease mechanism. So if you find that\nthere are some people who seem to progress very\nquickly through their disease and other people who seem\nto progress very slowly, you might then go back and do\nnew biological assays on them to try to understand what\ndifferentiates those two clusters. So the two clusters\nare differentiated in terms of their\nphenotype, but you want to go back\nand ask, well, what is different about their\ngenotype that differentiates them? And it might also be useful to\nhave a very concise description of what differentiates\npatients in order to actually have policies\nthat you can implement. So rather than\nhaving what might be a very complicated linear\nmodel, or even non-linear model, for predicting future\ndisease progression, it would be much easier\nif you could just say, OK, for patients who have\nthis biomarker abnormal,", "id": "yYWyLZrdRRI_60", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Patients who are likely have\nthis other biomarker abnormal, they're likely to have a\nslow disease progression. And so we'd like to\nbe able to do that. That's what I mean by\ndiscovering disease subtypes. But there's actually a second\ngoal as well, which-- remember, think back to that original\nmotivation I mentioned earlier of having very little data. If you have very little\ndata, which is unfortunately the setting that we're\nalmost always in when doing machine learning\nin health care, then you can overfit\nreally easily to your data when just using it strictly\nwithin a discriminative learning framework. And so if one were to now\nchange your optimization problem altogether to start to bring in\nan unsupervised loss function, then one can hope\nto get much more out of the limited data you\nhave and save the labels, which you might overfit\non very easily, for the very last step of\nyour learning algorithm. And that's exactly what we'll do\nin this segment of the lecture.", "id": "yYWyLZrdRRI_61", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  about the simplest possible\nunsupervised learning algorithm. And because the official\nprerequisite for this course was 6036, and because clustering\nwas not discussed in 6036, I'll spend just\ntwo minutes talking about clustering using the\nsimplest algorithm called K-means, which I hope\nalmost all of you know. But this will just\nbe a simple reminder. How many clusters are\nthere in in this figure that I'm showing over here? Let's raise some hands. One cluster? Two clusters? Three clusters? Four clusters? Five clusters? OK. And are these red points\nmore or less showing where those five clusters are? No. No, they're not. So rather there's\na cluster here. There's a cluster here,\nthere, there, there. All right. So you were you are able\nto do this really well, as humans, looking at\ntwo dimensional data. The goal of algorithms\nlike K-means is to show how one could\ndo that automatically for high-dimensional data.", "id": "yYWyLZrdRRI_62", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  It works as follows. You hypothesize a\nnumber of clusters. So here we have\nhypothesized five clusters. You're going to\nrandomly initialize those cluster centers,\nwhich I'm denoting by those red points shown here. Then in the first stage\nof the K-means algorithm, you're going to assign every\ndata point to the closest cluster center. And that's going to induce\na Voronoi diagram where every point within\nthis Voronoi cell is closer to this red point\nthan to any other red point. And so every data point\nin this Voronoi cell will then be assigned\nto this data point. Every data point in\nthis Voronoi cell will be assigned to that\ndata point and so on. So we're going to now assign\nall data points to the closest cluster center. And then we're just going\nto average all the data points assigned to\nsome cluster center to get the new cluster center. And you repeat. And you're going to stop this\nprocedure when no point in time is changed. So let's look at\na simple example. Here we're using K equals 2. We just decided there\nare only two clusters.", "id": "yYWyLZrdRRI_63", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  cluster centers, as\nthis red cluster center and this blue cluster center. Notice that they're\nnowhere near the data. We've just randomly chosen. They're nowhere near the data. It's actually pretty\nbad initialization. The first step is going\nto assign data points to their closest cluster center. So I want everyone to say\nout loud either red or green, to which cluster center\nit's going to point to, what it is going to be\nassigned to this step. [INTERPOSING VOICES] AUDIENCE: Red. Blue. Blue. DAVID SONTAG: All right. Good. We get it. So that's the first assignment. Now we're going to average the\ndata points that are assigned to that red cluster center. So we're going to average\nall the red points. And the new red cluster center\nwill be over here, right? AUDIENCE: No. DAVID SONTAG: Oh, over there? Over here? AUDIENCE: Yes. DAVID SONTAG: OK. Good. And the blue cluster center will\nbe somewhere over here, right?", "id": "yYWyLZrdRRI_64", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  DAVID SONTAG: OK. Good. So that's the next step. And then you repeat. So now, again, you\nassign every data point to its closest cluster center. By the way, the\nreason why you're seeing what looks like\na linear hyperplane here is because there are\nexactly two cluster centers. And then you repeat. Blah, blah, blah. And you're done. So in fact, I think\nI've just shown you the convergence point. So that's the K-means algorithm. It's an extremely\nsimple algorithm. And what I'm going to\nshow you for the next 10 minutes of lecture\nis how one could use this very simple clustering\nalgorithm to better understand asthma. So asthma is something\nthat really affects a large number of individuals. It's characterized by having\ndifficulties breathing. It's often managed by\ninhalers, although, as asthma gets more and more severe,\nyou need more and more complex management schemes. And it's been found\nthat 5% to 10% of people who have severe\nasthma remain poorly controlled", "id": "yYWyLZrdRRI_65", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And so a really\nbig question that the pharmaceutical community\nis extremely interested in is, how do we come up with\nbetter therapies for asthma? There's a lot of\nmoney in that problem. I first learned\nabout this problem when a pharmaceutical\ncompany came to me when I was a professor\nat NYU and asked me, could they work with\nme on this problem? I said no at the time. But I still find it interesting. [CHUCKLING] And at that time, the\ncompany pointed me to this paper, which I'll\ntell you about in a second. But before I get there,\nI want to point out what are some of the\nbig picture questions that everyone's interested\nin when it comes to asthma. The first one is to\nreally understand what is it about either genetic\nor environmental factors that underlie different\nsubtypes of asthma. It's observed that\npeople respond differently the therapy. It is observed that some\npeople aren't even controlled", "id": "yYWyLZrdRRI_66", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Why is that? Third, what are\nbiomarkers, what are ways to predict who's\ngoing to respond or not respond to any one therapy? And can we get better\nmechanistic understanding of these different subtypes? And so this was a\nlong-standing question. And in this paper from\nthe American Journal of Respiratory Critical Care\nMedicine, which, by the way, has a huge number\nof citations now-- it's sort of a prototypical\nexample of subtyping. That's why I'm going through it. They started to answer\nthat question using a data-driven\napproach for asthma. And what I'm showing you\nhere is the punch line. This is that main result, the\nmain figure over the paper. They've characterized\nasthma in terms of five different subtypes,\nreally three type. One type, which\nI'll show over here, was sort of inflammation\npredominant;", "id": "yYWyLZrdRRI_67", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  predominant; and another here,\nwhich is sort of concordant disease. And what I'll do over\nthe next few minutes is walk you through\nhow they came up with these different clusters. So they used three\ndifferent data sets. These data sets\nconsisted of patients who had asthma and already had\nat least one recent therapy for asthma. They're all nonsmokers. But they were managed in-- they're three disjoint set\nof patients coming from three different populations. The first group of\npatients were recruited from primary care practices\nin the United Kingdom. All right. So if you're a\npatient with asthma, and your asthma is being managed\nby your primary care doctor, then it's probably not too bad. But if your asthma,\non the other hand, were being managed at a\nrefractory asthma clinic, which is designed specifically for\nhelping patients manage asthma, then your asthma is\nprobably a bit more severe.", "id": "yYWyLZrdRRI_68", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  were from that second cohort\nof patients managed out of an asthma clinic. The third data set is much\nsmaller, only 68 patients. But it's very\nunique because it is coming from a 12-month study,\nwhere it was a clinical trial, and there were two different\ntypes of treatments applied given to these patients. And it was a randomized\ncontrol trial. So the patients were\nrandomized into each of the two arms of the study. I'll describe to you\nwhat the features are on just the next slide. But first I want to\ntell you about how their pre-processes to use\nwithin the K-means algorithm. Continuous-valued features\nwhere z-scored in order to normalize their ranges. And categorical variables\nwere represented just by a one-hot encoding. Some of the continuous\nvariables were furthermore", "id": "yYWyLZrdRRI_69", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  the logarithm of the features. And that's something that\ncan be very useful when doing something like K-means. Because it can, in essence,\nallow for that Euclidean distance function,\nwhich is using K-means, to be more meaningful\nby capturing more of a dynamic\nrange of the feature. So these were the features\nthat went into the clustering algorithm. And there are very, very few,\nso roughly 20, 30 features. They range from the\npatient's gender and age to their body mass index, to\nmeasures of their function, to biomarkers such as\neosinophil count that could be measured from the\npatient's sputum, and more. And there a couple of\nother features that I'll show you later as well. And you could look to see\nhow did these quantities, how", "id": "yYWyLZrdRRI_70", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So on this column, you see\nthe primary care population. You look at all of these\nfeatures in that population. You see that in the\nprimary care population, the individuals are-- on\naverage, 54% percent of them are female. In the secondary care\npopulation, 65% of them are female. You notice that\nthings like-- if you look at to some measures\nof lung function, it's significantly worse in\nthat secondary care population, as one would expect. Because these are patients\nwith more severe asthma. So next, after doing\nK-means clustering, these are the three\nclusters that result. And now I'm showing you\nthe full set of features. So let me first tell\nyou how to read this. This is clusters found in\nthe primary care population.", "id": "yYWyLZrdRRI_71", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of those features across\nthe full population. And then for each one\nof these three clusters, I'm showing you\nthe average value of the corresponding feature\nin just that cluster. And in essence, that's exactly\nthe same as those red points I was showing you\nwhen I describe to you K-means clustering. It's the cluster center. And one could also look\nat the standard deviation of how much variance\nthere is along that feature in that cluster. And that's what the numbers in\nparentheses are telling you. So the first thing to note\nis that in Cluster 1, which the authors of the study named\nEarly Onset Atopic Asthma, these are very young patients,\naverage of 14, 15 years old, as opposed to Cluster 2,\nwhere the average age was 35 years old-- so a\ndramatic difference there.", "id": "yYWyLZrdRRI_72", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to the hospital recently. So most of these patients\nhave been to the hospital. On average, these patients have\nbeen to hospital at least once recently. And furthermore, they've had\nsevere asthma exacerbations in the past 12 months, at least,\non average, twice per patient. And those are very large\nnumbers relative to what you see in these other clusters. So that's really\ndescribing something that's very unusual about these\nvery young patients with pretty severe asthma. Yep? AUDIENCE: What is the\np-value [INAUDIBLE]?? DAVID SONTAG: Yeah. I think the p-value-- I don't know if this is\na pair-wise comparison. I don't remember off\nthe top of my head. But it's really looking at the\ndifference between, let's say-- I don't know which of these cl-- I don't know if it's\ncomparing two of them or not. But let's say, for\nexample, it might be looking at the difference\nbetween this and that.", "id": "yYWyLZrdRRI_73", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  I don't remember. Cluster 2, one other hand,\nwas predominately female. So 81% of the patients\nwere female there. And they were\nlargely overweight. So their average body mass\nindex was 36, as opposed to the other two clusters,\nwhere the average body mass index was 26. And Cluster 3 consisted of\npatients who really have not had that severe asthma. So the average number of\nprevious hospital admissions and asthma exacerbations\nwas dramatically smaller than in the other two clusters. So this is the result\nof the finding. And then you might\nask, well, how does that generalize to\nthe other two populations? So they then went to the\nsecondary care population. And they reran the clustering\nalgorithm from scratch. And this is a completely\ndisjoint set of patients. And what they found,\nwhat they got out,", "id": "yYWyLZrdRRI_74", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  resembled Clusters 1 and\n2 from the previous study on the primary care population. But because this is a different\npopulation with much more severe patients, that\nthird cluster earlier of benign asthma doesn't show\nup in this new population. And there are two\nnew clusters that show up in this new population. So the fact that those\nfirst two clusters were consistent across two\nvery different populations gave the authors\nconfidence that there might be something real here. And then they went and they\nexplored that third population, where they had\nlongitudinal data. And that third population\nthey were then using to ask, does it not-- so\nup until now, we've only used baseline information. But now we're going to ask\nthe following question. If we took the baseline\ndata from those 68 patients and we were to separate them\ninto three different clusters based on the characterizations\nfound in the other two data sets, and then if\nwe were to look at long-term outcomes\nfor each cluster,", "id": "yYWyLZrdRRI_75", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And in particular,\nhere we actually looked at not just predicting\na progression, but we're also looking\nat prediction-- we're looking at differences\nin treatment response. Because this was a\nrandomized-control trial. And so there are going to\nbe two arms here, what's called the clinical arm, which\nis the standard clinical care, and what's called the\nsputum arm, which consists of doing regular monitoring\nof the airway inflammation, and then tight trading\nsteroid therapy in order to maintain\nnormal eosinophil counts. And so this is comparing two\ndifferent treatment strategies. And the question is, do these\ntwo treatment strategies result in differential outcomes? So when the clinical trial was\noriginally performed and they computed the average treatment\neffect, which, by the way, because the RCT was\nparticularly simple-- you just averaged outcomes\nacross the two arms-- they found that there was no\ndifference across the two arms. So there was no difference\nin outcomes across the two different therapies.", "id": "yYWyLZrdRRI_76", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  is they're going\nto rerun the study. And they're going to now,\ninstead of just looking at the average treatment effect\nfor the whole population, they're going to use-- they're going to look at\nthe average treatment each of the clusters by themselves. And the hope there\nis that one might be able to see now a\ndifference, maybe that there was heterogeneous\ntreatment response and sometimes that therapy\nworked for some people and not for others. And these were the results. So indeed, across\nthese three clusters, we see actually a\nvery big difference. So if you look\nhere, for example, the number of commenced\non oral corticosteroids, which is a measure\nof an outcome-- so you might want this to-- I can't remember,\nsmall or large. But there was a big difference\nbetween these two clusters. And this cluster, the number\ncommenced under the first arm is two; in this other\ncluster for patients who got the second arm, nine;\nand exactly the opposite", "id": "yYWyLZrdRRI_77", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  The first cluster, by the way,\nhad only three patients in it. So I'm not going to make\nany comment about it. Now, since these go in\ncompletely opposite directions, it's not surprising that\nthe average treatment effect across the whole\npopulation was zero. But what we're seeing now\nis that, in fact, there is a difference. And so it's possible\nthat the therapy is actually effective but just\nfor a smaller number of people. Now, this study would've never\nbeen possible had we not done this clustering beforehand. Because it has so few\npatients, only 68 patients. If you attempted to both\nsearch for the clustering at the same time\nas, let's say, find clusters to\ndifferentiate outcomes, you would overfit the\ndata very quickly. So it's precisely because we\ndid this unsupervised sub-typing first, and then\nuse the labels not for searching for the subtypes\nbut only for evaluating the subtypes, that\nwe're actually able to do something\ninteresting here. So in summary, in\ntoday's lecture, I talked about two\ndifferent approaches, a supervised approach\nfor predicting", "id": "yYWyLZrdRRI_78", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And there were a few\nmajor limitations that I want to\nemphasize that we'll return to in the next\nlecture and try to address. The first major\nlimitation is that none of these approaches\ndifferentiated between disease stage and subtype. In both of the\ntwo approaches, we assumed that there were\nsome amount of alignment of patients at baseline. For example, here we assume\nthat the patients at time zero were somewhat\nsimilar to another. For example, they\nmight have been newly diagnosed with Alzheimer's\nat that point in time. But often we have\na data set where we have no natural\nalignment of patients in terms of disease stage. And if we attempted to do\nsome type of clustering like I did in this last\nexample, what you would get out, naively, would be one\ncluster for disease stage. So patients who are very\nearly in their disease stage might look very different\nfrom patients who are late in their disease stage. And it will completely\nconflate disease stage from disease subtype, which\nis what you might actually want to discover.", "id": "yYWyLZrdRRI_79", "title": "18. Disease Progression Modeling and Subtyping, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  is that they only used one\ntime point per patient, whereas in reality,\nsuch as you saw here, we might have\nmultiple time points. And we might want\nto, for example, do clustering using\nmultiple time points. Or we might want to\nuse multiple time points to understand something\nabout disease progression. The third limitation\nis that they assume that there\nis a single factor, let's say disease subtype,\nthat explained all variation in the patients. In fact, there might\nbe other factors, patient-specific factors,\nthat one would like to use in your noise model. When you use an algorithm\nlike K-means for clustering, it presents no opportunity\nfor doing that, because it has such a\nnaive distance function. And so in next\nweek's lecture, we're going to move in\nto start talking a probabilistic modeling\napproaches to these problems, which will give us a very\nnatural way of characterizing variation along other axes. And finally, a natural\nquestion you should ask is, does it have to be\nunsupervised or supervised? Or is there a way to combine\nthose two approaches. All right. We'll get back to\nthat on Tuesday.", "id": "yYWyLZrdRRI_80"}, {"text": "  PROFESSOR: So I'm going\nto begin by trying to build some intuition for how\none might be able to do staging from cross-sectional\ndata, and we'll return to this question of\ncombined staging subtyping only much later. So imagine that we had data\nthat lived in one dimension. Here, each data point\nis an individual, we observe their data at\njust one point in time, and suppose we knew exactly\nwhich biomarker to look at. Right? So I gave you an\nexample of that here, when you might look at some\nantibody expression level, and that might be what\nI call biomarker A, is if you knew exactly\nwhat biomarker to look at, you might just put each\nperson along this line", "id": "aJqgO8e37_g_0", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  this is the early disease, and\nthat the other sort of line, maybe that's the late disease. Why might that be a\nreasonable conjecture? What would be an\nalternative conjecture? Why don't you talk\nto your neighbors and see if you guys\ncan come up with some alternative conjectures. Let's go. All right, that's enough. So hopefully simple\nquestions, so I won't give you too much time. All right, so what would\nbe another conjecture? So again, our goal is we have\none observation per individual, each individual is in some\nunknown stage of the disease, we would like to be able to\nsort individuals and turn it into early and late\nstages of the disease. I give you one conjecture\nof how to do that, sorting, what would be another\nreasonable conjecture?", "id": "aJqgO8e37_g_1", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Yep? AUDIENCE: That there's\nthe different-- that they have different\ntypes of the same diseases. They all have the same\ndisease and it could-- just one of the subtypes\nmight be sort of the-- PROFESSOR: Yeah. So you're going back to the\nexample I gave here where you could conflate these things. I want to stick with\na simpler story, let's suppose there's only\none subtype of the disease. What would be another way\nto sort the patients given this data where the data is\nthese points that you see here? Yeah? AUDIENCE: For any disease in\nthe middle range, and then as you [INAUDIBLE] PROFESSOR: OK, so\nmaybe early disease is right over here, and when\nthings get bad, the patient-- this biomarker starts\nto become abnormal, and abnormality,\nfor whatever reason, might be sort of to the\nright or to the left. Now I think that is a\nconjecture one could have.", "id": "aJqgO8e37_g_2", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  not a very natural\nconjecture given what we know about common\nbiomarkers that are measured from the human body\nand the way that they respond to disease progression. Unless you're in the situation\nof having multiple disease subtypes where, for example,\ngoing to the right marker might correspond to\none disease subtype and going to the left\nmarker might correspond to another disease subtype. What would be\nanother conjecture? You guys are missing\nthe easy one. Yeah, in the back. AUDIENCE: Well, it\nmight just be one where the high values\nare [INAUDIBLE] stage and low values are later ones? PROFESSOR: Exactly. So this might be early disease\nand that might be late disease. AUDIENCE: It says vice\nversa on this slide. PROFESSOR: Oh, does it really? Oh shoot. [LAUGHTER] AUDIENCE: [INAUDIBLE] PROFESSOR: Right, right, OK, OK. Thank you. Next time I'll take out\nthat lower vice versa. [LAUGHTER] That's why you guys\naren't saying that.", "id": "aJqgO8e37_g_3", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  OK, so this is good. Now I think we're\nall on the same page, and we had some idea of what\nare some of the assumptions that one might need\nto make in order to actually do anything here. Like for example,\nwe are making some-- we'll probably have to make some\nassumption about continuity, that there might be some gradual\nprogression of the biomarker relevance from early to late,\nand it might be getting larger, it might be getting smaller. If it's indeed the scenario that\nwe talked about earlier where we said like early\ndisease might be here and late disease might be going\nto either side, in that case, I think one could easily argue\nthat with just information we have here, disease\nprogression-- disease stage is unidentifiable, right? Because you wouldn't\nknow where would it-- where should you-- where should\nthat transition point be? So here, here, here,\nhere, here, here. In fact, the same\nproblem arises here. Like you don't know,\nis it early disease-- is it going this way or\nis it going that way?", "id": "aJqgO8e37_g_4", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  to try to get us all on\nthe same page, right? So suppose it was only\ngoing this direction or going that\ndirection, how could we figure out which is which? Yeah? AUDIENCE: Maybe we had data\non low key and other data about how much time we had taken PROFESSOR: Yeah. No, that's great. So maybe we have data on\nlet's say death information, or even just age. And if we started from a\nvery, very rough assumption that disease stage let's say\ngrows monotonically with age, then-- and if you had made an\nadditional assumption that the disease stages are--", "id": "aJqgO8e37_g_5", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  are uniformly drawn from across\ndisease stages, with those two assumptions alone, then you\ncould, for example, look at the average age of\nindividuals over here and the average age of\nindividuals over here, and you'd say, the one\nwith the larger average age is the late disease one. Or you could look at time to\ndeath if you had for each-- for each data\npoint you also knew how long until that\nindividual died, you could look at\naverage time to death for these individuals\nversus those individuals and try to tease it\napart in that way. That's what you meant. OK, so I'm just trying to give\nyou some intuition for how this might be possible. What about if your\ndata looked like this? So now you have two biomarkers. So we've only gone up\nby one dimension only, and we want to figure out\nwhere's early, where's late?", "id": "aJqgO8e37_g_6", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So the intuition that\nI want you to have is that we're going\nto have to make some assumptions about\ndisease progression, such as the ones we\nwere just discussing, and we also have to\nget lucky in some way. So for example, one\nway of getting lucky would be to have a\nreal lot of data. So if you had a\nton, ton of data, and you made an additional\nassumption that your data lives in some low dimensional manifold\nwhere on one side of manifold is early disease and the\nother side of manifold is late disease,\nthen you might be able to discover that\nmanifold from this data, and you might conjecture that\nthe manifold is something like that, that trajectory\nthat I'm outlining there with my hand. But for you to be able\nto do that, of course, you need to have\nenough data, all right? So it's going to be now\na trade-off between just having cross-sectional\ndata, it might be OK so long", "id": "aJqgO8e37_g_7", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  so you can sort of\nfill in the spaces and really identify\nthat manifold. A different approach\nmight be, well maybe you don't have just pure\ncross-sectional data, maybe you have two or\nmaybe three samples from each patient. And then you can\ncolor code this. So you might say, OK,\ngreen is patient 1-- or patient A, we'll call it,\nand this is the first time point from patient A, second\ntime point from patient A, third and fourth time\npoints from patient A. Red is patient B, and you have\ntwo time points for patient B, and blue here is\npatient C, and you have 1, 2, 3 time points\nfrom patient C. OK? Now again, it's not\nvery dense data, we can't really draw\ncurves out, But now we can start to get a\nsense of the ordering. And again, now we can-- even\nthough we don't-- we're not in a dense setting like\nwe were here, here, we'd still nonetheless be able\nto figure out that probably the manifold looks a little\nbit like this, right?", "id": "aJqgO8e37_g_8", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  around when disease\nprogression modeling for cross-sectional\ndata might be possible, but this is a wide open field. And so today, I'll\nbe telling you about a few algorithms\nthat try to build on some of these intuitions\nfor doing disease progression modeling, but they will\nbreak down very, very easily. They'll break down\nwhen these assumptions I gave you don't hold,\nthey'll break down when your data is\nhigh dimensional, they'll break down\nwhen your data looks like this where you don't\njust have a single subtype of perhaps a multiple subtypes. and so this is a really very\nactive area of research, and it's an area that I think\nwe can make a lot of progress on in the field in the\nnext several years. So I'll begin with one case\nstudy coming from my own work where we developed an\nalgorithm for learning from cross-sectional\ndata, and we valued it", "id": "aJqgO8e37_g_9", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  or COPD. COPD is a condition of\nthe lungs typically caused by air pollution or smoking,\nand it has a reasonably good staging mechanism. One uses what's called a\nspirometry device in order to measure the lung function\nof individual at any one point in time. So for example, you take\nthis spirometry device, you stick it in your\nmouth, and you breathe in, and then you exhale,\nand one measure is how long it takes in\norder to exhale all your air, and that is going to\nbe a measure of how good your lungs are. And so then one can take\nthat measure of your function and one can stage how\nsevere the person's COPD is,", "id": "aJqgO8e37_g_10", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So for example, in\nstage 1 of the COPD, common treatments\ninvolve just vaccinations using a short-acting\nbronchodilator only when needed. When the disease stage gets\nmuch more severe, like stage 4, than often treatment\nis recommended to be inhaling\nglucocorticosteroids if there are repeated\naspirations of the disease, long-term oxygen If respiratory\nfailure occurs, and so on. And so this is a disease that's\nreasonably well-understood because there exists a\ngood staging mechanism. And I would argue\nthat when we want to understand how to\ndo disease staging in a data-driven\nfashion, we should first start by working with\neither synthetic data, or we should start with\nworking with a disease where we have some idea of what the\nactual true disease staging is.", "id": "aJqgO8e37_g_11", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  recover in those\nscenarios, and does it align with what\nwe would expect either from the way\nthe data was generated or from the existing\nmedical literature. And that's why we chose COPD. Because it is\nwell-understood, and there's a wealth of literature\non it, and because we have data on it which is much\nmessier than the type of data that went into the original\nstudies, and we could ask, could we come to\nthe same conclusions as those original studies? So in this work, we're\ngoing to use data from the electronic\nmedical record. We're only going to look\nat a subset of the EMR, in particular,\ndiagnosis codes that are recorded for a patient\nat any point in time, and we're going to assume\nthat we do not have access to spirometry data at all. So we don't have any obvious\nway of staging the patient's disease.", "id": "aJqgO8e37_g_12", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  to build a generative model\nfor disease progression. At a very high level,\nthis is a Markov model. It's a model that specifies the\ndistribution of the patient's data, which is shown\nhere in the bottom, as it evolves over time. According to a number\nof hidden variables that are shown in the top,\nthese S variables that denote disease stages, and\nthese X variables that denote comorbidities that\nthe patient might have at that point in time,\nthese X and S variables are always assumed\nto the unobserved. So if you were to clump them\ntogether into one variable, this would look exactly\nlike a hidden Markov model. And moreover, we're\nnot going to assume that we have a lot\nof longitudinal data for a patient. In particular, COPD evolves over\na 10 to 20 years, and the data that we'll be learning from\nhere has data only over one", "id": "aJqgO8e37_g_13", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  The challenge will be, can\nwe take data in this one to three-year time range and\nsomehow stitch it together across large numbers\nof patients to get a picture of what the 20-year\nprogression of the disease might look like? The way that we're\ngoing to do that is by learning the parameters\nof this probabilistic model. And then from the\nparameters, we're going to either infer the\npatient's actual disease stage and thus sort them, or\nactually simulate data from this model to see what\na 20-year trajectory might look like. Is the goal clear? All right. So now what I'm\ngoing to do is I'm going to step into\nthis model piece by piece to tell you what\neach of these components are, and I'll start out with a\nvery topmost piece shown here by the red box. So this is the model of the\npatient's disease progression at any one point in time. So this variable,\nS1, for example,", "id": "aJqgO8e37_g_14", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  S2 might denote the patient's\ndisease stage April 2011; S capital T might denote\nthe patient's disease stage June 2012. So we're going to have\none random variable for each observation of the\npatient's data that we have. And notice that the observations\nof the patient's data might be at very\nirregular time intervals, and that's going to be OK\nwith this approach, OK? So notice that there is a\none-month gap between S1 and S2, but a four-month gap\nbetween St minus 1 and St, OK? So we're going to model\nthe patient's disease stage at the point in\ntime when we have an observation for the patient. S denotes a discrete\ndisease stage in this model. So S might be a value from\n1 up to 4, maybe 1 up to 10", "id": "aJqgO8e37_g_15", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  might denote a much\nlater disease stage. If we have a sequence of\nobservations per patient-- for example, we might have\nan observation on March and then in April, we're\ngoing to denote the disease stage by S1 and S2, what this\nmodel is going to talk about is the probability distribution\nof transitioning from whatever the disease stage\nat S1 is to whatever the disease stage at S2. Now because the time\ninterval is between stages are not homogeneous, we have to\nhave a transition distribution that takes into\nconsideration that time gap. And to do that, we use what's\nknown as a continuous time Markov process. Formally, we say that the\ntransition distribution-- so the probability of\ntransitioning from stage I at time t minus 1 to\nstate j at time t,", "id": "aJqgO8e37_g_16", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  the difference in\ntime points delta-- so delta is the number of months\nbetween the two observations. So this conditional\ndistribution is going to be given by the\nmatrix exponential of this time interval times a matrix Q. And then here, the matrix\nQ gives us the parameters that we want to learn. So let me contrast\nthis to things that you might\nalready be used to. In a typical hidden\nMarkov model, you might have\nasked t goes to St-- or St minus 1 goes to St,\nand you might imagine just", "id": "aJqgO8e37_g_17", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So for example, if\nthe number of states-- for each running variable is\n3, then you would have a 3 by 3 table where for\neach state St minus 1, you have some\nprobability of transition to the corresponding state\nSt, so this might be something like 0.9, 0.9,\n0.9, where notice, I'm having a very large\nvalue along the diagonal, because if, let's say,\na very small period-- so a priori, we might\nbelieve that patients stay in the same\ndisease, and then one might imagine\nthat the probably transitioning from state\n1 at time t minus 1 to state 2 at time t might\nbe something like 0.09,", "id": "aJqgO8e37_g_18", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  to state 3 from state\n1 might be something much smaller like 0.01, OK? And we might say something\nlike that the probability-- we might imagine\nthat the probability of going in a\nbackwards direction, going from stage 2\nat time t minus 1 to let's say stage 1 at time\nt, that might be 0 all right? So you might imagine that\nactually this is the model, and what that's saying is\nsomething like you never go in the backwards\ndirection, and you're more likely to\ntransition to the state immediately adjacent\nto the current stage and very unlikely\nto skip a stage. So this would be\nan example of how you would parametrize the\ntransition distribution in a typical discrete\ntime Markov model, and the story here is going\nto be different specifically", "id": "aJqgO8e37_g_19", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So intuitively, if a lot of\ntime has passed between the two observations, then we want\nto allow for an accelerated process. We want to allow for\nthe fact that you might want to skip many\ndifferent stages to go to your next time step, to go\nto the stage of the next time step, because so\nmuch time has passed. And that intuitively is what\nthis scaling of this matrix Q by delta corresponds to. So the number of parameters\nin this parameterization is actually identical to\nthe number of parameters in this parametrization, right? So you have a matrix Q which\nis given to you in essence by the number of\nstates squared-- really, the number of states-- there's an additional\nredundancy there because it has to sum up to\n1, but that's irrelevant. And so the same\nstory here, but we're going to now\nparametrize the process by in some sense the\ninfinitesimally small time probability of transitioning. So if you were to take the\nderivative of this transition", "id": "aJqgO8e37_g_20", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  and then you were to integrate\nover the time interval that was observed and the probability\nof transitioning from any state to any other state with\nthat infinitesimally small probability transitioning,\nwhat you get out is exactly this form. And I'll leave-- this paper\nis in the optional readings for today's lecture, and\nyou can read through it to get more intuition about the\ncontinuous time Markov process. Any questions so far? Yep? AUDIENCE: Those Q are the\nsame for both versions or-- PROFESSOR: Yes. And this model Q is essentially\nthe same for all patients. And you might imagine, if there\nwere disease subtypes, which there aren't in this\napproach, that Q might be different for each subtype. For example, you might\ntransition between stages much more quickly for some\nsubtypes than for others. Other questions? Yep?", "id": "aJqgO8e37_g_21", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  it's just like a screen\nnumber used beforehand you kind of like specified\nthese stages that you pick [INAUDIBLE] PROFESSOR: Correct. Yes. So you pre-specify the number of\nstages that you want to model, and there are many ways to\ntry to choose that parameter. For example, you could look\nat how about likelihood under this model,\nwhich is learned for the different of stages. You could use typical\nmodel selection techniques from machine learning\nas another approach where you try to penalize\ncomplexity in some way. Or, what we found here, because\nof some of the other things that I'm about to tell\nyou, it doesn't actually matter that much. So similarly to when one does\nhard [INAUDIBLE] clustering or even K-means clustering\nor even learning a problematic\ntopic model, if you use a very small number of\ntopics or number of clusters, you tend to learn very\ncoarse-grained topics or clusters. If you use very many more-- if you use a much\nlarger number of topics, you tend to learn much\nmore fine-grained topics.", "id": "aJqgO8e37_g_22", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  If you use a small\nnumber of disease stages, you're going to learn very\ncoarse-grained notions of disease stages; if you\nuse more disease stages, you're going to learn\na fine-grained notion; but the overall\nsorting of the patients is going to end up\nbeing very similar. But to make that\nstatement, we're going to need to make some\nadditional assumptions, which I'm going to show\nyou in a few minutes. Any other questions? These are great questions. Yep? AUDIENCE: So do we know\nthe staging of the disease because I PROFESSOR: No, and\nthat's critical here. So I'm assuming that these\nvariables-- these S's are all hidden variables here. And the way that we're\ngoing to learn this model is by maximum\nlikelihood estimation where we marginalize over\nthe hidden variables, just like you would do\nin any EM type algorithm. Any other questions? All right, so what\nI've just shown you is the topmost\npart of the model,", "id": "aJqgO8e37_g_23", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So I'm going to talk about\none of these time points. So if you were to look\nat the translation-- the rotation of one of\nthose time points, what you would get out is this model. These X's are also\nhidden variables, and we have pre-specified them\nto characterize different axes by which we want to understand\nthe patient's disease progression. So in Thursday's\nlecture, we characterized the patient's disease as\nsubtype by just a single number, and similarly in this example\nis just by a single number, but we might want to\nunderstand what's really unique about each subtype. So for example--\nsorry, what's really unique about each disease stage. So for example, how is the\npatient's endocrine function in that disease stage?", "id": "aJqgO8e37_g_24", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Has the patient developed lung\ncancer yet and that disease stage? And so on. And so we're going\nto ask that we want to be able to read\nout from this model according to these\naxes, and this will become very clear at\nthe end of this section where I show you a\nsimulation of what 20 years looks like for COPD\naccording to these quantities. When does the patient\ntypically develop diabetes, when does the patient\ntypically become depressed, when does the patient typically\ndevelop cancer, and so on. So these are the\nquantities in which we want to be able\nto really talk about what happens to a patient\nat any one disease stage, but the challenge is, we\nnever actually observe these quantities in\nthe data that we have. Rather, all we observe are\nthings like laboratory test results or diagnosis\ncodes or procedures that have been formed\nand so on, which", "id": "aJqgO8e37_g_25", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And as we've been discussing\nthroughout this course, one could think about\nthings as diagnosis codes as giving you information\nabout the disease status of the patient,\nbut they're not one and the same\nas the diagnosis, because there's so much\nnoise and bias that goes into the assigning of\ndiagnosis codes for patients. And so the way that we're\ngoing to model the raw data as a function of\nthese hidden variables that we want to\ncharacterize is using what's known as a noisy-OR network. So we're going to\nsuppose that there is some generative distribution\nwhere the observations you see-- for example,\ndiagnosis codes are likely to be observed as a\nfunction of whether the patient has these phenotypes\nor comorbidities with some probability, and that\nprobability can be specified by these edge weights. So for example, a\ndiagnosis code for diabetes", "id": "aJqgO8e37_g_26", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  if the patient\ntruly has diabetes, but of course, it\nmay not be recorded in the data for every\nsingle visit the patient has to a clinician, there\nmight be some visits to clinicians that have nothing\nto do with their patients endocrine function and\ndiabetes-- the diagnosis code might not be\nrecorded for that visit. So it's going to\nbe a noisy process, and that noise rate is going\nto be captured by that edge. So part of the\nlearning algorithm is going to be to learn that\ntransition distributions-- for example, that Q matrix\nI showed you in the earlier slide, but the other\nrole-- learning algorithm is to learn all\nof the parameters of this noisy-OR distribution,\nnamely these edge weights. So that's going to be discovered\nas part of the learning algorithm. And a key question\nthat you have to ask me is, if I know I want\nto read out from the model according to these axes,\nbut these axes are never--", "id": "aJqgO8e37_g_27", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  observed in the data, how\ndo I ground the learning algorithm to give meaning\nto these hidden variables? Because otherwise if we left\nthem otherwise unconstrained and you did maximum\nlikelihood estimation just like in any factor\nanalysis-type model, you might discover\nsome factors here, but they might not be the\nfactors you care about, and if the learning problem\nwas not identifiable, as is often the case in\nunsupervised learning, then you might not discover\nwhat you're interested in. So to ground the\nhidden variables, we introduced-- we used a\ntechnique that you already saw in an earlier lecture\nfrom lecture 8 called anchors. So a domain expert\nis going to specify for each one of the comorbidites\none or more anchors, which are observations, which\nwe are going to conjecture could only have arisen from the\ncorresponding hidden variable. So notice here\nthat this diagnosis code, which is for\ntype 2 diabetes,", "id": "aJqgO8e37_g_28", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  That is an assumption\nthat we're making in the learning algorithm. We are actually\nexplicitly zeroing out all of the other edges from\nall of the other comorbidities to a 1. We're not going to pre-specify\nwhat this edge rate is, we're going to allow for the\nfact that this might be noisy, it's not always observed even\nif the patient has diabetes, but we're going to\nsay, this could not be explained by any of\nthe other comorbidities. And so for each one of the\ncomorbidites or phenotypes that we want to\nmodel, we're going to specify some small\nnumber of anchors which correspond to a type\nof sparsity assumption on that graph. And these are the anchors\nthat we chose for asthma, we chose a diagnosis code\ncorresponding to asthma; for lung cancer, we\nchose several diagnosis codes correspond to lung\ncancer; for obesity, we chose a diagnosis\ncode corresponding to morbid obesity; and so on. And so these are\nways that we're going to give meaning to\nthe hidden variables,", "id": "aJqgO8e37_g_29", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  it is not going to pre-specify\ntoo much of the model. The model's still going\nto learn a whole bunch of other interesting things. By the way, the way that we\nactually came up with this set was by an iterative process. We specified some of the hidden\nvariables to have anchors, but we also left some of\nthem to be unanchored, meaning free variables. We did our learning\nalgorithm, and just like you would do\nin a topic model, we discovered that there\nwere some phenotypes that really seemed to be\ncharacterized by the patient's disease-- that seemed to characterize a\npatient's disease progression. Then in order to really dig\ndeeper, working collaboratively with a domain expert, we\nspecified anchors for those and we iterated,\nand in this way, we discovered the full set\nof interesting variables that we wanted to model. Yep? AUDIENCE: Did you measure how\ngood an anchor these were? Like are some comorbidities\nbetter anchors than others? PROFESSOR: Great. You'll see-- I think we'll\nanswer that question in just", "id": "aJqgO8e37_g_30", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  like that's learned. Yep? AUDIENCE: Were all\nthe other weights in that X to O network 0? They weren't part of it here. So it looks like\na pretty sparse-- PROFESSOR: They're\nexplicitly nonzero, actually, it's opposite. So for an anchor, we say that\nit only has a single parent. Everything that's\nnot an anchor can have arbitrarily many parents. Is that clear? OK. Yeah? AUDIENCE: Do the anchors that\nyou have in that linear table, you itereated yourself on\nthat or did the doctors say that these are the [INAUDIBLE]? PROFESSOR: We started\nout with just a subset of these conditions. As things that we\nwanted to model-- things that we\nwanted to understand what happens along\ndisease progression according to these axes,\nbut just a subset of them originally. And then we included a few\nadditional hidden variables that didn't have any\nanchors associated to them, and after doing unsupervised\nlearning and just a preliminary development stage,\nthey discovered some topics", "id": "aJqgO8e37_g_31", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  in there, and then we added them\nin with corresponding anchors. And so you could think about\nthis as an exploratory data analysis pipeline. Yep? AUDIENCE: Is there a chance\nthat these aren't anchors? PROFESSOR: Yes. So there's definitely the\nchance that these may not be anchors related\nto the question was asked a second ago. So for example, there\nmight be some chance that the morbid\nobesity diagnosis code might be\ncoded for a patient more often for a patient\nwho has, let's say, asthma-- this is a bad example. And in which case, that would\ncorrespond to there truly existing an edge from\nasthma to this anchor, which would be a violation\nof anchor assumption. All right, so we chose\nthese to make that unlikely,", "id": "aJqgO8e37_g_32", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And it's not easily testable. So this is another example\nof an untestable assumption, just like we saw lots\nof other examples already in today's lecture and\nthe causal inference lectures. If we had some ground\ntruth data-- like if we had done chart review\nfor some number of patients and we actually label\nthese conditions, then we could test\nthat anchor assumption. But here, we're assuming that we\ndon't actually know the ground truth of these conditions. AUDIENCE: Is there a\nreason why you choose such high-level comorbidites? Like I imagine you\ncould go more specific. Even, say, the\ndiabetes, you could try to subtype the diabetes\nbased on this other model, sort of use that as a single\nlayer, but it seems to-- at least this model seems to\nchoose [INAUDIBLE] high level. I was just curious\nof the reason. PROFESSOR: Yes. So that was a design\nchoice that we made. There are many, many\ndirections for follow-up work, one of which would be to use\na hierarchical model here. But we hadn't gone\nthat direction. Another obvious direction\nfor follow-up work would be to do something within\nthe subtyping with this staging", "id": "aJqgO8e37_g_33", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  is, let's say, the\ndisease subtype, and making everything\na function of that. OK, so I've talked\nabout the vertical slice and I've talked about\nthe topmost slice, but what I still need\nto tell you about is how these phenotypes relate\nto the observed disease stage. So for this, we use-- I don't remember the exact\ntechnical terminology-- a factored Markov model? Is that right, Pete? Factorized Markov\nmodel-- I mean, this is a term that existed\nin the graphical model's literature, but I don't\nremember right now. So what we're saying is that\neach of these Markov chains-- so each of these X1 up to Xt--", "id": "aJqgO8e37_g_34", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  This is the second one which\nI'll say is depression. We're going to assume that\neach one of these Markov chains is conditionally independent\nof each other given the disease stage. So it's the disease stage\nwhich ties everything together. So the graphical model looks\nlike this, and so on, OK? So in particular, there are\nno edges between, let's say,", "id": "aJqgO8e37_g_35", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  All correlations\nbetween these conditions is assumed to be mediated by\nthe disease stage variable. And that's a critical\nassumption that we had to make. Does anyone know why? What would go wrong if we\ndidn't make that assumption? So for example,\nwhat would go wrong if we had something\nlook like this, x1-- what was my notation? X1,1, X1,2, X1,3, and suppose\nwe had edges between them, a complete graph, and\nwe had, let's say, also the S variable with\nedges to everything? What would happen\nin that case where we're not assuming that the X's\nare conditionally independent given S?", "id": "aJqgO8e37_g_36", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So remember, we're\ngoing to learn how to do disease\nprogression through learning the parameters of this model. And so if we set this up and--\nif we set up the learning problem in a way which\nis unidentifiable, then we're going to\nbe screwed, we're not going to able\nto learn anything about disease progression. So what would happen\nin this situation? Someone who hasn't\nspoken today ideally. So any of you remember\nfrom, let's say, perhaps an earlier\nmachine learning class what types\nof distribution's a complete graph-- a complete Bayesian\nnetwork could represent? So the answer is\nall distributions,", "id": "aJqgO8e37_g_37", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  of the joint distribution. And so if you allowed\nthese x variables to be fully connected to\neach other-- so for example, saying that depression depends\non diabetes in addition to the stage, then in\nfact, you don't even need this stage\nvariable in here. The marginal-- you can\nfit any distribution on these X variables even\nwithout the S variable at all. And so the model could\nlearn to simply ignore the S variable, which would\nbe exactly not our goal, because our goal is to learn\nsomething about the disease stage, and in fact,\nwe're going to be wanting to make assumptions on\nthe progression of disease stage, which is going\nto help us learn. So by assuming conditional\nindependence between these X variables, it's going to\nforce all of the correlations to have to be mediated\nby that S variable, and it's going to remove some\nof that unidentifiability that would otherwise exist. It's this subtle but\nvery important point.", "id": "aJqgO8e37_g_38", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  the conditional distribution--\nso first of all, I'm going to assume\nthese X's are all binary. So either the\npatient has diabetes or they don't have diabetes. I'm going to suppose that-- and this is, again, another\nassumption we're making, I'm going to suppose that once\nyou already have a comorbidity, then you always have it. So for example, once this is\n1, then all subsequent ones are also going to be 1. Hold the questions\nfor just a second. I'm also going to\nmake an assumption that later stages of the disease\nare more likely to develop the comorbidity. So in particular, one can\nformalize that mathematically as probability of X-- I'll just say Xi\nbeing 1 given S--", "id": "aJqgO8e37_g_39", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  0, and suppose\nthat this is larger than or equal to probability\nof Xt equals 1 given St equals S prime and Xt minus 1 equals\n0 for all S prime less than S, OK? So I'm saying, as you get\nfurther along in the disease stage, you're more\nlikely to observe one of these complications. And again, this is an\nassumption that we're putting into the\nlearning algorithm,", "id": "aJqgO8e37_g_40", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  are really critical in order\nto learn disease progression models when you don't have\na large amount of data. And note that this is\njust a linear inequality on the parameters of the model. And so one can use a convex\noptimization algorithm during learning-- during the maximum\nlikelihood estimation step with this algorithm, we\njust put a linear inequality into the convex\noptimization problem to enforce this constraint. There are a couple of questions. AUDIENCE: Is there generally\nlike a quick way to check whether a model is\nunidentifiable or-- PROFESSOR: So there\nare ways to try to detect to see if a\nmodel is unidentifiable. It's beyond the\nscope of the class, but I'll just briefly mention\none of the techniques.", "id": "aJqgO8e37_g_41", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  by looking at moments\nof the distribution. For example, you\ncould talk about it as a function of\nall of the observed moments of distribution\nthat you get from the data. Now the observed data here are\nnot the S's and X's, but rather the O's. So you look at the\njoint distribution on the O's, and then you\ncan ask questions about-- if I was to-- so suppose I was to choose\na random set of parameters in the model, is\nthere any way to do a perturbation of the\nparameters in the model which leave the observed marginal\ndistribution on the O's identical? And often when you're in the\nsetting of non-identifiability, you can take the gradient\nof a function and see-- and you could sort of find that\nthere is some wiggle space, and then you show that\nOK, there are actually-- this objective function is\nactually unidentifiable. Now that type of\ntechnique is widely", "id": "aJqgO8e37_g_42", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  algorithms or\nestimation algorithms in learning verbal models,\nbut they would be much, much harder to apply in this type of\nsetting because first of all, these are much more\ncomplex models, and estimating the\ncorresponding moments is going to be very hard because\nthey're very high dimensional. And second, because they're-- I'm actually conflating\ntwo different things when I talk about identifiability. One statement is the infinite\ndata identifiability, and the second question\nis your ability to actually learn a good model\nfrom a small amount of data, which is a sample complexity. And these constraints\nthat I'm putting in, even if they don't affect\nthe actual identifiability of the model, they\ncould be extremely important for improving the\nsample complexity of learning algorithm. Is there another question?", "id": "aJqgO8e37_g_43", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  set of almost 4,000\npatients where, again, each patient we observed\nfor only a couple of years-- one to three years. And the observations\nthat we observed were 264 diagnosis\ncodes, the presence or absence of each of\nthose diagnosis codes during any three-month interval. Overall, there were almost\n200,000 observations of diagnosis codes\nin this data set. The learning\nalgorithm that we used was expectation maximization. Remember, there are a number\nof hidden variables here, and so if you want to\nmaximize the likely-- if you want to learn the\nparameters that maximize the likelihood of\nthose observations O, then you have to marginize\nover those hidden variables, and EM is one way to try to\nfind a local optima of that likelihood function, with the\nkey caveat that one has to do approximate inference\nduring the E step here,", "id": "aJqgO8e37_g_44", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  there's no closed form-- for example, dynamic programming\nalgorithm for doing posterior inference in this model\ngiven its complexity. And so what we used\nwas a Gibbs sampler to do approximate inference\nwithin that E step, and we used-- we did block sampling\nof the Markov chains where we combined\na Gibbs sampler with a dynamic programming\nalgorithm, which improved the mixing\nrate of the Markov chain for those of you who are\nfamiliar with those concepts. And in the end step of the\nlearning algorithm when one has to learn the\nparameters of the distribution, the only complex\npart of this model is the continuous\ntime Markov process, and there's actually\nbeen previous literature from the physics community\nwhich shows how you can really-- which gives you analytic\nclosed-form solutions for that M step of that\ncontinuous time Markov process. Now if I were to do\nthis again today, I would have done it a\nlittle bit differently. I would still think about\nmodeling this problem", "id": "aJqgO8e37_g_45", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  do learning using a\nvariational lower bound of the likelihood with\na recognition network in order to very quickly\nget you a lower bound in the likelihood. And for those of\nyou who are familiar with variational\nautoencoders, that's precisely the idea\nthat is used there for learning variational\nautoencoders. So that's the way I\nwould approach this if I was to do it again. There's just one or two other\nextensions I want to mention. The first one is\nsomething which we-- one, more customization\nwe made for COPD, which is that we enforced\nmonotonic stage progression. So we said that-- so here I talked about\na type of monotonically in terms of the conditional\ndistribution of X given S, but one could also\nput an assumption in the-- I already talked about\nthat, but one could also put an assumption on P of S--", "id": "aJqgO8e37_g_46", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  an assumption on\nQ, and I gave you a hint of how one might\ndo that over here when I said that you might put 0's to\nthe left-hand side, meaning you can never go to the left. And indeed, we did\nsomething like that here as well, which is\nanother type of constraint. And finally, we regularize\nthe learning problem by asking about\nthat graph involving the conditions,\nthe comorbidities, and the diagnosis\ncodes be sparse, by putting a beta prior\non those edge weights. So here's what one learned. So the first thing\nI'm going to do is I'm going to show you the-- we talked about how\nwe specified anchors, but I told you that the anchors\nweren't the whole story. That we were able to infer\nmuch more interesting things about the hidden variables\ngiven all of the observations we have. So here I'm showing you\nseveral of the phenotypes that", "id": "aJqgO8e37_g_47", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  algorithm. First, the phenotype\nfor kidney disease. In red here, I'm showing\nyou the anchor variables that we chose for kidney\ndisease, and what you'll notice are a couple of things. First, the weight, which\nyou should think about as being proportional\nin some way to how often you would see\nthat diagnosis code given that the patient\nhad kidney disease, the weights are all far\nless than one, all right? So there is some noise in this\nprocess of when you observe a diagnosis code for a patient. The second thing you\nobserve is that there are a number of other diagnosis\ncodes that are observed to be-- which are explained by this\nkidney disease comorbidity, such as anemia, urinary\ntract infections, and so on, and that aligns well with what's\nknown in the medical literature", "id": "aJqgO8e37_g_48", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Look at another example\nfor lung cancer. In red here I'm\nshowing you the anchors that we had pre-specified\nfor these, which mean that these diagnosis\ncodes could only be explained by the lung cancer\ncomorbidity, and these are the noise rates that\nare learned for them, and that's everything else. And here's one more\nexample of lung infection where there was only a signal\nanchor that we specified for pneumonia, and you see\nall of the other things that are automatically\nassociated to that as by the unsupervised\nlearning algorithm. Yep? AUDIENCE: So how\ndo you [INAUDIBLE] for the mobidity, right? PROFESSOR: Right. So that's what the unsupervised\nlearning algorithm is doing. So these weights are\nlearned, and I'm showing you something like a point\nestimate of the parameters that are learned by\nthe learning algorithm. AUDIENCE: And so we-- PROFESSOR: Just like if you're\nlearning a Markov model, you learned some transition and\n[INAUDIBLE],, same thing here.", "id": "aJqgO8e37_g_49", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And this should\nlook a lot like what you would see when you do topic\nmodeling on a text copora, right? You would discover a topic--\nthis is analogous to a topic. It's a discrete topic,\nmeaning it either occurs or it doesn't occur\nfor a patient. And you would discover some\nword topic distribution. This is analogous to that\nword topic distribution for a topic in a topic model. So one could then use\nthe model to answer a couple of the original\nquestions we set out to solve. The first one is given\na patient's data, which I'm illustrating\nhere on the bottom, I have artificially\nseparated out into three different\ncomorbidities, and a star denotes\nan observation of a data type of that one. But this was\nartificially done by us, it was not given to\nlearning algorithm. One can infer, when\nthe patient initiated-- started with each one of these\ncomorbidites, and also, when-- so for the full\nthree-year time range", "id": "aJqgO8e37_g_50", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  stage was the patient in\nin the disease at any one point in time? So this model infers that the\npatient starts out in stage 1, and about half a year\nthrough the data collection process, transitioned\ninto stage 2 of COPD. Another thing that one\ncould do using this model is to simulate from the model\nand answer the question of what would, let's say, a 20-year\ntrajectory of the disease look like? And here, I'm showing\na 10-year trajectory. And again, only one\nto three years of data was used for any one\npatient during learning. So this is the first time\nwe see the those axes, those comorbidities\nreally start to show up as being important as the way\nof reading out from the model. Here, we've thrown away those\nO's, those diagnosis codes altogether, we only care about\nwhat we conjecture is truly happening to the patient,\nthose X variables, which are unobserved during training. So what we conjecture is that\nkidney disease is very uncommon", "id": "aJqgO8e37_g_51", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  transition from stage 2, stage\n3, to stage 4 of the disease, and then really bumps\nup towards stage 5 and stage 6 of the disease. So you should read\nthis as saying that in stage 6 of\nthe disease, over 60% people have kidney disease. Now the time interval is here. So how I've chosen these-- where to put these\ntriangles, I've chosen them based on the\naverage amount of time it takes to transition from\none stage to the next stage according to the learned\nparameters of the model. And so you see that\nstages 1, 2, and 3, and 4 take a long period of-- amount of time to transition\nbetween those four stages, and then there's a\nvery small amount of time between\ntransitioning from stage 5 to stage 6 on average.", "id": "aJqgO8e37_g_52", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  One could also read this\nout for other comorbidities. So in orange here--\nin yellow here is diabetes, in black here is\nmusculoskeletal conditions, and in red here is\ncardiovascular disease. And so one of the\ninteresting inferences made by this\nlearning algorithm is that even in stage 1 of COPD,\nvery early in the trajectory, we are seeing patients\nwith large amounts of cardiovascular disease. And again, this is\nsomething that one can look at the\nmedical literature to see does it align\nwith what we expect? And it does, so even in patients\nwith mild to moderate COPD, the leading cause of morbidity\nis cardiovascular disease. Again, this is\njust a sanity check that what this model is learning\nfor a common disease actually aligns with the\nmedical knowledge. So that's all I want to say\nabout this probabilistic model approach to disease\nprogression modeling", "id": "aJqgO8e37_g_53", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  I want you to hold\nyour questions so I can get through\nthe rest of the material and you can ask me after class. So next I want to talk about\nthese pseudo-time methods, which are a very different\napproach for trying to align patients into\nearly-to-late disease stage. These approaches were really\npopularized in the last five years due to the explosion\nin single-cell sequencing experiments in the\nbiology community. Single-cell sequencing is\na way to really understand not just what is the\naverage gene expression, but on a cell-by-cell\nbasis can we understand what is\nexpressed in each cell. So at a very high level,\nthe way this works is you take a solid tissue, you\ndo a number of procedures in order to isolate\nout individual cells from that tissue,\nthen you're going to extract the RNA from\nthose individual cells,", "id": "aJqgO8e37_g_54", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  which somehow barcodes\neach of the RNA from each individual cell,\nmixes them all together, does sequencing of it,\nand then deconvolves it so that you can see\nwhat was the original RNA expression for each of\nthe individual cells. Now the goal of these\npseudo-time algorithms is to take that type\nof data and then to attempt to align\ncells to some trajectory. So if you look at the very\ntop of this figure part figure a that's the picture that\nyou should have in your mind. In the real world, cells\nare evolving with time-- for example, B cells will have\na well-characterized evolution", "id": "aJqgO8e37_g_55", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  and what we'd like\nto be understand, given that you have\ncross-sectional data-- so you can imagine-- imagine you have\na whole collection of cells, each one in a different\npart, a different stage of differentiation,\ncould you somehow order them into where they\nwere in different stages of differentiation? So that's the goal. We want to take this-- so there exists some\ntrue ordering that I'm showing from dark to light. The capture process\nis going to ignore what the ordering information\nwas, because all we're doing is getting a collection of cells\nthat are in different stages. And then we're going to\nuse this pseudo-time method to try to re-sort them so\nthat you could figure out, oh, these were the\ncells in the early stage and these are the cells\nin the late stage. And of course,\nthere's an analogy here to the pictures I\nshowed you in the earlier part of the lecture. Once you have this alignment\nof cells into stages, then you can answer some really\nexciting scientific questions.", "id": "aJqgO8e37_g_56", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  which genes are expressed\nat which point in time. So you might see that\ngene a is very highly expressed very early in\nthis cell's differentiation and is not expressed very\nmuch towards the end, and that might give you\nnew biological insights. So these methods\ncould immediately be applied, I believe, to\ndisease progression modeling where I want you to think about\neach cell as now a patient, and that patient has a number\nof observations for this data. The observations are an\nexpression for that cell, but in our data,\nthe observations might be symptoms that we\nobserve for the patient, for example. And then the goal is,\ngiven those cross-sectional observations, to sort them. And once you have\nthat sorting, then you could answer\nscientific questions, such as, I mentioned, of a\nnumber of different genes,", "id": "aJqgO8e37_g_57", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So here, I'm showing you\nsort of the density of when this particular\ngene is expressed as a function of pseudo-time. Analogously for disease\nprogression modeling, you should think of\nthat as being a symptom. You could ask, OK, suppose\nthere are some true progression of the disease, when do patients\ntypically develop diabetes or cardiovascular symptoms? And so for cardiovascular,\ngoing back to the COPD example, you might imagine that there's\na peak very early in the disease stage; for diabetes, it might\nbe in a later disease stage. All right? So that-- is the analogy clear? So this community, which\nhas been developing methods for studying single-cell\ngene expression data, has just exploded in\nthe last 10 years. So I lost count of how many\ndifferent methods there are, but if I had to guess, I'd\nsay 50 to 200 different methods", "id": "aJqgO8e37_g_58", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  There was a paper, which is\none of the optional readings for today's lecture, that just\ncame out earlier this month which looks at a comparison\nof these different trajectory inference methods,\nand this picture gives a really\ninteresting illustration of what are some of\nthe assumptions made by these algorithms. So for example,\nthe first question, when you try to figure out which\nmethod of these tons of methods to use is, do you expect\nmultiple disconnected trajectories? What might be a reason\nwhy you would expect multiple disconnected\ntrajectories for disease progression modeling? TAs should not answer. AUDIENCE: [INAUDIBLE] PROFESSOR: Different\nsubtexts would be an example. So suppose the answer\nis no, as we've been assuming for most of this\nlecture, then you might ask,", "id": "aJqgO8e37_g_59", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  assuming that a single\ndisease subtype, but do we expect a\nparticular topology? Now everything that we've been\ntalking about up until now has been a linear\ntopology, meaning there's a linear\nprojection, there's such notion of early\nand late to stage, but in fact, the\nlinear trajectory may not be realistic. Maybe the trajectory\nlooks a little bit more like this bifurcation. Maybe patients look the same\nvery early in the disease stage, but then\nsuddenly something might happen which causes\nsome patients to go this way and some\npatients to go that way. Any idea what that might\nbe in a clinical setting? AUDIENCE: A treatment? PROFESSOR: Treatments,\nthat's great. All right? So maybe these patients\ngot t equals 0, and maybe these patients\ngot t equal as 1, and maybe for whatever reason\nwouldn't even have good data on what\ntreatments patients got, so we don't actually observe\nthe treatment, right? Then you might want to be able\nto discover that bifurcation", "id": "aJqgO8e37_g_60", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  to the original source\nof the data, to ask, what differentiated these\npatients at this point in time? And you might\ndiscover, oh, there was something in the data\nthat we didn't record, such as treatment, all right? So there are a\nvariety of methods to try to infer these\npseudo-times under a variety of different assumptions. What I'll do in the\nnext few minutes is just give you an inkling of\nhow two of the methods work. And I chose these to be\nrepresentative examples. The first example\nis an approach based on building a minimum\nspanning tree. And this algorithm\nI'm going to describe goes by the name of Monocle. It was published in 2014 in\nthis paper by Trapnell et al, but it builds very heavily\non an earlier published paper from 2003 that\nI mostly citing here. So the way that this\nalgorithm works is as follows.", "id": "aJqgO8e37_g_61", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  all along, cross-sectional\ndata, which lives in some high-dimensional space. I'm drawing that in\nthe top-left here. Each data point corresponds\nto some patient or some cell. The first step of\nthe algorithm is to do dimensionality reduction. And there are many ways to\ndo dimensionality reduction. You could do principal\ncomponents analysis, or, for example, you could\ndo independent components analysis. This paper uses the\nindependent component analysis. What ICA is going\nto do, it's going to attempt to find a number\nof different components that seem to be as independent\nfrom one another as possible. Then you're going to\nrepresent the data now in this low-dimensional space,\nand in many of these papers, it's quite astonishing to me,\nthey actually use dimension 2. So they'll go all the way\ndown to two-dimensional space where you can actually\nplot all of the data. It's not at all obvious to me\nwhy you would want to do that, and for clinical data,\nI think that might be a very poor choice. Then what they do is they build\na minimum spanning tree on all", "id": "aJqgO8e37_g_62", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So the way that one\ndoes that is you create a graph by drawing\nan edge between every pair of nodes where the\nweight of the edge is the Euclidean distance\nbetween those two points. And then-- so for example, there\nis this edge from here to here, there's an edge from\nhere to here and so on. And then given that\nweighted graph, we're going to find the minimum\nspanning tree of that graph, and what I'm showing you here\nis the minimum spanning tree of the corresponding graph, OK? Next, what one\nwill do is go look for the longest\npath in that tree. Remember, finding the\nlongest path in a graph-- in an arbitrary\ngraph has a name, it's called the traveling\nsalesman problem and it's the NP-hard problem. How has that gotten around here? Well we're not-- this is\nnot an arbitrary graph, this is actually a tree.", "id": "aJqgO8e37_g_63", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  here's a algorithm for\nfinding the longest path. I won't talk about that. So one finds along\nthis path in a tree-- in the tree, and then\nwhat one does is one says, OK, one side of the\npath corresponds to, let's say, early disease stage\nand the other side of the path corresponds to\nlate disease stage, and it allows for\nthe fact that there might be some bifurcation. So for example, you\nsee here that there is a bifurcation over here. And as we discussed\nearlier, you have to have some way\nof differentiating what the beginning is and\nwhat the end should be, and that's where some side\ninformation might become useful. So here's an illustration\nof applying that method to some real data. So every point here\nis a cell after doing dimensionality reduction. The edges between\nthose points correspond to the edges of the\nminimum spanning tree, and now what the\nauthors have done", "id": "aJqgO8e37_g_64", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  had in order to color\neach of the nodes based on what part of the\ncell differentiation process is believed-- that cell\nis believed to be in. And what one discovers,\nthat in fact, this is very sensible, that\nall of these points are in a much\nearlier disease stage than [INAUDIBLE]\nthan these points, and this is a\nsensible bifurcation Next I want to talk about a\nslightly different approach to-- this is the whole story,\nby the way, right? It's conceptually a very,\nvery simple approach. Next I want to talk about\na different approach, which now tries to return back to\nthe probabilistic approaches that we had earlier\nin the lecture. This new approach is going to\nbe based on Gaussian processes. So Gaussian processes have\ncome up a couple of times in lecture, but\nI've never actually", "id": "aJqgO8e37_g_65", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So in order for what I'm going\nto say next to make sense, I'm going to formally define for\nyou what a Gaussian process is. A Gaussian process mu for a\ncollection of time points, T1 through T capital\nN, is defined by a joint distribution,\nmu, of those time points, which is a Gaussian\ndistribution. So we're going to\nsay that the function value for these T different\ntime points is just a Gaussian, which for the\npurpose of today's lecture I'm going to assume\nis zero mean, and where the covariance\nfunction is given to you by this capital K, it's a\ncovariance function of the time points-- of the input points. And so if you look-- this has to be a matrix\nof dimension capital N by capital N. And if you look\nat the I1 and I2 of the N tree, if you look at the N\ntree of that matrix,", "id": "aJqgO8e37_g_66", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  function. It looks at the exponential\nof the negative Euclidean distance squared between\nthose two time points. Intuitively what\nthis is saying is that if you have\ntwo time points that are very close to\none another, then this kernel function is\ngoing to be very large. If you have two time points that\nare very far from one another, then this is very large-- it's\na very large negative number, and so this is going\nto be very small. So the kernel function\nfor two inputs that are very far from\nanother are very small; the kernel function\nfor inputs that are very close to\neach other is large; and thus, what\nwe're saying here is that there's going to be some\ncorrelation between nearby data points. And that's the way which\nwe're going to specify a distribution of functions. If one were to sample\nfrom this Gaussian with a covariance function\nspecified in the way I did, what one gets out is something\nthat looks like this.", "id": "aJqgO8e37_g_67", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  that these curves\nlook really dense, and that's because I'm assuming\nthe N is extremely large here. If and what small, let's\nsay 3, there'd only be three time points here, OK? And so if you can make this\ndistribution of functions be arbitrarily complex by\nplaying with this little l-- so for example, if you made\na little l be very small, then what you get are these\nreally spiky functions that I'm showing you\nin a very light color. If you make a little\nl be very large, you get these very\nsmooth functions, right? So this is a way to get a\nfunction-- this is a way to get a distribution\nover functions just by sampling from\nthis Gaussian process. What this paper does from\nCampbell and Yau published two years ago in\nComputational Biology", "id": "aJqgO8e37_g_68", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  are drawn from a Gaussian\ndistribution whose mean is given to you by the\nGaussian process. So if you think back to the\nstory that we drew earlier, suppose that the data lived\nin just one dimension, and suppose we actually knew\nthe sorting of patients. So we actually\nknew which patients are very early in time, which\npatients are very late in time. You might imagine that that\nsingle biomarker, biomarker A, you might imagine that the\nfunction which tells you what the biomarker's value\nis as a function of time might be something\nlike this, right?", "id": "aJqgO8e37_g_69", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So it might be a function\nthat's increasing or a function that's decreasing. And this function\nis precisely what this mu, this Gaussian\nprocess is meant to model. The only difference is that\nnow, one can model the Gaussian process-- instead of just\nbeing a single dimension, one could imagine having\nseveral different dimensions. So this P denotes the\nnumber of dimensions, right? Which corresponds\nto, in some sense, to the number of\nsynthetic biomarkers that you might conjecture exist. Now here, we truly don't\nknow the sorting of patients into early versus late stage. And so the time points\nT are themselves assumed to be latent\nvariables that are drawn from a truncated\nnormal distribution that looks like this. So you might make\nsome assumption that the time intervals\nfor when a patient comes in might be, maybe patients\ncome in really typically very", "id": "aJqgO8e37_g_70", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  or maybe you're assuming\nit's something flat, an so patients come in\nthroughout the disease stage. But the time point\nitself is latent. So now the generative process\nfor the data is as follows. You first sample\na time point from this truncated normal\ndistribution, then you look to see-- oh, and you sample\nfrom the very beginning your sample this\ncurve mu, and then you look to see, what is the value\nof mu for the sample time point, and that gives you\nthe expected value you should expect to see for that patient. And the one, then,\njointly optimizes this to try to find the most-- the curve, the curve mu\nwhich has highest posterior probability, and that is how\nyou read out from the model both what the latent\nprogression looks like, and if you look at\nthe posterior distribution over the T's that are\ninferred for each individual,", "id": "aJqgO8e37_g_71", "title": "19. Disease Progression Modeling and Subtyping, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  for each individual. And I'll stop there. I'll post the slides\nonline for this last piece,", "id": "aJqgO8e37_g_72"}, {"text": "  PETER SZOLOVITS:\nSo today I'm going to talk about\nprecision medicine. And we don't really\nhave a very precise idea of what precision medicine is. And so I'm going\nto start by talking about that a little bit. David talked about\ndisease subtyping. And if you think about\nhow do you figure out what are the subtypes\nof a disease, you do it by some\nkind of clustering on a bunch of different\nsorts of data. And so we have data like\ndemographics, comorbidities, vital signs,\nmedications, procedures, disease trajectories, whatever\nthose mean, image similarities. And today, mostly I'm\ngoing to focus on genetics. Because this was the great hope\nof the Human Genome Project,", "id": "kZrb6ZIwJqg_0", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  on disease, it\nwould help us create precise ways of dealing\nwith various diseases and figuring out the right\ntherapies for them and so on. So I want to start by\nreviewing a little bit a study that was done by the\nNational Research Council, so the National Academies, and\nit's called \"Toward Precision Medicine.\" This was fairly recent, 2017. And they have some\ninteresting observations. So they start off\nand they say, well, why is this relevant\nnow, when it may not have been relevant before? And of course, the biggie\nis new capabilities to compile molecular data\non patients on a scale that was unimaginable 20 years ago. So people estimated that\ngetting the first human genome cost about $3 billion.", "id": "kZrb6ZIwJqg_1", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  I have some figures later in\nthe talk showing some of the ads that people are running. Increasing success in\nutilizing molecular information to improve diagnosis\nand treatment, we'll talk about some of those. Advances in IT so that we have\nbigger capabilities of dealing with so-called big data-- a perfect storm\namong stakeholders that has made them\nmuch more receptive to this kind of information. So the fact that costs in the\nhealth-care system in the US keep rising and quality doesn't\nkeep rising proportionately makes everybody\ndesperate to come up with new ways of dealing\nwith this problem. And so this looks like the next\ngreat hope for how to do it. And shifting public attitudes\ntoward molecular data--", "id": "kZrb6ZIwJqg_2", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  A few. So that's a dystopian\nview of what happens when people are\ngenotyped and can therefore be tracked by their genetics. And it is true that there are\nhorror stories that can happen. But nevertheless, people\nseem to be more relaxed today about allowing that kind of\ndata to be collected and used. Because they see the potential\nbenefits outweighing the costs. Not everybody--\nbut that continues to be a serious issue. So this report goes\non and says, you know, let's think about\nhow to integrate all kinds of different\ndata about individuals. And they start off and\nthey say, you know, one good example of this\nhas been Google Maps. So Google Maps has\na coordinate system,", "id": "kZrb6ZIwJqg_3", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  for every point on the Earth. And they can use that\ncoordinate system in order to layer on top of each other\ninformation about postal codes, built structures, census tracts,\nland use, transportation, everything. And they said, wow, this\nis really cool, if only we could do this in health care. And so their vision is to\ntry to do that in health care by saying, well,\nwhat corresponds to latitude and longitude\nis individual patients. And these individual\npatients have various kinds of data about them,\nincluding their microbiome, their epigenome, their genome,\nclinical signs and symptoms, the exposome, what\nare they exposed to. And so there's\nbeen a real attempt to go out and create\nlarge collections of data that bring together all of\nthis kind of information.", "id": "kZrb6ZIwJqg_4", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  well, NIH basically started\na project about a year and a half ago called All of\nUs, sounds sort of menacing. But it's really a million of us. And they have asked institutions\naround the United States to get volunteers to\nvolunteer to provide their genetic information, their\nclinical data, where they live, where they commute,\nthings like that, so that they can get\nenvironmental data about them. And then it's meant to be an\nongoing collection of data about a million people\nwho are supposed to be a representative\nsample of the United States. So you'll see in some\nof the projects I talk about later today that\nmany of the studies have been done in populations\nof European ancestry. And so they may not apply to\npeople of other ethnicities.", "id": "kZrb6ZIwJqg_5", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  that the fraction of\nAfrican Americans and Asians and Hispanics and\nso on corresponds to the sample in the\nUnited States population. There's a long history. How many of you have heard of\nthe Framingham Heart Study? So a lot of people. So Framingham, in\nthe 1940s, agreed to become the subject of\na long-term experiment. I think it's now run by Boston\nUniversity, where every year or two they go out\nand they survey-- I can't remember the\nnumber of people. It started as something\nlike 50,000 people-- about their habits and\nwhether they smoke, and what their\nweight and height is, and any clinical\nproblems they've had, surgical procedures, et cetera. And they've been collecting\nthat database now over several generations of\npeople that descend from those.", "id": "kZrb6ZIwJqg_6", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So All of Us is really doing\nthis on a very large scale. Now, the vision of\nthese of this study was to say that we're going\nto build this information commons, which collects all\nthis kind of information, and then we're going to develop\nknowledge from that information or from that data. And that knowledge will\nbecome the substrate on which biomedical research can rest. So if we find\nsignificant associations, then that suggests\nthat one should do studies, which\nwill not necessarily be answered by the data\nthat they've collected. You may have to grow knock-out\nmice or something in order to test whether an\nidea really works. But this is a way\nof integrating all of that type of information. And of course, it can\naffect diagnosis, treatment, and health outcomes, which\nare the holy grail for what", "id": "kZrb6ZIwJqg_7", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Now, here's an\ninteresting problem. So the focus, notice,\nwas on taxonomies. So Sam Johnson was a very famous\n17th century British writer. And he built encyclopedias\nand dictionaries, and was a poet and a\nreviewer and a commentator, and did all kinds\nof fancy things. And one of his quotes\nis, \"My diseases are an asthma and a dropsy\nand, what is less curable, 75,\" years old. So he was funny, too. Now, if you look up\ndropsy in a dictionary-- how many of you have\nheard of dropsy? A couple. So how did you hear of it? AUDIENCE: From\nJane Austen novels.", "id": "kZrb6ZIwJqg_8", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  PETER SZOLOVITS: Sorry? From a novel? AUDIENCE: Novels. PETER SZOLOVITS: Yeah. AUDIENCE: I've heard\nof dropsy [INAUDIBLE].. PETER SZOLOVITS: Yeah. I mean, I learned about it by\nwatching Masterpiece Theatre with 19th century people, where\nthe grandmother would take to her bed with the dropsy. And it didn't turn\nout well, typically. But it took a long time\nfor those people to die. So dropsy is water sickness,\nswelling, edema, et cetera. It's actually not a disease. It's a symptom of a\nwhole bunch of diseases. So it could be pulmonary\ndisease, heart failure, kidney disease, et cetera. And it's interesting. I look back on this. I couldn't find it for\nputting together this lecture. But at one point, I did discover\nthat the last time dropsy was listed as the cause of death of\na patient in the United States was in 1949. So since then, it's disappeared\nas a disease from the taxonomy.", "id": "kZrb6ZIwJqg_9", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  they suspect that\nasthma, which is still a disease in our\ncurrent lexicon, may be very much like dropsy. It's not a disease. It's a symptom of a whole\nbunch of underlying causes. And the idea is\nthat we need to get good enough and precise enough\nat being able to figure out what these are. So I talked to my friend\nZack Kohane at Harvard a few weeks ago when I started\npreparing this lecture. And he has the following idea. And the example I'm going\nto show you is from him. So he says, well,\nlook, we should have this precision\nmedicine modality space, which is this\nhigh-dimensional space that contains all of that information\nthat is in the NRC report.", "id": "kZrb6ZIwJqg_10", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  if we're lucky, we're going\nto find clusters of data. So this always happens. If you ever take a very\nhigh-dimensional data set and put it into its very\nhigh-dimensional representation space, it's almost\nnever the case that the data is scattered\nuniformly through the space. If that were true, it\nwouldn't help us very much. But generally, it's not true. And what you find\nis that the data tends to be on\nlower-dimensional manifolds. So it's in subsets of the space. And so a lot of\nthe trick in trying to analyze this kind\nof data is figuring out what those lower-dimensional\nmanifolds look like. And often you will find among\na very large data set a cluster of patients like this.", "id": "kZrb6ZIwJqg_11", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  it's hard to represent\nthree dimensions in two. But if you're patient that falls\nsomewhere in the middle of such a cluster, then\nthat probably means that they're kind of\nnormal for that cluster, whereas if they fall somewhere\nat the edge of such a cluster, that probably means that\nthere's something odd going on that is worth\ninvestigating, because they're unusual. So then he gave me an\nexample of a patient of his. And let me give you a\nminute to read this. Yeah? AUDIENCE: What's\nan armamentarium? PETER SZOLOVITS: Where\ndoes it say armamentarium? AUDIENCE: [INAUDIBLE] PETER SZOLOVITS: Oh, yeah.", "id": "kZrb6ZIwJqg_12", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  is the set of arms that\nare available to an army. So this is the set of treatments\nthat are available to a doctor. AUDIENCE: Is that the\nonly word you don't know? [LAUGHTER] It's the only word-- AUDIENCE: If I start asking-- AUDIENCE: Based on [INAUDIBLE]. AUDIENCE: Oh, OK. AUDIENCE: In the world. Some of it, I thought\nI could understand. PETER SZOLOVITS:\nWell, you probably know what antibiotics are. And immunosuppressants,\nyou've probably heard of. Anyway, it's a bunch\nof different therapies. So this is what's\ncalled a sick puppy. It's a kid who is\nnot doing well. They started life, at age\nthree, with ulcerative colitis, which was well-controlled\nby the kinds of medications that they normally give\npeople with that disease. And then all of a\nsudden, 10 years later, he breaks out with this horrible\nabdominal pain and diarrhea", "id": "kZrb6ZIwJqg_13", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And they try a bunch of stuff\nthat they think ought to work, and it doesn't work. So the kid was facing some\nfairly drastic options, like cutting out the part of\nhis colon that was inflamed. So your colon is an important\npart of your digestive tract. And so losing it is not fun\nand would have bad consequences for the rest of his life. But what they did\nis they said, well, why is he not responding\nto any of these therapies? And the difficulty,\nyou can imagine, in that cloud-of-points\npicture, is,", "id": "kZrb6ZIwJqg_14", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  or is in the middle of\none of these clusters, when it depends on\na lot of things? In this kid's case, what it\ndepended on most significantly was the last six months\nof his experience, where, before, he was doing OK\nwith the standard treatment. So that cloud might\nhave represented people with ulcerative colitis\nwho were well-controlled by the standard treatment. And now, all of a sudden,\nhe becomes an outlier. So what happened in\nthis case is they said, well, maybe there\nare different groups of ulcerative colitis patients. So maybe there are ones who\nhave a lifelong remission after treatment with a commonly\nused monoclonal antibody. So that's the center\nof the cluster. Maybe there are people who\nhave multi-year remission but become refractory\nto these drugs.", "id": "kZrb6ZIwJqg_15", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So that's the\nremoval of the colon. And then there are people who\nhave, initially, a remission, but then those\nstandard therapy works. So that's what this kid\nis in, this cluster. So how do you treat this as\na machine learning problem from the point of view\nof having lots of data about lots of\ndifferent patients? And the challenges, of\ncourse, include things like, what's your distance function\nin doing the kind of clustering that people typically do? How do you define\nwhat an outlier is? Because there's always a\ncontinuum where it just gets more and more diffuse. What's the best representation\nfor time-varying data, which is critical in this case?", "id": "kZrb6ZIwJqg_16", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So does every dimension in this\nvery high-dimensional space count the same? Or are differences\nalong certain dimensions more important than\nthose among others? And does that, in fact, vary\nfrom problem to problem? The answer is probably yes. So how do we find the\nneighborhood for the patient? Well, I'm going to\ngive you some clues by starting with a shallow\ndive into genetics. So if you've taken a\nmolecular cell biology class, this should not be news to you. And I'm going to run\nthrough it pretty quickly. If you haven't,\nthen I hope at least you'll pick up some\nof the vocabulary. So a wise biologist\nsaid, \"Biology is the science of exceptions.\" There are almost no rules. About 25 years ago,\nthe biology department here taught a special class\nfor engineering faculty", "id": "kZrb6ZIwJqg_17", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  were teaching in their\nintroductory biology, molecular biology classes. And I remember, I\nwas sitting next to Jerry Sussman,\none of my colleagues. And after we heard some\nlecture about the 47 ways that some theory doesn't\napply in many, many cases, Jerry turns to me and\nhe says, you know, the problem with this field is\nthere are just too many damned exceptions. There are no theories. It's all exceptions. And so even biologists\nrecognize this. Now, people have observed,\never since human beings walked the earth, that\nchildren tend to be similar to their\nparents in many ways. And until Gregor Mendel,\nthis was a great mystery. Why is it that you\nare like your parents? I mean, you must\nhave gotten something from them that sort of\ncarries through and makes", "id": "kZrb6ZIwJqg_18", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So Mendel had this\nnotion of having discrete factors of inheritance,\nwhich he called genes. He had no idea what these were. But conceptually, he knew\nthat they must exist. And then he did a bunch of\nexperiments on pea plants, showing that peas\nthat are wrinkled tend to have offspring peas\nthat are also wrinkled. And he worked out the\ngenetics of what we now call Mendelian\ninheritance, namely dominant versus recessive\ninheritance patterns. Then Johann Miescher came\nalong some years later, and he discovered a\nweird thing in cells called nuclein, which\nis now known as DNA.", "id": "kZrb6ZIwJqg_19", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  it's DNA that is carrying\nthis genetic information from generation to generation. And then, of course,\nWatson, Crick, and Franklin, the following year, deciphered\nthe structure of DNA, that it's this double\nhelix, and then figured out what the mechanism\nmust be that allows DNA to transmit this information. So you have a double helix. You match the four letters A,\nC, T, G opposite each other, and you can replicate\nthis DNA by splitting it apart and growing\nanother strand that is the complement\nof the first one. Now you have two. And you can have children, pass\non this information to them. So that was a big deal.", "id": "kZrb6ZIwJqg_20", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  for Biotechnology Information\nas a fundamental physical and functional unit of heredity\nthat's a DNA sequence located on a specific site\non a chromosome which encodes a specific\nfunctional product, namely RNA or a protein. I'll come back to\nthat in a minute. The remaining mystery\nis it's still very hard to figure out what\nparts of the DNA code genes. So you would think we\nmight have solved this, but we haven't quite. And what does the rest, which\nis the vast majority of the DNA, do if it's not encoding genes? And then, how does the\nfolding and the geometry, the topology of\nthese structures, influence their function? So I went back and I read\nsome of Francis Crick's work from the 1950s. And it's very interesting.", "id": "kZrb6ZIwJqg_21", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  at the time. So he said, \"The specificity\nof a piece of nucleic acid is expressed solely by\nthe sequence of its bases, and this sequence\nis a simple code for the amino acid sequence\nof a particular protein.\" And there were people arguing\nthat he was just flat wrong, that this was not true. Of course, it turned\nout he was right. And then the central\ndogma is the transfer of information from nucleic\nacid to nucleic acid or from nucleic acid to\nprotein may be possible. But transfer from protein\nto protein or from protein to nucleic acid is impossible. And that's not quite true. But it's a good\nfirst approximation. So this is where things stood\nback about 60 years ago. And then a few\nNobel prizes later,", "id": "kZrb6ZIwJqg_22", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  this works. And of course, how\nit works is that you have DNA, which is these\nfour bases, double stranded. RNA gets produced in the\nprocess of transcription. So this thing unfolds. An RNA strand is built along the\nDNA and separates from the DNA, creating a single-stranded RNA. And then it goes and\nhooks up with a ribosome. And the ribosome takes that\nRNA and takes the codes in triplets, and\neach triplet stands for a particular amino acid,\nwhich it then assembles in sequence and\ncreates proteins, which are sequences of amino acids. Now, it's very complicated. Because there's\nthree-dimensionality and there's time involved.", "id": "kZrb6ZIwJqg_23", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So again, a few more\nNobel prizes later, we have that transcription,\nthat process of turning DNA into RNA, is regulated\nby promoter, repressor, and enhancer regions\non the genome. And the proteins mediate this\nprocess by binding to the DNA and causing the beginning\nof transcription, or causing it to run faster\nor causing it to run slower, or they interfere\nwith it, et cetera. There are also these\nenhancers, some of which are very far away from\nthe coding region, that make huge differences\nin how much of the RNA, and therefore how much\nof the protein, is made. And the current\nunderstanding of that is that, if here\nis the gene, it may", "id": "kZrb6ZIwJqg_24", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And the enhancer, even though\nit's distant in genetic units, is actually in close\nphysical proximity, and therefore can encourage\nmore of this transcription to take place. By the way, if you're interested\nin this stuff, of course MIT teaches a lot of\ncourses in how to do this. Dave Gifford and Manolis\nKellis both teach computational courses in how\nto apply computational methods to try to decipher\nthis kind of activity. So repressors prevent activator\nfrom binding or alters the activator in order to\nchange the rate constants. And so this is\nanother mechanism. Now, one of the\nproblems is that if you look at the total amount of DNA\nin your genes, in your cells,", "id": "kZrb6ZIwJqg_25", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  are the parts that code for\nmRNA, and eventually protein. So the question is what does\nthe other 98 and 1/2% do? There was this unfortunate\ntendency in the biology community to call\nthat junk DNA, which of course is a terrible notion. Because evolution would\ncertainly have gotten rid of it if it was truly junk. Because our cells spend a lot\nof energy building this stuff. And every time a\ncell divides, it rebuilds all that\nso-called junk DNA. So it can't possibly be junk. But the question\nis, what does it do? And we don't really\nknow for a lot of it. So there are introns-- I'll show you a picture. There are segments of the\ncoding region that don't wind up", "id": "kZrb6ZIwJqg_26", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  They're spliced out. And we don't quite know why. There are these\nregulatory sequences, which is only about 5%,\nthat are those promoters and repressors and enhancers\nthat I talked about. And then there's a whole\nbunch of repetitive DNA that includes transposable\nelements, related sequences. And mostly, we don't\nunderstand what it all does. Hypotheses are things\nlike, well, maybe it's a storehouse of\npotentially useful DNA so that if environmental\nconditions change a lot, then the cell doesn't\nhave to reinvent the stuff from scratch. It saved it from previous\ntimes in evolution when that may have been useful. But that's pretty much pure\nspeculation at this point.", "id": "kZrb6ZIwJqg_27", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  was given by Gerald Fink,\nwho's a geneticist here. And his claim is that a gene\nis not any segment of DNA that produces RNA or protein. But it's any segment of\nDNA that is transcribed into RNA that has some\nfunction, whatever it is, not necessarily building\nproteins, but just anything. And I think that view\nis becoming accepted. So I promised you a little\nbit of more complexity. So when you look at\nyour DNA in eukaryotes, like us, here's the promoter. And then here is the\nsequence of the genome. When this gets transcribed, it\ngets transcribed into something called pre-mRNA, messenger RNA. And then there's this process\nof alternative splicing", "id": "kZrb6ZIwJqg_28", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  But sometimes it doesn't\nleave all the exons. It only leaves some of them. And so the same gene can,\nunder various circumstances, produce different\nmRNA, which then produces different proteins. And again, there's a lot\nof mysteries about exactly how all this works. Nevertheless, that's\nthe basic mechanism. And then here, I've\njust listed a few of the complexity problems. So there are things like,\nRNA can turn into DNA. This is a trick that\nviruses use a lot. They incorporate\nthemselves into your cell, create a DNA\ncomplement to the RNA, and then use that to\ngenerate more viruses. So this is very typical\nof a viral infection.", "id": "kZrb6ZIwJqg_29", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  This is like mad cow disease,\nwhere these proteins are able to cause changes in other\nproteins without going through the RNA/DNA-mediated mechanisms. There are\nDNA-modifying proteins, the most important of\nwhich is the stuff involved in CRISPR-CAS9, which is\nthis relatively new discovery about how bacteria are able to\nuse a mechanism that they stole from viruses to edit the genetic\ncomplement of themselves, and more importantly, of other\nviruses that attack them. So it's an antiviral\ndefense mechanism. And we're now figuring out how\nto use it to do gene editing. You may have read about this\nChinese guy who actually went", "id": "kZrb6ZIwJqg_30", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  in China, incorporating\nsome, I think, resistance against HIV infections\nin their genome. And of course, this is\nprobably way too early to do experiments\non human beings, because they haven't\ndemonstrated that this is safe. But maybe that'll\nbecome accepted. George Church at Harvard\nhas been going around-- he likes to rattle\npeople's chains. And he's been going\naround saying, well, the guy, he was unethical and\nwas a slob, but what he's doing is a really great idea. So we'll see where that goes. And then there are\nthese retrotransposons, where pieces of DNA in\neukarya just pop out of wherever they are\nand insert themselves in some other place\nin the genome. And in plants,\nthis happens a lot.", "id": "kZrb6ZIwJqg_31", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  of DNA segments that\nmaybe it had only one of, but it's replicated\nthrough this mechanism. Last bit of complexity-- so we have various kinds of RNA. There's long\nnon-coding RNA, which seems to participate\nin gene regulation. There is RNA interference,\nthat there are these small RNA pieces that will actually\nlatch onto the RNA produced by the standard\ngenetic mechanism and prevent it from being\ntranslated into protein. This was another Nobel\nPrize a few years ago. Almost everything in this\nfield, if you're first, you get a Nobel Prize for it.", "id": "kZrb6ZIwJqg_32", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  degraded differentially. So there are\ndifferent mechanisms in the cell that destroy\ncertain kinds of proteins much faster than others. And so the production\nrate doesn't tell you how much is going to be\nthere at any particular time. And then there's this secondary\nand tertiary structure, where there's actually-- what is it? It's a mile of DNA in\neach of your cells. So it wouldn't fit. And so it gets wrapped up\non these acetylated histones to produce something\ncalled chromatin. And again, we don't quite\nunderstand how this all works. Because you'd think that if\nyou wrap stuff up like this, it would become inaccessible\nto transcription. And therefore, it's not\nclear how it gets expressed. But somehow or other, the\ncell is able to do that.", "id": "kZrb6ZIwJqg_33", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Now, the reason we're\ninterested in all this is because, if you plot Moore's\nlaw for how quickly computers are becoming cheaper\nper performance, and you plot the cost\nof gene sequencing, it keeps going down. And it goes down much faster\neven than Moore's law. So this is pretty remarkable. And it means that, as I said,\nthat $3 dollar first genome now costs just a few\nhundred dollars. In fact, if you're just\ninterested in the whole exome, so only the 2%,\nroughly, of the DNA that produces genetic\ncoding, you can now go to this company, which\nI have nothing to do with. I just pulled this off the web.", "id": "kZrb6ZIwJqg_34", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  on about six gigabases. And if you pay\nthem an extra $100, they'll do it at 100x coverage. So these techniques\nare very noisy. And so it's important to\nget lots of replicates in order to reassemble\nwhat you think is going on. A slightly more recent\nphenomenon is people say, well, not only can we\nsequence your DNA but we can sequence the RNA that\ngot transcribed from the DNA. And in fact, you can buy a\nkit for $360 that will take the RNA from individual cells-- so these are like\npicoliter amounts of stuff. And it will give you the RNA\nsequence for $360 for up to 100", "id": "kZrb6ZIwJqg_35", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So people are very excited. And there are now also\ncompanies that will sell you advanced analysis. So they will correlate\nthe data that you are getting with\ndifferent databases and figure out whether\nthis represents a dominant or a recessive\nor an x-linked model, if you have family familial\ndata and functional annotation of candidate genes, et cetera. And so, for example, starting\nabout three years ago, if you walk into the Dana-Farber\nwith a newly diagnosed cancer, a solid-tumor cancer, they will\ntake a sample of that cancer, send it off to companies\nlike this, or their own labs, and do sequencing and do\nanalysis and try to figure out exactly which damaged genes\nthat you have may be causing", "id": "kZrb6ZIwJqg_36", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  a pretty empirical field, which\nunusual variants of your genes suggest that certain\ndrugs are likely to be more effective in treating\nyour cancer than other drugs. So this has become completely\nroutine in cancer care and in a few other domains. So now I'm going to switch to a\nmore technical set of material. So if you want to\ncharacterize disease subtypes using gene expression\narrays, microarrays, here's one way to do it. And this is a famous\npaper by Alizadeh. It was essentially the first\nof this class of papers back in 2001, I think. Yeah, 2001. And since then, there have\nbeen probably tens or hundreds of thousands of other\npapers published", "id": "kZrb6ZIwJqg_37", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So what they did is\nthey said, OK, we're going to extract the coding RNA. We're going to create\ncomplementary DNA from it. We're going to use a\ntechnique to amplify that, because we're starting\nwith teeny-tiny quantities. And then we're going to take\na microarray, which is either a glass slide with tens\nor hundreds of thousands of spotted bits of\nDNA on it or it's a silicon chip with\nwells that, again, have tens or hundreds of\nthousands of bits of DNA in it. Now, where does\nthat DNA come from? Initially, it was just\na random collection of pieces of genes\nfrom the genome.", "id": "kZrb6ZIwJqg_38", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  But the idea is that I'm going\nto take the amplified cDNA, I'm going to mark with one of\nthese jellyfish proteins that glows under light,\nand then I'm going to flow it over this slide\nor over this set of wells. And the complementary parts\nof the complementary DNA will stick to the samples of\nDNA that are in this well. OK-- stands to reason. An alternative is that you\ntake normal tissue as well as, say, the cancerous tissue,\nyou mark the normal tissue with green fluorescent\njellyfish stuff and you mark the\ncancer with red, and then you flow both\nof them in equal amounts over the array. That lets you measure a ratio.", "id": "kZrb6ZIwJqg_39", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  about trying to figure\nout the exact value. And then you cluster\nthese samples by nearness in the expression space. And you cluster the genes\nby expression similarity across samples. So it used to be\ncalled bi-clustering. And I'll talk in a few minutes\nabout a particular technique for doing this. So this is a typical\nmicroarray experiment. The RNA is turned into\nits complementary DNA, flowed over the microarray chip. And you get out a\nbunch of spots that are to various degrees\nof green and red. And then you\ncalculate their ratio. And then you do\nthis bi-clustering. And what you get is a\nhierarchical clustering of genes and a hierarchical\nclustering, in their case, of breast cancer biopsy\nspecimens that express", "id": "kZrb6ZIwJqg_40", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So this was pretty\nrevolutionary, because the answers made sense. So when they did\nthis on 19 cell lines in 65 breast tumor samples\nand a whole bunch of genes, they came up with a\nclustering that said, hmm, it looks like there are\nsome samples that have this endothelial cell cluster. So it's a particular\nkind of problem. And you could correlate\nit with pathology from the tumor slides\nand different subclasses. And then this is a very\ntypical kind of heat map that you see in\nthis type of study.", "id": "kZrb6ZIwJqg_41", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  using the gene list that\nthey curated before, looks like it clusters\nthe expression levels into these five clusters. It's a little hard to look at. I mean, when I stare at\nthese, it's not obvious to me why the mathematics came up with\nexactly those clusters rather than some others. But you can see that\nthere is some sense to it. So here you see a lot of\ngreens at this end of it and not very much at\nthis end, and vise versa. So there is some difference\nbetween these clusters. Yeah? AUDIENCE: How did they\ncome up with the gene list? And does anyone ever do this\nkind of cluster analysis without coming up with\na gene list first? PETER SZOLOVITS: Yes. So I'm going to talk in a\nminute about modern gene-wide association studies,\nwhere basically you say, I'm going to look at\nevery gene known to man. So they still have a list, but\nthe list is 20,000 or 25,000.", "id": "kZrb6ZIwJqg_42", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And that's another\nway of doing it. So what was compelling about\nthis work, this group's work, is a later analysis showed that\nthese five subtypes actually had different survival rates,\nand at p-equal 0.01 level of statistical significance. You've seen these survival\ncurves, of course, before from David's lecture. But this is pretty impressive\nthat doing something that had nothing to do\nwith the clinical condition of the patient-- this is purely based on their\ngene expression levels-- you were able to find\nclusters that actually behave differently, clinically. So some of them do\nbetter than others. So this paper and\nthis approach to work set off a huge set\nof additional work.", "id": "kZrb6ZIwJqg_43", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  They did a similar\nrelationship between 96 samples of normal and\nmalignant lymphocytes. And they get a\nsimilar result, where the clusters that\nthey identify here correspond to sort of\nwell-understood existing types of lymphoma. So this, again, gives\nyou some confidence that what you're extracting\nfrom these genetic correlations is meaningful in the terms that\npeople who deal with lymphomas think about, about the topic. But of course, it can\ngive you much more detail. Because people's\nintuitions may not be as effective as these\nlarge-scale data analyses. So to get to your question\nabout generalizing this,", "id": "kZrb6ZIwJqg_44", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  If I list all the genes and\nI list all the phenotypes-- now, we're a little\nmore sure of what the genes are than of\nwhat the phenotypes are. So that's an\ninteresting problem. Then I can do a\nbunch of analyses. So what is a phenotype? Well, it can be a diagnosed\ndisease, like breast cancer. Or it can be the type of\nlymphoma from the two examples I've just shown you. It can also be a qualitative\nor a quantitative trait. It could be your weight. It could be your eye color. It could be almost anything that\nis clinically known about you. And it could even be behavior. It could be things like, what\nis your daily output of Twitter posts?", "id": "kZrb6ZIwJqg_45", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  I don't know if it's\ngenetically predictable. But you'll see some\nsurprising things that are. So then, how do\nyou analyze this? Well, if you start by looking\nat a particular phenotype and say, what genes are\nassociated with this, then you're doing what's\ncalled a GWAS, or a Gene-Wide Association Study. So you look for\ngenetic differences that correspond to specific\nphenotypic differences. And usually, you're looking at\nthings like single nucleotide polymorphisms. So this is places where\nyour genome differs from the reference genome,\nthe most common genome in the human population,\nat one particular locus. So you have a C instead of\na G or something one place in your genes. Copy number variations,\nthere are stretches of DNA that have repeats in them.", "id": "kZrb6ZIwJqg_46", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So one of the most\nfamous ones of these is the one associated\nwith Huntington's disease. It turns out that if you have\nup to 20-something repeats of a certain section of DNA,\nyou're perfectly healthy. But if you're\nabove 30 something, then you're going to die\nof Huntington's disease. And again, we don't quite\nunderstand these mechanisms. But these are empirically known. So copy number\nvariations are important, gene expression levels, which\nI've talked about a minute ago. But the trick here\nin a GWAS is to look at a very wide set\nof genes rather than just a limited set\nof samples that you know you're interested in. Now, the other approach\nis the opposite, which is to say, let's\nlook at a particular gene and figure out what's\nit correlated with. And so that's called a PheWAS, a\nPhenome-Wide Association Study.", "id": "kZrb6ZIwJqg_47", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And you say, well, we can\ndo the same kind of analysis to say which of them\nare disproportionately present in people who\nhave that genetic variant. So here's what a\ntypical GWAS looks like. This is called a Manhattan plot,\nwhich I think is pretty funny. But it does kind of look like\nthe skyline of Manhattan. So this is all of your\ngenes laid out in sequence along your chromosomes. And you take a particular\nphenotype and you say, what is the difference\nin the ratio of expression levels between people who have\nthis disease and people who don't have this disease? And something like this gene,\nwhatever it is, clearly there", "id": "kZrb6ZIwJqg_48", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And so you would be surprised if\nthis gene didn't have something to do with the disease. And similarly, you can calculate\ndifferent significance levels. You have to do something\nlike a Bonferroni correction, because you are testing so\nmany hypotheses simultaneously. And so typically, the\ntop of these lines is the Bonferroni-corrected\nthreshold. And then you say, OK, this guy,\nthis guy, this guy, this guy, and this guy come\nabove that threshold. So these are good\ncandidate genes to think that may be\nassociated with this disease. Now, can you go out and start\ntreating people based on that? Well, it's probably\nnot a good idea. Because there are many\nreasons why this analysis might have failed. All the lessons that you've\nheard about confounders come in very strongly here. And so typically,\nwhat biologists", "id": "kZrb6ZIwJqg_49", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  They then create a\nstrain of knock-out mice who have some analog\nof whatever disease it is that you're studying. And they see whether,\nin fact, knocking out a certain gene, like this\nguy, cures or creates the disease that you're\ninterested in in this mouse model. And then you have a more\nmechanistic explanation for what the\nrelationship might be. So basically, you're\nlooking at the ratio of the odds of having the\ndisease if you have a SNP, or if you have a genetic\nvariant, to having the disease if you don't have\nthe genetic variant. Yeah? AUDIENCE: I'm just\ncurious on the class size. It seems like the\nBonferroni correction is being very limiting here,\npotentially conservative.", "id": "kZrb6ZIwJqg_50", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  techniques adapted\nto this scenario that allow you to sort of mine a bit\nmore effectively than those. PETER SZOLOVITS: Yeah. So if you talk to the\nstatisticians, who are more expert at this than the\ncomputer scientists typically, they will tell you\nthat Bonferroni is a very conservative\nkind of correction. And if you can impose\nsome sort of structure on the set of genes that you're\ntesting, then you can cheat. And you can say, well, you\nknow, these 75 genes actually are all part of\nthe same mechanism. And we're really testing\nthe mechanism and not the individual gene. And therefore, instead of\nmaking a Bonferroni correction for 75 of these guys, we\nonly have to do it for one. And so you can reduce the\nBonferroni correction that way. But people get nervous\nwhen you do that.", "id": "kZrb6ZIwJqg_51", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  is to show statistically\nsignificant results. But that whole\nquestion of p-values keeps coming under discussion. So the head of the American\nStatistical Association, about 15 years ago-- he's\nthe Stanford professor. And he published what became a\nvery notorious article saying, you know, we got it all wrong. Statistical significance\nis not significance in the standard English\nsense of the word. And so he called for\nvarious other ways and was more sympathetic to\nBayesian kinds of reasoning and things like that. So there may be some\ngradual movement to that. But this is a huge can of\nworms to which we don't have a very good mechanistic answer. All right. So if you do these GWASs--", "id": "kZrb6ZIwJqg_52", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  is that most of what\nyou see is down here. So you have things\nwith common variants. But they have very\nsmall effect sizes when you look at\nwhat their effect is on a particular disease. And so that same Zach Kohane\nthat I mentioned earlier has always been challenging\npeople doing this kind of work, saying, look-- for example, we did\na GWAS with Kat Liao, who was a guest interviewee\nhere when I was lecturing earlier in the semester. She's a rheumatologist. And we did a gene-wide\nassociation study. We found a bunch of genes\nthat had odds ratios of like 1.1 to 1, 1.2 to 1. And they're statistically\nsignificant.", "id": "kZrb6ZIwJqg_53", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  everything is\nstatistically significant. But are they significant in the\nother sense of significance? Well, so Zach's\nargument was that if you look at something like the\nodds ratio of lung cancer for people who do\nand don't smoke, the odds ratios is eight. So when you compare 1.1 to\neight, you should be ashamed. You're not doing very well\nin terms of elucidating what the effects really are. And so Zack actually\nhas argued very strongly that rather than focusing\nall our attention on these genetic factors that\nhave very weak relationships, we should instead focus\nmore on clinical things that often have stronger\npredictive relationships. And some combination,\nof course, is best. Now, it is true that we know a\nwhole bunch of highly penetrant", "id": "kZrb6ZIwJqg_54", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So these are ones where,\none change in your genome, and all of a sudden you\nhave some terrible disease. And I think when the Genome\nProject started in the 1990s, there was an expectation\nthat we would find a whole bunch\nmore things like that from knowing the genome. And that expectation was dashed. Because what we discovered\nis that our predecessors were actually pretty\ngood at recognizing those kinds of\ndiseases, from Mendel on, with the wrinkled peas. If you see a family in\nwhich there's a segregation pattern where you can\nsee who has the disease and who doesn't and what\ntheir relationships are, you can get a pretty\ngood idea of what genes or what\ngenetic variants are associated with that disease.", "id": "kZrb6ZIwJqg_55", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And so there weren't a whole lot\nmore that are highly penetrant Mendelian mutations. And so what we had is\nmostly these common variants with small effects. What's really interesting\nand worth working on is these rare variants\nwith small effects. So the mystery kid, like the\nkid whose case I showed you, probably has some\ninteresting genetics that is quite uncommon, and\nobviously, for a long time, had a small effect. But then all of a sudden,\nsomething happened. And there is this whole\nfield called unknown disease diagnosis that says, what do\nyou do when some weirdo walks in off the street and you have\nno idea what's going on? And there are now companies--\nso I was a judge in a challenge", "id": "kZrb6ZIwJqg_56", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  we took eight kids like\nthis and we genotyped them, and we genotyped their\nparents and their grandparents and their siblings. And we took all\ntheir clinical data. This was with the consent\nof their parents, of course. And we made it\navailable as a contest. And we had 20-something\nparticipants from around the world who\ntried to figure out something useful to say about these kids. And you go through a pipeline. And we did this in two rounds. The first round, the pipelines\nall looked very different. And the second round,\na couple of years later, the pipelines had\npretty much converged. And I see now that there\nis a company that did well in one of these challenges that\nnow sells this as a service, like I showed you before,\ndifferent company. And so you send them\nthe genetic makeup of some kid with\na weird condition", "id": "kZrb6ZIwJqg_57", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  and it tries to\nguess which genes might be involved in causing\nthe problem that this child has. That's not the\nanswer, of course. Because that's just a sort\nof suspicion of a problem. And then you have to go out\nand do real biological work to try to reproduce\nthat scenario and see what the\neffects really are. But at least in a couple of\ncases out of those eight, those hints have, in fact, led\nto a much better understanding of what caused the\nproblems in these children. That was fun, by the way. I got my name as\nan author on one of these things that looks\nlike a high-energy physics experiment. The first two pages of the paper\nis just the list of authors.", "id": "kZrb6ZIwJqg_58", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Now, here's a more\nrecent study, which is a gene-wide association\nof type 2 diabetes. It's not quite\ngene-wide, because they didn't study every locus. But they studied a\nhundred loci that have been associated with type\n2 diabetes in previous studies. So of course, if you're\nnot the first person doing this kind of work, you can\nrely on the literature, where other people have already come\nup with some interesting ideas. So they wound up\nselecting 94 type 2 diabetes-associated variants. So these are the glycemic\ntraits, fasting insulin, fasting glucose, et cetera;\nthings about your body, your body mass index, height,\nweight, circumference, et cetera; lipid levels\nof various sorts, associations with\ndifferent diseases,", "id": "kZrb6ZIwJqg_59", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And let me come back to this. So what they did\nis they said, OK, here's the way we're\ngoing to model this. We have an association\nmatrix that has 47 traits by\n94 genetic factors. So we make a matrix out of that. And then they did\nsomething funny. So they doubled the traits. The technology for\nmatrix factorization is called non-negative\nmatrix factorization. And since many of\nthose associations were negative, what they\ndid is, for each trait that had both positive\nand negative values, they duplicated the column. They created one column that had\npositive associations and one column that had the negation\nof the negative associations with zeros everywhere else. So that's how they\ndealt with that problem. And then they said,\nOK, we're going", "id": "kZrb6ZIwJqg_60", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  into two matrices, W and H. And\nI drew those here on the board. So you have one matrix that-- well, this is your\noriginal 47 by 94 matrix. And the question is, can you\nfind two smaller matrices that are 47 by K and K by 94,\nthat when you multiply these together, you get back\nsome close approximation to that matrix. Now, if you've been\nlooking at the literature, there are all kinds of\nideas like auto-encoders. And these are all basically\nthe same underlying idea. It's an unsupervised\nmethod that says, can we find interesting\npatterns in the data by doing some kind of\ndimension reduction? And this is one of those methods\nfor doing dimension reduction. So what's nice about\nthis one is that when", "id": "kZrb6ZIwJqg_61", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And then they know, of\ncourse, what the error is. And they say, well, minimizing\nthat error is our objective. So that also lets them\nget at the question of, what's the right K? And that's an important problem. Because normally\nclustering methods like hierarchical\nclustering, you have to specify what\nthe number of clusters is that you're looking for. And that's hard to do a\npriori, whereas this technique can suggest at least which\none fits the data best. And so the loss function is\nsome regularized L2 distance between the reconstruction, W\ntimes H and X, and some penalty terms based on the\nsize of W and H coupled by these\nrelevance weights that-- you can look at the paper, which\nI think I referred to in here and I asked you to read.", "id": "kZrb6ZIwJqg_62", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  of computational tricks\nto speed up the process. So they got about 17,000 people\nfrom four different studies. They're all of\nEuropean ancestry. So there's the usual\ngeneralization problem of, how do you apply this to people\nfrom other parts of the world? And they did\nindividual-level analysis of all the individuals with\ntype 2 diabetes from these. And the results were that\nthey found five subtypes-- again, five-- which were\npresent on 82.3% of iterations. By the way, total\nrandom aside, there's a wonderful video at\nCaltech of the woman who just made the picture of\nthe black hole shadow. And she makes arguments\nvery much like this.", "id": "kZrb6ZIwJqg_63", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  of coming up with this picture. And what we decided\nwas true is whatever showed up in almost all\nof the different methods of reconstructing it. So this is kind of\na similar argument. And their interpretations,\nmedically, are that one of them is involved with variations\nin the beta cells. So these are the cells in your\npancreas that make insulin. One of them is in\nvariations in proinsulin, which is a predecessor\nof insulin that is under different controls. And then three others have to\ndo with obesity, bad things about your lipid metabolism,\nand then your liver function. And if you look\nat their results, the top spider diagrams, so\nthe way to interpret these", "id": "kZrb6ZIwJqg_64", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  the one in the very middle,\nis the one with negative data. The one in between\nthat and the outside is with zero correlation. And the outside one is\nwith positive correlation. And what you see is\nthat different factors have different influences\nin these different clusters. So these are the\nfactors that are most informative in figuring out\nwhich cluster somebody belongs to. And they indeed look\nconsiderably different. I'm not going to\nhave you read this. But it'll be in the slides. Now, one thing\nthat's interesting-- and again, this won't\nbe on the final exam. But look at these numbers. They're all tiny. Some of them are hugely\nstatistically significant.", "id": "kZrb6ZIwJqg_65", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  to having beta-cell type of\nthis disease at a p-value of 6.6 times 10 to the minus 37th. So it's definitely there. It's definitely an effect. But it's not a very big effect. And what strikes me every\ntime I look at studies like this is just how\nsmall those effects are, whether you're\npredicting some output like the level of\ninsulin in the patient, or whether you're predicting\nsomething like a category membership, as in this table. So as I said, PheWAS\nis a reverse GWAS. And the first paper that\nintroduced the terminology was by Josh Denny and colleagues\nat Vanderbilt in 2010.", "id": "kZrb6ZIwJqg_66", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  But they said, we're going\nto take 25,000 samples from the Vanderbilt\nbiobank, and we're going to take the first\n6,000 European Americans with samples, no other\ncriteria for selection. Why European Americans? Because all the GWAS data\nis about European Americans. So they wanted to be\nable to compare to that. And then they said,\nlet's pick not one SNP but five different\nSNPs that we're interested in. So they picked these,\nwhich are known to be associated with\ncoronary artery disease and carotid artery stenosis,\natrial fibrillation, multiple sclerosis and\nlupus, rheumatoid arthritis and Crohn's disease. So it's a nice grab-bag\nof interesting disease associations. And then the hard\nwork they did was", "id": "kZrb6ZIwJqg_67", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  of different billing\ncodes that were available. And they, by hand, clustered\nthem into 744 case groups and said, OK, these\nare the phenotypes that we're interested in. And that data set, by the\nway, is still available. And it's been used by\na lot of other people, because nobody wants to\nrepeat that analysis. So now what you see\nis something very similar to what you saw in\nGWAS, except here, what we have is the ICD-9 code group. I guess by the time this got\npublished, it was up to 1,000. And these are the same\nkinds of odds ratios for the genetic expression\nof those markers.", "id": "kZrb6ZIwJqg_68", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  That's the\nBonferroni-corrected version. And only multiple\nsclerosis comes up for this particular SNP,\nwhich was one of the ones that they expected to come up. But they were interested to\nsee what else lights up when you do this sort of analysis. And what they discovered\nis that malignant neoplasm of the rectum, benign\ndigestive tract neoplasms-- so there's something going on\nabout cancer that is somehow related to this\nsingle-nucleotide polymorphism, not at a statistically\nhigh enough level, but it's still\nkind of intriguing that there may be some\nrelationship there. Yeah? AUDIENCE: So is this\ndata at all public? Or is this at one\nparticular hospital? Or who has this data? Would it be combined? PETER SZOLOVITS: Yeah. I don't believe that you\ncan get their data unless--", "id": "kZrb6ZIwJqg_69", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  I mean, they're pretty\ngood about collaborating with people. So if you're willing\nto become a volunteer employee at Vanderbilt, they\ncould probably take you. But I just made that up. But every hospital has\nvery strong controls. Now, what is\navailable is the NCBI has GEO, the Gene\nExpression Omnibus, which has enormous amounts-- like, I think, hundreds of\nbillions of sample data. But you don't often know\nexactly what the sample is from. So it comes with\nan accession number and an English description\nof what kind of data it is. And there are actually\nlots of papers where people have\ndone natural language processing on those English\ndescriptions in order to try to figure out what\nkind of data this is. And then they can\nmake use of it.", "id": "kZrb6ZIwJqg_70", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And there's a ton\nof data out there, but it's not well-curated data. Now, what's interesting\nis you don't always get what you expect. So for example, that\nSNP was selected because it's thought\nto be associated with multiple\nsclerosis and lupus. But in reality, the association\nwith lupus is not significant. Its p-value of 0.5, which\nis not very impressive. The association with multiple\nsclerosis is significant. And so they found, in\nthis particular study, a couple of things that had been\nexpected but didn't work out. So for example, this SNP, which\nwas associated with coronary artery disease and thought to\nbe associated with this carotid plaque deposition in your\ncarotid artery, just isn't.", "id": "kZrb6ZIwJqg_71", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  OK, onward. So that was done for SNPs. Now, a very popular\nidea today is to look at expression levels,\npartly because of those prices I showed you where you can\nvery cheaply get expression levels from lots of samples. And so there's this whole notion\nof Expression Quantitative Trait Loci, or EQTL,\nthat says, hey, instead of working as hard\nas the Vanderbilt guys did to figure out these hundreds\nof categories of disease, let's just take your\ngene expression levels and use those as defining the\ntrait that we're interested in. So now we're looking\nat the relationship between your genome and\nthe expression levels. And so you might say, well,\nthat ought to be easy. Because if the gene is there,\nit's going to get expressed.", "id": "kZrb6ZIwJqg_72", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  whether the gene is being\nactivated or repressed or enhanced, or whether any\nof these other complications that I talked about\nearlier are present. And so this is an interesting\nempirical question. And so people say, well, maybe\na small genetic variation will cause different\nexpression levels of some RNA. And we can measure\nthese, and then use those to do this\nkind of analysis. So differential expression\nin different populations-- there is evidence\nthat, for example, if you take 16 people\nof African descent, then 17% of the genes\nin a small sample of 16 people differ in\ntheir expression level", "id": "kZrb6ZIwJqg_73", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  26% in this Asian population and\n17% to 29% in a HapMap sample. Of course, some of\nthese differences may be because of\nconfounders like environment, different tissues, limited\ncorrelation of these expression levels to disease phenotypes. Nevertheless, this\ntype of analysis has uncovered relationships\nbetween these EQTLs and asthma and Crohn's disease. So I'll let you read\nthe conclusion of one of these studies. So this is saying what I said\nbefore, that we probably know", "id": "kZrb6ZIwJqg_74", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So the diseases that we're\ninterested in understanding better today are the ones\nthat are not Mendelian, but they're some complicated\ncombination of effects from different genes. And that makes it, of course,\na much harder problem. There is an interesting\nrecent paper-- well, not that recent-- 2005-- that uses Bayesian\nnetwork technology to try to get at this. And so they say, well, if you\nhave some quantitative trait locus and you treat the\nRNA expression level as this expression quantitative\ntrait locus, and then you take C as some complex\ntrait, which might be a disease or it might be a\nproclivity for something, or it might be one of\nJosh Denny's categories", "id": "kZrb6ZIwJqg_75", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  of different Bayesian\nnetwork-style models that you can build. So you can say, ah,\nthe genetic variant causes a difference in\ngene expression, which in turn causes the disease. Or you could say,\nhmm, the genetic trait causes the disease,\nwhich in turn causes the observable difference\nin gene expression. Or you can say that the\ngenetic variant causes both the expression\nlevel and the disease, but they're not\nnecessarily coupled. So they may be\nconditionally independent given the genetic variant. Or you can have\nmore complex issues, like you could have the\ngene causing changes in expression level of a\nwhole bunch of different RNA,", "id": "kZrb6ZIwJqg_76", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Or you can have\ndifferent genetic changes all impacting the expression\nof some RNA, which causes the disease. Or-- just wait for it. Oops. You can have models\nlike this that say, we have some environmental\ncontributions and a bunch of\ndifferent genes which affect the expression of a\nbunch of different EQTLs, which cause a bunch of\nclinical traits, which cause changes in a bunch\nof reactive RNA, which cause comorbidities. So the approach\nthat they take is to say, well, we can generate\na large set of hypotheses like this, and\nthen just calculate", "id": "kZrb6ZIwJqg_77", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And whichever one assigns the\ngreatest likelihood to the data is most likely to be the\none that's close to correct. So let me just blast through\nthe rest of this quickly. Scaling up genome-phenome\nassociation studies-- the UK Biobank is sort of\nlike this All of Us project. But they do make\ntheir data available. All of Us will, also, but it\nhasn't been collected yet. UK Biobank has\nabout half a million de-identified individuals\nwith full exome sequencing, although they only have about\n10% of what they want now. And many of them will have worn\n24-hour activity monitors so that we have behavioral data. Some of them have had\nrepeat measurements. They do online questionnaires. About a fifth of them\nwill have imaging.", "id": "kZrb6ZIwJqg_78", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So we know if they\ndied or if they had cancer or various\nhospital episodes, et cetera. And there's a website here which\npublishes the latest analyses. And so you see, on April\n18, genetic variants that protect against obesity and\ntype 2 diabetes discovered, moderate with meat-eaters\nare at risk of bowel cancer, and research identifies\ngenetic causes of poor sleep. So this is all over the place. But these are all the studies\nthat are being done by this. I'll skip this. But there's a group here\nat MGH and the Broad that is using this data\nto do, large-scale, many, many gene-wide\nassociation studies. And one of the things\nthat I promised you, which is interesting, is\nfrom these studies, they say,", "id": "kZrb6ZIwJqg_79", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  It's about 0.46 with a p-value\nof 10 to the minus 109th. So your height is definitely\ndetermined, in large part, by your parents' height. But what's interesting\nis that whether you get a college degree\nor not is determined by whether your parents got\na college degree or not. This is probably not genetic. Or it's only partly genetic. But it clearly has confounders\nus from money and social status and various things like that. And then what I found\namusing is that even TV-watching is partly\nheritable from your genetics. Fortunately, my parents\nwatch a lot of TV. The last thing I\nwanted to mention, but I'm not going to\nhave time to get into it, is this notion of gene\nset enrichment analysis.", "id": "kZrb6ZIwJqg_80", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  don't act by themselves. And so if you think back\non high school biology, you probably learned\nabout the Krebs cycle that powers cellular mechanisms. So if you break any\npart of that cycle, your cells don't\nget enough energy. And so it stands to\nreason that if you want to understand that\nsort of metabolism, you shouldn't be looking\nat an individual gene. But you should be looking\nat all of the genes that are involved in that process. And so there have been many\nattempts to try to do this. The Broad Institute here has\na set of, originally, 1,300 biologically-defined gene sets. So these were ones that\ninteracted with each other in controlling some important\nmechanism in the body. They're now up to 18,000. For example, genes involved\nin oxidative phosphorylation", "id": "kZrb6ZIwJqg_81", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  although the average decrease\nper gene is only 20%. So they have these sets. And from those, there\nis a very nice technique that is able to pull-- it's essentially a\nway of strengthening the gene-wide associations by\nallowing you to associate them with these sets of genes. And the approach that\nthey take is quite clever. They say, if we take all\nthe genes in a gene set and we order them by their\ncorrelation with whatever trait we're interested\nin, then the genes that are closer to\nthe beginning of that are more likely to be involved. Because they're the ones that\nare most strongly associated. And so they have this\nrandom walk process", "id": "kZrb6ZIwJqg_82", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  you can say anything\nbefore that is likely to be associated\nwith the disease that you're interested in. And they've had a number of\nsuccesses of showing enrichment in various diseases and\nvarious biological factors. The last thing I want to say\nis a little bit disappointing. I was just really looking\nfor the killer paper to talk about that\nuses some really sophisticated deep\nlearning, machine learning. And as far as I can tell,\nit doesn't exist yet. So most of these methods are\nbased on clustering techniques on clever ideas,\nlike the one for gene set enrichment analysis. But they're not neural\nnetwork types of techniques. They're not immensely\nsophisticated. So what you see coming up is\nthings like Bayesian networks", "id": "kZrb6ZIwJqg_83", "title": "20. Precision Medicine", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  sort of sound like 10-, 15-,\n20-year-old technologies. And I haven't seen examples\nyet of the hot off the presses, we built a 83-layer\nneural network that outperforms\nthese other methods. I suspect that that's coming. It just hasn't hit\nyet, as far as I know. If you know of such papers,\nby all means, let me know. All right.", "id": "kZrb6ZIwJqg_84"}, {"text": "  PETER SZOLOVITS: So\ntoday's topic is workflow, and this is something that-- a topic that I didn't\nrealize existed when I started\nworking in this area, but I've had my nose\nground and ground into it for many decades. And so finally, it has\nbecome obvious to me that it's something\nto pay attention to. So here's an\ninteresting question. Suppose that your goal\nin the kind of work that we're doing in this class\nis to improve medical care-- not an unreasonable goal.", "id": "Td01vFP3uJo_0", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Well, we had an idea\nback in the 1970s when I was getting\nstarted on this, which was that we wanted to\nunderstand what the world's best experts did and to\ncreate decision support systems by encapsulating\ntheir knowledge about how to do diagnosis, how to\ndo prognosis and treatment selection, in order to\nimprove the performance of every other doctor\nwho was not a world class expert by allowing the world\nclass expertise captured in a computer system to\nhelp people figure out how to do better-- so to make them more accurate\ndiagnosticians, more efficient therapists, et cetera. And the goal here was\nreally to bring up the average performance\nof everybody in the health care system.", "id": "Td01vFP3uJo_1", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  practicing medicine closer\nto the level of practice of the world class experts. Now, that turned out not\nto be what was important. And so there was another idea\nthat came along a little bit later that said, well,\nit's not really so much the average performance\nof doctors that's bad. It's the subaverage performance\nthat's really terrible. And so if you're subaverage\nperformance leads to your patients dying, but your\nabove average performance only makes a moderate difference\nin their outcomes, then it's clearly more important\nto focus on the people who are the worst doctors and to\nget them to act in a better way.", "id": "Td01vFP3uJo_2", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  says, let's treat similar\npatients in similar ways. And the value of that is\nto reduce the variance-- so improve average\nversus reduce variance. So which of these is better? Well it depends on\nyour loss function. So as I was suggesting, if your\nloss function is a symmetric so that doing badly or\ndoing below average is much worse than\ndoing above average is much better, than this\nprotocol idea of reducing variance is really important. And this is pretty much what\nthe medical system has adopted. So I wanted to try to\nhelp you visualize this. Suppose that on some\narbitrary scale of 0 to 8, we have an usual, normal\ndistribution, of on the left", "id": "Td01vFP3uJo_3", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  so this is how people, on\naverage, normally behave-- we assume that there's something\nlike a normal distribution. So here is a world class\nexpert whose performance is up at 6 or 7\nand here's the dud of a doctor whose performance\nis down between 0 and 1. And the average doctor\nis just shy of 4. So here are two scenarios. Scenario one is that we\nimprove these guys performance by just a little bit. So we improve it by\n0.1 performance points, I think is what I've\ndone in this model. versus another\napproach, which is suppose we could cut\ndown the variance dramatically so that this same\nnormal distribution becomes narrower. Its average is still in\nexactly the same place,", "id": "Td01vFP3uJo_4", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So there aren't doctors\nwho perform a lot worse, and there aren't doctors who\nperform a lot better either. Well, what happens in that case? Well, you have to look\nat the cost function. So if you have a cost\nfunction like this that says, that somebody's performing at\nthe 0 level has a cost of 1. Whereas somebody\nperforming at the 8 level has a cost of almost 0, and\nit's exponentially declining like this, so that the\naverage performance has a much lower cost than the\naverage between the worst performance and the\nbest performance. So this suggests that,\nif you could bunch people into this region of performance,\nthat your overall costs would go down.", "id": "Td01vFP3uJo_5", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  I've built-- but if you do the\ncalculations, you discover that for the\nbase distribution, here is the\ndistribution of costs. For the slightly\nimproved distribution, you get a cost, which is\n1,694 versus 781, again, in arbitrary units. But if you manage to\nnarrow the distribution, you can get the total cost\ndown to less than what you do by improving the average. Now, this is not a proof,\nbut this is the right idea. The proof is\nprobably in the fact that medical systems\nhave adopted this, and have decided that getting\nall doctors to behave more like the average doctor is the\nbest practical way of improving medical care. Well, how do we narrow the\nperformance distribution?", "id": "Td01vFP3uJo_6", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  you have some learned body who\nprescribes appropriate methods to diagnose and treat patients. So what happens is, for example,\nthe article here from November of 2018, a report of the\nAmerican College of Cardiology, the American Heart Association\nTask Force on Clinical Practice Guidelines, and this\nhas been adopted by this cornucopia of\nthree and four letter abbreviated organizations. And it's a guideline\non the management of blood cholesterol. So as you know, having high\ncholesterol is dangerous. It can lead to heart\nattacks and strokes, and so there is a\nconsensus that it would be good to lower that in people. So these guys went about\nthis by gathering together", "id": "Td01vFP3uJo_7", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  well, how do we do this? What do we promulgate\nas the appropriate way to care for patients\nwith this condition? And the first thing\nthey did is they came up with a\ncolor coded notion of how strong the recommendation\na certain recommendation should be And\nanother color coded or shaded level of certainty\nin that recommendation. So, for example, if you say\nsomething is in class 1, so it's a strong\nrecommendation, then you use words like is\nrecommended, or is indicated, useful, effective, beneficial,\nshould be performed, et cetera. If it's in class 2,\nwhere the benefit is", "id": "Td01vFP3uJo_8", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  say things like it's reasonable,\nit can be useful, et cetera. If the benefit is maybe equal\nto or a little bit better than the risk, you say\nwaffle words, like might be reasonable, may be considered. If there is no benefit,\nin other words, if it roughly equals\nthe risk, then you say, it's not recommended. And if the risk is\ngreater than the benefit, then you say things like\nit's potentially harmful, causes harm, et cetera. So if you were giving\na recommendation on whether to spray\ndisinfectant down your lungs, you might put that in red and\nsay, this is not recommended. And then here,\nthis shading coding is basically how\ngood is the evidence for this recommendation. So the best evidence,\nthe level A,", "id": "Td01vFP3uJo_9", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  clinical trials,\nor a meta-analyses of a high-quality RCTs,\nor RCTs corroborated by high-quality\nregistry studies. And then we go down\nto level C, which is consensus of expert opinion\nbased on clinical experience, but without any sort\nof formal analysis. So if you look at this\nparticular document on cholesterol it\nsays, well, here are the recommendations\non the measurement of LDL and non-HDL cholesterol. And they say here,\nthe confidence and the recommendation\nis one, and it's based", "id": "Td01vFP3uJo_10", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And it says, in adults who\nare 20 years or older and not on lipid-lowering\ntherapy, measurements of either a fasting or a\nnon-fasting blood-- dot, dot, dot. So you could read this\nin the notes later. But notice that there are\nhigh force recommendations. There are lower force\nrecommendations, and each recommendation\nis also shading coded to tell you what\nthe strength of evidence is for this kind\nof recommendation. Here's just another example. This is secondary\natherosclerotic cardiovascular disease prevention. So this is for somebody\nwho's already ill, and it's a bunch\nof recommendations. If you're over 75 years\nof age, or younger with a clinical case of\ncoronary vascular disease,", "id": "Td01vFP3uJo_11", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  should be initiated or\ncontinued with the aim of achieving a 50% or greater\nreduction in LDLC and et cetera. So again, a whole bunch of\ndifferent recommendations. Once again, the strength of the\nrecommendation-- by the way, this is just the first\npage of a couple of pages-- and the quality of\nevidence for it. So this is very\nmuch the way that learned societies are\nnow trying to influence the practice of medicine in\norder to reduce the variance and get everybody to\nbehave in a normal way. You've probably seen articles\nabout Atul Gawande, who's a surgeon here in Boston, and\nhe's gotten publicly famous for advocating checklists. And he says, for\nexample, if you're", "id": "Td01vFP3uJo_12", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  that before you take\noff in the airplane, you go through a\nsanity checklist to make sure that all the\nsystems are working properly, that all the switches\nare set correctly, which in a surgical\nsetting would be things like you have all the right\nnecessary equipment available, that you know what to do in\nvarious potential emergencies, et cetera. So here are their take-home\nmessages, which makes sense. Here, I've abstracted\nthese from the paper that has all of these details. So number one, you\ngo, well, duh-- in all individuals, emphasize\na heart healthy lifestyle across the life course. That seems not\nterribly controversial, and in people who\nare already diseased,", "id": "Td01vFP3uJo_13", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  by statins. And in very high risk\nASCVD, use a threshold of 70 milligrams per\ndeciliter, et cetera. So these are the\nsummary recommendations. And the hope is that doctors\nreading these sorts of articles come away from them convinced\nand will remember that they're supposed to act this way\nwhen they're interacting with their patients. This is a flow chart,\nagain, abstracted from that paper by them\nwhich says, everybody, you should emphasize\na healthy lifestyle. And then depending on\nyour age, depending on what your estimate\nof lifetime risk is,", "id": "Td01vFP3uJo_14", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And these different categories\nhave different recommendations for what you ought to\ndo with your patients. This is for\nsecondary prevention. So it's a similar flow chart for\npeople who are already diseased and not just at risk. And then for people at very\nhigh risk for future events, which is defined\nby these histories and these high-risk\nconditions, these are the people who fall\ninto that second flow chart and should be treated that way. Now, by the way, I\ndidn't make a poll, so I'll give you the answer. But it's interesting to ask. So when papers like\nthis get published, how well do doctors\nactually adhere to these?", "id": "Td01vFP3uJo_15", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  and it takes many, many\nyears before these kinds of recommendations are\ntaken up by the majority of the community, so even\nvery, very uncontroversial recommendations. For example, I think\n20 years ago there was a recommendation that said\nthat anybody who's had a heart attack should be treated, even\nif they're now asymptomatic, with beta blockers. Because in various\ntrials, they showed that there was a 35% reduction\nin repeat heart attacks as a result of this treatment. It took, I think, over a dozen\nyears before most doctors were aware of this\nand started making that kind of recommendation\nto their patients.", "id": "Td01vFP3uJo_16", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  the Agency for Health\nResearch and Quality. And until the current\nadministration, they ran a national\nguideline clearinghouse that contained myriad of\nthese guidelines, published by different authorities,\nand was available for people to download and use. There's been an attempt\nby Guideline Central to take over some of these roles\nsince the government shutdown the government run one, and\nthey have about 2,000 guidelines that are posted on their site. And these are some\nof the examples. So risk reduction\nof prostate cancer with drugs or\nnutritional supplements, stem cell transplantation\nin multiple myeloma, stem cell transplantation\nin myelodysplastic syndromes", "id": "Td01vFP3uJo_17", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And then they also publish\na bunch of risk calculators that say-- I don't know what the 4T\nscore is for heparin-induced thrombocytopenia-- but there are tons\nof these as well. So there's a clearinghouse\nof these things. And you, as a practicing\ndoctor, can go to these. Or your hospital can\ndecide that they're going to provide these\nguidelines to their doctors, and either encourage,\nor in some cases, coerce them to use the\nguidelines in order to determine what\ntheir activity is. Now, notice that this is a\nvery top-down kind of activity. So it's typically done by\nthese learned societies that bring together\nexperts to cogitate on what the right thing\nto do is, and then they tell the rest of the\nworld how to do it. But there's also a kind\nof bottom-up activity.", "id": "Td01vFP3uJo_18", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Now, a care plan is\nreally a nursing term. So if you hang\nout at a hospital, the thing you discover is that\nthe doctors are evanescent. They appear and disappear. They're like\nelementary particles, and they're not\naround all the time. The people who are\nactually taking care of you are the nurses. And so the nurses have\ndeveloped a set of methodologies for how to ensure that\nthey take good care of you, and one of them\nis the development of these care plans. And then what\nclinical pathways are is an attempt to take the\ncare plans that nurses use in taking care\nof individuals and to generalize from\nthose and say, well, what are the typical ways in\nwhich we take care of patients", "id": "Td01vFP3uJo_19", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So I'm going to talk a\nlittle bit about that, and one of the papers I gave\nyou as an optional reading for today is about\ncow paths, which are these attempts to build\ngeneralizations of care plans. So this is a care plan from the\nMichigan Center for Nursing, which is an educational\norganization that tries to help nurses figure\nout how to be good nurses. I was very amused when\nI was looking for this. I ran across a video, which is\nsome experienced nurse talking about how you build\nthese care plans. And she sort of says, well,\nwhen you're in nursing school, you learn how to build these\nvery elaborate carefully constructed care plans. When you're actually\npracticing as a nurse, you'll never have\ntime to do this. And so you're going to do a\nrough approximation to this.", "id": "Td01vFP3uJo_20", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  But for now, satisfy your\nprofessors by doing these exercises correctly. So take a look at this. So there are a bunch of columns. The leftmost one\nsays assessment. So this is objective,\nsubjective, and medical diagnostic data. So the objective data is this\npatient has gangrene-infected left foot-- not a good thing, an open\nwound, et cetera, et cetera. Subjective data, the patient\nsaid the pain is worse when walking and turning. She dreads physical\ntherapy, and she wishes she did not have\nto be in this situation-- surprise. But that's definitely\nsubjective. You can't see external\nevidence of that. The nursing diagnosis\nis that this patient has impaired tissue\nintegrity in reference to the wound and the\npresence of an infection. Now, that diagnosis actually\ncomes with a kind of guideline", "id": "Td01vFP3uJo_21", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  In other words, in order\nto be able to put that down on the care plan,\nshe has to make sure that characteristics\nof the patient satisfy certain criteria\nwhich are the definition of that diagnosis. The patient outcomes--\nso this is the goals that the nurse is\ntrying to achieve. And notice, there\nare five goals here. One is that the patient will\nreport any altered sensation of pain at the tissue impairment\nbetween January 23 and 24. So this is a very specific goal. It says, the\npatient will tell me that they feel\nbetter, that there's a change in their feeling\nin their infected left foot. They will understand the plan to\nheal tissue and prevent injury.", "id": "Td01vFP3uJo_22", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  They will describe measures to\nprotect and heal the tissue, including wound care by 124. So notice, this is the\npatient describing to you what you are planning to do\nfor them, in other words, demonstrating an understanding\nof what the plan is and what's likely\nto happen with them. Experience a wound decrease\nthat decreases in size and has increased\ngranulation tissue, and achieve functional\npain goal of 0 by 124 per the\npatient's verbalization. So when they come\nin and they ask you on that pain scale, are\nyou at a 0, or a 10, or somewhere in\nbetween, the goal is that the patient\nwill say, I'm at a 0, in other words, no pain. Now, what are the interventions?", "id": "Td01vFP3uJo_23", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  plans to do in order to\ntry to achieve those goals. And then the rationale\nis an explanation of why it's reasonable to expect\nthose interventions to achieve those goals. And the evaluation\nof outcomes says, what criteria or what are\nthe actual outcomes for what we're trying to achieve? So that gets filled\nin later, obviously, then when the plan is made. So if you look at a\nwebsite like this, there are templated care plans\nfor many, many conditions. You can see that I'm\nonly up to C in an A to Z listing from this one website,\nand there are plenty of others. But there is an admission care\nplan, adult failure to thrive, alcohol withdrawal, runny\nnose, altered cardiac output, amputation.", "id": "Td01vFP3uJo_24", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  anemia, angina, anticoagulant\ncare, et cetera. So there are tons of\ndifferent conditions that different\npatients fall into, and this is a way of trying to\nlist the template care plans. Now, this paper is\nkind of interesting, by Yiye Zhang and colleagues. And what they did\nis they said, well, let's take all these\ncare plans and let's try to build a machine learning\nsystem that learns what are the typical patterns that\nare embedded in those care plans. But they didn't\nstart with the plans. This is retrospective analysis. So what they started with\nis the actual records of what was done\nto each patient. And so the idea is that\nyou get treatment data from the electronic\nhealth record. Then you identify patient\nsubgroups from that data,", "id": "Td01vFP3uJo_25", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And you have medical\nexperts evaluate these, and these then become\nclinical pathways, which are this generalization\nof the care plans to particular\nsubpopulations of patients. So the idea is that they\ndefine a bunch of abstractions. So they say, look,\nan event is a visit. So, for example,\nfor an outpatient, anything that happens to you\nduring one visit to a doctor or to a hospital. So it's a set of procedures,\na set of medications, a set of diagnoses. And by the way,\nthey were focusing on people with kidney disease\nas the target population that they were looking at. So then they say,\nOK, individual events are going to be abstracted into\nthese supernodes, which capture", "id": "Td01vFP3uJo_26", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  associated with some visit. So you might worry that this\nis going to be combinatorial, because there are many possible\ncombinations of things. And that is, in fact,\na bit of a problem, I think, in their analysis. So now, you have\nthese supernodes, and then each patient\nhas a visit sequence, which is a time-ordered\nlist of the supernodes. So every time you\ngo see your doctor, you have one new supernode. And so you have a\ntime series of these. And then they do\nthe following thing. They say, gee, when we talk\nto our doctors and nurses, they tell us that\nthey care mostly about what happened at the last\nvisit that the patient had. But they also care\na little bit less, but they still care about what\nhappened at the visit previous to that, but not so much about\nhistory going further back.", "id": "Td01vFP3uJo_27", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  we only have things depend on\nthe last node in the Markov chain. So let's change\nthe model here so that we will combine\npairs of visits into nodes so that each\nnode in the Markov chain will represent the last two\nvisits that the patient had. So this could, again, cause\nsome combinatorial problems. But here's the image\nthat they come up with. So there are individual items. Is it a hospital visit,\nan office visit, a visit for the purpose of education? Are you in chronic kidney\ndisease stage four? Was an ultrasound done? Were you given ACE inhibitors? Were you given\ndiuretics, et cetera? So these are all the\ndata that we mentioned. They treat that as a bag.", "id": "Td01vFP3uJo_28", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  to identify all the bags that\nhave the same exact content. An asterisk, they didn't\nlook, for example, at the dose of medication\nthat you were given, only which medication it was. So there are some\ncollapsing that way. Then the supernodes\nare these combinations where we say, OK, you\nhad a particular purpose, a particular diagnosis,\na particular set of interventions, a\nparticular set of procedures. And again, we list all\npossible combinations of those, and then that sequence\nrepresents your sequence. These are aggregated\ninto supernodes. That represents\nyour visit sequence, and then these super\npairs are this hack to let you look two steps\nback in the Markov chain. And so they wind up with\nabout 3,500 different", "id": "Td01vFP3uJo_29", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So it is combinatorial,\nbut it's not terribly combinatorial in their data. They then compute the\nmaximum of the length of common subsequences between\neach pair of visit sequences. So they're going to\ncluster these sequences. They define a\ndistance function that says that the more they\nshare a common sequence, the less distant they\nare from each other. And the particular\ndistance function they used is the length of\neach sequence minus twice the length of the common\nsubsequence, the longest common subsequence, which\nseems pretty reasonable. And then hierarchical clustering\ninto distinct subgroups, they came up with 31 groups\nfor this group of patients, and here they are.", "id": "Td01vFP3uJo_30", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  don't differ a whole\nlot from each other. So, for example,\nthese two differ only in that the patient got some\nmedication and diuretics in one case and just that\nmedication in the other case. So these are-- it is a\nhierarchical cluster, and the things lower\ndown in the clustering are probably fairly\nclose to each other. Nevertheless, what\nthey're able to do, then, is to estimate\na transition matrix among these supernode\npair states, and they can look at\ndifferent trajectories depending on the degree\nof support for the data. So you can set\ndifferent thresholds on how many cases have to be\nin a particular state in order for you to take transitions to\nor from that state seriously.", "id": "Td01vFP3uJo_31", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  is that they had way too little\ndata, and so many of the groups that they came up\nwith had relatively small numbers of patients in\nthem, which is unfortunate. Now, once you have these\ntransition matrices, then you can say, OK,\nfor cluster 29, which was this cluster, so\nthere were a grand total of 14 patients in this cluster. They were all at chronic\nkidney disease stage 4, so quite severe. They were all hypertensive. They were all on ACE\ninhibitors and statins, and everybody in that group\nhad that categorization. So if you look there\nthen you can say, OK, for all the things we\nknow about that patient,", "id": "Td01vFP3uJo_32", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And what we find is that-- man, I can't read these. So these nodes\nimply other nodes, and the strength of the arrows\nis proportional to their width. And so this is a\nrepresentation of everything that we've learned\nabout that cluster, but remember, only\nfrom those 14 patients. So I'm not sure I would\ntake this to the bank and rely on it too intensely. But they then, by\nhand, abstract it and say, well, let's look at\nan interpretation of this. And so if they look\nin typical patterns that they see in\nthat cluster, they say, hmm, we see an\noffice visit in which the patient is on\nthese medications", "id": "Td01vFP3uJo_33", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Then they're hospitalized. Then there's\nanother-- let's see. No, I'm sorry. Yeah, yellow node\nis an office visit. So they're hospitalized. They then get an\neducation visit, so that's typically with the\nnurse or nurse practitioner to explain to them what\nthey ought to be doing. They have another hospital-- they have another office visit. They have a hospital visit. They have another hospital\nvisit, and then they die. So that, unfortunately,\nis a not atypical pattern that you see in patients who\nare at a pretty severe state of chronic kidney disease. And we don't know\nfrom this diagram how long this process\ntakes to take place. So I have some questions.", "id": "Td01vFP3uJo_34", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Some of them were fairly\nsimilar to others. They have between 10 and 158\npatients in each subgroup. So I would feel\nmuch better if they had between 1,000 and\n15,000 or something patients in each group, or\n150,000 patients in each group. I would feel much more\nbelieving in the representations that they found. And the other\nproblem is that even within an individual\nsubgroup, you can find very different patterns. So, for example, here is a\npattern where, again, a person has a couple of office visits. They go to the hospital. Or they go to the hospital\ntwice with slightly different--", "id": "Td01vFP3uJo_35", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So this person at this point\nis in acute kidney injury. So you can get there either\ndirectly from the office visit or from an earlier\nhospitalization, and then they die. And so this is part\nof that pattern. But here's another pattern mined\nfrom exactly the same subgroup. Now, this subgroup has\n122 patients in it, so there's a little\nbit more heterogeneity. But what you see here\nis that a patient is going back and forth between\neducation visits and doctor's visits, back and\nforth between doctors visits and hospitalizations,\nthen a hospitalization, then another hospitalization,\nbut they're surviving. So it's a little bit tricky,\nbut I think this is a good idea, but there are\nprobably improvements that are possible on\nthe technique that's being used here. And, of course, much more\ndata would be very helpful", "id": "Td01vFP3uJo_36", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  going on in these patients. Here's a similar idea\nthat I was involved. Jeff Klann did his\nPhD at Regenstrief, which is a very well-known,\nvery early adopter of computerized information\nsystems in Indiana. And so what he started\noff-- and he said, hmm. You know the Amazon\nrecommendation system that says you just\nbought this camera lends, and other people who bought\nthis camera lens also bought a cleaning kit\nand a battery that goes with that camera, and so on? So he said, why don't\nwe apply that same idea to medical orders? And so he took the record of\nall the orders at Regenstrief,", "id": "Td01vFP3uJo_37", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  to the Amazon recommendation\nsystem that said, hey, other doctors who have\nordered the following set of tests have also ordered\nthis additional test that you didn't order. Maybe you should\nconsider doing it. Or conversely, other doctors who\nhave ordered this set of tests have never ordered this\nother one in addition. And so are you sure\nyou really need it? So that was the idea. And what he did was\nhe focused on four different clinical issues. So one of them was an\nemergency department visit for back pain, pregnancy,\nso labor and delivery, hypertension in the\nurgent visit clinic-- so the urgent visit clinic\nis one of these lower-level non-emergency department,\ncheaper, lower level of care, but still urgent care kinds\nof clinics that many hospitals", "id": "Td01vFP3uJo_38", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  sick out of the emergency\ndepartment and in this lower-intensity clinic-- and hypertension, and\nhigh blood pressure, and then altered mental state\nin the intensive care unit. So people in the ICU\nare often medicated, and they become wacko,\nand so this is trying to take care of such patients. They used three years\nof encountered data from Regenstrief. And for each domain,\nthey limited themselves to the 40 most frequent orders,\nand, again, low granularity. So, for example,\ndrug, but not the dose of the drug for\nmedications, and the 10 most frequent comorbidities or\nco-occurring diagnoses. So this is an example of wisdom\nof the crowd kind of approach", "id": "Td01vFP3uJo_39", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  is probably a good\nrepresentation of what you ought to be doing. Now, what's an obvious\npitfall of this approach? I'm just checking to\nsee if you're awake. Yeah? AUDIENCE: Just reinforce\nwhatever's [INAUDIBLE].. PETER SZOLOVITS: Yeah,\nif they're all bozos, they're going to train\nyou to be a bozo too. And there's a lot\nof stuff in medicine that is not very\nwell-supported by evidence, where, in fact, people\nhave developed traditions of doing things a certain way\nthat may not be the right way to do it. And this just reinforces that. On the other hand, it\nprobably does reduce variance in the sense that we talked\nabout at the beginning. And so, as a result, it may\nbe a reasonable approach, if you're willing to\ntolerate some exceptions. My favorite story is\nSemmelweiss figured out", "id": "Td01vFP3uJo_40", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  was extremely dangerous\nfor the mother, because they would\ndie of what was called \"child bed fever,\" which\nwas basically an infection. And Semmelweiss figured\nout that maybe there was-- this was before Pasteur. But he figured out\nthat maybe there was something that was being\ntransmitted from one woman to the next that was causing\nthis child bed fever, and, of course, he was right. And he did an experiment,\nwhere on his maternity ward, he had all of the\nyounger doctors wash their hands with some\nsort of alcohol or something to kill whatever they\nwere transmitting. And their death rate\nfrom this child bed fever dropped to almost 0. And he went to his colleagues\nand he said, hey, guys, we", "id": "Td01vFP3uJo_41", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  and stop killing women. And they looked at\nhim, and they said, you know, these hands\nheal, they don't kill. Many of them were upper\nclass or noblemen who had gone into this profession. The idea that somehow they were\nresponsible for transmitting what turns out to\nbe bacteria was just a non-starter for them. And Semmelweiss wound\nup ending his days in a mental institution,\nbecause he went nuts. He was unable to\nchange practice even though he had done an experiment\nto demonstrate that it worked. So this is a case where\nthe wisdom of the crowd was not so good and\nled to bad outcomes. So like Amazon's\nrecommendation system, it automates the learning\nof decision support rules.", "id": "Td01vFP3uJo_42", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  induced from real data, it tends\nto deal with more complex cases than the sort of simple,\nstereotypical cases for which people can\ndevelop guidelines, for example, where\nthey can anticipate what's going to happen\nin various circumstances. So he used the Bayesian\nnetworking model that used diagnoses possible\norders and evidence, which is the results from orders\nthat were already completed. There's a system out of\nUniversity of Pittsburgh, called Tetrad, that implements\na nice version of something called Greedy\nEquivalent Search, which is a faster way of\nsearching through the space of Bayesian networks for\nan appropriate network that represents your data. So it's a highly\ncombinatorial problem,", "id": "Td01vFP3uJo_43", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  classes of Bayesian networks\nthat, by definition, would fit the data equally well. And it does it by class rather\nthan by individual network, and so it gets a nice\ncombinatorial reduction. And what Jeff found is, for\nexample, in the pregnancy network, these\nare the nodes that correspond to\nvarious interventions and various conditions. And this is the Bayesian network\nthat best fits that data. It's reasonably complicated. Here are some others. This is for the emergency\ndepartment case. So you see that you have things\nlike chest pain and abdominal pain presenting\ndiagnoses, and then you have various\nprocedures, like an abdomen", "id": "Td01vFP3uJo_44", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  CT, or a basic metabolic\npanel, et cetera, and this gives you the\nprobabilistic relationships between them. And so what they were able to\ndo is to take this Bayesian network representation,\nand then if you lay a particular patient's\ndata on that representation, that corresponds to fixing\nthe value of certain nodes. And then you do Bayesian\ninference to figure out the probabilities of\nthe unobserved nodes, and you recommend the highest\nprobability interventions that have not yet been done. So it's a little bit\nlike, if you remember, we talked about\nsequential diagnosis. This is a little\nbit in that spirit, but it's a much more complicated\nBayesian network model rather than a naive-based model.", "id": "Td01vFP3uJo_45", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  You have-- it's called the\nIterative Treatment Suggestions algorithm, and it\nshows the doctor that these are the\nproblems of the patient, and the current orders, and\nthe probability that you might ask to have any\none of these orders done. And what they're able to show is\nthat this does reasonably well. Obviously, it wouldn't\nhave been published if they hadn't been able to show that. And so what you see is that, for\nexample, the next order that's done in an inpatient pregnancy\nusing this Bayesian network formalism has a position of\nabout fourth on the list. So their criterion for\njudging this algorithm is, is it raising the\nthings that people actually", "id": "Td01vFP3uJo_46", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  list, on the recommended\nset of actions that you consider doing? And you see that it's\nfourth, on average, in inpatient pregnancy,\nabout sixth in the ICU, about sixth in the\nemergency department, and about fifth in the\nurgent care clinic. So that's pretty\ngood, because that means that even if you're\nlooking at an iPhone, there's enough screen\nreal estate that it'll be on the so-called first\npage of Google hits, which is the only thing\npeople ever pay attention to. And, in fact, they can\nshow that the average list position corresponds to the\norder rank by frequency, but that their model does a\nreasonably good job of keeping you within the first 10 or\nso for much of this range.", "id": "Td01vFP3uJo_47", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So Adam Right, you've met. He was discussant in one\nof our earlier classes. And Adam's been very active\nin trying to deploy decision support systems. And he had an interesting\nepisode back in-- when was this-- 2016. So it must have been\na little before 2016. He went to demonstrate this\ngreat decision support system that they had implemented\nat the Brigham, and he put in a fake case where\nan alert should have gone off for a patient who has been on\na particular drug for more than a year and needs to have their\nthyroid stimulating hormone measured in order to check\nfor a potential side effect", "id": "Td01vFP3uJo_48", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  ALT is a liver test,\nliver enzyme test. So they needed both\nof those tests. He was demonstrating\nthis wonderful system. He put in a fake patient\nwho had these conditions, and the alert didn't go off. So he goes, hmm,\nwhat's going on? And they went back,\nand they discovered that in 2009 the system's\ninternal code for amiodarone had been changed\nfrom 40 to 70-99. Who knows why? But the rule logic in\nthe system was never updated to reflect this change. And so, in fact, if\nyou look at the history of the use of amiodarone-- by the way, it's an\ninteresting graph. The blue dots are weekdays, and\nthe black dots are weekends.", "id": "Td01vFP3uJo_49", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  But what you see is that-- I don't know what happened\nbefore about the end of 2009. They probably weren't running\nthat rule or something. But what you see is sort\nof a gradual increase in the use of this rule, and\nthen you see a long decrease from 2010 up through 2013 when\nthey discovered this problem. Now, why a decrease? I mean, it's not a\nsudden jump to 0. And the reason was\nthat this came about-- first of all, it\ncame about gradually, because the people who had had\nthis drug before that change in the software had\ngotten the old code, which was still triggering the rule. It's just that as time went\non, more and more people", "id": "Td01vFP3uJo_50", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And with that new code, it was\nno longer triggering the rule. And then this is the point at\nwhich they discovered the bug, and then they fixed it. Of course, it came\nright back up again. Oh. Well, I'll talk about some\nof the others as well. So this was the amiodarone case. So it fell suddenly, as\nsome patients were taken off the drug and others were started\nwith this new internal code. And as I said, the alert\nlogic was fixed back in 2013. Yeah? AUDIENCE: So I don't know\nhow hospital IT systems work, and it might vary\nfrom place to place. But is there ever a notion\nof like this computer needs to be updated for the\nsoftware, but that one already", "id": "Td01vFP3uJo_51", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Or are they all synced\nup so that they all get updated at the same time? PETER SZOLOVITS:\nThey tend to all get updated at the same time. There are disasters\nthat have happened in that updating process. Famously, the Beth Israel was\ndown for about three days. Their computer\nsystem just crashed. And what they discovered is that\nthey had this very complicated network in which there were\ncyclic dependencies in order to boot up different systems. So some system had\nto be up in order to let some other\nsystem be up, which had to be up in order to\nlet the first system be up. And, of course, in\nnormal operation, they never take down\nthe whole system, and so nobody had discovered\nthis until there was-- Cisco screwed them. There was some fix in the\nrouters that caused everything to crash, and then they\ncouldn't bring it back up again.", "id": "Td01vFP3uJo_52", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  John Halamka, who's\nthe CIO there, is a former student of mine. And after this all\nplayed out, I asked John, so what's the first thing\nyou did when this happened? And he said, I sent a\ncouple of panel trucks down to the Staples warehouse\nto buy pads of paper, which is pretty smart. So here's another example. This is lead screening. And so this was a case where\nthere is a lead screening rule for two-year-olds. There is also one for one-,\nthree-, and four-year-olds. And there was no change in\nscreening for one-, three-, and four-year-olds, but the\nscreening for two-year-olds went from 300 or 400 a day down\nto 0 for several years before", "id": "Td01vFP3uJo_53", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And they never did quite\nfigure out what happened here, but something added two\nincomplete clauses to the rule having to do with gender\nand smoking status. But the clauses were incomplete,\nand so they were actually looking for the case of neither\nthe gender nor the smoking status having been specified. So smoking status\nfor a two-year-old, you could imagine, is\nnot often specified, but gender typically is. And so the rule never\nfired because of that, and they have no idea how\nthese changes were made. There's a complicated\nlogging system that logs all the changes, and\nit crashed and lost its logging data. And it's a just so story.", "id": "Td01vFP3uJo_54", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And so they wound\nup-- they found this very quickly, because they\nhad a two-month-old boy who had numerous duplicate\nreminders, including suggestions for\nmammograms, pap smears, pneumococcal vaccination,\nand cholesterol screening, and a\nsuggestion to start the patient on various meds. So this was just a human\nerror in revising the rule, and that one they\nfound pretty quickly. So that's amusing. But what's interesting is these\nguys went on to say, well, how could we monitor for this\nin some ongoing fashion? And so they said, well,\nthere's this notion of change point detection,\nwhich is an interesting machine learning problem, again.", "id": "Td01vFP3uJo_55", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  that includes seasonality,\nbecause we have to deal with the fact\nthat a lot of stuff happens Monday through\nFriday and nothing happens on weekends? And so they created\na model that says that your output is some\nfunction, f, of your inputs, plus some noise. The noise is Gaussian with\nsome variance, capital V, and that x evolves according\nto some evolution that says it depends on the\nprevious value of x, plus some other noise,\nwhich is also Gaussian. So that's the general sort of\ntime series modeling approach that people often take. And then they said, well, we\nhave to deal with seasonality. So what we're going to do is\ndefine a period, namely a week, and then we're going\nto separate out the states on different\ndays of the week", "id": "Td01vFP3uJo_56", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  I worked on a different\nproject having to do with outbreak detection\nfor infectious diseases, and there the\nperiodicity was a year, because things like the\nflu come in yearly cycles rather than in weekly cycles. And so that idea\nis pretty common. And then they built this\nmultiprocess dynamic linear model that says,\nbasically, imagine that our data is\nbeing generated by one of a set of these\ndynamic linear models. And so we have an\nadditional state variable at each time that says\nwhich of the models is in control to generate\nthe data at this point. And so if you have the set of\nobservations up to some time,", "id": "Td01vFP3uJo_57", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  that model i is driving the\ngenerator at this point. And so you can have\nthree basic models. You can have a\nmodel that says it's a stable model, in\nother words, what you expect is the steady state. So that would be the normal\nweekly variation in volume for any of these alerts. You can have a model which\nis an additive outlier. So that's something that says,\nall of a sudden, something happened, like that\nchlamydia screen or one of the other things\nthat had a very quick blip. Or you can have a\nlevel shift change, like the change that\nhappened when the screening rules or the alert\nrule for amiodarone stopped firing, because\nit went from one level to a very different level\nover a period of a relatively short period of time. And then what you can do is\ncalculate the probability", "id": "Td01vFP3uJo_58", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  and that's called the\nchange point score. And you can calculate this from\nthe data that you're given. And of course, they\nhave tons of data. It's a big hospital and\nlots of these alerts go on. And if you plot this, there's\nthe data for a time series. So you see the weekly variation. But what you see is\nthat the probability of the steady behavior is quite\nhigh except at certain points where it all of a sudden dips. And so those are places where\nyou suspect that something interesting is going on. And similarly, the probability\nof a temporary offset goes up at these various points,\nand the probability of a level", "id": "Td01vFP3uJo_59", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And you can see\nthat, indeed, there is a level shift\nfrom essentially 0 up to this periodic behavior in\nthe original data sequence. And so they actually implemented\nthis in the hospital, and so now you get\nnot just alerts, but you get\nmeta-alerts that say, this kid ought to be screened\nfor their lead levels, but also the lead\nlevel screening rule hasn't fired as often as\nwe expected it to fire. Yeah, so there are\na lot of details in the paper that you can\nlook up, if you're interested. And what they find\nis that, if you look at the area under the\ndelay false positive rate curve, so you're trading off how\nlong it takes to be certain that one of these conditions has\noccurred versus how often you", "id": "Td01vFP3uJo_60", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  much better than a\nbunch of other things that they tried it against,\nwhich are earlier attempts to do this. And these are all highly\nstatistically significant, so they got a nice\npaper out of it. In the remaining\ntime, I wanted to talk about a number of other\nissues that really have to do with workflow. So we've talked about\nalerting, but there are an interesting\nset of studies about how these alerting\nsystems actually work. So there was a cool idea from\nthe Beth Israel Deaconess Hospital here in Boston\nwhere they said, well, what we really need to\ndo is to escalate alerts. So, for example, it's\nquite typical in a hospital", "id": "Td01vFP3uJo_61", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  just sent their\nblood to the lab, and let's say there serum\npotassium comes back as 7 or 8, that patient is at high risk of\ngoing into cardiac arrhythmia and dying. And so your pager, in\nthose days, goes off, and you read this text\nmessage that says, Mr. Jones has a\nserum potassium of 8. You'd better look in on him. So what they did\nwas very clever. They said, well, the problem is\nbusy doctors might ignore this. And so we'll then start\na countdown timer. And we'll say, did Dr.\nSmith actually come and look at Mr. Jones within 20 minutes? And if the answer\nis no, then they send the page to the\ndoctor's boss that says, hey, we sent this guy a\npage, and within 20 minutes", "id": "Td01vFP3uJo_62", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And then they start\nanother timer. And they say, if that boss\ndoesn't respond within an hour, then they send a page to\nthe head of the hospital saying, you're her\ninfectious disease people are doing a lousy job,\nbecause they're not-- or in this case, you're\nendocrine people, or whatever, are doing a lousy job,\nbecause they're not responding to these alerts. Now, how do you think\nthe doctors liked this? Not much. And there is a real\nproblem with overalerting. And there is no\ngeneral rule that says, how often can you bug\nthe head of the hospital with an alert like this before\nhe or she just says, well, turn off the damn thing,\nI don't want to see these? And clearly, if you set the\nthresholds at different places,", "id": "Td01vFP3uJo_63", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So, for example, I remember\nTufts implemented a system like this back in the 1980s,\nbut they would send a page on every order where any of\nthe lab results were abnormal, and that was way too much. Because a lot of these\ntests generate 20 results. Normal is defined as the\n95% confidence interval. What are the chances\nthat out of 20 tests, which aren't really independent,\nbut if they were, one of them would be pretty guaranteed\nto be out of range for most of the patients? And so basically every\ntest generated an alert to the doctor. And the doctors did\nthreaten to kill the people who had\nimplemented the system, and it got turned off. A system like this, if\nyou set the threshold to be not abnormal, but\nlife-threateningly abnormal,", "id": "Td01vFP3uJo_64", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  durations such that it's\nreasonable for people to respond to it, then\nmaybe it can be acceptable. When we did this project on\nlooking at how an emergency department could anticipate\na flood of patients because it looked like\nflu season was starting, for example, the\nquestion we asked is, how many false alarms a\nmonth can you guys tolerate? And they thought about it. And the ED docs got together\nand said, three times a month you can cry wolf,\nbecause we really want to know when\nit actually happens. And we'd rather be prepared,\nand we can tolerate a 10% error rate on this prediction. But I don't know what\nit is in this domain.", "id": "Td01vFP3uJo_65", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  it's become quite popular. I got a bunch of\nemails from my doctor today, because I had ordered\na refill on some prescription, and he wanted to know how it's\ngoing, and blah, blah, blah. So the BI asked the question,\nwhat fraction of those messages are never read by the\npatients that they're sent to? Which is an important\nquestion, because if you're relying on that mode\nof communication as part of your workflow,\nyou'd like it to be 0. It turned out only to be 3%,\nwhich is remarkably good. That means that most people\nare actually paying attention to those kinds of messages. Then I wanted to say a few\nwords about the importance of communication\nand then finish up by mentioning some so\nfar failed attempts at really good integration of\nall different data sources.", "id": "Td01vFP3uJo_66", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  with a system that\nsaid, if you're taking a renally-excreted\nor a nephrotoxic drug, then we're going to\nwarn people if there is a rising creatinine\nlevel, which is an indication that\nyour kidneys are not functioning so well. Because, of course, if the\ndrug is renally excreted, that means that if your kidneys\nare not excreting things at the rate they're\nsupposed to, you're going to wind up building up\nthe amount of drug in your body, and that can become toxic. So they saw a 21-hour,\nso almost a full day, reduction in response time\nfrom the medical staff given these alerts versus\nwhat happened before. That's remarkable. I mean, saving a\nday in responding to a condition like\nthis is really quite an impressive\nresult. And they also", "id": "Td01vFP3uJo_67", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  that the risk of\nrenal impairment was reduced to about half of\nthe preintervention level. So that earlier\nresponse actually was saving people's\nkidney function by getting people to\nintervene earlier. I found it interesting\nthey said 44% of doctors found these alerts helpful,\n28% found them annoying, but 65% of them\nwanted them continued to be used in a survey. Enrico Carrera is\none of my heroes. He used to be in the UK. He's now in Australia. And he had this very deep\ninsight back in the 1980s. He said, you know,\nall you computer guys who are treading on\nthis medical field", "id": "Td01vFP3uJo_68", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  but it's not. All of the action is\nreally about communication, that health care is\nbasically a team sport. And unless we spend\nmuch more time studying what goes\non in communication, we're going to miss the boat. And then mostly, we didn't\npay any attention to him, but he's kept at it. So he said, well, how big\nis the communication space? So he cited a 1985 study\nthat said that about 50% of requests for information\nare ones that people ask their colleague\nfor versus 26% that they look up\nin their own notes. So if a doctor is on rounds,\nwalks into a patient's room", "id": "Td01vFP3uJo_69", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  going up or down, a quarter of\nthe time he'll look at notes. And half the time, he'll\nturn to the nurse and say, is this patient's\ntemperature going up or down? So he says that's interesting. Paul Tang did a study\nin the '90s that said that in a clinic,\nabout 60% of the time is spent talking among the\nstaff, not doing anything else. Enrico and one of his\ncolleagues said that almost 100% of non-patient\nrecord information, in other words, the thing\nthat's not in the written health record, is done by talking. That's almost tautological,\nbecause where else would you", "id": "Td01vFP3uJo_70", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And then Charlie Saffron at the\nBI did a time and motion study and was looking at, I think,\nnursing behavior, and saying that about half their time was\nface-to-face communication, about 10% with electronic\nmedical records, and also a lot of email,\nand voicemail, and paper reminders as ways of\ncommunicating among people. So this was a study looking at-- this is that 1998 study\nby Colera and Tombs. And they're looking at\na consultant, the house officer, another consultant. These are British\ntitles, because this was done in Australia-- a nurse, et cetera. And they say, OK,\namong hospital staff-- I think this was in\none shift, I believe,", "id": "Td01vFP3uJo_71", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  this is the number of pages\nthat they sent and received. So they range from\n0 up to about 4. The number of telephone\ncalls made and received-- this ranges from 0 up to 13. Oh, here's the length\nof observation. So this was over a period\nof about three hours for each of these patients. And this is the total\nnumber of events. So think about it. In 3 and 1/2 hours, the\nsenior house officer had 24 distinct communication\nevents happen to that person. So that means, what,\nthat's like 7-- yeah, like 7 an hour. So that's like 1 every\n10 minutes, roughly. So it's an interrupt-driven\nkind of environment.", "id": "Td01vFP3uJo_72", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  three and a quarter\nhours of observation. This person spent 86%\nof their time talking. 31% were taken up\nwith 28 interruptions. So even the interruptions\nwere being interrupted. 25% were multitasking with\ntwo or more conversations. 87%, face-to-face or\non a phone or a pager. So most of that is talk time. And 13% dealing with\ncomputers and patient notes. So the communication\nfunction is really important. And I don't have anything\nprofound to say about it other than I'll put up a pointer\nto some of these papers. But the kinds of things\nthey're considering are, well, we could\nintroduce new channels, or new types of messages, or\nnew communication policies", "id": "Td01vFP3uJo_73", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  the person who's\ntaking care of patients while they're doing it,\nor something like that. And then moving from synchronous\nto asynchronous methods, like voicemail, or\nemail, or Slack, or some modern\ncommunication mechanism. Let me skip by these. Next to the last\ntopic, quickly, how do you keep from\ndropping the ball? So there are a lot\nof analyses that say that the biggest\nmistakes in health care are made not because somebody\nmakes the wrong decision, but it's because somebody\nfails to make a decision. They just forget\nabout something. They don't follow-up on\nsomething that they ought to. The patient is going along,\nand you think everything's OK,", "id": "Td01vFP3uJo_74", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So inspired partly by\nthat escalation of pagers that I read about\nat the Beth Israel, I said, well, this sounds\nlike what we really need is a workflow engine that's\napproximately a discrete event simulator. So has anybody built a discrete\nevents simulator in this class? It's a fairly standard sort\nof programming problem, and it's useful in simulating\nall kinds of things that involve discrete events. And the idea is that\nyou have a timeline, and you run down the\ntimeline, and you execute the next\nactivity that comes up. And that activity\ndoes something. It sends an email, or it shoots\na rocket, or whatever field you're doing the simulation in. But most importantly, what\nit does is-- the last thing it does is it schedules\nsomething else to happen later", "id": "Td01vFP3uJo_75", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So, for example, for something\nthat happens once a day, when it happens, the task\nthat runs schedules it to happen again the next day. And that means that it's going\nto be continually operating all the time. So the idea I had was that what\nyou'd like to do is to say, if at some time, t,\nI have a task that says do x or asks\nz to do y, or both, then the last thing\nshould be at some time in the future schedule another\ntask that says, is y done? And if not, then go notify\nsomebody or go remind somebody. And as far as I\nknow, no hospital and no electronic record system\nhas any capability like this, but I still think\nit's a terrific idea. And then I wanted to\nfinish with a pointer to a problem that is\nstill very much with us.", "id": "Td01vFP3uJo_76", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  wrote this thing we called\n\"The Guardian Angel Manifesto.\" And the idea was that we\nshould engage patients more in their own\ncare, because they can keep track of\na lot of the things that systems didn't do a very\ngood job of keeping track of. And the idea was that you would\nhave a computational process that would start off at the\ntime your parents conceived you and run until your\nautopsy after you died. And during this\ntime, it would be responsible for collecting\nall the relevant health care data about you. So it would be your\nelectronic medical record, but it would also be active. So it would help you\ncommunicate with your providers. It would help educate you\nabout any conditions you have. It would remind\nyou about things. It would schedule stuff\nfor you, et cetera. So this was a nice\nscience fiction vision.", "id": "Td01vFP3uJo_77", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  who was a VP of\nGoogle, came to me. And he said, you know,\nI read your thing. It's a good idea. I'm going to do it. So Google started up this thing\ncalled Google Health, which was more focused on being\nat least the personal health record. They did a pilot with 1,600\npeople at Cleveland Clinic, and then they went\npublic as a beta. And three years\nlater, they killed it. And they had a\nbunch of partners. So they had Allscripts,\nand Beth Israel, and Blue Cross of Massachusetts,\nand the Cleveland Clinic, and CVS, and so on. So they did their job\nof trying to connect to a bunch of important players. But, of course, they\ndidn't have everybody. And so, for example,\nI, of course, immediately signed\nup for an account,", "id": "Td01vFP3uJo_78", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  was Walgreens, where\nI had bought a skin cream one time for a skin rash. And so my total medical\nrecord consisted of a skin rash\nand a cream that I had bought to take care of it-- not very helpful. And so nobody, other\nthan these partners, could enter data\nautomatically, which meant that you had to be even\nmore anal compulsive than I am in order to\nsit there and type in my entire medical\nhistory into the system, especially, because if I did so,\nnobody would ever look at it. Because if I go to\nmy doctor and say, hey, Doc, here's the Google\nURL for my medical record, and here's the password by\nwhich you can access it, what do you think are the\nodds that they're actually going to look? AUDIENCE: 0.", "id": "Td01vFP3uJo_79", "title": "21. Automating Clinical Work Flows", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So the thing was an\nabsolute abject failure. And people keep trying it. And so far, nobody has\nfigured out how to do it, but it's still a good idea.", "id": "Td01vFP3uJo_80"}, {"text": "  PROFESSOR: All right. Let's get started. Welcome, ladies and gentlemen. Today it's my pleasure to\nintroduce two guest speakers who will talk about\nthe regulation of AI and machine learning and about\nboth the federal FDA level regulation and about\nIRB issues of regulation within institutions. So the first speaker\nis Andy Coravos. Andy is the CEO and founder\nof Elektra Labs, which is a small company that's doing\ndigital biomarkers for health care. And Mark is a data\nand software engineer", "id": "k95abdkdCPk_0", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Sinai in New York\nand was kind enough to come up to speak to us today. So with that, I'm\ngoing to introduce them and sit back and enjoy. ANDY CORAVOS: Thank you. Thank you for having us. Yeah, so I am working\non digital biomarkers, and I'm also a research\ncollaborator at the Harvard MIT Center for Regulatory Sciences. So you all have a center\nthat is looking just at how regulators should think\nabout some of these problems. And then I'm also an\nadvisor at the Biohacking Village at DEFCON, which we can\ntalk a little bit more about. My background-- I'm\na software engineer, had worked with the FDA formerly\nas an entrepreneur resident in the digital health unit,\nand then spent some time in corporate land. MARK SHERVEY: I'm Mark Shervey. I work at the Institute for\nNext Generation Healthcare at Mount Sinai. I've been there about\nthree years now. My background is in software\nand data engineering,", "id": "k95abdkdCPk_1", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So this is a new spot. And most of my\nresponsibilities focus around data security and IRB and\nethical responsibilities. ANDY CORAVOS: We also know\nhow much people generally love regulatory\nconversations, so we will try to make this very\nfun and exciting for you. If you do have questions,\nas regulations are weird and they're constantly\nchanging, you can also shoot us a note on Twitter. We'll respond back if you\nhave things that come up. Also, the regulatory\ncommunity on Twitter, amazing. When somebody comes\nout with, like, what does real world\ndata actually mean, everybody is talking\nto one another. So once you start tapping into-- I'm sure you have your\nown Twitter communities, but if you tap into the\nregulatory Twitter community, it is a very good one. The digital health unit\ntweets a lot at the FDA. OK. Disclaimers-- these are our\nopinions and the information that you'll see here\ndoes not necessarily reflect the United States\ngovernment or the institutions", "id": "k95abdkdCPk_2", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And policies and regulations\nare constantly changing. So by the time we have\npresented this to you, most likely parts\nof it are wrong. So you should definitely\ninteract early and often with relevant\nregulatory institutions. Your lawyers might\nsay not to do that. There are definitely\ndifferent ways, and we can talk through\nhow you'd want to do that. But especially as a software\nengineer and developing anything on the\ndata side, if you spend too much time developing\na product that is never going to get through, it is\nreally a wasted period of time. So working with the\nregulators, and given how open they are right\nnow to getting feedback, as you saw with the\npaper that you read, is going to be important. And then the last thing, which\nMark and I talk a lot about, is many of these definitions\nand frameworks have not actually happened yet. And so when somebody\nsays a biomarker, they might actually\nnot mean a biomarker, they might mean a measurement.", "id": "k95abdkdCPk_3", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  When someone's like, I work\nin AI, and you're like, what does that actually mean? So you should ask us questions. And if you think about it, the\ntype of knowledge that you have is a very specific, rare set\nof knowledge compared to almost everybody else in the country. And so as the FDA\nand other regulators start thinking about how\nto regulate and oversee these technologies, you can\nhave a really big amount of influence. And so what we're going\nto do is a little bit of the dry stuff\naround regulatory, and then I am going to\nsomewhat plead with you and also teach you how to submit\npublic comments so that you can be part of this\nregulatory process. And then-- MARK SHERVEY: I will speak about\nthe Institutional Review Board. How many people in here\nhave worked with an IRB or aware of them? OK, good. That's a good mix. So it'll be a quick\nthing, just kind of reviewing when\nto involve the IRB,", "id": "k95abdkdCPk_4", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  need the IRB for and\nsome things that you don't, as an alternative\nto taking the FDA approach. ANDY CORAVOS: All right, good. And then I'll go first, and\nthen we'll go through IRBs, and then we'll leave the last\npart for your impressions of the paper. OK. So before I start,\nI'll ground us in some ideas around\nalgorithmically-driven health care products. So as you know, these\ncan have wide ranges of what they can do. A general framework that I\nlike to use to think about them is products that measure,\nthat diagnose, or treat. So measurement products\nmight include things like digital biomarkers or\nclinical decision support. Diagnostics might take that\nmeasurement and then say whether or not somebody\nhas some sort of condition given those metrics. And then treatment are ideas\naround digital therapeutics. How many people here think that\nsoftware can treat a person?", "id": "k95abdkdCPk_5", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  OK. And I think one thing that\npeople don't think about always when they have these\nsorts of tools is-- and you all probably think\nabout this a lot more-- is even something as simple as\na step count is an algorithm. So it takes your gyroscope,\naccelerometer, height, weight, and age, and then it\npredicts whether or not you've made a step. And if you think about the\ntypes of different steps that people make, older\npeople drag their feet a little bit more\nthan younger people. So an algorithm for\na step count looks very different from an algorithm\nfor younger people for step count. And so all of these tools\nhave some level of error, and they're all\nalgorithms, effectively. One of my favorite frameworks\nas you start thinking about-- a lot of people are very\ninterested in the measurement side around what's called\ndigital biomarkers. And it turns out\nthat the FDA realized that many people, even\nwithin their own agency, didn't know what\na biomarker was, and everyone was using the\nterm slightly differently, probably how people approach\nyou slightly differently of what machine learning actually is.", "id": "k95abdkdCPk_6", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  around the seven different\ntypes of biomarkers that I'd highly recommend you\nread if you go into this area. A digital biomarker\nonly, in my definition and how other people\nhave started to use this, is the way that that\nmeasurement is collected. And so you might have\na monitoring biomarker, a diagnostic\nbiomarker, but it is collected in an\nambulatory, remote way that is collecting digital data. And this type of\ndata is very tricky. To give you an example of why\nthis is particularly difficult to regulate, so\nthink about a couple of products that just\nlook at something that would be simple, like AFib. So AFib is an abnormal\nheart condition. You might have seen in\nthe news that a number of different companies are\ncoming out with AFib products. Put simply, there is\nobviously a large stack of different types of data,\nand one person's raw data is another person's\nprocessed data. So what you see on\nthis chart is a list of five different\ncompanies that are all developing AFib products,\nfrom whether or not", "id": "k95abdkdCPk_7", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  which is the green part,\nversus whether or not they might use a third\nparty product, so developing an app on top of\nsomebody else's product. And so in a broad way, thinking\nabout going from the operating system to the sensor data. So somebody might be using\nsomething like a PPG sensor and collecting this sort\nof data from their watch and then doing some sort\nof signal processing, then making another\nalgorithm that makes some sort of diagnostic. And then you have some sort of\nuser interface on top of that. So if you are the FDA, where\nwould you draw the line? Which part of this\nproduct, when somebody says my product is\nvalidated, should it be actually validated? And then thinking\nabout what does it actually mean if something\nis verified versus validated. So verified being like,\nif I walk 100 steps, does this thing\nmeasure 100 steps? And then validation\nbeing, does 100 steps mean something for\nmy patient population or for my clinical use case?", "id": "k95abdkdCPk_8", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to think through is how might\nyou decouple the hardware components from the\nsoftware components, where you think about some of the\nhardware components as the ways that you would-- effectively, the supply chain\nfor collecting that data, and then you would be\nusing something on top. And so maybe you\nhave certain types of companies that might do\nsome sort of verification or validation lower\ndown the stack, and then you can\ninnovate higher up. And so these measurements have\npretty meaningful impacts. So in the past, a\nlot of these tools, you really had to\ngo into the clinic. It was very expensive to get\nthese sorts of measurements. And more and more, a number\nof different companies are getting their\nproducts cleared to use in some care\nsettings with a doctor or possibly to inform\ndecisions at home. All right. And so in the last\nof some examples is around digital therapeutics.", "id": "k95abdkdCPk_9", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  was using a technology based\nout of UCSF that is developing, effectively, a video\ngame for pediatric ADHD. And so when kids\nplay the game, they reduce their ADHD symptoms. And one of things that's pretty\nexciting about this game it is a 30-day protocol. And unlike something\nlike Ritalin or Adderall, where you have to take\nthat drug every day for the rest of your life\nas you reduce the symptoms, this seems to have an\neffect that, after 30 days, is more long-term and that when\nyou test somebody months down the line, they still retain\nthe effects of the treatment. So this technology\nwas taken out of UCSF and licensed to a company\ncalled Akili, who decided, hey, we should just structure\nourselves like a drug company. So they raised venture\ncapital like a drug company, they ran clinical trials\nlike a drug company, and they're now\nsubmitting to the FDA and might be the first\nprescription drug. So anybody who was told\nthat video games might rot your brain, you now\nhave a revenge, maybe.", "id": "k95abdkdCPk_10", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  I don't have to tell\nyou, you're probably thinking a lot about it. And the FDA has been clearing a\nnumber of these different types of algorithms. And one of the questions\nthat has come up is, what part of the agency\nshould you think about? What are you claiming when you\nuse these sorts of algorithms? And what ones should be\ncleared and what's not? And how should we really think\nabout the regulatory oversight for them? And a lot of these\ntechnologies enable things that are really quite exciting, too. So it's not just\nabout the measurement, but what you can do with them. So one thing that has a lot\nof people really excited about is an idea around\ndecentralized clinical trials. No block chains here. You might be able to build\nit with a blockchain, but not necessary. So on the y-axis, you\ncan think about where are the data collected. So is it collected\nat a clinical site,", "id": "k95abdkdCPk_11", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And then the method\nis how it's collected. So do you need a human\nto do the interaction, or is it fully virtual? So at the top you can\nthink about somebody doing telemedicine, where they\ncall into somebody at home and then they might\nask some questions and fill out a survey. On the bottom, you can\nimagine in a research facility where I'm using a number\nof different instruments, and perhaps I'm in\na Parkinson's study and you're measuring my\ntremor with some sort of accelerometer. And so the challenge that's\nhappening is a lot of people use all of these terms\nfor different things when they mean\ndecentralized trials. Is it telemedicine? Is it somebody\nwho's instrumented with a lot of wearables? How do you know that\nthe data are accurate? But this is, I think, in many\ninstances really exciting because the number\none reason why people don't want to\nenroll in a clinical trial is to get a placebo. I think nobody really\nwants to participate in research if you're not\ngetting the actual drug. And then the other\nreason is location. People don't want to\nhave to drive in, find parking, participate. And this allows people\nto participate from home.", "id": "k95abdkdCPk_12", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  around how to rethink the\nclinical trial design process and incorporate some\nof this real world data into decision-making. Before I jump into some\nof the regulatory things, I want to just set a\nframework of how to think about what these tools can do. So these are three\ndifferent scenarios of how you might use software\nin a piece of clinical research. So imagine that somebody\nhas Parkinson's and you want to measure how\ntheir Parkinson's is changing over time using\na smartphone-based test. You have a standard Parkinson's\ndrug that they would use, and then you would collect\nthe endpoint data, which is how you see if that\ndrug has performed using a piece of software. Another idea would be, say\nyou have an insulin pump and then you have a CGM that\nis measuring your blood sugar levels, and you want to\ndose the insulin based on those readings. You might have software both\non the interventional side and on the endpoint side.", "id": "k95abdkdCPk_13", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  which has a digital\nproduct, they said the only thing we\nwant to change in the study is that the\nintervention is digital, but we want you to compare us\nlike any other intervention for pediatric ADHD. So we want to use\nstandard endpoints and not make that an innovation. The challenge here is the\nfirst one, most likely, would go to the drug\nside of the FDA. The second one would go to\nboth the drug and the device side of the FDA as a\ncombination product. And the final one would\njust go to devices, which has been generally\nhandling software. We've never really had products\nat the FDA, in my opinion, where-- we don't have drugs that\ncan measure, diagnose, and treat and change\nall the different ways. And so you're now\nhaving software hitting multiple different\nparts of a system, or it might even be the same\nproduct, but in one instance it's used as an intervention,\nanother instance it's used as a diagnostic,\nanother it's to inform or expand labeling. And so the lines are\nnot as clean anymore", "id": "k95abdkdCPk_14", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So how do you manage these? There's a couple\nagencies that are responsible for thinking through\nand overseeing health care software. The big one that we'll spend\nmost of our time on is the FDA. But it's also worth\nthinking about how they interact with\nsome of the other ones, including ONC, FCC, and FTC. So the FDA is responsible\nfor safety and effectiveness and for facilitating\nmedical product innovation and ensuring that\npatients have access to high quality products. The ONC is responsible for\nhealth information technology. And you can imagine where the\nlines between storing data and whether or not you're making\na diagnosis on that data start to get really\nvague, and it really might be the exact\nsame product but just the change of what claim\nyou're making on that product. And most of these products\nhave some level of connectivity", "id": "k95abdkdCPk_15", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and have to abide by the ways\nthat these tools are regulated by this agency. And then finally, and\nprobably most interesting, is around the FTC,\nwhich is really focused on informing consumer choice. And if you think\nabout FDA and FTC, they're actually really similar. So both of these agencies\nare responsible for consumer protection, and the FDA really\ntakes that with a public health perspective. So in many instances, if you've\nseen some of the penalties around somebody having\ndeceptive practices, it actually wasn't the FDA who\nstepped in, it was the FTC. And I think some\nof the agencies are thinking about where\ndo their lines end and where do others begin. And in many instances,\nas we've really seen with a lot of probably bad\nbehavior that happens in tech, there's really gaps\nacross multiple places where nobody's stepping in.", "id": "k95abdkdCPk_16", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to think about. An important one is around\nstandards and technology. You probably think\nabout this all the time with interoperability\nand whether or not you can actually\nimport the data. There are people who\nspend a lot of time thinking about standards. It is a very painful\nand very important job to promote innovation. OK. So the FDA has multiple centers. I'm going to use\na lot of acronyms, so you might want to write\nthis down or take a picture. And I'll try to\nminimize my acronyms. But there are three centers that\nwill be the most interesting for you. So CDER is the one\nfor drugs, and this is the one where you\nwould have a regular drug and possibly use a\nsoftware product to see how that drug is performing. CDHR is for devices. And CBER is for\nbiological products. I will probably use\ndrugs and biologics in a very similar sort of way. And the distinctions that\nwe'll spend most of our time on are around drugs versus device. There's a bunch\nof policy that is coming out that is both exciting\nand making things change.", "id": "k95abdkdCPk_17", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  This has accelerated a lot\nof innovation in health care. It's also changed the\ndefinition of what device is, which has a pretty meaningful\nimpact on software. And the FDA has\nbeen thinking a lot about how you would actually\nincorporate these products in. I think there is a\nlot of people who are really excited about them. There's a lot of\ninnovation, and so how do we create standards\nboth to expand labeling, be able to actually\ningest digital data, and have these sorts\nof digital products that are actually\nunder FDA oversight and not just weird snake\noil on the app store? But what is a medical device? Pretty much, a device\nis like anything that's not the\nother centers, which has a big catch-all for\nall the other components. And so one of the big\nchallenges for people", "id": "k95abdkdCPk_18", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  If you think about\ngenerally what the FDA does, it doesn't always make\nsure that your products are safe and effective. They check whether or\nnot you claim that they are safe and effective. So it's really all\nabout claims management and what you're claiming\nthat this product can do and evaluating\nthat for marketing. Obviously if your product\ncauses very significant harm, that is an issue. But the challenge really happens\nto be when somebody makes-- the product can do something\nthat it doesn't necessarily claim to do, but then\nyou are able to imply that it does other things. Most people don't really have\na really good understanding of what the difference\nis between informing a product versus\ndiagnosing a product, and so I think in many\ninstances for the public, it gets a bit confusing. So as we talked\nabout before, the FDA has been thinking\nabout how do you decouple the hardware\nfrom the software,", "id": "k95abdkdCPk_19", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  as a medical device,\nso the software that is effectively defined\nas having no hardware components where you can\nevaluate just this product, and this is pronounced SaMD. And SaMDs are\npretty interesting. This is very hard to\nread, but I pulled it straight from legal\ndocuments, so you know I'm not changing it. So something that's\ninteresting about SaMD-- so if you go all\nthe way to the end-- so if you have electronic health\ncare data that's just storing health data, that is\nnot a SaMD and often can go straight to market and\nis not regulated by the FDA. If you have a piece of software\nthat is embedded into a system, so something like a pacemaker\nor a blood infusion pump, then that is software\nin a medical device, and that's not a SaMD. So there's a line between these\nabout what the functionality is that the product is doing,\nand then how serious is it, and that informs how you would\nbe evaluated for that product.", "id": "k95abdkdCPk_20", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  the term device. So when I talked about\nthese connected wearables and other sorts of tools, I\nwill use the word tool and not device because this has a very\nspecific meaning for the FDA. And so if you're curious whether\nor not your product is a device or your algorithm is\na device, first, you should talk to the regulators\nand talk to your lawyer. And we'll play a little game. So there are two products here. One is an Apple product and\none is a Fitbit product. Which one is a device? I'm going to call on\nsomebody randomly, or someone can raise their\nhand and offer as tribute. OK, which one? AUDIENCE: I think Apple\nreceived 510(k) clearance, so I'd say the\nApple Watch device. I'm not sure about the Fitbit,\nbut if it's one or the other, then it's probably not.", "id": "k95abdkdCPk_21", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So we'll talk about this. Apple did submit to\nthe FDA for clearance, and they submitted\nfor de novo, which is very similar to a 510(k). And they submitted for\ntwo products, two SaMDs. One was based on the\nsignal from their PPG, and the second was on the app. So it has two devices,\nneither of which are hardware. And the Fitbit has,\ntoday, no devices. How about now? Is it a device, or\nis it not a device? Trick question,\nobviously, because there are two devices there,\nand then a number of things that are not devices. So it really just\ndepends on what you are claiming the product does. And back to that\nset of modularity, what is actually the product? So is the product a signal\nprocessing algorithm? Is the product an app? Is the product the\nwhole entire system? And so people are\nthinking about,", "id": "k95abdkdCPk_22", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  are devices because you might\nwant somebody else to be building on your system. So maybe you want to make\nyour hardware a device, and then other people\ncan build off of it. And so there are strategic\nways of thinking about it. So the crazy thing here,\nif you can imagine this, is that the exact same product\ncan be a device or not a device through just a change of words\nand no change in hardware or code. So if you think about whether\nor not my product is a device, it's actually generally not\nthe most useful question. The more useful question\nis, what is the intended use of the product? And so, are you making a\nmedical device claim with what your product is doing? Obviously this is a\nlittle bit overwhelming, I think, in trying to figure\nout how to navigate all of this. And the FDA recognizes\nthat, and their goal is to increase innovation. And so particularly for\nproducts like software, they're", "id": "k95abdkdCPk_23", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  It seems a little bit\ndifficult if you're constantly figuring out all\nthe different words and how you're going to get\nthese products to market. So something that I think is\nreally innovative by the FDA is piloting-- this is one example of a program\nthat they're thinking through, which is working with\nnine different companies. And the idea is, can you\npre-certify an entire company that is developing software\nas an excellent company across a series of\nobjectives, and then allow them to ship\nadditional updates? So today, if you had\nan update and you wanted to make a\nchange, you have to go through an entire\n510(k) or de novo process or other type of process,\nwhich is pretty wild. If you imagine\nthat we only would let Facebook ship one update\na year, that would be crazy. And we don't expect Facebook\nto maintain or sustain a human life. And so being able\nto have updates in a more regular fashion\nis very important. But how do you know that\nthat change is going to have a big impact or not?", "id": "k95abdkdCPk_24", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  I'm actually very glad\nthat you read this document without talking to us because\nyou were the exact audience of somebody who\nwould not necessarily have the background,\nand so it needs to be put in a way that\nis readable for people who are developing these\ntypes of products to know how to go into them. We'll save some time at\nthe end of the discussion because I'm curious how\nyou perceived the piece. But you should definitely\ntrust your first reading as a honest, good reading. You also probably read\nit way more intensely than any other person\nwho is reading it, and so the notes that\nyou took are valid. And I'm curious what you saw. OK. Another thing to help you\nbe cool at cocktail hour. FDA cleared, not the same\nthing as FDA approved. OK. So for devices, there are\nthree pathways to think about. One is the 510(k),\nthe next is de novo, the next is a premarket\napproval, also known as a PMA.", "id": "k95abdkdCPk_25", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  something is risky. And the type of\ndata that you have to submit to be able to get\none of these clearances varies. So the more risky you\nare, the more type of data that you have to have. So de novos are granted, but\npeople often will say cleared. 510(k)s are cleared. Very few products that you've\nseen go through a PMA process. AUDIENCE: I have a question. ANDY CORAVOS: Tell me. AUDIENCE: Do you know\nwhy Apple chose to do a de novo instead of a 510(k)? ANDY CORAVOS: I am not\nApple, but if I had to guess, once you create a de\nnovo, you can then become a predicate for other things. And so if they wanted to create\na new class of predicates that they could then\nbuild on over time, and they didn't\nwant to get stuck in an old type of\npredicate system, I think, strategically,\nthe fact that they picked a PPG and their app-- I don't know what they'll\neventually do over time, but I think it's part of\ntheir long-term strategy.", "id": "k95abdkdCPk_26", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  OK. So the tools are safe\nand effective, perhaps, depending on how much\ndata is submitted. But what about the\ninformation that's collected from the tools? So today, our health care system\nhas pretty strong protections for biospecimens, your blood,\nyour stool, your genomic data. But we really don't\nhave any protections around digital specimens. You can imagine how many data\nbreaches we constantly have and what ads get served\nto us on Facebook. A lot of this is considered\nwellness data, not actually health data. But in many instances,\nyou are finding quite a lot of\nhealth information from somebody in that. And I have a lot more. We can nerd about this forever. But generally, there's\na couple of things that are good to know, is\nthat with most of this data, you can't really\nde-identify it anymore. Who here thinks I could\nde-identify my genome? You can't, right? My genome's unique to me.", "id": "k95abdkdCPk_27", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  information, but you're not\nreally going to de-identify it. I am uniquely identifiable\nwith 30 seconds of walk data. So all of this biometric\nsignatures is pretty specific. And so there are\nsome agencies today who are thinking\nabout how you might handle these sorts of tools. But in the end, there is, I\nthink, a pretty substantial gap. So in general, the FDA is really\nfocused on safety and efficacy, and safety is considered\nmuch more of a body safety and not as a we are very\nprogrammable as humans in the type of\ninformation that we see or change type of safety. So the data that we collect-- FTC could have a\nlot of power here, but they're a much\nsmaller agency that isn't as well-resourced. And there's a couple of\ndifferent organizations that are trying to\nthink through how to do rulemaking for\nInternet of Things and how that data is being used. But generally, in my\nopinion, we probably need some sort of\ncongressional action", "id": "k95abdkdCPk_28", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  would require a Congress\nthat could think through, I think, a really\ndifficult problem of how you would handle\ndata rights and management. OK. So I'll go through\na couple examples of how government\nagencies are interacting with members of the\npublic, which I think you might find interesting. So, many of the\ngovernment agencies are really thinking through,\nrealizing that they are not necessarily the\nexperts in their field in how do they get the\ndata that they need. So a couple pieces that will be\ninteresting for you, I think. One is there is a joint\ngroup with the FDA and Duke, where they're thinking through\nwhat's called novel endpoints. So if you are working\non a study today where you realize that you're\nmeasuring something better than the quote gold standard,\nand the gold standard is actually quite a\nterrible gold standard, how do you create and\ndevelop a novel metric that might not have a reference\nstandard or a legacy standard? And this is a way of\nthinking through that.", "id": "k95abdkdCPk_29", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  This used to be\ncalled mobile devices, and they changed it\nfor the same reason around not calling things a\ndevice unless it is a device. And so these are\nthinking through what type of connected\ntech would you want to use to generate the\npatient data that you might use in your study. All right. Who here knows what DEFCON is? Three of you. OK. So DEFCON is a\nhacker conference. It is probably one of the\nbiggest hacker conferences. It is a conference that if you\ndo have the joy of going to, you should not bring your\nphone and you should not bring your computer, and\nyou should definitely not connect to the internet\nbecause there is a group called the Wall of Sheep, and they\nwill just straight stream all your Gmail passwords\nplain text and your account logins and anything that you\nare putting on the internet. This group is amazing.", "id": "k95abdkdCPk_30", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  bought a number of voting\nmachines last year, hacked them, found\nthe voting records, and sent them back to\nCongress and said, hey, you should probably fix this. DEFCON has a number of villages\nthat sit under the main DEFCON. One of them is called\nBiohacking Village. And there is some\nbiohacking, so, like, doing the RFID\nchipping, citizen science. But there's also a set of people\nat Biohacking Village that do what's called\nwhite hat hacking. So for people who\nknow about this, there's black cat,\nwhere you might encrypt somebody's website\nand then hold them for ransom and do things that\nare disruptive. White hat hackers are\nconsidered ethical hackers, where they are doing security\nresearch on a product. So the hackers in the\nBiohacking Village started to do a lot of\nwork on pacemakers, which are connected technologies. A lot of pacemaker companies-- an easy way to think about how\nthey're thinking about this was the pacemaker companies\nare generally trying", "id": "k95abdkdCPk_31", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  They don't want to\ndo anything that's computationally expensive. Turns out, encrypting things\nis computationally expensive. They did a relatively\ntrivial exploit where they were able to\nreverse engineer the protocol. Pacemakers stay in a low power\nmode as long as they can. If you ping it, it will\nturn into high power mode, so you can drain a multi-year\nbattery of the pacemaker into a couple days or weeks. They were also able\nto reverse engineer the shock that a pacemaker can\ndeliver upon a cardiac event. And so this has pretty\nsignificant implications for what this exploit can do. With any normal\ntech company, when you have an exploit of this\ntype, you can go to Facebook, you can go to Amazon,\nthere is something called a coordinated disclosure,\nyou might have a bug bounty, and then you share the update,\nyou can submit the update, and then you're done. With the device companies,\nwhat was generally happening is the researchers\nwere going to the device companies, hey, we\nfound this exploit, and the device companies\nwere saying, thank you,", "id": "k95abdkdCPk_32", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And the security\nresearchers were like, why are you suing us? And they said, you're\ntampering with our product, we are regulated by\nagencies, we can't just ship updates whenever we want,\nand so we have to sue you. Turns out that is not true. And the FDA found out about\nthis and they're like, you can't just do\nsecurity researchers. If you have a security\nissue, you have to fix that. And so the FDA did something\nthat was pretty bold, which was three years\nago, they went to DEFCON. And if anyone has\nactually gone to DEFCON, you would know\nthat you do not go to DEFCON if you are\npart of the government because there is a game\ncalled Find the Fed, and you do not want to be found. And of course, NSA, CIA, a lot\nof members of the government will go to DEFCON,\nbut it is generally not a particularly\nfriendly environment. The Biohacking Village said,\nhey, we will protect you,", "id": "k95abdkdCPk_33", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  want to work together with you. And so over the\nlast three years, the agency has been\nworking closely with security researchers\nto really think through the best ways\nof doing cybersecurity, particularly for\nconnected devices. And so if you look at the\npast couple of guidances, there's a premarket and\npost-market guidance where they've been\ncollaborating, and they're very good\nand strong guidances. So the FDA did something\nreally interesting, which was in January,\nthey announced a new initiative, which\nI think is quite amazing, called #WeHeartHackers. And if you go to\nWeHeartHackers.org, the FDA has been encouraging\ndevice manufacturers, like Medtronic and BD,\nand Philips, and Thermo Fisher and others, to\nbring their devices and work together with\nsecurity researchers. Another group that is\nprobably worth knowing is that if you think\nabout what a lot of these connected products\ndo, they, in many instances,", "id": "k95abdkdCPk_34", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  their work. And so today, if\nyou are a clinician and you graduate\nfrom med school, you would take something like a\nHippocratic oath to do no harm. Should the software engineers\nand the manufacturers of these products also take\nsome sort of oath to do no harm? And would that oath look\nsimilar or different? And that line of\nthinking helped people realize that there are entire\nprofessional communities and societies for people\nwho do this sort of thing for doctors in\ntheir specialties, so a society for neuro\noncology, society for radiology. But there's really\nno society for people who practice digital medicine. So there is a group that is\nstarting now, which you all might like to join because\nI think you would all be part of this\ntype of community, which is the society for-- it's called the DIME Society. And so if you're\nthinking through, how do I do informed\nconsent with these sorts of digital products,\nwhat are the new ways that I need to think\nthrough regulation, how am I going to\nwork with my IRB,", "id": "k95abdkdCPk_35", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  All right. So how do you participate\nin the rulemaking process? One is, I would\nhighly encourage, if you get a chance to, to\nserve some time in government. There are more opportunities to\ndo that through organizations like the Presidential\nInnovation Fellow, to be an entrepreneur\nresident somewhere, to be part of the\nUS Digital Service. The payment system of CMS is\nmillions of line of COBOL, and so that obviously\nneeds some fixing. And so if you want\nto do a service, I think this is a\nreally important way. Another way that\nyou can do it is submitting to a public docket. And so this is something I\nwill be asking you to do, and we'll talk about it\nafter, is how can you take what you learned in\nthat white paper and ways that you can share back with the\nagency of how you would think about developing rules and laws\naround AI and machine learning.", "id": "k95abdkdCPk_36", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Mina wrote, which is that-- these are a couple\nof things to know. So anyone can comment,\nyou will be heard. If you write a very long\ncomment, someone at the agency, probably multiple, will have\nto read every single thing that you write, so please be\njudicious in how you do that. But you will be heard. And most of the time comments\ncome from big organizations and people who have\ncome together and not from the people who are\nexperiencing and using a lot of the products. So in my opinion,\nI think someone like you is a really\nimportant comment and voice for the agency to have, and to\nhave a technical perspective. Another way that you\ncan do this, which I'm going to put Irene\non the spot, is we need new regulatory paradigms. And so when you are out\nat beers or ice cream, or whatever you do for fun, you\ncan think through new models. And so we were kicking\naround an idea of,", "id": "k95abdkdCPk_37", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to think about AI in general? So algorithms\nperform differently on different patient populations\nand different groups. You need inclusion/exclusion\ncriteria. Should this be\nsomething maybe we even expand beyond health\ncare algorithms to how you decide whether or\nnot someone gets bail or teacher benefits? And then the fun thing about\nputting your ideas online, if you do that, is then\npeople start coming to you. And we realized there\nwas a group in Italy who had proposed a version\nof FDA for algorithms, and you start to\ncollect people who are thinking about things\nthat you're thinking about. And now we will\ndig into the thing that you most likely\nwill spend more time with than the government,\nwhich is your IRB. MARK SHERVEY: Thank you. OK. I could probably not give\nthe rest of this talk if you just follow the\nthing on the bottom. If you don't know if you're\ndoing human subject research, ask the IRB, ask your\nprofessor, ask somebody. I think most of what\nI'm going to say", "id": "k95abdkdCPk_38", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  around, and it's\nreally just to try to get the thought process going\nthrough your head of if we're doing actual human research,\nif the IRB has to be involved, what actually constitutes\nhuman research? And just to be sure that\nyou're aware of what's going on there all the time. We've done this. So research is\nsystematic investigation to develop or contribute\ngeneralizable knowledge. So you can do that on a rock. What's important\nabout human subjects research is that people's\nlives are on the line. Generally, the\neasiest thing to know is if there's any sort of\nidentifiable information with the data that\nyou're working with, that is going to fall under\nhuman subjects research.", "id": "k95abdkdCPk_39", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  There's all sorts of\nimaging training data sets that you can use\nthat are anonymized to what is an acceptable level. But to Andy's point,\nthere's really no way to truly\nde-identify a data set. And with the amount of data that\nwe're working with all right now in the world, it's\nbecoming impossible to de-identify any data set if\nyou have any other reference data set. So anytime you're\nworking with any people, you are almost\ncertainly going to have to involve the IRB, again. So why the IRB is there,\nit's not specifically to slap you on the wrists. It's not that\nanything's expected to purposely do anything wrong. Although that has happened,\nthat's such a small amount that it's just\nunhelpful to think", "id": "k95abdkdCPk_40", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So you're not going to do\nanything particularly wrong, but there are things that\nyou just may not know. And this is not the\nIRB's 1,000th rodeo, so if you bring\nsomething up to them, they'll know almost immediately. Participants are giving up\ntheir time and information, so the IRB, more than keeping\nthe institution from harm, is really protecting\nthe patients first and the institution\nat the same time. But the main role is to\nprotect the participants. Specifically, here's\nsomething that might not go through everybody's\nhead, research that may be questionable\nor overly manipulative. That gets into\ncompensation for studies. You can imagine certain places\nin an impoverished nation that you say, we'll\npay $50,000 per person", "id": "k95abdkdCPk_41", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  you can imagine people\nwant to be in that study and it can become a problem. So the IRB is also a\nhuge part of making sure that the studies aren't actually\naffecting anybody negatively in that kind of sense. Now, before I do, this next\nslide gets dark for a second, so we'll try to move through it. But it talks about how\nthe IRB came about. So we start with the Nuremberg\nCode, human research conducted on prisoners and\nothers, not participants but subjects of research. Tuskegee experiment,\nanother thing that people were not properly\nconsented into the studies.", "id": "k95abdkdCPk_42", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  so they couldn't\npossibly have consented. The study went for 40 years\ninstead of six months. And even after a standard of\ncare had been established, the study continued on. That essentially is what\nbegan the National Commission for Protection of Human\nSubjects, which led to the IRB being the requirement\nfor research. And then five years\nlater, the Belmont Report came out, essentially\nenumerating these three basic principles,\nrespect for participants, beneficence as far as\ndo no harm, don't take extra blood if it just\nmakes it more convenient, don't add extra drug if you\njust want to see what happens, and then just making\nsure that participants", "id": "k95abdkdCPk_43", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So we follow the Belmont Report. That's essentially\nthe state of the art that we have now with\nmodernization moving forward. This is not something\nto really worry about, but HHS has a\ngreat site that has a flow chart for just about any\ncircumstance that you can think of to decide if you're actually\ndoing human subjects research or not. This is pretty much\nthe most basic one. You can go through\nit on your own. Just to highlight the main thing\nthat I think you guys will all probably be worried\nabout, is you will be collecting identifiable\ndata, which just immediately puts you in IRB land.", "id": "k95abdkdCPk_44", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  a thing that's happening,\nyou're just there, so you don't really have\nto go through any of this. What is health data? So you have names obviously. Most of these are either\nidentifications or some sort of identifying thing. The two, I guess,\nthat a lot of people maybe gloss over that aren't\nso obvious is zip codes. You have to limit them to the\nfirst three numbers of a zip code, which gives a\ngeneralizable area without actually dialing\nin on a person's place. Dates are an extremely\nsensitive topic. So anytime you're working\nwith actual dates, which I assume in wearable\ntechnologies you're going to be\ndealing with time series data and that kind of stuff. There are different ways of\nmaking that less sensitive.", "id": "k95abdkdCPk_45", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  we're dealing with the\nelectronic health records, we deal in years, not in\nactual dates, which can-- it creates problems\nif you are trying to do time series analysis\nfor somebody's entire health record, in which case you can\nget further clearance to work with more identifiable data. But that is progressive\nas it can be. There's no reason to start with\nthat kind of data if you don't. So it's always on\na need to know. Finally, if you're working\nwith patients older than 90, 90 or older, they are just\ngeneralized as a category of greater than 90. The rest of these, I think,\nare fairly guessable, so we don't have\nto go through them.", "id": "k95abdkdCPk_46", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Again, just limit the collection\nof PHI as strictly as possible. If you don't need\nit, don't get it. If you're sharing\nthe data, instead of sharing an entire data set\nif you do have strong PHI, limit what you're giving or\nsharing to another researcher. That's just a hygiene\nissue, and it's really limiting the amount of\nerrors that can happen. So why is this so important? The IRB, again, is particularly\ninterested in protecting patients and making\nsure that there's as little harm, if any, done\nas possible to patients. Just general human\ndecency and respect. There's institutional\nrisk if something", "id": "k95abdkdCPk_47", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  publish if you have\ndone human subjects research without an IRB. Those two are kind of the\nstick, but the carrot really should be the top two, as\nfar as just human decency and making sure that\nyou've protected any patients or any\nparticipants that you have involved in your research. These are a couple\nof violations. We don't have to\nget too far into it, but they were both\nallegedly conducted without any IRB approval. There's possible\nfraud involved, and it ruined both of their careers. But it put people\nat huge exposures to unhealthy conditions. This is probably a much\nbigger common issue that you're going to have. PHI data breaches,\nthey happen a lot. They're generally not\nbreaches from the outside.", "id": "k95abdkdCPk_48", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Somebody will set\nup a web server on the machine serving PHI\nbecause they found it easier to work at home one day. It could just be they don't\nknow how software is set up. So anytime you're\nworking with PHI, you've really got to overdo it\non knowing exactly how you're working with it. Other breaches are losing\nunencrypted computers, putting data on a thumb\ndrive and losing it. The gross amount\nof data breaches happen just from\nnegligence and not being as careful\nas you want to be. So that's always\ngood to keep in mind. I guess a new thing with\nthe IRB and digital research is things have been changing now\nfrom face to face recruitment", "id": "k95abdkdCPk_49", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to be able to reach millions\nof people across the world and allowing them to\nconsent on their own. So this has become, obviously,\na new thing since the Belmont Report, and it's something\nthat we are working closely with our IRB to make sure that\nwe're being as respectful as we can to the patients,\nbut also making sure that we can develop\nsoftware solutions that are not hurting anybody and\ndevelop into swim lanes. So what we've come up with a\nframework for is that there's a project which is-- we're studying all cancers. So you can post reports about\ndifferent research that's going on, things that seem important.", "id": "k95abdkdCPk_50", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  consented to a protocol,\nwhich is human research and subject to IRB. Then we'll have a platform\nthat the users will use, and that will be like a\nwebsite or an iPhone app that they can get literature\ninformation about what the project is going on. And then we'll have a\nparticipant who is actually part of a study, who's,\nagain, covered under IRB through consent. So why this kind of\ndevelopment has been important, the old way of\nsoftware development was the waterfall approach,\nwhere you work for three weeks, implement something,\nwork for three weeks, implement something, where we\nhave moved to a Agile approach in software. And so while Agile makes our\nlives a lot easier as far", "id": "k95abdkdCPk_51", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  we're doing isn't going\nto affect patients in certain contexts. So within a study, working\nAgile makes no sense. We have we want to work with\nthe IRB to approve things, but IRB approval takes\nbetween two and four weeks for expedited things. When we talk about\nprojects and stuff, that's where we want to work\nsafely in an Agile environment and try to figure out\nplaces where the IRB doesn't necessarily have to be involved\nor doesn't want to be involved and that there isn't\nany added patient risk whatsoever in working in\nthat kind of environment. So it's working with software\nproducts versus studies, and so working with\nthe IRB to be sure that we can separate\nthose things and make sure that\nthings move on as well as possible\nwithout any added harm.", "id": "k95abdkdCPk_52", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So project activity would\nbe social media outreach, sharing content that is relevant\nto the project and kind of just informing about a general idea. A study activity is\nwhat you would generally be used to with\nconsent, data sharing, actually participating\nin a study, whether it's through a wearable,\nanswering questions, and then withdrawing in the process. And the study\nactivities are 100% IRB, where the project activities\nthat aren't directly dealing with the\nstudy can hopefully be separated in most cases.", "id": "k95abdkdCPk_53", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  ask, limit the collection of\nPHI as strictly as possible, and working in Agile\ndevelopments are great but it is unsafe in a lot\nof human research, so we have to focus on where\nthat can be used and where it can't. And that's it. Thank you. [APPLAUSE] Oh. AUDIENCE: I have a question\nabout how it's actually done. So as the IRB, how\ndo you make sure that your researcher\nis complying? Is that, like, writing\na report, doing a PDF, or is there a third\nparty service? MARK SHERVEY: Yeah, yeah. So we certify all\nof our researchers with human research and HIPAA\ncompliance, just blanket. And if you provide that and your\ncertifications are up to date,", "id": "k95abdkdCPk_54", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  knows what they should\nbe looking out for and that the IRB understands. AUDIENCE: So is\nthat a third party? MARK SHERVEY: Oh, yeah, yeah. We use a third party. You can have-- I don't-- we use a third party. PROFESSOR: Can I just add-- MARK SHERVEY: Oh, yeah. PROFESSOR: So at MIT,\nthere's something called COUHES, the\nCommittee on Use of Humans as\nExperimental Subjects, and they are our official IRB. It used to be all paper. Now there's an electronic\nway where you can apply for a COUHES protocol. And it's a reasonably\nlong document in which you describe the\npurpose of the experiment, what you're going to do,\nwhat kind of people you're going to recruit, what\nrecruiting material you're going to use, how you\nwill handle the data,", "id": "k95abdkdCPk_55", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Of course, if you're\ndoing something like injecting\npeople with toxins, then that's a much more\nserious kind of thing, and you have to describe the\npreliminary data on why you think this is safe and so on. And that gets reviewed\nat, essentially, one of three levels. There is exempt\nreview, which is-- you can't exempt yourself,\nbut they can exempt you. And what they would say\nis, this is a minimal risk kind of problem. So let's say you're doing a data\nonly study using mimic data, and you've done\nthe city training, you've signed the\ndata use agreement. You're supposed to get\nIRB permission for it. There is an exception\nfor students in a classroom, in which\ncase I'm responsible rather than making you responsible.", "id": "k95abdkdCPk_56", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  The second level is\nan expedited approval, which is a low risk\nkind of approval, typically data only studies. But it may involve things\nlike using limited data sets, where, for example,\nif you're trying to study the geographical\ndistribution of disease, then you clearly need better\ngeographical identifiers than a three-digit\nzip code, or if you're trying to study a time series,\nas Mark was talking about, you need actual dates. And so you can get approval\nto use that kind of data. And then there's\nthe full on review, which takes much longer,\nwhere they do actually bring in people to\nevaluate the safety of what you're proposing to do. So far, my experience\nis that mostly", "id": "k95abdkdCPk_57", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  do that are representative of\nthe material we're studying in class, we don't have to\nget into that third category because we're not actually\ndoing anything that is likely to harm\nindividual patients, except in a kind of reputational\nor data-oriented sense, and that doesn't require\nthe full blown review. So that's the local situation. MARK SHERVEY: Yeah, thank you. Yeah, I think I misunderstood\nthe full range of the question. Yeah, and that's\nroughly our same thing. So we have-- Eddie Golden is our\nresearch project manager, who is my favorite person in the\noffice for this kind of stuff. She keeps on top of\neverything and makes sure that the right people\nare listed on research and that people are taken\noff, that kind of stuff. But it's a good\nrelationship with the IRB on that kind of stuff. Yeah? AUDIENCE: So I'm somewhat\nunfamiliar with Agile software", "id": "k95abdkdCPk_58", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  On a high level, it's\njust more parallelized and we update more frequently? MARK SHERVEY: Yeah, yeah. I don't know if I\ntook that slide out, but there's something\nwhere Amazon will deploy 50 million\nupdates per year or something like that. So it's constantly on\nan update frequency instead of just building\neverything up and then dropping it. And that's just been a new\ndevelopment in software. AUDIENCE: Can we ask\nquestions to both you guys? ANDY CORAVOS: Yeah. AUDIENCE: Can you tell us\nmore about Elektra Labs? I couldn't fully understand. Are you guys more consultantancy\nfor all these, we'll call them, tool companies? Or is it more like a\nlobbying kind of thing? The reason I ask this is\nalso because I wonder what your opinion is on a third\nparty source for determining whether these things are a\ngood or bad kind of thing because it seems\nlike the FDA would have trouble understanding.", "id": "k95abdkdCPk_59", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  would that be a useful solution? Or where does that go wrong? ANDY CORAVOS: Mm-hm, yeah. So what we're\nbuilding with Elektra is effectively a pharmacy\nfor connecting technologies. So the way that today\nyou have pharmacies that have a formulary of\nall the different drugs that are available,\nthis is effectively like a digital pharmacy,\nlike a Kelley Blue Book of all the different tools. And then we're building out\na label for each of them based on as much\nobjective data as we can, so that we're not\nscoring whether or not something's good or bad. Because in most\ninstances, things aren't good or bad\nand absolute, they're good or bad for a purpose. And so you can imagine\nsomething-- maybe you need really high\nlevels of accuracy, so you need to\nknow whether or not that tool has been\nverified and validated in certain contexts in\ncertain patient populations. Even if the tool's\naccurate, if you're to recharge it all the\ntime or you can't wear it in the shower, you won't have\nthe usability, or if the APIs are really hard to work with. And then security profile,\nwhether or not they have coordinated disclosure,\nhow they handle things", "id": "k95abdkdCPk_60", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and what kind of\nsoftware is used. And then even if the\ntool is accurate, even if it's relatively\nusable, even if it's secure, that doesn't solve the\nCambridge Analytica problem, so how tools are doing\na third party transfer. And so one of the philosophies\nis we don't score, but we are building\nout the data set so when you are\nevaluating a certain tool, it's like a nutrition label. Sometimes you need\nmore sugar, sometimes you need more protein. Maybe you need more\nsecurity, maybe you really need to think\nabout the data rates. Maybe you can take a leave on\nsome of the accuracy levels. And so we're all\nbuilding out this ability to evaluate the tools, and\nthen also to deploy them like the way that a pharmacy\nwould deploy them out. One thing I would like\nto do with the group, if you all are down for\nit, out of civic duty-- and I'm serious, though. Voting is very important\nand submitting your comments", "id": "k95abdkdCPk_61", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And I read all your comments\nbecause Irene sent them to me, and they were very good. And I know probably\npeople who came here want to polish everything\nand make them perfect. You can submit them\nexactly how they are. And I am very much hoping\nthat we get about 95% of you to submit, and\nthe 5% of you that didn't, like, your internet\nbroke or something. You can submit tonight. I will email Irene because you\nalready have done the work, and you can submit it. But I would like to just\nhear some of your thoughts. So what I'm going\nto do is I'm going to use that same framework\naround, what would you keep? What would you change? And then change can\nalso include, like, what was so confusing in\nthere, that it didn't even really make sense? Part of the confusion\nmight be that it was-- some regulations are confusing. But some of the confusion is\nthat part of that document was not written by people who-- some people have technical\nbackgrounds and some do not.", "id": "k95abdkdCPk_62", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  might not actually\nbe used in the way that industry is using it\ntoday, so refining the language. And then what did you\nsee that was missing? So here's what\nwe're going to do. Keep, change slash confusing,\nand then start or add. And before I ask you,\nI want you to look at the person next\nto you, seriously, and if there's three\nof you, that's fine, and I want you to\ntell them when you will be submitting the comment. Is it tonight, tomorrow,\nor you are choosing not to? Just look at them and talk. [INDISTINCT CHATTER] There will be a link. I will send you all links.", "id": "k95abdkdCPk_63", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  OK. All right. Who wants to start? We got three things\non the board. Yes? AUDIENCE: So one thing that-- I don't know if this is\nconfusing or just intentionally vague, but for things like\nquality systems and machine learning practices, who\nsets those standards and how can they be\nadapted or changed? ANDY CORAVOS: Mm-hm. I don't also know\nthat answer, and so I would like you to submit-- one of the things\nthat is nice is then that you have to respond, yeah. And I think it's also\na little bit confusing, even the language. So people are using\ndifferent things. People call it GXP, good\nmanufacturing practice, good clinical practice. These are maintained, I\nthink, in some instances by different orgs. I wonder if good algorithm\npractice gap or good machine", "id": "k95abdkdCPk_64", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  yeah, that's a good thing. So who owns GXP? OK. Yes? You didn't have a question? No. AUDIENCE: Just wanted\nto share something. ANDY CORAVOS: Yes. AUDIENCE: One of the things\nI found really to keep were the examples\nin the appendix. I don't know [INAUDIBLE]. --general guidelines,\nand so it's more that the language\nitself is more generalized and so the examples are\nreally hopeful for what is a specific situation\nthat's analogous to make. ANDY CORAVOS: Yep. Like that? Yeah, examples are helpful. Yep? AUDIENCE: Speaking of specifics,\nI thought around transparency they could have been\nmuch more specific and that we should generally\nadhere to guidelines as opposed", "id": "k95abdkdCPk_65", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  this algorithm is exactly\nwhat's coming out of it, the exact quality\nmetrics, things like that that hold people\naccountable as opposed to there are many instances to\nnot be transparent. And so if those\naren't as specific, I worry that not that much\nreally would happen there. The analog like I\nthought of was when Facebook asks for\nyour data, they say here are the things that\nwe need or that we're using, and it's very explicit. And then you can have a\nchoice of whether or not you actually want that. ANDY CORAVOS: OK. AUDIENCE: So seeing something\nlike that [INAUDIBLE].. ANDY CORAVOS: So part\nof it is transparency, but also user choice\nin data selection or-- AUDIENCE: Yeah, I think\nthat was, for me, more of an analog because choice\nin the medical setting is a bit more complex. Someone who doesn't\nhave the ability in that case or the knowledge\nto actually make that choice. ANDY CORAVOS: Yeah. AUDIENCE: I think at\nthe very least saying", "id": "k95abdkdCPk_66", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So you can work with\nsomeone, and maybe there's some parameters around\nwhat you would or would not have that choice. ANDY CORAVOS: Yep. Yes? AUDIENCE: What if you added\nsomething about algorithm bias? Because I know that that's\nbeen relevant for a lot of other industries\nin terms of confidence within the legal\nsystem, and then also in terms of facial recognition\nnot working fully across races. So I think that breaking\nthings down by population and ensuring equitable\nacross different populations is important. ANDY CORAVOS: Yep. I don't know if I slept\nenough, so if I just gave this example-- but a friend\nof mine called me last week and asked for PPGs, so\nthe sensor on the back. She was asking me if it\nworks on all skin colors", "id": "k95abdkdCPk_67", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And if it responds\ndifferently, whether or not somebody has a tattoo. And so for some of the\nbig registries that are doing bring your\nown device data, you can have unintended\nbiases in the data sets just because of how\nthat it's processing. So yeah. What do you think? What are ways-- I think Irene's worked\nwith some of this. How do you think\nabout whether or not something is-- what would be\na good system for the agency to consider around bias? AUDIENCE: I think maybe\ncoming into consideration with [INAUDIBLE] system\nmight be part of the GNLP. But I think it would be the\nresponsibility of the designer to assess [INAUDIBLE]. ANDY CORAVOS: OK. AUDIENCE: As a\nnote, bearing this", "id": "k95abdkdCPk_68", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to talk about it more, we will\nhave plenty material next time. ANDY CORAVOS: You\nwant to pick someone? MARK SHERVEY: I'm sorry. Go ahead. AUDIENCE: Me? MARK SHERVEY: Yeah. ANDY CORAVOS: Cold call. [LAUGHTER] Yeah? AUDIENCE: Just adding off at\nanother place, so it looked like there was a\nperiod for providing periodic reportings to the\nFDA on updates and all that. There could also\nbe like a scorecard of bias on subpopulations\nor something to that effect. ANDY CORAVOS: Mm-hm. That's cool. Have you seen any places\nthat do something like that? AUDIENCE: I remember when I\nread Weapons of Math Destruction from Cathy O'Neil, she mentioned\nsome sort of famous audit. But I don't really\nremember the details. ANDY CORAVOS: Yeah.", "id": "k95abdkdCPk_69", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  or it can be posts\nor blogs or whatever, just link them in because\none thing that you'll find is that we read a lot\nof things, probably the same things on Twitter, but\nother groups don't necessarily see all of that. So I think that Cathy O'Neil\nis really interesting work, but yeah, just tag stuff. It doesn't have to be\nformatted amazingly. PROFESSOR: So in some\nof the communities that I follow, not on Twitter\nbut email and on the web, there's been a lot of discussion\nabout really terrible design of information\nsystems in hospitals and how these lead to errors. Now, I know from\nyour slide, Andy, that the FDA has defined those\nto be out of its purview. But it seems to me\nthat there's probably,", "id": "k95abdkdCPk_70", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  systems that encourage\nreally bad practice or that allow bad\npractice than there is by retinopathy,\nAI, machine learning techniques that make mistakes. So just this morning,\nfor example, somebody posted a message\nabout a patient who had a heart rate of 12,000,\nwhich seems extremely unlikely. [LAUGHTER] ANDY CORAVOS: Yep. PROFESSOR: And the\nproblem is that when you start automating\nprocesses that are based on the information\nthat is collected in these systems, things\ncan go really screwy when you get garbage data. ANDY CORAVOS: Yeah. Have you thought about\nthat with your system? MARK SHERVEY: We\ncannot get good data. I mean, you're not going to get\ngood data out of those systems.", "id": "k95abdkdCPk_71", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and there's not much\nyou can do about it other than validate good\nranges and go from there. PROFESSOR: Well, I can\nthink of things to do. For example, if FDA were\ninterested in regulating such devices-- oh, sorry, such tools-- ANDY CORAVOS: Yeah. Well, they would\nregulate devices. So one of the funny\nthings with FDA is-- and I should have\nmentioned this-- is the FDA does not regulate\nthe practice of medicine. So doctors can do\nwhatever they want. They regulate-- well,\nyou should look up exactly-- the way I interpret it\nis they regulate the marketing that a manufacturer would do. So I actually wonder if the EHRs\nwould be considered practice of medicine or if it would be a\nmarketing from the EHR company, and maybe that's how it\ncould be under their purview. PROFESSOR: Yeah.", "id": "k95abdkdCPk_72", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Yes? AUDIENCE: I guess\nsomething that I was surprised not\nto see as much about were privacy issues in this. I know there's ways where you\ncan train machine learning models and extract information\nthat the data was trained on. At least I'm pretty\nsure that exists. It's not my expertise. But I was wondering if anything\nlike that [INAUDIBLE] have someone try to extract\nthe data that you can't. But you talked about that a lot\nin your section of the talk, but I don't remember\nit as much [INAUDIBLE].. ANDY CORAVOS: OK. Yep. Realistically,\nhow many of you do think you'll actually\nsubmit a comment? A couple. So if you're thinking you\nwouldn't submit a comment, just out of curiosity, I\nwon't argue with you, I'm just curious, what\nwould hold you back", "id": "k95abdkdCPk_73", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  If you didn't raise your hand\nnow, I get to cold call you. Yes? AUDIENCE: I raised\nmy hand before. We were just talking. Most of us have our\ncomputers open now. If you really want us to submit\nit as is, if you put it up, we could all submit. ANDY CORAVOS: OK. OK, OK. Wow. AUDIENCE: We are\n95% [INAUDIBLE].. ANDY CORAVOS: All right. PROFESSOR: So while\nAndy is looking that up, I should say when the HIPAA\nregulations, the privacy regulations were first proposed,\nthe initial version got 70,000 public comments about it. And it is really true that\nthe regulatory agency, in that case, it was\nHealth and Human Services, had to respond to every\none of those by law.", "id": "k95abdkdCPk_74", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  about responding to\nall those requests. So they will take your comments\nseriously because they have to. AUDIENCE: I was going to say,\nis there any way of anonymously commenting? Or does it have to be tied\nto us, out of curiosity? ANDY CORAVOS: I don't know. I think it's generally-- I don't know, I'd have\nto look at it again. I think most of them\nare public comments. I mean, I guess\nif you wanted to, maybe you could coordinate\nyour comments and you could-- yeah, OK. Irene is willing\nto group comment. So you can also send if\nyou'd like to do it that way, and it can be a set of class\ncomments, if you would prefer. The Bitly is capital MIT all\nlowercase loves FDA, will", "id": "k95abdkdCPk_75", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  I'm amazed that that Bitly\nhas not been taken already. PROFESSOR: [INAUDIBLE] has\nbeen asleep on the job. ANDY CORAVOS: What other\nquestions do you all have? Yes? AUDIENCE: So what is the line\nbetween an EHR and a SaMD? Because it said it earlier\nthat EHR is exempted, but then it also says, oh,\nfor example, with SaMD, it could be collecting\nphysiological signals, and then they might\nsend an audible alarm to indicate [INAUDIBLE]. And my understanding is\nsome of EHRs do that. ANDY CORAVOS: Mm-hm. AUDIENCE: And so would they need\nto be retroactively approved and partially SaMD-ified? Or how's that work? ANDY CORAVOS: So\nI'm not a regulator, so you should ask\nyour regulator. A couple resources that could\nhelp you decide this is,", "id": "k95abdkdCPk_76", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  perhaps not what\nit actually does. The next thing,\nwhich I don't think-- I mean, I think if it really\ndoes that, you should also claim that it does what\nit does, especially if it's confusing for people. There's a couple of regulations\nthat might be helpful. One is called Clinical\nDecision Support. And if you read any FDA things,\nthey love their algorithms-- I mean, they love\ntheir algorithms, but they also love\ntheir acronyms. So Clinical Decision Support\nis CDS, and then also Patient Decision Support. There's a guidance\nthat just came out around the two types of\ndecision support tools, and I would guess maybe\nthat is supporting a decision, that EHR. So it might actually\nbe considered something that would be regulated. There's also a lot of weird-- we didn't go into it, but\nthere are many instances where something might\nactually be a device", "id": "k95abdkdCPk_77", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  enforcement discretion,\nwhich says it's a device but we will not\nregulate it as such. Which is actually a little\nbit risky for a manufacturer because you are device, but you\ncan now go straight to market. In some instances, you still\nhave to register and list the product, but you don't have\nto necessarily get reviewed. And it also could\neventually be reviewed. So the line of, is it a\ndevice, is it a device and you have to\nregister, is it a device and you have to get\ncleared or approved, is why you should\nearly and often-- yes? AUDIENCE: I enjoyed\nyour game with regards to Fitbit and Apple. And I have a question\nabout the app. I know that you're\nnot Apple either, but why do you think that Apple\nwent for FDA approval versus Fitbit who didn't? What were the motivations\nfor the companies to do that? ANDY CORAVOS: I would\nsay, in public documents, Fitbit has expressed an interest\nin working with the FDA.", "id": "k95abdkdCPk_78", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  submitted or had their package. They're also working with\nthe pre-cert program. So I don't know what's\nhappening behind the scenes. Yeah? AUDIENCE: Does it give them\na business edge, perhaps, to get FDA approval? ANDY CORAVOS: I cannot\ncomment on that. AUDIENCE: OK, no worries. ANDY CORAVOS: Yeah. I would say,\ngenerally, people want to use tools that\nare trustworthy, and developing more tools\nthat have somebody of evidence is a really important thing. I think the FDA is one\nway of having evidence. I think there are other ways\nthat tools and devices can continue to build evidence. My hope is over time, that a\nlot of these things that we consider to be wellness tools\nalso have evidence around them. Maybe in some instances we\ndon't always regulate vitamins, but you want to still trust\nthat your vitamin doesn't have sawdust in it, right,\nand that it's a real product. And so the more\nthat we, I think,", "id": "k95abdkdCPk_79", "title": "22. Regulation of Machine Learning / Artificial Intelligence in the US", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  use products that do that, I\nhope over time this helps us. PROFESSOR: Does it give\nthem any legal protection to have it be classified\nas an FDA device? ANDY CORAVOS: I'm\nnot sure about that. Historically, it has\nhelped with reimbursement. So a class two product has\nbeen easier to reimburse. That also is generally\nchanging, but that helps with the business\nmodel around that. PROFESSOR: Yeah. Well, I want to thank\nyou both very much. That was really interesting. And I do encourage all\nof you to participate in this regulatory process\nby submitting your comments. And I enjoyed the presentations. Thank you. ANDY CORAVOS: Yeah. MARK SHERVEY: Thank you. ANDY CORAVOS: Thank you.", "id": "k95abdkdCPk_80"}, {"text": "  PETER SZOLOVITS: OK,\nso a little over a year ago, I got a call\nfrom this committee. NASEM is the National Academy\nof Science, Engineering, and Medicine. So this is an august body of old\npeople with lots of gray hair who have done something\nimportant enough to get elected to these academies. And their research arm is called\nthe National Research Council and has a bunch of\ndifferent committees. One of them is this Committee\non Science, Technology, and the Law. It's a very\ninteresting committee. It's chaired by\nDavid Baltimore, who used to be an MIT professor\nuntil he went and became", "id": "zYgkr0KfWM0_0", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And he also happens to have\na Nobel Prize in his pocket and he's a pretty famous guy. And Judge David\nTatel is a member of the US Court of Appeals\nfor the District of Columbia circuit, so this is probably the\nmost important circuit court. It's one level below\nthe Supreme Court. And he happens to\nsit in the seat that Ruth Bader\nGinsburg occupied before she was elevated\nto the Supreme Court from that Court of Appeals,\nso this is a pretty big deal. So these are heavy hitters. And they convened a meeting to\ntalk about the set of topics that I've listed here. So blockchain and distributed\ntrust, artificial intelligence and decision making, which is\nobviously the part that I got invited to talk about,\nprivacy and informed consent in an era of big data,\nscience curricula for law", "id": "zYgkr0KfWM0_1", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  The issue of using litigation\nto target scientists who have opinions that you don't like. And the more general\nissue of how do you communicate advances in life\nsciences to a skeptical public. So this is dealing with the\nsort of anti-science tenor of the times. So the group of us that talked\nabout AI and decision making, I was a little bit\nsurprised by the focus because Hank really is a law\nschool professor at Stanford who's done a lot of work\non fairness and prejudice in health care. Cherise Burdee is at something\ncalled the Pretrial Justice Institute, and her\nissue is a legal one which is that there are\nnow a lot of companies that have software that predict,\nif you get bail while you're", "id": "zYgkr0KfWM0_2", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And so this is influential\nin the decision that judges make about\nhow much bail to impose and whether to let\nyou out on bail at all or to keep you in prison,\nawaiting your trial. Matt Lundgren is a radiology\nprofessor at Stanford and has done some of\nthe really cool work on building convolutional\nneural network models to detect pulmonary\nemboli and various other things in imaging data. You know the next guy, and\nSuresh Venkatasubramanian is a professor. He was originally a theorist\nat the University of Utah but has also gotten into\nthinking a lot about privacy and fairness. And so that that was our panel,\nand we each gave a brief talk", "id": "zYgkr0KfWM0_3", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  One of the things that I was\nvery surprised by is somebody raised the question of shouldn't\nTatel as a judge on the Circuit Court of Appeals\nhire people like you guys to be clerks in his court? So people like you\nguys who also happen to go to law school, of\nwhich there are a number now of people who are trained\nin computational methods and machine learning but also\nhave the legal background. And he said something\nvery interesting to me. He said, no, he\nwouldn't want people like that, which\nkind of shocked me. And so we quizzed him\na little bit on why, and he said, well, because he\nviews the role of the judge not to be an expert\nbut to be a judge.", "id": "zYgkr0KfWM0_4", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And he was afraid that\nif he had a clerk who had a strong\ntechnical background, that person would have strong\ntechnical opinions which would bias his decision\none way or another. So this reminded me-- my wife was a lawyer,\nand I remember, when she was in law school, she\nwould tell me about the classes that she was taking. And it became obvious\nthat studying law was learning how to win, not\nlearning how to find the truth. And there's this\nphilosophical notion in the law that says that\nthe truth will come out from spirited argument on\ntwo sides of a question, but your duty as a lawyer is\nto argue as hard as you can for your side of the argument. And in fact, in law school,\nthey teach them, like in debate,", "id": "zYgkr0KfWM0_5", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  and be able to make a\ncogent argument for it. And so Tatel sort of\nreinforced that notion in what he said, which I\nthought was interesting. Well, just to talk a little\nbit about the justice area because this is the one\nthat has gotten the most public attention, governments\nuse decision automation for determining eligibility\nfor various kinds of services, evaluating where to deploy\nhealth inspectors and law enforcement personnel,\ndefining boundaries along voting districts. So all of the gerrymandering\ndiscussion that you hear about is all about using\ncomputers and actually machine learning techniques\nto try to figure out how to-- your objective function is to\nget Republicans or Democrats elected, depending on who's in\ncharge of the redistricting.", "id": "zYgkr0KfWM0_6", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  in order to maximize the\nprobability that you're going to have the majority in\nwhatever congressional races or state legislative races. So in the law, people are\nin favor of these ideas to the extent that they\ninject clarity and precision into bail, parole, and\nsentencing decisions. Algorithmic technologies\nmay minimize harms that are the\nproducts of human judgment. So we know that people\nare in fact prejudiced, and so there are prejudices\nby judges and by juries that play into the decisions\nmade in the legal system. So by formalizing\nit, you might win. However, conversely,\nthe use of technology to determine whose liberty\nis deprived and on what terms", "id": "zYgkr0KfWM0_7", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  and interpretability. So next week, we're going to\ntalk some about transparency and interpretability,\nbut today's is really about fairness. So here is an article from\nOctober of last year-- no, September of last year,\nsaying that as of October of this year, if you get\narrested in California, the decision of whether\nyou get bail or not is going to be made by\na computer algorithm, not by a human being, OK? So it's not 100%. There is some discretion on the\npart of this county official who will make a recommendation,\nand the judge ultimately decides, but I suspect\nthat until there are some egregious\noutcomes from doing this, it will probably be\nquite commonly used.", "id": "zYgkr0KfWM0_8", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  is based on a number\nof different factors. One is that the algorithms\nreflect a severe racial bias. So for example, if you are two\nidentical people but one of you happens to be white and one\nof you happens to be black, the chances of you\ngetting bail are much lower if you're black\nthan if you're white. Now, you say, well,\nhow could that be given that we're learning\nthis algorithmically? Well, it's a complicated\nfeedback loop because the algorithm is\nlearning from historical data, and if historically, judges have\nbeen less likely to grant bail to an African-American than\nto a Caucasian-American,", "id": "zYgkr0KfWM0_9", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  to do and will\nnicely incorporate exactly that prejudice. And then the second\nproblem, which I consider to be\nreally horrendous, is that in this\nparticular field, the algorithms are\ndeveloped privately by private companies\nwhich will not tell you what their algorithm is. You can just pay them and\nthey will tell you the answer, but they won't tell you\nhow they compute it. They won't tell\nyou what data they used to train the algorithm. And so it's really a black box. And so you have no idea\nwhat's going on in that box other than by looking\nat its decisions. And so the data\ncollection system is flawed in the same way as\nthe judicial system itself. So not only are\nthere algorithms that", "id": "zYgkr0KfWM0_10", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  which is after all a\nrelatively temporary question until your trial comes\nup, although that may be a long time,\nbut there are also algorithms that advise on\nthings like sentencing. So they say, how likely is this\npatient to be a recidivist? Somebody who, when\nthey get out of jail, they're going to offend again. And therefore, they deserve\na longer jail sentence because you want to keep\nthem off the streets. Well, so this is a particular\nstory about a particular person in Wisconsin, and shockingly,\nthe state Supreme Court ruled against this guy,\nsaying that knowledge of the algorithm's output\nwas a sufficient level of transparency in order\nto not violate his rights, which I think many\npeople consider to be kind of an outrageous decision.", "id": "zYgkr0KfWM0_11", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Conversely-- I keep doing on\nthe one hand and on the other-- algorithms could help\nkeep people out of jail. So there's a Wired\narticle not long ago that says we can use algorithms\nto analyze people's cases and say, oh, this person\nlooks like they're really in need of psychiatric help\nrather than in need of jail time, and so perhaps\nwe can divert him from the penal system\ninto psychiatric care and keep him out of prison\nand get him help and so on. So that's the\npositive side of being able to use these\nkinds of algorithms. Now, it's not only\nin criminality. There is also a\nlong discussion-- you can find this\nall over the web--", "id": "zYgkr0KfWM0_12", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  hire better than a human being. So if you're a big company\nand you have a lot of people that you're trying to\nhire for various jobs, it's very tempting\nto say, hey, I've made lots and lots\nof hiring decisions and we have some outcome data. I know which people have\nturned out to be good employees and which people have turned\nout to be bad employees, and therefore, we can base\na first-cut screening method on learning such an\nalgorithm and using it on people who apply for jobs\nand say, OK, these are the ones that we're going to interview\nand maybe hire because they look like they're a better bet. Now, I have to tell\nyou a personal story. When I was an\nundergraduate at Caltech, the Caltech faculty\ndecided that they wanted to include student\nmembers of all the faculty", "id": "zYgkr0KfWM0_13", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And so I was lucky\nenough that I served for three years as a member of\nthe Undergraduate Admissions Committee at Caltech. And in those days, Caltech\nonly took about 220, 230 students a year. It's a very small school. And we would actually fly\naround the country and interview about the top half of all the\napplicants in the applicant pool. So we would talk not\nonly to the students but also to their teachers\nand their counselors and see what the\nenvironment was like, and I think we got a very good\nsense of how good a student was likely to be based on that. So one day, after the admissions\ndecisions have been made, one of the professors, kind\nof as a thought experiment, said here's what we ought to do. We ought to take the 230\npeople that we've just offered admission to and\nwe should reject them all", "id": "zYgkr0KfWM0_14", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  see whether the faculty notices. Because it seemed like a\nfairly flat distribution. Now, of course, I\nand others argued that this would be\nunfair and unethical and would be a waste\nof all the time that we had put into\nselecting these people, so we didn't do that. But then this guy\nwent out and he looked at the data we had\non people's ranking class, SAT scores, grade point\naverage, the checkmarks on their recommendation\nletters about whether they were truly exceptional\nor merely outstanding. And he built a linear\nregression model that predicted the person's\nsophomore level grade point average, which seemed\nlike a reasonable thing", "id": "zYgkr0KfWM0_15", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And he got a\nreasonably good fit, but what was\ndisturbing about it is that in the Caltech\npopulation of students, it turned out that the beta for\nyour SAT English performance was negative. So if you did particularly\nwell in English on the SAT, you were likely to do worse\nas a sophomore at Caltech than if you didn't do as well. And so we thought\nabout that a lot, and of course, we\ndecided that that would be really unfair\nto penalize somebody for being good at something,\nespecially when the school had this philosophical\norientation that said we ought to look for\npeople with broad educations. So that's just an example. And more, Science\nFriday had a nice show", "id": "zYgkr0KfWM0_16", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So let me ask you, what\ndo you mean by fairness? If we're going to define\nthe concept, what is fair? What characteristics\nwould you like to have an algorithm\nhave that judges you for some particular purpose? Yeah? AUDIENCE: It's impossible to\npin down sort of, at least might in my opinion,\none specific definition, but for the pre-trial\nsuccess rate for example, I think having the error rates\nbe similar across populations, across the covariants you\nmight care about, for example, fairness, I think\nis a good start. PETER SZOLOVITS: OK, so similar\nerror rates is definitely", "id": "zYgkr0KfWM0_17", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And you'll see later Irene-- where's Irene? Right there. Irene is a master of\nthat notion of fairness. Yeah? AUDIENCE: When the model says\nsome sort of observation that causally shouldn't\nbe true, and what I want society to look like PETER SZOLOVITS:\nSo I'm not sure how to capture that\nin a short phrase. Societal goals. But that's tricky, right? I mean, suppose that I\nwould like it to be the case that the fraction of people\nof different ethnicity who are criminals\nshould be the same. That seems like a good\ngoal for fairness.", "id": "zYgkr0KfWM0_18", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  I mean, I could pretend\nthat it's the same, but it isn't the same\ntoday objectively, and the data wouldn't\nsupport that. So that's an issue. Yeah? AUDIENCE: People who are similar\nshould be treated similarly, so engaged sort of\nindependent of the [INAUDIBLE] attributes or independent\nof your covariate. PETER SZOLOVITS:\nSimilar people should lead to similar treatment. Yeah, I like that. AUDIENCE: I didn't make it up. PETER SZOLOVITS: I know. It's another of the classic\nsort of notions of fairness. That puts a lot of weight on\nthe distance function, right? In what way are\nto people similar? And what characteristics--\nyou obviously", "id": "zYgkr0KfWM0_19", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  the forbidden characteristics\nin order to decide similarity, because then people will\nbe dissimilar in ways that you don't want, but\ndefining that function is a challenge. All right, well, let me show\nyou a more technical approach to thinking about this. So we all know about biases like\nselection bias, sampling bias, reporting bias, et cetera. These are in the conventional\nsense of the term bias. But I'll show you an example\nthat I got involved in. Raj Manrai was a MIT\nHarvard HST student, and he started looking at the\nquestion of the genetics that was used in order to\ndetermine whether somebody", "id": "zYgkr0KfWM0_20", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  That's a big word. It means that your\nheart gets too big and it becomes sort of flabby\nand it stops pumping well, and eventually, you die of this\ndisease at a relatively young age, if, in fact, you have it. So what happened\nis that there was a study that was done mostly\nwith European populations where they discovered that a lot\nof people who had this disease had a certain genetic variant. And they said, well, that must\nbe the cause of this disease, and so it became accepted\nwisdom that if you had that genetic variant, people\nwould counsel you to not plan on living a long life. And this has all\nkinds of consequences.", "id": "zYgkr0KfWM0_21", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  a kid when you're\nin your early 40s, and your life expectancy is 55. Would you want to die\nwhen you have a teenager that you leave to your spouse? So this was a consequential\nset of decisions that people have to make. Now, what happened\nis that in the US, there were tests\nof this sort done, but the problem was that a lot\nof African and African-American populations turned out to have\nthis genetic variant frequently without developing\nthis terrible disease, but they were all told that they\nwere going to die, basically. And it was only after\nyears when people noticed that these\npeople who were supposed to die genetically weren't\ndying that they said, maybe we misunderstood\nsomething.", "id": "zYgkr0KfWM0_22", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  was used to develop the\nmodel was a European ancestry population and not an\nAfrican ancestry population. So you go, well, we must\nhave learned that lesson. So this paper was\npublished in 2016, and this was one of\nthe first in this area. Here's a paper\nthat was published three weeks ago in Nature\nScientific Reports that says, genetic risk factors\nidentified in populations of European descent do\nnot improve the prediction of osteoporotic fracture\nand bone mineral density in Chinese populations. So it's the same story. It's exactly the same story. Different disease,\nthe consequence is probably less\ndire because being told that you're going to break\nyour bones when you're old is not as bad as being told\nthat your heart's going to stop working when you're in\nyour 50s, but there we have it.", "id": "zYgkr0KfWM0_23", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Well, I mentioned\nthe standard sources, but here is an\ninteresting analysis. This comes from\nConstantine Aliferis from a number of\nyears ago, 2006, and he says, well, look,\nin a perfect world, if I give you a\ndata set, there's an uncountably infinite\nnumber of models that might possibly explain\nthe relationships in that data. I cannot enumerate an\nuncountable number of models, and so what I'm going to do is\nchoose some family of models to try to fit, and then I'm\ngoing to use some fitting technique, like stochastic\ngradient descent or something,", "id": "zYgkr0KfWM0_24", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Maybe it'll find\nthe local optimum. And then there is noise. And so his observation\nis that if you count O as the optimal possible\nmodel over all possible model families, and if you count\nL as the best model that's learnable by a particular\nlearning mechanism, and you call A the actual\nmodel that's learned, then the bias is\nessentially O minus L, so its limitation\nof learning method related to the target model. The variance is\nlike L minus A, it's the error that's due to the\nparticular way in which you learned things, like\nsampling and so on, and you can estimate the\nsignificance of differences", "id": "zYgkr0KfWM0_25", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  randomizing, essentially, the\nrelationships in the data. And then you get a curve of\nperformance of those models, and if yours lies outside\nthe 95% confidence interval, then you have a P\nequal 0.05 result that this model is not random. So that's the typical\nway of going about this. Now, you might say, but isn't\ndiscrimination the very reason we do machine learning? Not discrimination\nin the legal sense, but discrimination in\nthe sense of separating different populations. And so you could say,\nwell, yes, but some basis for differentiation\nare justified and some basis for\ndifferentiation", "id": "zYgkr0KfWM0_26", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So they're either\npractically irrelevant, or we decide for\nsocietal goals that we want them to be irrelevant\nand we're not going to take them into account. So one lesson from people who\nhave studied this for a while is that discrimination\nis domain specific. So you can't define\na universal notion of what it means to discriminate\nbecause it's very much tied to these questions of\nwhat is practically and morally irrelevant in the\ndecisions that you're making. And so it's going to be\ndifferent in criminal law than it is in medicine, than\nit is in hiring, than it is in various other\nfields, college admissions, for example. And it's\nfeature-specific as well, so you have to take\nthe individual features into account.", "id": "zYgkr0KfWM0_27", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  has tried to regulate\nthese domains, and so credit is regulated by\nthe Equal Credit Opportunity Act, education by\nthe Civil Rights Act and various amendments,\nemployment by the Civil Rights Act, housing by the\nFair Housing Act, public accommodation by\nthe Civil Rights Act, more recently, marriage\nis regulated originally by the Defense of\nMarriage Act, which as you might tell\nfrom its title, was against things like\npeople being able to marry who were not a traditional marriage\nthat they wanted to defend, but it was struck down\nby the Supreme Court about six years ago as\nbeing discriminatory. It's interesting, if you look\nback to probably before you guys were born in\n1967, until 1967,", "id": "zYgkr0KfWM0_28", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  to marry each other in Virginia. It was literally illegal. If you went to get a marriage\nlicense, you were denied, and if you got married out\nof state and came back, you could be arrested. This happened much later. Trevor Noah, if you know\nhim from The Daily Show, wrote a book called\nBorn a Crime, I think, and his father\nis white Swiss guy and his mother is a\nSouth African black, and so it was literally\nillegal for him to exist under the apartheid\nlaws that they had. He had to pretend to be-- his mother was his caretaker\nrather than his mother in order to be able\nto go out in public, because otherwise, they\nwould get arrested.", "id": "zYgkr0KfWM0_29", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  but these are some of\nthe regulatory issues. So here are some of the legally\nrecognized protected classes, race, color, sex, religion,\nnational origin, citizenship, age, pregnancy, familial status,\ndisability, veteran status, and more recently,\nsexual orientation in certain jurisdictions,\nbut not everywhere around the country. OK, so given those\nexamples, there are two legal doctrines about\ndiscrimination, and one of them talks about disparate\ntreatment, which is sort of related to this one. And the other talks\nabout disparate impact and says, no matter\nwhat the mechanism is, if the outcome is very\ndifferent for different racial", "id": "zYgkr0KfWM0_30", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  is prima facie evidence that\nthere is something not right, that there is some\nsort of discrimination. Now, the problem is, how do\nyou defend yourself against, for example, a disparate\nimpact argument? Well, you say, in order to\nbe disparate impact that's illegal, it has to be\nunjustified or avoidable. So for example,\nsuppose I'm trying to hire people to climb\n50-story buildings that are under construction, and\nyou apply, but it turns out you have a medical\ncondition which is that you get\ndizzy at times, I might say, you know what,\nI don't want to hire you, because I don't want\nyou plopping off", "id": "zYgkr0KfWM0_31", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  and that's probably\na reasonable defense. If I brought suit\nagainst you and said, hey, you're\ndiscriminating against me on the basis of this\nmedical disability, a perfectly good defense\nis, yeah, it's true, but it's relevant to the job. So that's one way\nof dealing with it. Now, how do you demonstrate\ndisparate impact? Well, the court has\ndecided that you need to be able to show\nabout a 20% difference in order to call something\ndisparate impact. So the question, of course,\nis can we change our hiring policies or whatever\npolicies we're using in order to achieve the same\ngoals, but with less of a disparity in the impact. So that's the challenge.", "id": "zYgkr0KfWM0_32", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  and disparate impact are really\nin conflict with each other. And you'll find that\nthis is true in almost everything in this domain. So disparate impact is\nabout distributive justice and minimizing\nequality of outcome. Disparate treatment is\nabout procedural fairness and equality of opportunity,\nand those don't always mesh. In other words, it may well be\nthat equality of opportunity still leads to\ndifferences in outcome, and you can't square\nthat circle easily. Well, there's a lot\nof discrimination that keeps persisting. There's plenty of evidence\nin the literature. And one of the problems\nis that, for example,", "id": "zYgkr0KfWM0_33", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  or different ethnicities. It turns out that we don't\nhave a nicely balanced set where the number of\npeople of European descent is equal to the number of\npeople of African-American, or Hispanic, or\nAsian, or whatever population you choose\ndescent, and therefore, we tend to know a lot more\nabout the majority class than we know about\nthese minority classes, and just that additional data\nand that additional knowledge might mean that we're able to\nreduce the error rate simply because we have a\nlarger sample size. OK, so if you want\nto formalize this, this is Moritz Hardt's\npart of the tutorial that I'm stealing\nfrom in this talk.", "id": "zYgkr0KfWM0_34", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And Moritz is a\nprofessor at Berkeley who actually teaches an\nentire semester-long course on fairness in machine learning,\nso there's a lot of material here. And so he formalizes\nthe problem this way. He says, look, a decision\nproblem, a model, in our terms, is that we have some X,\nwhich is the set of features we know about an individual,\nand we have some said A, which is the set of\nprotected features, like your race, or your\ngender, or your age, or whatever it is we're trying\nto prevent from discriminating on, and then we have either\na classifier or some score or predictive function\nthat's a function of X and A", "id": "zYgkr0KfWM0_35", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  that we're interested\nin predicting. So now you can begin to tease\napart some different notions of fairness by looking\nat the relationships between these elements. So there are three criteria\nthat appear in the literature. One of them is the\nnotion of independence of the scoring function\nfrom sensitive attributes. So this says that R is\nindependent from A. Remember, on the previous slide, I said\nthat R is a function of-- oops. R is a function of X and A,\nso obviously, that criterion says that it can't be a\nfunction of A. Null function. Another notion is\nseparation of score and the sensitive attribute\ngiven the outcome.", "id": "zYgkr0KfWM0_36", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  going to be treated similarly. In other words, if I tell\nyou the group, the outcome, the people who did\nwell at the job and the people who\ndid poorly at the job, then the scoring function is\nindependent of the protected attribute. So that allows a\nlittle more wiggle room because it says that the\nprotected attribute can still predict something\nabout the outcome, it's just that you can't use it\nin the scoring function given the category of which\noutcome category that individual belongs to. And then sufficiency\nis the inverse of that. It says that given\nthe scoring function, the outcome is independent\nof the protected attribute. So that says, can we\nbuild a fair scoring function that separates the\noutcome from the protected", "id": "zYgkr0KfWM0_37", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So here's some detail on those. If you look at independence-- this is also called by\nvarious other names-- basically, what it says\nis that the probability of a particular\nresult, R equal 1, is the same whether\nyou're in class A or class B in the protected attribute. So what does that tell you? That tells you that the\nscoring function has to be universal over\nthe entire data set and has to not distinguish\nbetween people in class A versus class B. That's a\npretty strong requirement. And then you can operationalize\nthe notion of unfairness either by looking for\nan absolute difference between those probabilities.", "id": "zYgkr0KfWM0_38", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  then you have evidence that this\nis not a fair scoring function, or a ratio test that says,\nwe look at the ratio, and if it differs\nfrom 1 significantly, then you have evidence that this\nis an unfair scoring function. And by the way, this\nrelates to the 4/5 rule, because if you make\nepsilon 20%, then that's the same as the 4/5 rule. Now, the problem-- there are\nproblems with this notion of independence. So it only requires\nequal rates of decisions for hiring, or giving somebody\na liver for transplant, or whatever topic\nyou're interested in. And so what if hiring is based\non a good score in group A, but random in B? So for example, what if we\nknow a lot more information", "id": "zYgkr0KfWM0_39", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  so we have a better\nway of scoring them than we do of scoring group\nB. So you might wind up with a situation\nwhere you wind up hiring the same\nnumber of people, the same ratio of people in\nboth groups, but in one group, you've done a good\njob of selecting out the good candidates,\nand in the other group, you've essentially\ndone it at random. Well, the outcomes are likely\nto be better for a group A than for group B, which means\nthat you're developing more data for the future\nthat says, we really ought to be hiring\npeople in group A because they have\nbetter outcomes. So there's this feedback loop. Or alternatively--\nwell, of course, it could be caused\nby malice also. I could just decide as\na hiring manager I'm not", "id": "zYgkr0KfWM0_40", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  going to take some random\nsample of African-Americans and hire them, and then\nmaybe they'll do badly, and then I'll have more\ndata to demonstrate that this was a bad idea. So that would be malicious. There's also a\ntechnical problem, which is it's possible that\nthe category, the group is a perfect predictor\nof the outcome, in which case, of course, they\ncan't be separated. They can't be independent\nof each other. Now, how do you\nachieve independence? Well, there are a number\nof different techniques. One of them is-- there's this article\nby Zemel about learning fair representations,\nand what it says is you create a new\nworld representation, Z, which is some\ncombination of X and A, and you do this by maximizing\nthe mutual information", "id": "zYgkr0KfWM0_41", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  the mutual information\nbetween the A and Z. So this is an idea\nthat I've seen used in machine learning\nfor robustness rather than for fairness,\nwhere people say, the problem is that given\na particular data set, you can overfit to that data\nset, and so one of the ideas is to do a Gann-like\nmethod where you say, I want to train my\nclassifier, let's say, not only to work well on\ngetting the right answer, but also to work as poorly as\npossible on identifying which data set my example came from. So this is the\nsame sort of idea. It's a representation\nlearning idea. And then you build\nyour predictor, R, based on this representation,\nwhich is perhaps not", "id": "zYgkr0KfWM0_42", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  but is as independent\nas possible. And usually, there are knobs\nin these learning algorithms, and depending on how\nyou turn the knob, you can affect\nwhether you're going to get a better classifier\nthat's more discriminatory or a worse classifier\nthat's less discriminatory. So you can do that\nin pre-processing. You can do some kind of\nincorporating in the loss function a dependence notion or\nan independence notion and say, we're going to train on\na particular data set, imposing this notion of\nwanting this independence between A and R as\npart of our desiderata. And so you, again,\nare making trade-offs against other characteristics. Or you can do post-processing.", "id": "zYgkr0KfWM0_43", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  about discrimination, then\nI can do another learning problem that says I'm now going\nto build a new F, which takes R and the protected\nattribute into account, and it's going to minimize the\ncost of misclassifications. And again, there's a knob where\nyou can say, how much do I want to emphasize\nmisclassifications for the protected\nattribute or based on the protected attribute? So this was still talking\nabout independence. The next notion is separation,\nthat says given the outcome, I want to separate A and R.\nSo that graphical model shows that the protected attribute\nis only related to the scoring function through the outcome.", "id": "zYgkr0KfWM0_44", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  than through the outcome. So this recognizes that\nthe protected attribute may, in fact, be correlated\nwith the target variable. An example might be\ndifferent success rates in a drug trial for\ndifferent ethnic populations. There are now some cardiac\ndrugs where the manufacturer has determined that\nthis drug works much better in certain\nsubpopulations than it does in other populations,\nand the FDA has actually approved the\nmarketing of that drug to those subpopulations. So you're not\nsupposed to market it to the people for whom\nit doesn't work as well, but you're allowed to\nmarket it specifically for the people for\nwhom it does work well. And if you think about\nthe personalized medicine", "id": "zYgkr0KfWM0_45", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  The populations that\nwe're interested in becomes smaller and smaller\nuntil it may just be you. And so there might be a drug\nthat works for you and not for anybody else in the\nclass, but it's exactly the right drug for you,\nand we may get to the point where that will happen and\nwhere we can build such drugs and where we can approve their\nuse in human populations. Now, the idea here\nis that if I have two populations, blue and green,\nand I draw ROC curves for both of these populations,\nthey're not going to be the same, because\nthe drug will work differently for those two populations. But on the other hand, I can\ndraw them on the same axes, and I can say, look any place\nwithin this colored region", "id": "zYgkr0KfWM0_46", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  to get the same outcome\nfor both populations. So I can't achieve this\noutcome for the blue population or this outcome for\nthe green population, but I can achieve any of these\noutcomes for both populations simultaneously. And so that's one way of\ngoing about satisfying this requirement when it\nis not easily satisfied. So the advantage of\nseparation over independence is that it allows\ncorrelation between R and Y, even a perfect\npredictor, so R could be a perfect\npredictor for Y. And it gives you incentives\nto learn to reduce the errors in all groups. So that issue about\nrandomly choosing members of the minority\ngroup doesn't work here because that would suppress\nthe ROC curve to the point where there would be no feasible\nregion that you would like.", "id": "zYgkr0KfWM0_47", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  then you'd have\nthe diagonal line and the only feasible region\nwould be below that diagonal, no matter how good the predictor\nwas for the other class. So that's a nice characteristic. And then the final\ncriterion is sufficiency, which flips R and Y. So\nit says that the regressor or the predictive variable can\ndepend on the protected class, but the protected class is\nseparated from the outcome. So for example, the\nprobability in a binary case of a true outcome\nof Y given that R is some particular value, R\nand A is a particular class, is the same as the probability\nof that same outcome given the same R value, but\nthe different class.", "id": "zYgkr0KfWM0_48", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  similar treatment notion,\nqualitative notion, again. So it requires a parody\nof both the positive and the negative predictive\nvalues across different groups. So that's another popular\nway of looking at this. So for example, if the scoring\nfunction is a probability, or the set of all instances\nassigned the score R has an R fraction of positive\ninstances among them, then the scoring function is\nsaid to be well-calibrated. So we've talked about\nthat before in the class. If it turns out that R\nis not well-calibrated, you can hack it and you can make\nit well-calibrated by putting it through a logistic function\nthat will then approximate the appropriately\ncalibrated score,", "id": "zYgkr0KfWM0_49", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  or the degree of\ncalibration will give you a good approximation to\nthis notion of sufficiency. These guys in the\ntutorial also point out that some data sets actually\nlead to good calibration without even trying very hard. So for example, this is\nthe UCI census data set, and it's a binary prediction\nof whether somebody makes more than $50,000 a year if\nyou have any income at all and if you're over 16 years old. And the feature, there are 14\nfeatures, age, type of work, weight of sample is\nsome statistical hack from the Census Bureau,\nyour education level, marital status, et\ncetera, and what you see is that the calibration\nfor males and females is pretty decent.", "id": "zYgkr0KfWM0_50", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  without having done anything\nparticularly dramatic in order to achieve that. On the other hand, if you\nlook at the calibration curve by race for whites\nversus blacks, the whites, not surprisingly,\nare reasonably well-calibrated, and the blacks are not\nas well-calibrated. So you could imagine building\nsome kind of a transformation function to improve\nthat calibration, and that would get\nyou separation. Now, there's a\nterrible piece of news, which is that you can prove,\nas they do in this tutorial, that it's not possible\nto jointly achieve any pair of these conditions. So you have three\nreasonable technical notions of what fairness\nmeans, and they're incompatible with each other\nexcept in some trivial cases. This is not good.", "id": "zYgkr0KfWM0_51", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  but there's a very\nnice thing from Google where they illustrate\nthe results of adopting one or another of these\nnotions of fairness on a synthesized\npopulation of people, and you can see how\nthe trade-offs vary and what the results\nare of choosing different notions of fairness. So it's a kind of\nnice graphical hack. Again, it'll be on\nthe slides, and I urge you to check\nthat out, but I'm not going to have time\nto go into it. There is one other problem\nthat they point out which is interesting. So this was a\nscenario where you're trying to hire\ncomputer programmers, and you don't want to take\ngender into account because we know that women are\nunderrepresented among computer people, and so we\nwould like that not to be an allowed\nattribute in order", "id": "zYgkr0KfWM0_52", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So they say, well,\nthere are two scenarios. One of them is that gender,\nA, influences whether you're a programmer or not. And this is empirically true. There are fewer women\nwho are programmers. It turns out that visiting\nPinterest is slightly more common among women than men. Who knew? And then visiting GitHub is much\nmore common among programmers than among non-programmers. That one's pretty obvious. So what they say is, if you\nwant an optimal predictor of whether somebody's\ngoing to get hired, it should actually take both\nPinterest visits and GitHub visits into account, but\nbecause those go back", "id": "zYgkr0KfWM0_53", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  they don't like this model. And so they say, well, we could\nuse an optimal separated score, because now, being a programmer\nseparates your gender from the scoring function. And so we can create\na different score which is not the same\nas the optimal score, but is permitted because it's\nno longer dependent on your sex, on your gender. Here's another scenario that,\nagain, starts with gender and says, look, we know that\nthere are more men than women who obtain college degrees\nin computer science, and so there's an\ninfluence there, and computer scientists\nare much more", "id": "zYgkr0KfWM0_54", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  If you're were a woman-- has anybody visited the Grace\nMurray Hopper Conference? A couple, a few of you. So this is a really\ncool conference. Grace Murray Hopper invented\nthe notion bug or the term bug and was a really famous\ncomputer scientist starting back in the 1940s when there\nwere very few of them, and there is a yearly\nconference for women computer scientists in her honor. So clearly, the probability that\nyou visited the Grace Hopper Conference is dependent\non your gender. It's also dependent on whether\nyou're a computer scientist, because if you're\na historian, you're not likely to be interested\nin going to that conference. And so in this story,\nthe optimal score is going to depend basically\non whether you have a computer science degree or not,\nbut the separated score", "id": "zYgkr0KfWM0_55", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  is kind of funny, because\nthat's the protected attribute. And what these guys point out is\nthat despite the fact that you have these two scenarios,\nit could well turn out that the numerical data, the\nstatistics from which you estimate these models\nare absolutely identical. In other words, the\nsame fraction of people are men and women, the\nsame fraction of people are programmers, they\nhave the same relationship to those other factors, and\nso from a purely observational viewpoint, you can't tell\nwhich of these styles of model is correct or which version of\nfairness your data can support.", "id": "zYgkr0KfWM0_56", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  that these different\nnotions of fairness are in conflict with each other. So I wanted to finish by showing\nyou a couple of examples. So this was a paper\nbased on Irene's work. So Irene, shout if I'm\nbutchering the discussion. I got an invitation last year\nfrom the American Medical Association's Journal of Ethics,\nwhich I didn't know existed, to write a think piece for\nthem about fairness in machine learning, and I decided that\nrather than just bloviate, I wanted to present\nsome real work, and Irene had been\ndoing some real work. And so Marcia, who was\none of my students, and I convinced her\nto get into this,", "id": "zYgkr0KfWM0_57", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  these machine learning models\ncan identify and perhaps reduce disparities in general\nmedical and mental health. Now, why those two areas? Because we had access\nto data in those areas. So the general medical was\nactually not that general. It's intensive care\ndata from MIMIC, and mental health\ncare is some data that we had access to from Mass\nGeneral and McLean's hospital here in Boston, which both\nhave big psychiatric clinics. So yeah, this is\nwhat I just said. So the question we were\nasking is, is there bias based on race,\ngender, and insurance type? So we were really interested\nin socioeconomic status, but we didn't have\nthat in the database, but the type of insurance you\nhave correlates pretty well with whether you're\nrich or poor.", "id": "zYgkr0KfWM0_58", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  and if you have\nprivate insurance, the first approximation,\nyou're rich. So we did that, and then\nwe looked at the notes. So we wanted to see\nnot the coded data, but whether the things that\nnurses and doctors said about you as you\nwere in the hospital were predictive of readmission,\nof 30-day readmission, of whether you were likely\nto come back to the hospital. So these are some of the topics. We used LDA, standard\ntopic modeling framework. And the topics, as usual,\ninclude some garbage, but also include a lot of\nrecognizably useful topics. So for example, mass,\ncancer, metastatic, clearly associated with\ncancer, Afib, atrial, Coumadin, fibrillation, associated with\nheart function, et cetera, in the ICU domain.", "id": "zYgkr0KfWM0_59", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  things like bipolar, lithium,\nmanic episode, clearly associated with bipolar disease,\npain, chronic, milligrams, the drug quantity, associated\nwith chronic pain, et cetera. So these were the\ntopics that we used. And so we said,\nwhat happens when you look at the\ndifferent topics, how often the\ndifferent topics arise in different subpopulations? And so what we found is that,\nfor example, white patients have more topics that\nare enriched for anxiety and chronic pain, whereas black,\nHispanic, and Asian patients had higher topic\nenrichment for psychosis. It's interesting. Male patients had more\nsubstance abuse problems.", "id": "zYgkr0KfWM0_60", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  and treatment-resistant\ndepression. So if you want to create a\nstereotype, men are druggies and women are depressed,\naccording to this data. What about insurance type? Well, private insurance\npatients had higher levels of anxiety and depression,\nand poorer patients or public insurance\npatients had more problems with substance abuse. Again, another stereotype\nthat you could form. And then you could look at-- that was in the\npsychiatric population. In the ICU population, men still\nhave substance abuse problems. Women have more\npulmonary disease. And we were\nspeculating on how this relates to sort of known\ndata about underdiagnosis", "id": "zYgkr0KfWM0_61", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  By race, Asian patients have\na lot of discussion of cancer, black patients have\na lot of discussion of kidney problems,\nHispanics of liver problems, and whites have\natrial fibrillation. So again, stereotypes\nof what's most common in these different groups. And by insurance type,\nthose with public insurance often have multiple\nchronic conditions. And so public insurance patients\nhave atrial fibrillation, pacemakers, dialysis. These are indications\nof chronic heart disease and chronic kidney disease. And private insurance patients\nhave higher topic enrichment values for fractures. So maybe they're richer,\nthey play more sports and break their\narms or something.", "id": "zYgkr0KfWM0_62", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Just reporting the data. Just the facts. So these results are\nactually consistent with lots of analysis that have been\ndone of this kind of data. Now, what I really\nwanted to look at was this question of, can\nwe get similar error rates, or how similar are the\nerror rates that we get, and the answer is, not so much. So for example, if you\nlook at the ICU data, we find that the error rates\non a zero-one loss metric are much lower for men than they\nare for women, statistically significantly lower. So we're able to more accurately\nmodel male response or male prediction of 30-day\nreadmission than we are-- sorry, of ICU mortality for\nthe ICU than we are for women.", "id": "zYgkr0KfWM0_63", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  to predict outcomes for\nprivate insurance patients than for public\ninsurance patients with a huge gap\nin the confidence intervals between them. So this indicates that\nthere is, in fact, a racial bias in the\ndata that we have and in the models\nthat we're building. These are particularly\nsimple models. In psychiatry, when you\nlook at the comparison for different\nethnic populations, you see a fair\namount of overlap. One reason we\nspeculate is that we have a lot less data\nabout psychiatric patients than we do about ICU patients. So the models are\nnot going to give us as accurate predictions. But you still see, for example,\na statistically significant", "id": "zYgkr0KfWM0_64", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  although there's a\nlot of overlap here. Again, between\nmales and females, we get fewer errors in\nmaking predictions for males, but there is not a 95%\nconfidence separation between them. And for private versus\npublic insurance, we do see that separation\nwhere for some reason, in fact, we're able to\nmake better predictions for the people on\nMedicare than we are-- or Medicaid than\nwe are for patients in private insurance. So just to wrap that up, this is\nnot a solution to the problem, but it's an examination\nof the problem. And this Journal of\nEthics considered it interesting enough to publish\njust a couple of months ago.", "id": "zYgkr0KfWM0_65", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  is some work of\nWillie's, so I'm taking the risk of speaking\nbefore the people who actually did the work here\nand embarrassing myself. So this is modeling mistrust\nin end-of-life care, and it's based on\nWillie's master's thesis and on some papers that\ncame as a result of that. So here's the interesting data. If you look at\nAfrican-American patients, and these are patients in the\nMIMIC data set, what you find is that for mechanical\nventilation, blacks are on\nmechanical ventilation a lot longer than\nwhites on average, and there's a pretty\ndecent separation at the P equal\n0.05 level, so 1/2%", "id": "zYgkr0KfWM0_66", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So there's something going\non where black patients are kept on mechanical ventilation\nlonger than white patients. Now, of course, we\ndon't know exactly why. We don't know whether\nit's because there is a physiological\ndifference, or because it has something to do\nwith their insurance, or because God knows. It could be any of a lot\nof different factors, but that's the case. The eICU data set\nwe've mentioned, it's a larger, but less\ndetailed data set, also of intensive care patients, that\nwas donated to Roger Marks' Lab by Phillips Corporation. And there, we see,\nagain, a separation of mechanical ventilation\nduration roughly comparable to what we saw\nin the MIMIC data set. So these are consistent\nwith each other. On the other hand, if you look\nat the use of vasopressors, blacks versus whites, at\nthe P equal 0.12 level,", "id": "zYgkr0KfWM0_67", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  of evidence, but\nnot strong enough to reach any conclusions. Or in the eICU\ndata, P equal 0.42 is clearly quite\ninsignificant, so we're not making any claims there. So the question that\nWillie was asking, which I think is a\nreally good question, is, could this difference be due\nnot to physiological differences or even these sort of\nsocioeconomic or social differences, but to a difference\nin the degree of trust between the patient\nand their doctors? It's an interesting idea. And of course, I wouldn't\nbe telling you about this if the answer were no. And so the approach\nthat he took was to look for cases where\nthere's clearly mistrust.", "id": "zYgkr0KfWM0_68", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  For example, if a patient\nleaves the hospital against medical advice, that\nis a pretty good indication that they don't trust\nthe medical system. If the family-- if the\nperson dies and the family refuses to allow them\nto do an autopsy, this is another\nindication that maybe they don't trust the medical system. So there are these sort of red\nletter indicators of mistrust. For example, patient\nrefused to sign ICU consent and expressed\nwishes to be do not resuscitate, do not\nintubate, seemingly very frustrated and mistrusting\nof the health care system, also with a history of\npoor medication compliance and follow-up. So that's a pretty\nclear indication. And you can build a\nrelatively simple extraction", "id": "zYgkr0KfWM0_69", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  This is what I was\nsaying about autopsies. So the problem, of course,\nis that not every patient has such an obvious label. In fact, most of them don't. And so Willie's idea\nwas, can we learn a model from these obvious\nexamples and then apply them to the\nless obvious examples in order to get a kind\nof a bronze standard or remote supervision notion\nof a larger population that has a tendency to be mistrustful\naccording to our model without having as explicit\na clear case of mistrust, as in those examples. And so if you look at chart\nevents in MIMIC, for example, you discover that\nassociated with those cases of obvious mistrust are\nfeatures like the person was", "id": "zYgkr0KfWM0_70", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  They were literally\nlocked down to their bed because the nurses\nwere afraid they would get up and do something bad. Not necessarily\nlike attack a nurse, but more like fall out of bed\nor go wandering off the floor or something like that. If a person is in pain, that\ncorrelated with these mistrust measures as well. And conversely, if you saw that\nsomebody had their hair washed or that there was a discussion\nof their status and comfort, then they were probably\nless likely to be mistrustful of the system. And so the approach\nthat Willie took was to say, well, let's code\nthese 620 binary indicators of trust and build a\nlogistic regression model to the labeled\nexamples and then", "id": "zYgkr0KfWM0_71", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  we don't have such\na clear indication, and this gives us another\npopulation of people who are likely to be\nmistrustful and therefore, enough people that we can\ndo further analysis on it. So if you look at\nthe mistrust metrics, you have things like if\nthe patient is agitated on some agitation scale, they're\nmore likely to be mistrustful. If, conversely,\nthey're alert, they're less likely to be mistrustful. So that means they're in\nsome better mental shape. If they're not in\npain, they're less likely to be\nmistrustful, et cetera. And if the patient\nwas restrained, then trustful\npatients have no pain, or they have a spokesperson\nwho is their health care proxy,", "id": "zYgkr0KfWM0_72", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  but conversely, if restraints\nhad to be reapplied, or if there are various\nother factors, then they're more likely\nto be mistrustful. So if you look at that\nprediction, what you find is that for both predicting the\nuse of mechanical ventilation and vasopressors, the\ndisparity between a population of black and white\npatients is actually less significant\nthan the disparity between a population of high\ntrust and low trust patients. So what this suggests is that\nthe fundamental feature here that may be leading\nto that difference is, in fact, not\nrace, but is something that correlates with\nrace because blacks", "id": "zYgkr0KfWM0_73", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  of the medical\nsystem than whites. Now, why might that be? What do you know about history? I mean, you took the\ncity training course that had you read the Belmont\nReport talking about things like the Tuskegee experiment. I'm sure that leaves a\nsignificant impression in people's minds about how\nthe health care system is going to treat people of their race. I'm Jewish. My mother barely lived\nthrough Auschwitz, and so I understand some\nof the strong family feelings that happened\nas a result of some of these historical events. And there were medical people\ndoing experiments on prisoners in the concentration\ncamps as well, so I would expect that\npeople in my status might also have similar\nissues of mistrust.", "id": "zYgkr0KfWM0_74", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  is mistrust, in fact,\njust a proxy for severity? Are sicker people\nsimply more mistrustful, and is what we're seeing\njust a reflection of the fact that they're sicker? And the answer seems\nto be, not so much. So if you look at these severity\nscores like OASIS and SAPS and look at their correlation\nwith noncompliance in autopsy, those are pretty low\ncorrelation values, so they're not explanatory\nof this phenomenon. And then in the population,\nyou see that, again, there is a significant\ndifference in sentiment expressed in the notes between\nblack and white patients. The autopsy derived\nmistrust metrics don't", "id": "zYgkr0KfWM0_75", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  but the noncompliance\nderived mistrust metrics do. So I'm out of time. I'll just leave you\nwith a final word. There is a lot more work that\nneeds to be done in this area, and it's a very rich area\nboth for technical work and for trying to understand\nwhat the desiderata are and how to match them to\nthe technical capabilities. There are these\nvarious conferences. One of the people\nactive in this area, one of the pairs of people, Mike\nKearns and Aaron Roth at Penn are coming out with a book\ncalled The Ethical Algorithm, which is coming out this fall. It's a popular pressbook. I've not read it,\nbut it looks like it should be quite interesting. And then we're starting\nto see whole classes", "id": "zYgkr0KfWM0_76", "title": "23. Fairness", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  University of Pennsylvania has\nthe science of Data ethics, and I've mentioned already this\nfairness in machine learning class at Berkeley. This is, in fact, one of the\ntopics we've talked about. I'm on a committee\nthat is planning the activities of\nthe new Schwarzman College of Computing,\nand this notion of infusing ideas about\nfairness and ethics into the technical curriculum\nis one of the things that we've been discussing. The college obviously\nhasn't started yet, so we don't have anything\nother than this lecture and a few other things\nlike that in the works,", "id": "zYgkr0KfWM0_77"}, {"text": "  [SQUEAKING] [RUSTLING] [CLICKING] DAVID SONTAG: OK, so\nthen today's lecture is going to be about data\nset shifts, specifically how one can be robust\nto data set shift. Now, this is the topic\nthat we've been alluding to throughout the semester. And the setting that I want\nyou to be thinking about is as follows. You're a data scientist\nworking at, let's say, Mass General\nHospital, and you've been very careful in setting\nup your machine learning task to make sure that the\ndata is well specified, the labels that you're trying\nto predict are well specified. You train on a valid-- you train on your training data,\nyou test it on a held-out set, you see that the model\ngeneralizes well,", "id": "MdUnh4PaGKw_0", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  predicting is actually what\nyou think you're predicting, and you even do prospective\ndeployment where you then let your machine\nlearning algorithm drive some clinical decision\nsupport, and you'd see things are working great. Now what? What happens after this stage\nwhen you go to deployment? What happens when\nyour same model is going to be used\nnot just tomorrow but also next week, the\nfollowing week, the next year? What happens if your model,\nwhich is working well at this one hospital, then\nwants to-- then there's another institution,\nsay, maybe Brigham and Women's Hospital,\nor maybe UCSF, or some rural hospital\nin the United States wants to use the\nsame model, will it keep working in this \"short\nterm to the future\" time period or in a new institution? That's the question which\nwe're going to be talking about in today's lecture. And we'll be talking\nabout how one can deal with data set shift\nof two different varieties. The first variety is adversarial\nperturbations to data,", "id": "MdUnh4PaGKw_1", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  changes for natural reasons. Now, the reason why\nit's not at all obvious that your machine learning\nalgorithm should still work in the setting is because\nthe number one assumption we make when we do\nmachine learning is that your training\ndistribution, your training data, is drawn from the same\ndistribution as your test data. So if you now go to a setting\nwhere your data distribution has changed, even if you've\ncomputed your accuracy using your held-out data\nand it looks good, there's no reason\nthat should continue to look good in this new\nsetting, where the data distribution has changed. A simple example of what it\nmeans for a data distribution to change might be as follows. Suppose that we\nhave as input data, and we're trying to\npredict some label, which", "id": "MdUnh4PaGKw_2", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  or will be newly diagnosed\nwith type 2 diabetes, and this is an\nexample which we-- which we talked about when we\nintroduce risk stratification, you learn a model\nto predict y from x. And now suppose you go\nto a new institution where their definition of\nwhat type 2 diabetes means has changed. For example, maybe they don't\nactually have type 2 diabetes coded in their data, maybe\nthey only have diabetes coded in their data,\nwhich is lumping together both type 1 and type\n2 diabetes, type 1 being what's usually\njuvenile diabetes and is actually a very distinct\ndisease from type 2 diabetes. So now the notion of what\ndiabetes is is different.", "id": "MdUnh4PaGKw_3", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  And there's no\nreason, obviously, that your model, which was used\nto predict type 2 diabetes, would work for that new label. Now, this is an example\nof a very type-- of a type of data set\nshift which is perhaps for you obvious nothing\nshould work in the setting because here the distribution\nof P of y given x changes, meaning even if you have\nthe same individual, your distribution P(y)\ngiven x in, let's say, the distribution P(0) and the\ndistribution P of y given x and P(1), where this is, let's\nsay, one institution, this is another, these now are\ntwo different distributions if the meaning of the\nlabel has changed. So for the same person, there\nmight be different distribution over what y is. So this is one\ntype of data shift. And a very different\ntype of data set shift is where we assume\nthat these two are equal. And so that would,\nfor example, rule out", "id": "MdUnh4PaGKw_4", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  But rather what changes is P of\nx from location 1 to location-- to location 2. And this is the type of data set\nshift which will be focused on in today's lecture. It goes by the name\nof covariate shift. And let's look at two\ndifferent examples of that. The first example would be of\nan adversarial perturbation. And so we've-- you've all seen\nthe use of convolutional neural networks for image\nclassification problems. This is just one illustration\nof such an architecture. And with such an\narchitecture, one could then attempt to do all\nsorts of different object classification or image\nclassification tasks. You could take as input\nthis picture of a dog, which is clearly a dog. And you could modify\nit just a little bit.", "id": "MdUnh4PaGKw_5", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  What I'm going to\ndo is now I'm going to create a new image which\nis that original image. Now with every single\npixel, I'm going to add a very small epsilon in\nthe direction of that noise. And what you get out\nis this new image, which you could stare at\nhowever long you want, you're not going to able\nto tell the difference. Basically to the\nhuman eye, these two look exactly identical. Except when you take your\nmachine learning classifier, which is trained on\noriginal unperturbed data, and now apply it\nto this new image, it's classified as an ostrich. And this observation\nwas published in a paper in 2014 called\n\"Intriguing properties of neural networks.\" And it really kickstarted\na huge surge of interest in the machine\nlearning community on adversarial perturbations\nto machine learning.", "id": "MdUnh4PaGKw_6", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  a little bit, how\ndoes that change your classifier's output? And could that be used to attack\nmachine learning algorithms? And how can one\ndefend against it? By the way, as an\naside, this is actually a very old area of research. And even back in the land\nof linear classifiers, these questions\nhad been studied. Although I won't get\ninto it in this course. So this is a type of data\nset shift in the sense that what we want is that this\nshould still be classified as an ostrich-- as a dog. So the actual label\nhasn't changed. We would like this distribution\nover the labels, given the perturbed into it,\nto be slightly different, except that now the\ndistribution of inputs is a little bit\ndifferent because we're allowing for some noise to be\nadded to each of the inputs. And in this case, the noise\nactually isn't random, it's adversarial. And towards the end\nof today's lecture, I'll give you an example\nof how one can actually", "id": "MdUnh4PaGKw_7", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  can change the classifier. Now, the reason\nwhy we should care about these types of\nthings in this course are because I expect\nthat this type of data set shift, which is not at\nall natural, it's adversarial, is also going to start\nshowing up in both computer vision and non-computer vision\nproblems in the medical domain. There was a nice paper by Sam\nFinlayson, Andy Beam, and Isaac Kohane recently, which\npresented several different case studies of where these\nproblems could really arise in health care. So, for example, here\nwhat we're looking at is an image\nclassification problem arising from dermatology. You're given as input an image. For example, you would like\nthat this image be classified as an individual having\na particular type of skin", "id": "MdUnh4PaGKw_8", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  And what one can see is that\nwith a small perturbation of the input, one\ncan completely swap the label that would be assigned\nto it from one to the other. And in this paper,\nwhich we're going to post as optional\nreadings for today's course, they talk about how one\ncould maliciously use these algorithms for benefit. So, for example, imagine that\na health insurance company now decides in order to reimburse\nfor an expensive biopsy of a patient's skin,\na clinician or a nurse must first take a\npicture of the disorder and submit that picture\ntogether with the bill", "id": "MdUnh4PaGKw_9", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  And imagine now that the\ninsurance company were to have a machine\nlearning algorithm be an automatic check, was this\nprocedure actually reasonable for this condition? And if it isn't, it\nmight be flagged. Now, a malicious user could\nperturb the input such that it would, despite the\npatient having perhaps even completely normal-looking\nskin, could nonetheless be classified by a\nmachine learning algorithm as being abnormal in\nsome way, and thus perhaps could get reimbursed\nby that procedure. Now, obviously\nthis is an example of a nefarious setting\nwhere we would then hope that such an\nindividual would be caught by the police, sent to jail. But nonetheless, what we\nwould like to be able to do is build checks and balances\ninto the system such that that couldn't even\nhappen because to a human", "id": "MdUnh4PaGKw_10", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  trick anyone with such a\nvery minor perturbation. So how do you build\nalgorithms that could also be not\ntricked as easily as humans wouldn't be tracked? AUDIENCE: Can I ask a question DAVID SONTAG: Yeah. AUDIENCE: For any\nof these samples, did the attacker need\naccess to the network? Is there a way to\n[? attack it? ?] DAVID SONTAG: So the question\nis whether the attacker needs to know something about\nthe function that's being used for classifying. There are examples of both what\nare called white box and black box attacks, where in one\nsetting you have access to the function and\nother settings you don't. And so both have been\nstudied in the literature, and there are results\nshowing that one can attack in either setting. Sometimes you might need\nto know a little bit more. Like, for example,\nsometimes you need to have the ability to query\nthe function a certain number of times. So even if you don't know\nexactly what the function is, like you don't know the\nweights of the neural network,", "id": "MdUnh4PaGKw_11", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  you'll be able to construct\nadversarial examples. That would be one approach. Another approach\nwould be, oh, maybe we don't know the function,\nbut we know something about the training data. So there are ways to go about\ndoing this even if you don't perfectly know the function. Does that answer your question? So what about a\nnatural perturbation? So this figure just\npulled from lecture 5 when we talked about\nnon-stationarity in the context of risk\nstratification, that's just to remind you here the\nx-axis is time, that y-axis is different types of\nlaboratory test results that might be ordered,\nand the color denotes how many of those\nlaboratory tests were ordered in a certain\npopulation at a point in time. So what we would expect to\nsee if the data was stationary is that every row would\nbe a homogeneous color. But instead what we see is\nthat there are points in time, for example, a few month\nintegrals over here, when suddenly it looks like, for\nsome of the laboratory tests,", "id": "MdUnh4PaGKw_12", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  That's most likely\ndue to a data problem, or perhaps the feed of data from\nthat laboratory test provider got lost, there were\nsome systems problem. But they're also going\nto be settings where, for example, a\nlaboratory test is never used until it's suddenly used. And that might be because\nit's a new test that was just invented or\napproved for reimbursement at that point in time. So this is an example\nof non-stationarity. And, of course, this\ncould also result in changes in your\ndata distribution, such as what I described\nover there, over time. And the third example\nis when you then go across institutions,\nwherein, of course, both the language that might\nbe used-- you might think of a hospital in\nthe United States versus a hospital in China, the\nclinical notes will be written in completely different\nlanguages, that'll would be an extreme case. And a less extreme case might\nbe two different hospitals in Boston where the\nacronyms or the shorthand they use for some clinical\nterms might actually be different because\nof local practices.", "id": "MdUnh4PaGKw_13", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  This is all a setup. And for the rest of the\nlecture, what I'll talk about is first, very\nbriefly, how one can build in population-level checks\nfor has something changed. And then the bulk\nof today's lecture, we'll be talking about how\nto develop transfer learning algorithms and how one\ncould think about defenses to adversarial attacks. So before I show you that\nfirst slide for bullet one, I want to have a\nbit of discussion. You've suddenly done that\nthing of learning machine learning algorithm\nin your institution, and you want to know,\nwill this algorithm work at some other institution? You pick up the phone, you\ncall up your collaborating data scientists at another\ninstitution, what are the questions that\nyou should ask them when we're trying to\nunderstand, will your algorithm", "id": "MdUnh4PaGKw_14", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  Yeah. AUDIENCE: What kind of\nlab test information they collect [INAUDIBLE]. DAVID SONTAG: So\nwhat type of data do they have on their\npatients, and do they have similar data types\nor features available for their patient population? Other ideas, someone who\nhasn't spoken in the last two lectures, maybe someone\nin the far back there, people who have\ntheir computer out. Maybe you with your\nhand in your mouth right there, yeah, you\nwith your glasses on. Ideas. [STUDENT LAUGHS] AUDIENCE: Sorry, can\nyou repeat the question? DAVID SONTAG: You want me\nto repeat the question? The question was as follows. You learn your machine learning\nalgorithm at some institution, and you want to apply it\nnow in a new institution. What questions should you\nask of that new institution to try to assess whether your\nalgorithm will generalize in that new institution? AUDIENCE: I guess it depends on\nyour problem you're looking at, like whether you're\ntrying to learn possible differences\nin your population,", "id": "MdUnh4PaGKw_15", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  So I'd envision it\nthat you'd want to, like are your machines\ncalibrated [INAUDIBLE]?? Do they use techniques\nto acquire the data? DAVID SONTAG: All right. So let's break down each of\nthe answers that you gave. The first answer\nthat you gave was, are there differences\nin the population? What would be an exa--\nsomeone else now, what are we an example of a\ndifference in a population? Yep. AUDIENCE: Age\ndistribution You might have younger people\nin maybe Boston versus like a\nMassachusetts [INAUDIBLE].. DAVID SONTAG: So you\nmight have younger people in Boston versus\nolder people who are in Central Massachusetts. How might a change\nin age distribution affect your ability of your\nalgorithms to generalize? Yep. AUDIENCE: [? Possibly ?]\nhealth patterns, where young people are very\ndifferent from [INAUDIBLE] who have some diseases that\nare clearly more prevalent in populations that are\nolder [? than you. ?] DAVID SONTAG: Thank you. So sometimes we might expect a\ndifferent just set of diseases", "id": "MdUnh4PaGKw_16", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  population. So I type 2 diabetes,\nhypertension, these are diseases that are\noften diagnosed when patients-- when individuals are\n40s, 50s, and older. If you have people\nwho are in their 20s, you don't typically\nsee those diseases in a younger population. And so what that means is\nif your model, for example, was trained on a population\nof very young individuals, then it might not be able\nto-- and suppose you're doing something like\npredicting future cost, so something which is not\ndirectly tied to the disease itself, the features that\nare predictive of future cost in a very young population\nmight be very different from features-- for predictors of cost in a\nmuch older population because of the differences in conditions\nthat those individuals have. Now the second\nanswer that was given had to do with calibration\nof instruments.", "id": "MdUnh4PaGKw_17", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  AUDIENCE: Yeah. So I was thinking [? clearly ?]\nin the colonoscopy space. But if you're collecting--\nso in that space, you're collecting\nvideos of colons. And so you can\nhave machines that are calibrated very\ndifferently, let's say different light exposure,\ndifferent camera settings. But you also have that\nthe GIs and physicians have different techniques as\nto how they explore the colon. So the video data itself is\ngoing to be very different. DAVID SONTAG: So the\nexample that was given was of colonoscopies\nand data that might be collected as part of that. And the data that could be--\nthe data that could be collected could be different for\ntwo different reasons. One, because the-- because\nthe actual instruments that are collecting the data,\nfor example, imaging data, might be calibrated a\nlittle bit differently. And a second reason might be\nbecause the procedures that are used to perform that\ndiagnostic test might be different in each institution. Each one will result in slightly\ndifferent biases to the data, and it's not clear that\nan algorithm trained on one type of procedure\nor one type of instrument", "id": "MdUnh4PaGKw_18", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  So these are all great examples. And so when one reads a paper\nfrom the clinical community on developing a new risk\nstratification tool, what you will always\nsee in this paper is what's known as \"Table 1.\" Table 1 looks a\nlittle bit like this. Here I pulled one\nof my own papers that was published in\nJAMA Cardiology for 2016 where we looked at how to try\nto find patients with heart failure who are hospitalized. And I'm just going to walk\nthrough what this table is. So this table is\ndescribing the population that was used in the study. At the very top, it says these\nare characteristics of 47,000 hospitalized patients. Then what we've done is,\nusing our domain knowledge, we know that this is a\nheart failure population, and we know that there are\na number of different axes that differentiate patients\nwho are hospitalized", "id": "MdUnh4PaGKw_19", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  And so we enumerate over\nmany of the features that we think are critical to\ncharacterizing the population, and we give\ndescriptive statistics on each one of those features. You always start with things\nlike age, gender, and race. And so here, for example, the\naverage age was 61 years old, this was, by the way, NYU\nMedical School, 50.8% female, 11.2% Black, African-American,\n17.6% of individuals were on Medicaid, which\nwas a state-provided health insurance for either disabled\nor lower-income individuals. And then we looked at quantities\nlike what types of medications were patients on. 41% of-- 42% of\ninpatient patients were on something\ncalled beta blockers. 31.6% of outpatients\nwere on beta blockers.", "id": "MdUnh4PaGKw_20", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  So one can look at the\naverage creatinine values, the average sodium values\nof this patient population. And in this way,\nit described what is the population\nthat's being studied. Then when you go to\nthe new institution, that new institution receives\nnot just the algorithm, but they also\nreceive this Table 1 that describes a population in\nwhich the algorithm was learned on. And they could use that together\nwith some domain knowledge to think through questions like\nwhat we were eliciting-- what I elicited from you\nin our discussion so that we could\nthink, is it actually-- does it make sense that\nthis model will generalize to this new institution? Are the reasons\nwhy it might not? And you could do that\neven before doing any prospective evaluation\non the new population. So almost all of you should\nhave something like Table 1 in your project\nwrite-ups because that's", "id": "MdUnh4PaGKw_21", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  what is the population that\nyou're doing your study on? You agree with me, Pete? PETER SZOLOVITS: Yeah. I would just at that Table 1,\nif you're doing a case control study, you will have\ntwo columns that show the distributions\nin the two populations, and then a p-value of how\nlikely those differences are to be significant. And if you leave that out, you\ncan't get your paper published. DAVID SONTAG: I'll just\nrepeat Pete's answer for the recording. If you are-- this table is\nfor a predictive problem. But if you're thinking about a\ncausal inference type problem, where there's a notion of\ndifferent intervention groups, then you'd be expected to\nreport the same sorts of things, but for both the\ncase population, the people who received,\nlet's say, treatment one, and the control\npopulation of people", "id": "MdUnh4PaGKw_22", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  And then you would be\nlooking at differences between those populations as\nwell at the individual feature level as part of the descriptive\nstatistics for that study. Now, this-- yeah. AUDIENCE: Is this to\nidentify [? individually ?] [? between ?] those peoples? [INAUDIBLE] institutions to do\nlike t-tests on those tables-- DAVID SONTAG: To see\nif they're different? No, so they're always\ngoing to be different. You go to a new\ninstitution, it's always going to look different. And so just looking to see\nhow something changed is not-- the answer's always\ngoing to be yes. But it enables a conversation\nto think through, OK, this, and then you might look-- you might use some\nof the techniques that Pete's going to talk about\nnext week on interpretability to understand, well, what\nis the model actually using. Then you might\nask, oh, OK, well, the model is using\nthis thing, which makes sense in this population\nbut might not make sense in another population. And it's these two\nthings together that make the conversation.", "id": "MdUnh4PaGKw_23", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  to the forefront in recent\nyears in close connection to the topic that Pete\ndiscussed last week on fairness in machine learning. Because you might ask\nif a classifier is built in some population, is\nit going to generalize to another population if that\npopulation that has learned on was very biased, for\nexample, it might have been all white people. You might ask, is\nthat classifier going to work well in another\npopulation that might perhaps include people of\ndifferent ethnicities? And so that has led to a concept\nwhich was recently published. This working draft that I'm\nshowing the abstract from was just a few weeks ago called\n\"Datasheets for data sets.\" And the goal here\nis to standardize the process of\ndescribing-- of eliciting the information about what is it\nabout the data set that really played into your model?", "id": "MdUnh4PaGKw_24", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  just through a\ncouple of elements of what an example data set for\na datasheet might look like. This is too small\nfor you to read, but I'll blow up one\nsection in just a second. So this is a\ndatasheet for a data set called Studying Face\nRecognition in an Unconstrained Environment. So it's for computer\nvision problem. There are going to be a\nnumber of questionnaires, which this paper that I\npoint you to outlines. And you as the model developer\ngo through that questionnaire and fill out the\nanswers to it, so including things about\nmotivation for the data set creation\ncomposition and so on. So in this particular instance,\nthis data set called Labeled Faces in the Wild was created to\nprovide images that study face recognition in an unconstrained\n[INAUDIBLE] settings, where image characteristics\nsuch as pose, elimination,", "id": "MdUnh4PaGKw_25", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  So it's intended to be\nreal-world settings. Now, one of the most\ninteresting sections of this report that one should\nrelease with the data set has to do with how was the\ndata preprocessed or cleaned? So, for example,\nfor this data set, it walks through the\nfollowing process. First, raw images were\nobtained from the data set, and it consisted of\nimages and captions that were found together with\nthat image in news articles or around the web. Then there was a face detector\nthat was run on the data set. Here were the parameters of the\nface detector that were used. And then remember, the goal\nhere is to study face detection. And so-- so one has to\nknow, how were the-- how were the labels determined?", "id": "MdUnh4PaGKw_26", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  was no face in this image? And so there they described\nhow a face was detected and how a region was determined to not\nbe a face in the case that it wasn't. And finally, it describes\nhow duplicates were removed. And if you think\nback to the examples we had earlier in the\nsemester from medical imaging, for example in\npathology and radiology, similar data set constructions\nhad to be done there. For example, one would\ngo to the PAC System where radiology images\nare stored, one would-- one would decide which images\nare going to be pulled out, one would go to\nradiography reports to figure out how do we\nextract the relevant findings from that image,\nwhich would give the labels for that predictive--\nfor that learning task. And each step there will\nincur some bias and some-- which one then needs to\ndescribe carefully in order to understand what\nmight the bias be of the learned classifier.", "id": "MdUnh4PaGKw_27", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  but this will also be\none of the suggested readings for today's course. And it's a fast read. I encourage you to go through\nit to get some tuition for what are questions we might want\nto be asking about data sets that we create. And for the rest\nof this semester-- for the rest of the\nlecture today, I'm now going to move on to\nsome more technical issues. So we have to do it. We're doing machine\nlearning now. The populations\nmight be different. What do we do about it? Can we change the\nlearning algorithm in order to hope that your\nalgorithm might transfer better to a new institution? Or if we get a little bit of\ndata from that new institution, could we use that\nsmall amount of data from the new institution or a\nfuture time point in the future to retrain our model to\ndo well in that slightly different distribution? So that's the whole field\nof transfer learning. So you have data drawn from one\ndistribution on p of x and y,", "id": "MdUnh4PaGKw_28", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  distribution q of x,y. And under the covariate\nshift assumption, I'm assuming that q(x,y) is\nequal to q of x times p of y given x, namely that the\nconditional distribution of y given x hasn't changed. The only thing that\nmight have changed is your distribution over x. So that's what the covariate\nshift assumption would assume. So suppose that we\nhave some small amount of data drawn from the\nnew distribution q. How could we then\nuse that in order to perhaps retrain our\nclassifier to do well for that new institution? So I'll walk through four\ndifferent approaches to do so. I'll start with\nlinear models, which", "id": "MdUnh4PaGKw_29", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  move on to deep models. The first approach to something\nthat you've seen already several times in this course. We're going to\nthink about transfer as a multi-task learning\nproblem, where one of the tasks has much less data\nthan the other task. So if you remember when\nwe talked about disease progression modeling,\nI introduced this notion of\nregularizing the weight vectors so that they could\nbe close to one another. At that time, we were\ntalking about weight vectors predicting disease\nprogression in different time points in the future. We could use exactly\nthe same idea here, where you take your classifier,\nyour linear classifier that was trained on a\nreally large corpus, I'm going to call that-- I'm going to call the weights\nof that classifier w old, and then I'm going to solve a\nnew optimization problem, which", "id": "MdUnh4PaGKw_30", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  So this is where your training--\nyour new training data come in. So I'm going to assume that\nthe new training get D is drawn from the q distribution. And I'm going to add on a\nregularization that asks that w should stay close to w old. Now, if the amount of\ndata you have-- if D, the data from that new\ninstitution, was very large, then you wouldn't need this at\nall because you would be able to just-- you would be able to ignore\nthe classifier that you learned", "id": "MdUnh4PaGKw_31", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  to that new institution's data. Where something like this\nis particularly valuable is if there was a small\namount of data set shift, and you only have a very\nsmall amount of labeled data from that new\ninstitution, then this would allow you to\nchange your weight vector just a little bit. So if this coefficient\nwas very large, it would say that\nthe new w can't be too far from the old w. So it'll allow you to\nshift things a little bit in order to do well on the small\namount of data that you have. So, for example, if there is\na feature which was previously predictive, but that\nfeature is no longer present in the new data\nset, so, for example, it's all identically zero,\nthen, of course, the new weight vect-- the new weight for that feature\nis going to be set to 0, and that weight\nyou can think about as being redistributed to\nsome of the other features. Does this makes sense? Any questions? So this is the simplest\napproach to transfer learning.", "id": "MdUnh4PaGKw_32", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  always try this. Uh, yep. So the second approach is\nalso with a linear model, but here we're no longer going\nto assume that the features are still useful. So there might--\nwhen you go from-- when you go from a-- your first institution, let's\nsay, I'm GH on the left, you learn your model,\nand you can apply it to some new institution,\nlet's say, UCSF on the right, it could be that there\nis some really big change in the feature set such that-- such that the original\nfeatures are not at all useful for the new feature set. And a really extreme\nexample of that", "id": "MdUnh4PaGKw_33", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  your model's trained on English,\nand you're testing it out in Chinese. That would be an example-- if you use a bag of\nwords model, that would be an example where\nyour model, obviously, wouldn't generalize at all\nbecause your features are completely different. So what would you\ndo in that setting? What's the simplest\nthing that you might do? So you're taking a text\nclassifier learned in English, and you want to\napply it in a setting where that language is Chinese. What would you do? AUDIENCE: Train on them. DAVID SONTAG:\nTranslate, you said. And there was another answer. AUDIENCE: Or try train an RN. DAVID SONTAG: Train\nan RN to do what? AUDIENCE: To translate. DAVID SONTAG: Train\nan RN-- oh, OK. So assume that you\nhave some ability to do machine translation, you\ntranslate from English to-- from Chinese to English. It has to be that direction\nbecause the original classifier", "id": "MdUnh4PaGKw_34", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  And then your new function\nis the composition of the translation and the\noriginal function, right? And then you can\nimagine doing some fine tuning if you had a\nsmall amount of data. Now, the simplest\ntranslation function might be just use a dictionary. So you look up a\nword, and if that word has an analogy in\nanother language, you say, OK, this\nis the translation. But there are always going to\nbe some words in your language which don't have a\nvery good translation. And so you might imagine that\nthe simplest approach would be to translate, but\nthen to just drop out words that don't\nhave a good analog and force your classifier\nto work with, let's say, just the shared vocabulary. Everything we're\ntalking about here is an example of a\nmanually chosen decision. So we're going to manually\nchoose a new representation for the data such that we\nhave some amount of shared", "id": "MdUnh4PaGKw_35", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  So let's talk about\nelectronic health record 1 and electronic health record 2. By the way, the slides that\nI'll be presenting here are from a paper\npublished in KDD by Jan, Tristan, your\ninstructor, Pete, and John Guttag. So you have to go\ntwo electronic health records, electronic\nhealth record 1, electronic health record 2. How can things change? Well, it could be that the same\nconcept in electronic health record 1 might be mapped\nto a different encoding, so that's like an\nEnglish-to-Spanish type translation, in electronic\nhealth record 2. Another example\nof a change might be to say that some concepts\nare removed, like maybe you have laboratory test results\nin electronic health record 1 but not in electronic\nhealth record 2.", "id": "MdUnh4PaGKw_36", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  Another change might be\nthere might be new concepts. So the new institution\nmight have new types of data that the old\ninstitution didn't have. So what do you do\nin that setting? Well, one approach\nwe would say, OK, we have some small amount of\ndata from electronic health record 2. We could just train\nusing that and throw away your original data from\nelectronic health record 1. Now, of course, if you\nonly had a small amount of data from the target\nto distribution, then that's going to be a very poor\napproach because you might not have enough data\nto actually learn a reasonable enough model. A second obvious\napproach would be, OK, we're going to just train\non electronic health record 1 and apply it. And for those concepts that\naren't present anymore, so be it. Maybe things won't\nwork very well. A third approach, which we\nwere alluding to before when we talked about\ntranslation, would be to learn a model just in\nthe intersection of the two", "id": "MdUnh4PaGKw_37", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  And what this work\ndoes, as they say, we're going to manually\nredefine the feature set in order to try to find as\nmuch common ground as possible. And this is something\nwhich really involves a lot of domain knowledge. And I'm going to be using\nthis as a point of contrast from what I'll be talking about\nin 10 or 15 minutes, where I talk about how one could\ndo this without that domain knowledge that we're\ngoing to use here. So the setting\nthat they looked at is one of predicting outcomes,\nsuch as in-hospital mortality or length of stay. The model which is going to be\nused as a bag-of-events model. So we will take a patient's\nlongitudinal history up until the time of prediction. We'll look at different\nevents that occurred. And this study was\ndone using PhysioNet. And MIMIC, for example, events\nare encoded with some number,", "id": "MdUnh4PaGKw_38", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  1046 might correspond\nto pain being present, 25 might correspond to the drug\nheparin being given and so on. So we're going to create one\nfeature for every event which has some number-- which is encoded\nwith some number. And we'll just say\n1 if that event has occurred, 0 otherwise. So that's the representation\nfor a patient. Now, because when one goes\nthough this new institution, EHR2, the way that\nevents are encoded might be completely different. One won't be able to just\nuse the original feature representation. And that's the\nEnglish-to-Spanish example that I gave. But instead, what\none could try to do is come up with a new feature\nset where that feature set could be derived from each\nof the different data sets. So, for example, since each\none of the events in MIMIC has some text\ndescription that goes", "id": "MdUnh4PaGKw_39", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  event 2, hemorrhagic\nstroke, and so on, one could attempt to map-- use that English\ndescription of the feature to come up with a way to map\nit into a common language. In this case, the\ncommon language is the UMLS, the\nUnited Medical Language System that Pete talked\nabout a few lectures ago. So we're going to now say, OK,\nwe have a much larger feature set where we've now\nencoded ischemic stroke as this concept,\nwhich is actually the same ischemic\nstroke, but also as this concept\nand that concept, which are more general\nversions of that original one. So this is just\ngeneral stroke, and it could be multiple\ndifferent types of strokes. And the hope is\nthat even if in-- even if the model doesn't-- even if some of these\nmore specific ones don't show up in the\nnew institution's data, perhaps some of the more general\nconcepts do show up there.", "id": "MdUnh4PaGKw_40", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  you're going to learn your model\nnow on this expanded translated vocabulary, and\nthen translate it. And at the new\ninstitution, you'll also be using that\nsame common data model. And that way one hopes\nto have much more overlap in your feature set. And so to evaluate\nthis, the authors looked at two different\ntime points within MIMIC. One time point was when the\nBeth Israel Deaconess Medical Center was using electronic\nhealth record called CareView. And the second time point\nwas when that hospital was using a different\nelectronic health record called MetaVision. So this is an example\nactually of non-stationarity. Now because of them using two\ndifferent electronic health records, the encodings\nwere different. And that's why\nthis problem arose. And so we're going\nto use this approach,", "id": "MdUnh4PaGKw_41", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  on top of this new encoding\nthat I just described. And we're going to compare the\nresults by looking at how much performance was lost due\nto using this new encoding, and how well we\ngeneralize from one-- from one-- from the source\ntask to the target task. And so here's the\nfirst question, which is, how much do we lose\nby using this new encoding? So as a comparison\npoint for looking at predicting in-hospital\nmortality, we'll look at, what is the\npredictive performance if you're to just use an\nexisting, very simple risk score called the SAPS score? And that's this red line\nwhere that y-axis here is the area under the\nROC curve, and the x-axis is how much time\nin advance you're predicting, so the\nprediction gap. So using this very simple score,\nSAPS get somewhere between 0.75", "id": "MdUnh4PaGKw_42", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  But if you were to use all\nof the events data, which is much, much richer than what\nwent into that simple SAPS score, you would get the\npurple curve, which is-- the purple curve, which is\nSAPS plus the event data, or the blue curve, which\nis just the events data. And you can see you\ncan get substantially better predictive\nperformance by using that much richer feature set. The SAPS score has the\nadvantage that it's easier to generalize because it's so\nsimple, those feature elements, one could trivially translate\nto any new EHR, either manually or automatically, and thus\nit'll always be a viable route. Whereas this blue\ncurve, although it gets better predictive\nperformance, you have to really worry about\nthese generalization questions. And the same story happens\nin both of the source task and the target task. Now the second question\nto ask is, well,", "id": "MdUnh4PaGKw_43", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  of the data? And so here looking at,\nagain, both of the two-- both EHRs, what we\nsee first in red is the same red curvature-- is\nthe same as the blue curvature on the previous slide. It's using SAPS plus the item\nIDs, so using all of the data. And then the blue curve here,\nwhich is a bit hard to see, but it's right there,\nit's substantially lower. So that's what\nhappens if you now use this new representation. And you see that you\ndo lose something by trying to find a\ncommon vocabulary. The performance\ndoes get hit a bit. But what's particularly\ninteresting is when you attempt to generalize,\nyou start to see a swap. So if we now-- so now the colors are\ngoing to be quite similar. Red here was at the\nvery top before.", "id": "MdUnh4PaGKw_44", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  Before it was at the very top. Shown here is the training error\non this institution, CareView. You see, there's so\nmuch rich information in the original\nfeature set that it's able to do very good\npredictive performance. But once you attempt\nto translate it, so you train on CareView,\nbut you test on MetaVision, then the test performance shown\nhere by this solid red line is actually the worst\nof all of the system. So there's a substantial\ndrop in performance because not all\nof these features are present in the new EHR. On the other hand, when\nthe translated version, despite the fact that it's\na little bit worse when evaluated on the source,\nit generalizes much better. And so you see a significantly\nbetter performance that's shown by this\nblue curve here when you use this translated vocabulary.", "id": "MdUnh4PaGKw_45", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  AUDIENCE: So would you\ntrain with full features? So how do you apply [? with ?]\nthem if the other [? full ?] features are-- you\njust [INAUDIBLE].. DAVID SONTAG: So, you\nassume that you have come up with a mapping from the\nfeatures in both of the EHRs to this common feature\nvocabulary of QEs. And the way that this mapping is\ngoing to be done in this paper is based on the text of the-- of the events. So you take the text-based\ndescription of the event, and you come up with a\ndeterministic mapping to this new UMLS-based\nrepresentation. And then that's\nwhat's being used. There's no fine\ntuning being done in this particular example. So I consider this to be a very\nnaive application of transfer. The results are exactly what you\nwould expect the results to be. And, obviously, a lot of work\nhad to go into doing this.", "id": "MdUnh4PaGKw_46", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  the English-based\ndescription of the features to come up with the\nautomatic mapping, but the story ends there. And so a question\nwhich all of you might have is, how\ncould you try to do such an approach automatically? How could we automatically\nfind representations-- new representations of\nthe data that are likely to generalize\nfrom, let's say, a source distribution to\na target distribution? And so to talk about\nthat, we're going to now start thinking\nthrough representation learning-based approaches,\nof which deep models are particularly capable. So the simplest approach to\ntry to do transfer learning in the context of, let's\nsay, deep neural networks, would be to just chop off part\nof the network and reuse that-- some internal representation of\nthe data in this new location. So the picture looks a\nlittle bit like this. So the data might\nfeed in the bottom.", "id": "MdUnh4PaGKw_47", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  some fully connected layers. And what you decide\nto do is you're going to take this model that's\ntrained in one institution, you chop it at some layer,\nit might be, for example, prior to the last\nfully connected layer, and then you're\ngoing to take that-- take the new representation\nof your data, now the representation\nof the data is what you would get out\nafter doing some convolutions followed by a single\nfully connected layer, and then you're going to take\nyour target distribution's data, which you might only\nhave a small amount of, and you learn a simple model on\ntop of that new representation. So, for example, you might\nlearn a shallow classifier using a support\nvector machine on top of that new representation. Or you might add in some more-- a couple more layers of a deep\nneural network, and then fine tune the whole thing end to end. So all of these have been tried. And in some cases, one\nworks better than another.", "id": "MdUnh4PaGKw_48", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  And that was when Adam\nYala spoke in lecture 13 about breast cancer\nand mammography, where in his approach he\nsaid that he had tried both taking a randomly\ninitialized classifier and comparing that to what\nwould happen if you initialized with a well-known\nImageNet-based deep neural network for the problem. And he had a really\ninteresting story that he gave. In his case, he had enough\ndata that he actually didn't need to initialize\nusing this pre-trained model from ImageNet. If he had just done a random\ninitialization, eventually-- and this x-axis,\nI can't remember, it might be hours of training\nor epochs, I don't remember, it's time-- eventually the\nright initialization", "id": "MdUnh4PaGKw_49", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  But for his particular\ncase, if you were to do a initialization with\nImageNet and then fine tune, you get there\nmuch, much quicker. And so it was for the\ncomputational reason that he found it to be useful. But in many other applications\nin medical imaging, the same tricks become\nessential because you just don't have enough data\nin the new test case. And so one makes use of,\nfor example, the filters which one learns from an\nImageNet's task, which is dramatically different from\nthe medical imaging problems, and then using those\nsame filters together with a new top layer,\nset of top layers in order to fine tune it for\nthe problem that you care about. So this would be\nthe simplest way to try to hope for a common\nrepresentation for transfer in a deep architecture. But you might ask, how would\nyou do the same sort of thing with temporal data, not\nimage data, maybe data that's from language, or data\nfrom time series of health", "id": "MdUnh4PaGKw_50", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  And for that you\nreally want to be thinking about recurrent\nneural networks. So just to remind you,\nrecurrent neural network is a recurrent\narchitecture where you take as input some vector. For example, if you're\ndoing language modeling, that vector might be encoding,\njust a one-hot encoding of what is the word at that location. So, for example, this\nvector might be all zeros, except for the fourth\ndimension, which is a 1, denoting that this word\nis the word, quote, \"class.\" And then it's fed into\na recurrent unit, which takes the previous\nhidden state, combined it with the current input, and\ngets you a new hidden state. And in this way, you read in--\nyou encode the full input. And then you might predict-- make a classification\nbased on the hidden state of the last time\n[? step. ?] That would be a common approach. And here would be a very simple\nexample of a recurrent unit. Here I'm using S to\ndenote in a state. Often you will see H used\nto denote the hidden state. This is a particularly\nsimple example, where there's just a\nsingle non-linearity. So you take your\nprevious hidden state, you hit it with some matrix Ws,s\nand you add that to the input", "id": "MdUnh4PaGKw_51", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  You now have a\ncombination of the input plus the previous hidden state. You apply non-linearity\nto that, and you get your new hidden state out. So that would be an example\nof a typical recurrent unit, a very simple recurrent unit. Now, the reason why I'm going\nthrough these details is to point out that the dimension\nof that Ws,x matrix is the dimension of the hidden\nstate, so the dimension of s, by the vocabulary size if\nyou're using a one-hot encoding of the input. So if you have a huge\nvocabulary, that matrix, Ws,x, is also going to\nbe equally large. And the challenge\nthat that presents is that it would lead to\noverfitting on rare words very quickly. And so that's a problem that\ncould be addressed by instead using a low-rank representation\nof that Ws,x matrix.", "id": "MdUnh4PaGKw_52", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  a lower dimensional bottleneck,\nwhich in this picture I'm denoting as xt prime,\nwhich is your original xt input, which is the\none-hot encoding, multiplied by a new matrix We. And then your recurrent\nunit only takes inputs of that hidden-- of that xt prime's\ndimension, which is k, which might be\ndramatically smaller than v. And you can even think\nabout each column of that intermediate\nrepresentation, We, as a word embedding. It's a way of-- and this is something that\nPete talked quite a bit about when we were thinking\nabout natural language-- when we were talking about\nnatural language processing. And many of you would\nhave heard about it in the context of\nthings like Word2Vec.", "id": "MdUnh4PaGKw_53", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  one institution's data where\nyou had a huge amount of data, learn every current neural\nnetwork on that institution's data, and then generalize\nit to a new institution, one way of trying to do\nthat, if you think about, what is the thing that you chop,\none answer might be, all you do is you keep the word embedding. So you might say,\nOK, I'm going to keep the We's, I'm going to translate\nit back to my new institution. But I'm going to let the\nrecurrent unit parameters-- the recurrent\nparameters, for example, that Ws,s you might allow it\nto be relearned for each new institution. And so that might\nbe one approach of how to use the\nsame idea that we had from feed forward networks\nwithin a recurrent setting. Now, all of this\nis very general. And what I want to do\nnext is to instantiate it a bit in the context\nof health care.", "id": "MdUnh4PaGKw_54", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  the extensions of Word2Vec\nsuch as BERT and ELMo, and I'm not going to-- I'm not going to\ngo into them now, but you can go back to Pete's\nlecture from a few weeks ago to remind yourselves what\nthose were, since the time he presented that lecture,\nthere are actually three new papers\nthat actually tried to apply this in the health\ncare context, one of which was from MIT. And so these papers all\nhave the same sort of idea. They're going to\ntake some data set-- and these papers all use MIMIC. They're going to\ntake that text data, they're going to learn\nsome word embeddings or some low-dimensional\nrepresentations of all words in the vocabulary. In this case,\nthey're not learning a static representation\nfor each word. Instead these BERT\nand ELMo approaches are going to be\nlearning-- well, you", "id": "MdUnh4PaGKw_55", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  They're going to be a\nfunction of the word and their context on the\nleft and right-hand sides. And then what they'll\ndo is they'll then take those representations\nand attempt to use them for a completely new task. Those new tasks might\nbe on MIMIC data. So, for example, these two tasks\nare classification problems on MIMIC. But they might also\nbe on non-MIMIC data. So these two tasks are from\nclassification problems on clinical text that didn't\neven come from MIMIC at all. So it's really an\nexample of translating what you learned\nfrom one institution to another institution. These two data sets\nwere super small. Actually, all of these data\nsets were really, really small compared to the\noriginal size of MIMIC. So there might be some hope that\none could learn something that really improves generalization. And indeed, that's\nwhat plays out. So all these tasks are looking\nat a concept detection task. Given a clinical note,\nidentify the segments of text within a note that\nrefer to, for example,", "id": "MdUnh4PaGKw_56", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  you then in a second stage\nmight normalize to the UMLS. So what's really striking\nabout these results is what happens when you go\nfrom the left to the right column, which I'll\nexplain in a second, and what happens when\nyou go top to bottom across each one of\nthese different tasks. So the left column\nare the results. And these results are\nan F score, the results, if you were to use embeddings\ntrained on a non-clinical data set, or said definitely, not\non MIMIC but on some other more general data set. The second column\nis what would happen if you trained those embedding\non a clinical data set, in this case, MIMIC. And you see pretty\nbig improvements from the general embeddings\nto the MIMIC-based embeddings. What's even more striking\nis the improvements", "id": "MdUnh4PaGKw_57", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  So the first row are\nthe results if you were to use just\nWord2Vec embeddings. And so, for example, for\nthe I2B2 Challenge in 2010, you get 82.65 F score\nusing Word2Vec embeddings. And if you use a very\nlarge BERT embedding, you get 90.25 F score-- F measure, which is\nsubstantially higher. And the same findings were\nfound time and time again across different tasks. Now, what I find really\nstriking about these results is that I had tried many of\nthese things a couple of years ago, not using BERT or\nELMo, but using Word2Vec, and GloVe, and fastText. And what I found is that using\nword embedding approaches for these problems didn't-- even if you threw that in as\nadditional features on top of other state-of-the-art\napproaches to this concept", "id": "MdUnh4PaGKw_58", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  above the existing\nstate of the art. However, in this\npaper, here they use the simplest\npossible algorithm. They used a recurrent\nneural network fed into a conditional\nrandom field for the purpose of classifying\neach word into each of these categories. And the feature\nrepresent-- the features that they used are just\nthese embedding features. So with just the Word2Vec\nembedding features, the performance is crap. You don't get anywhere\nclose to the state of art. But with the better embeddings,\nthey actually obtain-- actually, they improved\non the state of the art for every single\none of these tasks. And that is without any\nof the manual feature engineering which\nwe have been using in the field for\nthe last decade. So I find this to be\nextremely promising. Now you might ask, well, that\nis for one problem, which is classification of concepts--\nor identification of concepts.", "id": "MdUnh4PaGKw_59", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  So a different paper\nalso published-- what month is it now, May--\nso last month in April, looked at a predicted problem\nof 30-day readmission prediction using discharge summaries. This also was valued on MIMIC. And their evaluation\nlooked at the area under the ROC curve of\ntwo different approaches. The first approach, which is\nusing a bag-of-words model, like what you did in\nyour homework assignment, and the second approach,\nwhich is the top row there, which is using BERT embeddings,\nwhich they call Clinical BERT. And this, again, is\nsomething which I had tackled for quite a long time. So I worked on these types\nof readmission problems. And bag-of-words model\nis really hard to beat. In fact, did any of you beat\nit in your homework assignment? If you remember, there\nwas an extra question, which is, oh, well,\nmaybe if we used a deep learning-based\napproach for this problem, maybe you could get\nbetter performance.", "id": "MdUnh4PaGKw_60", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  No. How many of you\nactually tried it? Raise your hand. OK, so one-- a couple of\npeople who are afraid to say, but yeah. So a couple of people\nwho tried, but not many. But I think the reason why it's\nvery challenging to do better with, let's say, a recurrent\nneural network versus a bag-of-words model\nis because there is-- a lot of the subtlety in\nunderstanding the text is in terms of understanding\nthe context of the text. And that's something that\nusing these newer embeddings is actually really good at\nbecause they can get-- they could use the\ncontext of words to better represent what\neach word actually means. And they see\nsubstantial improvement in performance\nusing this approach. What about for non-text data? So you might ask when we\nhave health insurance claims, we have longitudinal\ndata across time. There's no language in this. It's a time series data set. You have ICD-9 codes\nat each point in time,", "id": "MdUnh4PaGKw_61", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  And this is very similar\nto the market scan data that you used in your\nhomework assignment. Could one learn embeddings\nfor this type of data, which is also useful for transfer? So one goal might be to say, OK,\nlet's take every ICD-9, ICD-10 code, every medication,\nevery laboratory test result, and embed those event types into\nsome lower dimensional space. And so here's an\nexample of an embedding. And you see how-- this is\njust a sketch, by the way-- you see how you might\nhope that diagnosis codes for autoimmune\nconditions might be all near each other\nin some lower dimensional space, diagnosis\ncodes for medications that treat some conditions\nshould be near each other, and so on. So you might hope that such\nstructure might be discovered by an unsupervised learning\nalgorithm that could then be used within a transfer\nlearning approach. And indeed, that's\nwhat we found. So I wrote a paper\non this in 2015/16.", "id": "MdUnh4PaGKw_62", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  So this is just a look\nat nearest neighbors to give you some sense of\nwhether the embedding's actually capturing the\nstructure of the data. So we looked at\nnearest neighbors of the diagnosis ICD-9 diagnosis\ncode 710.0, which is lupus. And what you find is that\nanother diagnosis code, also for lupus, is the first\nclosest result, followed by connective tissue\ndisorder, or Sicca syndrome, which is Sjogren's disease,\nRaynaud's syndrome, and other autoimmune conditions. So that makes a lot of sense. You can also go\nacross data types, like ask, what is the nearest\nneighbor from this diagnosis code to laboratory tests? And since we've embedded\nlab tests and diagnosis codes all in the same\nspace, you can actually get an answer to that. And what you see is that these\nlab tests, which by the way are exactly lab tests\nthat are commonly used to understand progression\nin this autoimmune condition,", "id": "MdUnh4PaGKw_63", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  Similarly, you can ask the same\nquestion about drugs and so on. And by the way, we have made\nall of these embeddings publicly available on my lab's GitHub. And since the time that\nI wrote this paper, there have been a\nnumber of other papers, that I give citations\nto at the bottom here, tackling a very similar problem. This last one also put there\nembeddings publicly available, and is much larger than the one\nthat we had So these things, I think, would also be\nvery useful as one starts to think about how one can\ntransfer knowledge learned on one institution to\nanother institution where you might\nhave much less data than that other institution. So finally I want to\nreturn back to the question that I raised in\nbullet two here, where we looked\nat a linear model with a manually\nchosen representation, and ask, could we-- instead of just naively chopping\nyour deep neural network", "id": "MdUnh4PaGKw_64", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  could one have learned a\nrepresentation of your data specifically for the purpose of\nencouraging good generalization to a new institution? And there has been some\nreally exciting work in this field that goes by the\nname of Unsupervised Domain Adaptation. So the setting that's\nconsidered here is where you have\ndata from-- you have data from first some\ninstitution, which is x comma y. But then you want\nto do prediction from a new institution\nwhere all you have access to at training time is x. So as opposed to the\ntransfer settings that I talked about earlier,\nnow for this new institution,", "id": "MdUnh4PaGKw_65", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  Whereas before I was\ntalking about having just a small amount\nof label data, but I never talked\nof the possibility of having a large amount\nof unlabeled data. And so you might\nask, how could you use that large amount\nof unlabeled data from that second\ninstitution in order to learn representation\nthat actually encourages similarities from one\nsolution to the other? And that's exactly what these\ndomain adversarial training approaches will do. What they do is they\nadd a second term to the last function. So they're going to minimize-- the intuition is you're\ngoing to minimize-- you're going to try\nto learn parameters that minimize your loss function\nevaluated on data set 1. But intuitively, you're\ngoing to ask that there also be a small distance, which\nI'll just note as d here, between D1 and D2.", "id": "MdUnh4PaGKw_66", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  but when I calculate\ndistance here, I'm referring to distance\nin representation space. So you might imagine\ntaking the middle layer of your deep neural\nnetwork, so taking, let's say, this layer, which\nwe're going to call the feature layer, or the representation\nlayer, and you're going to say, I want that my data under\nthe first institution should look very\nsimilar to the data under the second institution. So the first few layers of\nyour deep neural network are going to attempt to\nequalize the two data sets so that they look similar to\nanother, at least in x space. And we're going to attempt\nto find representations of your model that get\ngood predictive performance on the data set for which\nyou actually have the labels and for which the induced\nrepresentations, let's say, the middle layer look very\nsimilar across the two data sets. And one way to do that is just\nto try to predict for each-- you now get a-- for each data point,\nyou might actually say, well, which data\nset did it come from, data set 1 or data set 2? And what you want is that\nyour model should not", "id": "MdUnh4PaGKw_67", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  So that's what it says,\ngradient reverse layer you want to be\nable to-- you want to ensure that predicting which\ndata set that data came from, you want to perform badly\non that loss functions. It's like taking the\nminus of that loss. And so we're not going to\ngo into the details of that, but I just wanted to\ngive you a reference to that approach in the bottom. And what I want to\ndo is just spend one minute at the very end\ntalking now about defenses to adversarial attacks. And conceptually\nthis is very simple. And that's why I can\nactually do it in one minute. So we talked about how one could\neasily modify an image in order to turn the prediction from,\nlet's say, pig to airliner. But how could we change your\nlearning algorithm actually to make sure that,\ndespite the fact that you do this perturbation, you still\nget the right prediction out, pig? Well, to think through that,\nwe have to think through, how do we do machine learning? Well, a typical approach\nto machine learning is to learn some\nparameters theta minimized your empirical loss. Often we use deep\nneural networks,", "id": "MdUnh4PaGKw_68", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  And we do gradient\ndescent where we attempt to minimize some loss surfaced,\nfind some parameters theta have as low loss as possible. Now, when you think about\nan adversarial example and where they come\nfrom, typically one finds an adversarial example\nin the following way. You take your same\nloss function, now for specific\ninput x, and you try to find some\nperturbation delta to x an additive perturbation,\nfor example, such that you increase the loss as\nmuch as possible with respect to the correct label y. And so if you've increased\nthe loss with respect to the correct\nlabel y, intuitively then when you try to see,\nwell, what should you predict for this\nnew perturbed input, there's going to be a lower\nloss for some alternative label, which is why the prediction--\nthe class that's predicted actually changes. So now one can try to find\nthese adversarial examples using the same type of gradient-based\nlearning algorithms that one uses for learning\nin the first place.", "id": "MdUnh4PaGKw_69", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  method now-- instead of gradient\ndescent, gradient ascent. So you take this optimization\nproblem for a given input x, and you try to maximize\nthat loss for that input x with this vector\ndelta, and you're now doing gradient ascent. And so what types of\ndelta should you consider? You can imagine\nsmall perturbations, for example, delta that have\nvery small maximum values. That would be an example\nof an L-infinity norm. Or you could say that the\nsum of the perturbations across, let's say, all of the\ndimensions has to be small. That would be corresponding to\nlike an L1 or an L2 norm bound on what delta should be. So now we've got\neverything we need actually to think about\ndefenses to this type of adversarial perturbation. So instead of minimizing your\ntypical empirical loss, what we're going to do is\nwe're going to attempt to minimize an adversarial\nrobust loss function. What we'll do is\nwe'll say, OK, we want to be sure that no matter\nwhat the perturbation is", "id": "MdUnh4PaGKw_70", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  still has low loss. So you want to find\nparameters theta which minimize this new quantity. So I'm saying that\nwe should still do well even for the worst-case\nadversarial perturbation. And so now this would be\nthe following new learning objective, where we're\ngoing to minimize over theta with respect to\nthe maximum of our delta. And you have to restrict the\nfamily that these perturbations could live in, so if\nthat delta that you're maximizing with respect\nto is the empty set, you get back the original\nlearning problem. If you let it be, let's\nsay, all L-infinity bounded perturbations\nof maximum size of 0.01, then you're saying we're\ngoing to allow for a very small amount of perturbations. And the learning\nalgorithm is going to find parameters theta such\nthat for every input, even with a small perturbation\nto it, adversarially chosen, you still get good\npredictive performance. And this is now a new\noptimization problem", "id": "MdUnh4PaGKw_71", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  And we've now\nreduced the problem of finding an adversarial robust\nmodel to a new optimization problem. And what the field\nhas been doing in the last couple\nof years is coming up with new optimization\napproaches to try to solve those problems fast. So, for example, this paper\npublished an ICML in 2018 by Zico Kolter and his student-- Zico just visited\nMIT a few weeks ago-- what it did is it\nsaid, we're going to use a convex relaxation\nto the rectified linear unit, which is used in many deep\nneural network architectures. And what it's going to do\nit's then going to say, OK, we're going\nto think about how a small perturbation\nto the input would be propagated in terms\nof getting how much that could actually change the output. And if one could be\nbound at every layer by layer how much a small\nperturbation affects the output of that\nlayer, then one could propagate\nfrom the very bottom all the way to the loss\nfunction of the top", "id": "MdUnh4PaGKw_72", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  And a picture of what you\nwould expect out is as follows. On the left-hand side here,\nyou have a data point, red and blue, and\nthe decision boundary that's learned if you didn't do\nthis robust learning algorithm. On the right, you have now-- you'll notice a small square\naround each data point. That corresponds to a\nmaximum perturbation of some limited amount. And now you notice how the\ndecision boundary doesn't cross any one of those squares. And that's what would be found\nby this learning algorithm. Interestingly, one can\nlook at the filters that are learned by convolutional\nneural network using this new learning algorithm. And you find that\nthey're much more sparse. And so this is a very\nfast moving field. Every time a new\nadversarial attack-- every time a new adversarial\ndefense mechanism comes up, someone comes up\nwith a different type of attack, which breaks it. And usually that's from\none of two reasons.", "id": "MdUnh4PaGKw_73", "title": "24. Robustness to Dataset Shift", "MIT OpenCourseWare": "MIT OpenCourseWare", "2021-07-09T20:24:33Z": "2021-07-09T20:24:33Z"}, {"text": "  and so one could try to come\nup with a theorem which says, OK, as long as you\ndon't perturbate more than some amount, these are\nthe results you should expect. The other flip of the coin\nis, even if you come up with some provable\nguarantee, there might be other types of attacks. So, for example, you\nmight imagine a rotation to the input instead of\nan L-infinity bounded norm that you add to it. And so for every new\ntype of attack model, you have to think through\nnew defense mechanisms. And so you should expect to see\nsome iteration in the space. And there's a website\ncalled robust-ml.org, where many of these attacks and\ndefenses are being published to allow for the academic\ncommunity to make progress here.", "id": "MdUnh4PaGKw_74"}, {"text": "  PROFESSOR: OK, so the\nlast topic for the class is interpretability. As you know, the modern\nmachine learning models are justifiably reputed to be\nvery difficult to understand. So if I give you something\nlike the GPT2 model, which we talked about in natural\nlanguage processing, and I tell you that it\nhas 1.5 billion parameters and then you say,\nwhy is it working? Clearly the answer\nis not because these particular parameters\nhave these particular values. There is no way to\nunderstand that. And so the topic\ntoday is something", "id": "wDLzLN1tArA_0", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  on fairness, where\none of the issues there was also that if you\ncan't understand the model you can't tell if the model\nhas baked-in prejudices by examining it. And so today we're going to\nlook at different methods that people have\ndeveloped to try to overcome this problem\nof inscrutable models. So there is a very\ninteresting bit of history. How many of you know\nof George Miller's 7 plus or minus 2 result? Only a few. So Miller was a psychologist at\nHarvard, I think, in the 1950s. And he wrote this paper in 1956\ncalled \"The Magical Number 7 Plus or Minus 2-- Some Limits On Our Capacity\nfor Processing Information.\" It's quite an interesting paper.", "id": "wDLzLN1tArA_1", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  I read this paper\nmany, many years ago. And I'd forgotten that he\nstarts off with the question of how many different\nthings can you sense? How many different levels\nof things can you sense? So if I put headphones\non you and I ask you to tell\nme on a scale of 1 to n how loud is the sound that\nI'm playing in your headphone, it turns out people get confused\nwhen you get beyond about five, six, seven different\nlevels of intensity. And similarly, if I give\nyou a bunch of colors and I ask you to tell me\nwhere the boundaries are between different\ncolors, people seem to come up with 7 plus or\nminus 2 as the number of colors that they can distinguish. And so there is a long\npsychological literature", "id": "wDLzLN1tArA_2", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And then Miller went\non to do experiments where he asked people to\nmemorize lists of things. And what he\ndiscovered is, again, that you could memorize\na list of about 7 plus or minus 2 things. And beyond that, you couldn't\nremember the list anymore. So this tells us something\nabout the cognitive capacity of the human mind. And it suggests that if I\ngive you an explanation that has 20 things in it,\nyou're unlikely to be able to fathom it because\nyou can't keep all the moving parts in your mind at one time. Now, it's a tricky result,\nbecause he does point out even in 1956 that if you chunk\nthings into bigger chunks, you can remember seven of those,\neven if they're much bigger. And so people who are very\ngood at memorizing things,", "id": "wDLzLN1tArA_3", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And they remember\nthose patterns, which then allow them\nto actually remember more primitive objects. So you know-- and we\nstill don't really understand how memory works. But this is just an\ninteresting observation, and I think plays\ninto the question of how do you explain things\nin a complicated model? Because it suggests\nthat you can't explain too many different\nthings because people won't understand what\nyou're talking about. OK. So what leads to complex models? Well, as I say,\noverfitting certainly leads to complex models. I remember in the\n1970s when we started working on expert\nsystems in healthcare, I made a very bad faux pas.", "id": "wDLzLN1tArA_4", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  between statisticians and\nartificial intelligence researchers. And the statisticians were\nall about understanding the variance and understanding\nstatistical significance and so on. And I was all about trying to\nmodel details of what was going on in an individual patient. And in some discussion after my\ntalk, somebody challenged me. And I said, well, what\nwe AI people are really doing is fitting\nwhat you guys think is the noise,\nbecause we're trying to make a lot more detailed\nrefinements in our theories and our models than what the\ntypical statistical model does. And of course, I was roundly\nbooed out of the hall. And people shunned me for\nthe rest of the conference because I had done\nsomething really stupid to admit that I\nwas fitting noise.", "id": "wDLzLN1tArA_5", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  that I was fitting noise. I believed that\nwhat I was fitting was what the average\nstatistician just chalks up to noise. And we're interested in more\ndetails of the mechanisms. So overfitting we\nhave a pretty good handle on by regularization. So you can-- you\nknow, you've seen lots of examples\nof regularization throughout the course. And people keep coming up\nwith interesting ideas for how to apply regularization in order\nto simplify models or make them fit some preconception\nof what the model ought to look like before you\nstart learning it from data. But the problem is\nthat there really is true complexity\nto these models, whether or not\nyou're fitting noise. There's-- the world is\na complicated place. Human beings were not designed.", "id": "wDLzLN1tArA_6", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And so there's all kinds\nof bizarre stuff left over from our evolutionary heritage. And so it is just complex. It's hard to understand\nin a simple way how to make predictions that\nare useful when the world really is complex. So what do we do in order\nto try to deal with this? Well, one approach\nis to make up what I call just-so stories that give\na simplified explanation of how a complicated thing\nactually works. So how many of you\nhave read these stories when you were a kid? Nobody? My God. OK. Must be a generational thing. So Rudyard Kipling\nwas a famous author. And he wrote the series\nof just-so stories, things like How the Lion Got His\nMane and How the Camel Got His Hump and so on. And of course, they're\nall total bull, right?", "id": "wDLzLN1tArA_7", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  of why male lions have manes. It's just some made up story. But they're really cute stories. And I enjoyed them as a kid. And maybe you would have,\ntoo, if your parents had read them to you. So I mean, I use this\nas a kind of pejorative because what the\npeople who follow this line of investigation\ndo is they take some very complicated model. They make a local\napproximation to it that says, this is not an approximation\nto the entire model, but it's an approximation\nto the model in the vicinity of a particular case. And then they explain\nthat simplified model. And I'll show you\nsome examples of that through the lecture today.", "id": "wDLzLN1tArA_8", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  show you some examples\nof is that you simply trade off somewhat lower\nperformance for a simple-- a model that's simple enough\nto be able to explain. So things like decision\ntrees and logistic regression and so on typically\ndon't perform quite as well as the best, most\nsophisticated models, although you've seen plenty\nof examples in this class where, in fact, they\ndo perform quite well and where they're\nnot outperformed by the fancy models. But in general, you\ncan do a little better by tweaking a fancy model. But then it becomes\nincomprehensible. And so people are\nwilling to say, OK, I'm going to give up\n1% or 2% in performance in order to have a model\nthat I can really understand. And the reason it makes sense\nis because these models are not self-executing.", "id": "wDLzLN1tArA_9", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  who makes ultimate decisions. Your surgeon is\nnot going to look at one of these\nmodels that says, take out the guy's left\nkidney and say, OK, I guess. They're going to go, well,\ndoes that make sense? And in order to answer\nthe question of, does that make sense? It really helps to know\nwhat the model is-- what the model's\nrecommendation is based on. What is its internal logic? And so even an approximation\nto that is useful. So the need for trust, clinical\nadoption of ML models-- there are two\napproaches in this paper that I'm going to talk\nabout where they say, OK, what you'd like to do is to look\nat case-specific predictions. So there is a particular\npatient in a particular state and you want to understand\nwhat the model is", "id": "wDLzLN1tArA_10", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And then you also want to\nhave confidence in the model overall. And so you'd like to be able to\nhave an explanatory capability that says, here are some\ninteresting representative cases. And here's how the\nmodel views them. Look through them and\ndecide whether you agree with the approach\nthat this model is taking. Now, remember my critique of\nrandomized controlled trials that people do these trials. They choose the simplest cases,\nthe smallest number of patients that they need in order to\nreach statistical significance, the shortest amount of\nfollow-up time, et cetera. And then the results\nof those trials are applied to very\ndifferent populations. So Davids talked\nabout the cohort shift as a generalization\nof that idea. But the same thing happens in\nthese machine learning models that you train on\nsome set of data.", "id": "wDLzLN1tArA_11", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  test on some held-out\nsubset of the same data. But that's not a very\naccurate representation of the real world. If you then try to apply that\nmodel to data from a totally different source,\nthe chances are you will have specialized\nit in some way that you don't appreciate. And the results\nthat you get are not as good as what you got\non the held-out test data because it's more heterogeneous. I think I mentioned\nthat Jeff Drazen, the editor-in-chief of\nthe New England Journal, had a meeting about a year ago\nin which he was arguing that the journal shouldn't ever\npublish a research study unless it's been validated on\ntwo independent data sets because he's tired of publishing\nstudies that wind up getting retracted because--", "id": "wDLzLN1tArA_12", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  of the investigators. They've done exactly\nthe kinds of things that you've learned how\nto do in this class. But when they go\nto apply that model to a different\npopulation, it just doesn't work nearly\nas well as it did in the published version. And of course, there\nare all the publication bias issues about if 50 of\nus do the same experiment and by random chance some\nof us are going to get better results than others. And those are the\nones that are going to get published\nbecause the people who got poor results don't have\nanything interesting to report. And so there's that whole\nissue of publication bias, which is another serious one. OK. So I wanted to just spend\na minute to say, you know, explanation is not a new idea.", "id": "wDLzLN1tArA_13", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  talked about a little bit in\none of our earlier classes, we talked about the idea\nthat we would take medical-- human medical experts\nand debrief them of what they knew and then try to encode\nthose in patterns or in rules or in various ways in a\ncomputer program in order to reproduce their behavior. So Mycin was one\nof those programs-- [INAUDIBLE] PhD\nthesis-- in 1975. And they published\nthis nice paper that was about explanation and\nrule acquisition capabilities of the Mycin system. And as an illustration,\nthey gave some examples of what you could\ndo with the system. So rules, they argued,\nwere quite understandable because they say if a bunch\nof conditions, then you can draw the\nfollowing conclusion.", "id": "wDLzLN1tArA_14", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  when the program\ncomes back and says, in light of the site from\nwhich the culture was obtained and the method of\ncollection, do you feel that a significant number\nof organism 1 were detected-- were obtained? In other words, if you took\na sample from somebody's body and you're looking\nfor an infection, do you think you got enough\norganisms in that sample? And the user says, well, why\nare you asking me this question? And the answer in terms of the\nrules that the system works by is pretty good. It says it's\nimportant to find out whether there's therapeutically\nsignificant disease associated with this occurrence\nof organism 1. We've already established\nthat the culture is not one of those that\nare normally sterile and the method of\ncollection is sterile. Therefore, if the\norganism has been observed in significant\nnumbers, then there's", "id": "wDLzLN1tArA_15", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  significant disease associated\nwith this occurrence of the organism. So if you find bugs in a\nplace carefully collected, then that suggests\nthat you ought to probably treat this patient\nif there are were bunch of-- enough bugs there. And there's also strongly\nsuggestive evidence that the organism is\nnot a contaminant, because the collection\nmethod was sterile. And you can go on with this and\nyou can say, well, why that? So why that question? And it traces back in its\nevolution of these rules and it says, well,\nin order to find out the locus of\ninfection, it's already been established that the\nsite of the culture is known. The number of days since\nthe specimen was obtained is less than 7. Therefore, there\nis therapeutically", "id": "wDLzLN1tArA_16", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  of the organism. So there's some rule that\nsays if you've got bugs and it happened within\nthe last seven days, the patient probably really\ndoes have an infection. And I mean, I've got a\nlot of examples of this. But you can keep going why. You know, this is\nthe two-year-old. But why, daddy? But why? But why? Well, why is it important to\nfind out a locus of infection? And, well, there's\na reason, which is that there is a rule\nthat will conclude, for example, that the abdomen\nis a locus of infection or the pelvis is a locus\nof infection of the patient if you satisfy these criteria. And so this is a kind of\nrudimentary explanation that comes directly\nout of the fact", "id": "wDLzLN1tArA_17", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  and so you can just\nplay back the rules. One of the things I\nlike is you can also ask freeform questions. 1975, the natural language\nprocessing was not so good. And so this worked\nabout one time in five. But you could walk up to\nit and type some question. And for example, do you\never prescribe carbenicillin for pseudomonas infections? And it says, well,\nthere are three rules in my database of rules that\nwould conclude something relevant to that question. So which one do you want to see? And if you say, I\nwant to see rule 64, it says, well, that\nrule says if it's known with certainty that\nthe organism is a pseudomonas and the drug under\nconsideration is gentamicin, then a more appropriate\ntherapy would be a combination of\ngentamicin and carbenicillin. Again, this is medical\nknowledge as of 1975.", "id": "wDLzLN1tArA_18", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  is that there probably\nwere pseudomonas that were resistant by\nthat point, to gentamicin, and so they used a\ncombination therapy. Now, notice, by the way, that\nthis explanation capability does not tell you that, right? Because it doesn't actually\nunderstand the rationale behind these individual rules. And at the time there was\nalso research, for example, by one of my students on how\nto do a better job of that by encoding not only the\nrules or the patterns, but also the rationale behind\nthem so that the explanations could be more sensible. OK. Well, the granddaddy of the\nstandard just-so story approach to explanation of complex models\ntoday comes from this paper", "id": "wDLzLN1tArA_19", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Locally Interpretable\nModel-agnostic Explanations. And just to give\nyou an illustration, you have some complicated\nmodel and it's trying to explain why the\ndoctor or the human being made a certain decision,\nor why the model made a certain decision. And so it says, well,\nhere are the data we have about the patient. We know that the\npatient is sneezing. And we know their weight\nand their headache and their age and the fact\nthat they have no fatigue. And so the explainer\nsays, well, why did the model decide\nthis patient has the flu? Well, positives are\nsneeze and headache. And a negative is no fatigue. So it goes into this\ncomplicated model and it says, well, I can't\nexplain all the numerology that happens in that neural\nnetwork or Bayesian network", "id": "wDLzLN1tArA_20", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  But I can specify that\nit looks like these are the most important positive\nand negative contributors. Yeah? AUDIENCE: Is this\nfor notes only, or it's for all types of data? PROFESSOR: I'll show you some\nother kind of data in a minute. I think they originally\nworked it out for notes, but it was also used for\nimages and other kinds of data, as well. OK. And the argument they make\nis that this approach also helps to detect data\nleakage, for example in one of their experiments,\nthe headers of the data had information in them that\nthat correlated highly with the result. I think there-- I can't\nremember if it was these guys, but somebody was assigning\nstudy IDs to each case.", "id": "wDLzLN1tArA_21", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  corresponded to people who had\nthe disease and the big numbers corresponded to the\npeople who didn't. And of course, the most\nparsimonious predictive model just used the ID number\nand said, OK, I got it. So this would help\nyou identify that, because if you see that the\nbest predictor is the ID number, then you would say, hmm,\nthere's something a little fishy going on here. Well-- so here's an example\nwhere this kind of capability is very useful. So this was another-- this was from a newsgroup. And they were trying to\ndecide whether a post was about Christianity or atheism. Now, look at these two models. So there's algorithm\n1 and algorithm 2 or model 1 and model 2. And when you explain\na particular case", "id": "wDLzLN1tArA_22", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  that I consider important\nare God, mean, anyone, this, Koresh, and through-- does anybody remember\nwho David Koresh was? He was some cult leader who-- I can't remember if he killed\na bunch of people or bad things happened. Oh, I think he was\nthe guy in Waco, Texas that the FBI and the ATF went\nin and set their place on fire and a whole bunch\nof people died. So the prediction in\nthis case is atheism. And you notice that God and\nKoresh and Mean are negatives. And anyone this and\nthrough are positives. And you go, I don't\nknow, is that good? But then you look at\nalgorithm 2 and you say,", "id": "wDLzLN1tArA_23", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  which is that this particular\narticle is about atheism. But the positives were\nthe word by and in, not terribly specific. And the negatives\nwere things like NNTP. You know what that is? That's the Network\nTime Protocol. It's some technical thing,\nand posting and host. So this is probably\nlike metadata that got into the header of\nthe articles or something. So it happened\nthat in this case, algorithm 2 turned out to be\nmore accurate than algorithm 1 on their held out test data,\nbut not for any good reason. And so the\nexplanation capability allows you to clue\nin on the fact that even though this thing\nis getting the right answers, it's not for sensible reasons.", "id": "wDLzLN1tArA_24", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So what would you like\nfrom an explanation? Well, they say you'd like\nit to be interpretable. So it should provide\nqualitative understanding of the relationship\nbetween the input variables and the response. But they also say\nthat that's going to depend on the audience. It requires sparsity for\nthe George Miller argument that I was making before. You can't keep too\nmany things in mind. And the features themselves\nthat you're explaining must make sense. So for example, if I say,\nwell, the reason this decided that is\nbecause the eigenvector for the first\nprinciple component was the following,\nthat's not going to mean much to most people. And then they also say, well,\nit ought to have local fidelity. So it must correspond\nto how the model behaves in the vicinity of the\nparticular instance", "id": "wDLzLN1tArA_25", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And their third criterion, which\nI think is a little iffier, is that it must\nbe model-agnostic. In other words, you can't\ntake advantage of anything you know that is specific\nabout the structure of the model, the way you\ntrained it, anything like that. It has to be a general\npurpose explainer that works on any kind of\ncomplicated model. Yeah? AUDIENCE: What is the\nreasoning for that? PROFESSOR: I think their\nreasoning for why they insist on this is because\nthey don't want to have to write a\nseparate explainer for each possible model. So it's much more efficient\nif you can get this done. But I actually question whether\nthis is always a good idea or not. But nevertheless, this is\none of their assumptions. OK. So here's the setup\nthat they use. They say, all\nright, x is a vector", "id": "wDLzLN1tArA_26", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And what we're\ngoing to do in order to make the data explainable,\nin order to make the data, not the model,\nexplainable, is we're going to define a\nnew set of variables, x prime, that are\nall binary and that are in some space of\ndimension D prime that is probably lower than D. So we're simplifying the\ndata that we're going to explain about this model. Then they say, OK, we're\ngoing to build an explanation model, g, where g is a class\nof interpretable models. So what's an\ninterpretable model? Well, they don't\ntell you, but they say, well, examples might be\nlinear models, additive scores, decision trees,\nfalling rule lists, which we'll see\nlater in the lecture.", "id": "wDLzLN1tArA_27", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  the simplified input data, the\nbinary variables in D prime dimensions, and the model\ncomplexity is going to be some measure of the depth\nof the decision tree, the number of non-zero weights,\nand the logistic regression-- the number of clauses in a\nfalling rule list, et cetera. So it's some complexity measure. And you want to\nminimize complexity. So then they say, all\nright, the real model, the hairy, complicated\nfull-bore model is f. And that maps the original data\nspace into some probability. And for example,\nfor classification, f is the probability that x\nbelongs to a certain class. And then they also need\na proximity measure. So they need to\nsay, we have to have a way of comparing two cases\nand saying how close are they", "id": "wDLzLN1tArA_28", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And the reason for that\nis because, remember, they're going to give\nyou an explanation of a particular case and the\nmost relevant things that will help with that\nexplanation are the ones that are near it in\nthis high dimensional input space. So they then define\ntheir loss function based on the actual\ndecision algorithm, based on the simplified one, and\nbased on the proximity measure. And they say, well,\nthe best explanation is that g which minimizes\nthis loss function plus the complexity of g. Pretty straightforward. So that's our best model. Now, the clever\nidea here is to say,", "id": "wDLzLN1tArA_29", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  what we're going to do\nis to sample the data so that we take more sample\npoints near the point we're interested in explaining. We're going to sample in\nthe simplified space that is explainable and\nthen we'll build that g model, the explanatory\nmodel, from that sample of data where we weight by\nthat proximity function so the things that are closer\nwill have a larger influence on the model that we learn. And then we recapture the-- sort of the closest point to\nthis simplified representation. We can calculate what\nits answer should be. And that becomes the\nlabel for that point. And so now we train\na simple model", "id": "wDLzLN1tArA_30", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  have predicted for the\npoint that we've sampled. Yeah? AUDIENCE: So the proximity\nmeasure is [INAUDIBLE]?? PROFESSOR: It's a distance\nfunction of some sort. And I'll say more\nabout it in a minute, because that's one\nof the critiques of this particular method\nhas to do with how do you choose that distance function? But it's basically a similarity. So here's a nice, graphical\nexplanation of what's going on. Suppose that the actual model-- the decision boundary is between\nthe blue and the pink regions. OK. So it's this god awful, hairy,\ncomplicated decision model. And we're trying to explain\nwhy this big, red plus wound up in the pink rather\nthan in the blue.", "id": "wDLzLN1tArA_31", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  to say, well, let's\nsample a bunch of points weighted by shortest distance. So we do sample a\nfew points out here. But mostly we're sampling\npoints near the point that we're interested in. We then learn a linear\nboundary between the positive and the negative cases. And that boundary\nis an approximation to the actual boundary in\nthe more complicated decision model. So now we can give\nan explanation just like you saw\nbefore which says, well, this is some D prime\ndimensional space. And so which variables in\nthat D prime dimensional space are the ones that\ninfluence where you are on one side or another\nof this newly computed decision", "id": "wDLzLN1tArA_32", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And that becomes\nthe explanation. OK? Nice idea. So if you apply this to\ntext classification-- yes? AUDIENCE: I was just\ngoing to ask if the-- there's a worry that if\nexplanation is just fictitious, like, we can understand it? But is there reason to believe\nthat we should believe it if that's really the\ntrue nature of things that the linear does-- you\nknow, it would be like, OK, we know what's\ngoing on here. But is that even\nclose to reality? PROFESSOR: Well,\nthat's why I called it a just-so story, right? Should you believe it? Well, the engineering\ndisciplines have a very long\nhistory of approximating extremely complicated\nphenomena with linear models. Right? I mean, I'm in a department\nof electrical engineering", "id": "wDLzLN1tArA_33", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And if I talk to my electrical\nengineering colleagues, they know that the world\nis insanely complicated. Nevertheless, most models\nin electrical engineering are linear models. And they work well\nenough that people are able to build really\ncomplicated things and have them work. So that's not a proof. That's an argument by\nhistory or something. But it's true. Linear models are very\npowerful, especially when you limit them to giving\nexplanations that are local. Notice that this model is\na very poor approximation to this decision boundary\nor this one, right? And so it only works to\nexplain in the neighborhood of the particular\nexample that I've chosen. Right? But it does work OK there. Yeah. AUDIENCE: [INAUDIBLE]\nvery well there?", "id": "wDLzLN1tArA_34", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  PROFESSOR: Well, they did. So they sample all\nover the place. But remember that that\nproximity function says that this one is less\nrelevant to predicting that decision boundary because\nit's far away from the point that I'm interested in. So that's the magic. AUDIENCE: But here\nthey're trying to explain to the\ndeep red cross, right? PROFESSOR: Yes. AUDIENCE: And they\npicked some point in the middle of\nthe red space maybe. Then all the nearby ones\nwould be red and [INAUDIBLE].. PROFESSOR: Well,\nbut they would-- I mean, suppose they\npicked this point, instead. Then they would sample\naround this point and presumably they would\nfind this decision boundary or this one or\nsomething like that and still be able to come up\nwith a coherent explanation.", "id": "wDLzLN1tArA_35", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  seen this example already. It's pretty simple. For their proximity function,\nthey use cosine distance. So it's a bag of words\nmodel and they just calculate cosine distance\nbetween different examples by how much overlap there is\nbetween the words that they use and the frequency of\nwords that they use. And then they choose k-- the number of words to\nshow just as a preference. So it's sort of\na hyperparameter. They say, you know, I'm\ninterested in looking at the top five words\nor the top 10 words that are either positively or\nnegatively an influence on the decision, but\nnot the top 10,000 words because I don't know\nwhat to do with 10,000 words.", "id": "wDLzLN1tArA_36", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  then apply the same idea\nto image interpretation. So here is a dog\nplaying a guitar. And they say, how do\nwe interpret this? And so this is one of\nthese labeling tasks where you'd like to label this\npicture as a Labrador or maybe as an acoustic guitar. But some reason--\nsome labels also decide that it's\nan electric guitar. And so they say, well,\nwhat counts in favor of or against each of these? And the approach they take is a\nrelatively straightforward one. They say let's\ndefine a super pixel as a region of pixels\nwithin an image that have roughly the same intensity. So if you've ever\nused Photoshop, the magic selection tool\ncan be adjusted to say,", "id": "wDLzLN1tArA_37", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  within some delta of the\npoint that I've picked. And so it'll outline some\nregion of the picture. And what they do is they\nbreak up the entire image into these regions. And then they treat those\nas if they were the words in the words style explanation. So they say, well, this\nlooks like an electric guitar to the algorithm. And this looks like\nan acoustic guitar. And this looks like a Labrador. So some of that makes sense. I mean, you know,\nthat dog's face does kind of look like a Lab. This does look kind of like\npart of the body and part of the fret work of a guitar. I have no idea\nwhat this stuff is or why this contributes\nto it being a dog. But such is-- such is the\nnature of these models.", "id": "wDLzLN1tArA_38", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  believes these various things. So then the last\nthing they do is to say, well, OK, that\nhelps you understand the particular model. But how do you\nconvince yourself-- I mean, a particular example\nwhere a model is applied to it. But how do you convince\nyourself that the model itself is reasonable? And so they say, well,\nthe best technique we know is to show you a\nbunch of examples. But we want those\nexamples to kind of cover the gamut of places that\nyou might be interested in. And so they say, let's\ncreate this matrix-- an explanation matrix where\nthese are the cases and these are the various features, you\nknow, the top words or the top pixel elements or\nsomething, and then we'll fill in the element of\nthe matrix that tells me", "id": "wDLzLN1tArA_39", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  with the classification\nfor that model. And then it becomes a\nkind of set covering issue of find a set of\nmodels that gives me the best coverage\nof explanations across that set of features. And then with that,\nI can convince myself that the model is reasonable. So they have this thing called\nthe sub modular pick algorithm. And you know, probably\nif you're interested, you should read the paper. But what they're\ndoing is essentially doing a kind of greedy\nsearch that says, what features should\nI add in order to get the best coverage in that\nspace of features by documents?", "id": "wDLzLN1tArA_40", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  where they said,\nOK, let's compare the results of\nthese explanations of these simplified models\nto two sentiment analysis tasks of 2,000 instances each. Bag of words as features-- they\ncompared it to decision trees, logistic regression,\nnearest neighbors, SVM with the radial\nbasis function, kernel, or random forests that use\nword to vacuum beddings-- highly non-explainable-- with 1,000 trees and K equal 10. So they chose 10\nfeatures to explain for each of these models. They then did a side\ncalculation that said, what are the 10 most suggestive\nfeatures for each case? And then they said, does\nthat covering algorithm", "id": "wDLzLN1tArA_41", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And so what they show here is\nthat their method line does better in every case\nthan a random sampling-- that's not very surprising-- or a greedy sampling or a\npartisan sampling, which I don't know the details of. But in any case, there's\nwhat this graph is showing is that of the\nfeatures that they decided were important\nin each of these cases, they're recovering. So their recall is up\naround 90, 90-plus percent. So in fact, the algorithm is\nidentifying the right cases to give you a broad\ncoverage across all the important\nfeatures that matter in classifying these cases. They then also did a bunch\nof human experiments where", "id": "wDLzLN1tArA_42", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  of two classifiers they think\nis going to generalize better. So this is like the picture I\nshowed you of the Christianity versus atheism algorithm,\nwhere presumably if you were a Mechanical Turker and somebody\nshowed you an algorithm that has very high accuracy but that\ndepends on things like finding the word NNTP in a\nclassifier for atheism versus Christianity, you would\nsay, well, maybe that algorithm isn't good to\ngeneralize very well, because it's depending\non something random that may be correlated with\nthis particular data set. But if I try it on a\ndifferent data set, it's unlikely to work. So that was one of the tasks. And then they asked them\nto identify features", "id": "wDLzLN1tArA_43", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  They then ran this Christianity\nversus atheism test and had a separate test set\nof about 800 additional web pages from this website. The underlying model was\na support vector machine with RBF kernels trained\non the 20 newsgroup data-- I don't know if you\nknow that data set, but it's a well-known,\npublicly available data set. They got 100 Mechanical Turkers\nand they said, OK, we're going to present each\nof them six documents and six features per document in\norder to ask them to make this. And then they did an auxiliary\nexperiment in which they said, if you see words that are no\ngood in this experiment, just", "id": "wDLzLN1tArA_44", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And that will tell us\nwhich of the features were bad in this method. And what they found was that\nthe human subjects choosing between two\nclassifiers were pretty good at figuring out which\nwas the better classifier. Now, this is better\nby their judgment. And so they said, OK, this\nsubmodular pick algorithm-- which is the one that I\ndidn't describe in detail, but it's this set\ncovering algorithm-- gives you better results than\na random pick algorithm that just says pick random features. Again, not totally surprising. And the other thing\nthat's interesting is if you do the feature\nengineering experiment, it shows that as the Turkers\ninteracted with the system,", "id": "wDLzLN1tArA_45", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So they started off\nwith real world accuracy of just under 60%. And using the better\nof their algorithms, they reached about 75% after\nthree rounds of interaction. So the users could say, I\ndon't like this feature. And then the system would\ngive them better features. Now, they tried a similar\nthing with images. And so this one\nis a little funny. So they trained a\ndeliberately lousy classifier to classify between\nwolves and huskies. This is a famous example. Also it turns out that huskies\nlive in Alaska and so-- and wolves-- I guess some wolves\ndo, but most wolves don't.", "id": "wDLzLN1tArA_46", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  which was used in that\noriginal problem formulation, there was an extremely accurate\nclassifier that was trained. And when they went to look\nto see what it had learned, basically it had learned\nto look for snow. And if it saw snow in the\npicture, it said it's a husky. And if it didn't see snow in the\npicture, it said it's a wolf. So that turns out to be\npretty accurate for the sample that they had. But of course, it's not a very\nsophisticated classification algorithm because\nit's possible to put a wolf in a snowy\npicture and it's possible to have your\nHusky indoors with no snow. And then you're just missing\nthe boat on this classification. So these guys built a\nparticularly bad classifier by having all wolves\nin the training set", "id": "wDLzLN1tArA_47", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And then they presented cases to\ngraduate students like you guys with machine\nlearning backgrounds. 10 balance test predictions. But they put one ringer\nin each category. So they put in one husky\nin snow and one wolf who was not in snow. And the comparison was between\npre and post experiment trust and understanding. And so before the\nexperiment, they said that 10 of the\n27 students said they trusted this bad\nmodel that they trained. And afterwards, only 3\nout of 27 trusted it. So this is a kind of\nsociological experiment that says, yes, we can\nactually change people's minds about whether a model is\na good or a bad one based on an experiment. Before only 12\nout of 27 students", "id": "wDLzLN1tArA_48", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  whereas afterwards\nalmost everybody did. So again, this tells you\nthat the method is providing some useful information. Now this paper set off\na lot of work, including a lot of critiques of the work. And so this is one particular\none from just a few months ago, the end of December. And what these guys say is that\nthat distance function, which includes a sigma, which is\nsort of the scale of distance that we're willing to\ngo, is pretty arbitrary. In the experiments that\nthe original authors did, they set that distance\nto 75% of the square root of the dimensionality\nof the data set.", "id": "wDLzLN1tArA_49", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  I mean, that's a number. But it's not obvious\nthat that's the best number or the right number. And so these guys\nargue that it's important to tune the\nsize of the neighborhood according to how far z,\nthe point that you're trying to explain,\nis from the boundary. So if it's close\nto the boundary, then you ought to\ntake a smaller region for your proximity measure. And if it's far\nfrom the boundary, this addresses the\nquestion you guys were asking about\nwhat happens if you pick a point in the middle. And so they show\nsome nice examples of places where, for instance,\nif you compare this explaining this green point, you get\na nice green line that follows the local boundary. But explaining the\nblue point, which is close to a corner of the\nactual decision boundary,", "id": "wDLzLN1tArA_50", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And similarly for the red point. And so they say,\nwell, we really need to work on that\ndistance function. And so they come\nup with a method that they call LEAFAGE, which\nbasically says, remember, what LINE did is it\nsampled nonexistent cases, simplified nonexistent cases. But here they're going\nto sample existing cases. So they're going to\nlearn from the training-- the original training set. But they're going to sample\nit by proximity to the example that they're trying to explain. And they argue that this is a\ngood idea because, for example, in law, the notion\nof precedent is that you get to argue that this\ncase is very similar to some", "id": "wDLzLN1tArA_51", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  should be decided the same way. I mean, Supreme Court arguments\nare always all about that. Lower court arguments\nare sometimes more driven by what\nthe law actually says. But case law has been well\nestablished in British law, and then by inheritance\nin American law for many, many centuries. So they say, well,\ncase-based reasoning normally involves retrieving\na similar case, adapting it, and then learning\nthat as a new precedent. And they also argue for\ncontrastive justification, which is not only why\ndid you choose x, but why did you choose x\nrather than y as giving a more satisfying\nand a more insightful explanation of how\nsome model is working? So they say, OK, similar setup. f solves the\nclassification problem", "id": "wDLzLN1tArA_52", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  you know 0, 1, if you like. The training set\nis a bunch of x's. y sub true is the actual\nanswer. y predicted is what f predicts on that x. And to explain f of z equals\nsome particular outcome, you can define the\nallies of a case as ones that come up\nwith the same answer. And you can define\nthe enemies as one that wants to come up\nwith a different answer. So now you're going to sample\nboth the allies and the enemies according to a new\ndistance function. And the intuition they\nhad is that the reason that the distance function\nin the original line work wasn't working very\nwell is because it", "id": "wDLzLN1tArA_53", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  in n dimensional space. And so they're going\nto bias it by saying that the distance,\nthis b, is going to be some combination\nof the difference in the linear predictions\nplus the difference in the two points. And so the contour\nlines of the first term are these circular\ncontour lines. This is what lime was doing. The contour lines\nof the second term are these linear gradients. And they add them to get\nsort of oval-shaped things. And this is what gives\nyou that desired feature of being more sensitive\nto how close this point is to the decision boundary. Again, there are a lot of\nrelatively hairy details, which I'm going to elide\nin the class today.", "id": "wDLzLN1tArA_54", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So they also did a user study\non some very simple prediction models. So this was how much is your\nhouse worth based on things like how big is it and\nwhat year was it built in and what's some subjective\nquality judgment of it? And so what they\nshow is that you can find examples that are\nthe allies and the enemies of this house in order\nto do the prediction. So then they apply\ntheir algorithm. And it works. It gives you better answers. I'll have to go find\nthat slide somewhere. All right. So that's all I'm going to\nsay about this idea of using simplified models in\nthe local neighborhood", "id": "wDLzLN1tArA_55", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  I wanted to talk about\ntwo other topics. So this was a paper\nby some of my students recently in which they're\nlooking at medical images and trying to generate\nradiology reports from those medical images. I mean, you know,\nmachine learning can solve all problems. I give you a\ncollection of images and a collection of\nradiology reports, should be straightforward to\nbuild a model that now takes new radiological\nimages and produces new radiology reports that are\nunderstandable, accurate, et cetera. I'm joking, of course. But the approach they took\nwas kind of interesting. So they've taken a\nstandard image decoder. And then before\nthe pooling layer, they take essentially an\nimage embedding from the next", "id": "wDLzLN1tArA_56", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And then they feed that\ninto a word decoder and word generator. And the idea is\nto get things that appear in the image that\ncorrespond to words that appear in the report to wind up in\nthe same place in the embedding space. And so again, there's\na lot of hair. It's an LSDM based encoder. And it's modeled as\na sentence decoder. And within that, there\nis a word decoder, and then there's a generator\nthat generates these reports. And it uses\nreinforcement learning. And you know, tons of hair. But here's what I wanted to\nshow you, which is interesting.", "id": "wDLzLN1tArA_57", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  The sentence decoder uses these\nimage features in addition to the linguistic features,\nthe word embeddings that are fed into it. And then for ground\ntruth annotation, they also use a remote\nannotation method, which is this chexpert program, which\nis a rule-based program out of Stanford that reads\nradiology reports and identifies features in\nthe report that it thinks are important and correct. So it's not always\ncorrect, of course. But that's used in order\nto guide the generator. So here's an example.", "id": "wDLzLN1tArA_58", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  so this is the actual\nradiology report-- says cardiomegalia is moderate. Bibasilar atelectasis is mild. There's no pneumothoraxal\nor cervical spinal fusion is partially visualized. Healed right rib fractures\nare incidentally noted. By the way, I've stared at\nhundreds of radiological images like this. I could never figure out\nthat this image says that. But that's why radiologists\ntrain for many, many years to become good at this stuff. So there was a\nprevious program done by others called TieNet which\ngenerates the following report. It says AP portable\nupright view of the chest. There's no call no focal\nconsolidation, effusion, or pneumothorax. The cardio mediastinal\nsilhouette is normal.", "id": "wDLzLN1tArA_59", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So if you compare\nthis to that, you say, well, if the cardio\nmediastinal silhouette is normal, then where would\nthe lower cervical spinal fusion, being partially\nvisualized, because that's along the middle. And so these are not\nquite consistent. So the system that\nthese students built says there's mild enlargement\nof the cardiac silhouette. There is no pleural\neffusion or pneumothorax. And there's no acute\nosseous abnormalities. So it also missed the\nhealed right rib fractures that were incidentally noted. But anyway, it's-- you know,\nthe remarkable thing about a singing dog is not how well\nit sings but the fact that it sings at all. And the reason I\nincluded this work", "id": "wDLzLN1tArA_60", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  going to replace\nradiologists anytime soon, but that it had an interesting\nexplanation facility. And the explanation\nfacility uses attention, which is\npart of its model, to say, hey, when we\nreach some conclusion, we can point back\ninto the image and say what part of the\nimage corresponds to that part of the conclusion. And so this is\npretty interesting. You say in upright and lateral\nviews of the chest in red, well, that's kind\nof the chest in red. There's moderate cardiomegaly,\nso here the green certainly shows you\nwhere your heart is. OK. About there and a\nlittle bit to the left. And there's no pleural\neffusion or pneumothorax. This one is kind of funny. That's the blue region.", "id": "wDLzLN1tArA_61", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And we were surprised,\nactually, the way it showed us that\nthere isn't something is to highlight everything\noutside of anything that you might be\ninterested in, which is not exactly convincing that\nthere's no pleural effusion. And here's another example. There is no relevant change,\ntracheostomy tube in place, so that roughly is\nshowing a little too wide. But it's showing roughly where\na tracheostomy tube might be. Bilateral pleural effusion\nand compressive atelectasis. Atelectasis is when your\nlung tissues stick together. And so that does often happen\nin the lower part of the lung. And again, the negative\nshows you everything that's not part of the action.", "id": "wDLzLN1tArA_62", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  AUDIENCE: [INAUDIBLE]. PROFESSOR: Yes. AUDIENCE: [INAUDIBLE] PROFESSOR: No. It's trying to predict\nthe whole model-- the whole node. AUDIENCE: And it's not easier to\nhave, like, one node for, like, each [INAUDIBLE]? PROFESSOR: Yeah. But these guys were ambitious. You know, they-- what was it? Jeff Hinton said a few\nyears ago that he wouldn't want his children to\nbecome radiologists because that field is going\nto be replaced by computers. I think that was a stupid\nthing to say, especially when you look at the\nstate of the art of how well these things work. But if that were true,\nthen you would, in fact, want something that is able\nto produce an entire radiology report. So the motivation is there. Now, after this\nwork was done, we ran into this interesting paper\nfrom Northeastern, which says--", "id": "wDLzLN1tArA_63", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  OK. So attention is\nclearly a mechanism that's very useful in all kinds\nof machine learning methods. But you shouldn't confuse\nit with an explanation. So they say, well, assumption--\nit's the assumption that the input units are\naccorded high attention-- that are accorded high\nattention weights are responsible for\nthe model outputs. And that may not be true. And so what they did is they\ndid a bunch of experiments where they studied\nthe correlation between the attention weights\nand the gradients of the model parameters to see whether,\nin fact, the words that had high attention\nwere the ones that were most decisive in making\na decision in the model.", "id": "wDLzLN1tArA_64", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  between intuitive feature\nimportance measures, including gradient and feature\nerasure approaches-- so this is ablation studies and learn\ndetention weights is weak. And so they did a\nbunch of experiments. There are a lot of controversies\nabout this particular study. But what you find is that if\nyou calculate the concordance, you know, on different data\nsets using different models, you see that, for example, the\nconcordance is not very high. It's less than a half\nfor this data set. And you know, some\nof it below 0, so the opposite\nfor this data set. Interestingly,\nthings like diabetes, which come from the mimic\ndata, have narrower bounds than some of the others.", "id": "wDLzLN1tArA_65", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  for the study. OK. Let me finish off by talking\nabout the opposite idea. So rather than building\na complicated model and then trying to\nexplain it in simple ways, what if we just\nbuilt a simple model? And Cynthia Rudin,\nwho's now at Duke, used to be at the\nSloan School at MIT, has been championing\nthis idea for many years. And so she has come up with\na bunch of different ideas for how to build\nsimple models that trade off maybe a little\nbit of accuracy in order to be explainable. And one of her favorites is\nthis thing called a falling rule list. So this is an example for a\nmammographic mass data set. So it says, if some lump\nhas an irregular shape", "id": "wDLzLN1tArA_66", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  then there's an 85%\nchance of malignancy risk, and there are 230 cases\nin which that happened. If this is not the case,\nthen if the lump has the speculated margin-- so it has little spikes\ncoming out of it-- and the patient is\nover 45, then there's a 78% chance of malignancy. And otherwise, if the margin is\nkind of fuzzy, the edge of it is kind of fuzzy, and\nthe patient is over 60, then there's a 69% chance. And if it has an\nirregular shape, then there's a 63% chance. And if it's lobular and\nthe density is high, then there's a 39% chance. And if it's round and\nthe patient is over 60,", "id": "wDLzLN1tArA_67", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  Otherwise, there's a 10% chance. And the argument is that that\ndescription of the model, of the decision-making\nmodel, is simple enough that even doctors\ncan understand it. You're supposed to laugh. Now, there are\nstill some problems. So one of them is--\nnotice some of these are age greater than\n60, age greater than 45, age greater than 60. It's not quite obvious what\ncategories that's defining. And in principle, it\ncould be different ages in different ones. But here's how they build it. So this is a very\nsimple model that's built by a very\ncomplicated process. So the simple model is the\none I've just showed you. There's a Bayesian approach, a\nBayesian generative approach, where they have a bunch of hyper\nparameters, falling rule list", "id": "wDLzLN1tArA_68", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  they calculate a\nlikelihood, which is given a particular\ntheta, how likely are you to get the answers that\nare actually in your data given the model that you generate? And they start with a\npossible set of if clauses. So they do frequent\nclause mining to say what conditions,\nwhat binary conditions occur frequently together\nin the database. And those are the\nonly ones they're going to consider\nbecause, of course, the number of possible\nclauses is vast and they don't want to have\nto iterate through those. And then for each set\nof-- for each clause, they calculate a\nrisk score which is generated by a\nprobability distribution under the constraint that the\nrisk score for the next clause", "id": "wDLzLN1tArA_69", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  There are lots of details. So there is this frequent\nitemset mining algorithm. It turns out that\nchoosing r sub l to be the logs of\nproducts of real numbers is an important step\nin order to guarantee that monotonicity\nconstraint in a simple way. l, the number of\nclauses, is drawn from a Poisson distribution. And you give it a\nkind of scale that says roughly how many\nclauses would you be willing to tolerate in\nyour following rule list? And then there's a lot\nof computational hair where they do--", "id": "wDLzLN1tArA_70", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  by using a simulated\nannealing algorithm. So they basically\ngenerate some clauses and then they use swap, replace,\nadd, and delete operators in order to try\ndifferent variations. And they're doing hill\nclimbing in that space. There's also some\nGibbs sampling, because once you have\none of these models, simply calculating how accurate\nit is is not straightforward. There's not a closed\nform way of doing it. And so they're doing sampling in\norder to try to generate that. So it's a bunch of hair. And again, the paper\ndescribes it all. But what's interesting is\nthat on a 30 day hospital readmission data set with\nabout 8,000 patients, they used about 34 features,\nlike impaired mental status, difficult behavior, chronic\npain, feels unsafe, et cetera.", "id": "wDLzLN1tArA_71", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  of the database and no\nmore than two conditions. They set the expected\nlength of the decision list to be eight clauses. And then they compared\nthe decision model they got to SVM's random\nforce logistic regression cart and an inductive\nlogic programming approach. And shockingly to\nme, their method-- the following rule list method-- got an AUC of about 0.8, whereas\nall the others did like 0.79, 0.75 logistic\nregression, as usual outperformed the one\nthey got slightly. Right? But this is interesting,\nbecause their argument is that this\nrepresentation of the model is much more easy to understand\nthan even a logistic regression", "id": "wDLzLN1tArA_72", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  And also, if you look at-- these are just various runs\nand the different models. And their model has a\npretty decent AUC up here. I think the green one is\nthe logistic regression one. And it's slightly better because\nit outperforms their best model in the region of low false\npositive rates, which may be where you want to operate. So that may actually\nbe a better model. So here's their\nreadmission rule list. And it says if the\npatient has bed sores and has a history of not\nshowing up for appointments, then there's a 33%\nprobability that they'll be readmitted within 30 days. If-- I think some note says\npoor prognosis and maximum care,", "id": "wDLzLN1tArA_73", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So this is the result\nthat they came up with. Now, by the way, we've talked\na little bit about 30 day readmission predictions. And getting over about 70%\nis not bad in that domain because it's just not that\neasily predictable who's going to wind up back in\nthe hospital within 30 days. So these models are\nactually doing quite well, and certainly understandable\nin these terms. They also tried on a\nvariety of University of California-Irvine\nmachine learning data sets. These are just random\npublic data sets. And they tried building\nthese falling rule list models to make predictions. And what you see is that\nthe AUCs are pretty good. So on the spam\ndetection data set, their system gets about 91.", "id": "wDLzLN1tArA_74", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  So you know, part of the\nunfortunate lesson that we teach in almost every\nexample in this class is that simple models\nlike logistic regression often do quite well. But remember, here they're\noptimizing for explainability rather than for getting\nthe right answer. So they're willing to sacrifice\nsome accuracy in their model in order to develop\na result that is easy to explain to people. So again, there are many\nvariations on this type of work where people have different\nnotions of what counts as a simple, explainable model. But that's a very\ndifferent approach than the LIME approach, which\nsays build the hairy model and then produce local\nexplanations for why", "id": "wDLzLN1tArA_75", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  All right. I think that's all I'm going\nto say about explainability. This is a very hot\ntopic at the moment, and so there are lots of papers. I think there's-- I just\nsaw a call for a conference on explainable machine\nlearning models. So there's more and\nmore work in this area. So with that, we come to\nthe end of our course. And I just wanted-- I just went through the front\npage of the course website and listed all the topics. So we've covered quite\na lot of stuff, right? You know, what makes\nhealth care different? And we talked about what\nclinical care is all about and what clinical data is\nlike and risk stratification, survival modeling,\nphysiological time series, how to interpret clinical text\nin a couple of lectures,", "id": "wDLzLN1tArA_76", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  The italicized ones\nwere guest lectures, so machine learning for\ncardiology and machine learning for\ndifferential diagnosis, machine learning for\npathology, for mammography. David gave a couple of\nlectures on causal inference and reinforcement learning\nwhere David and a guest-- which I didn't note here-- disease progression\nand sub typing. We talked about\nprecision medicine and the role of genetics,\nautomated clinical workflows, the lecture on regulation,\nand then recently fairness, robustness to data set\nshift, and interpretability. So that's quite a lot. I think we're-- we the staff are\npretty happy with how the class has gone. It was our first time as\nthis crew teaching it. And we hope to do it again. I can't stop without giving\nan immense vote of gratitude", "id": "wDLzLN1tArA_77", "title": "25. Interpretability", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  would have been totally sunk. [APPLAUSE] And I also want to acknowledge\nDavid's vision in putting this course together. He taught a sort of half-size\nversion of a class like this a couple of years\nago and thought that it would be a good idea to\nexpand it into a full semester regular course and got me\non board to work with him. And I want to thank you\nall for your hard work.", "id": "wDLzLN1tArA_78"}], "drug": [{"text": "  >>GOOD AFTERNOON, EVERYONE, TO THIS AFTERNOON'S SESSION OF THE WEDNESDAY AFTERNOON LECTURE SERIES HERE AT THE NIH. FOR THOSE OF YOU WHO DON'T KNOW ME, I'M IN THE INTRAMURAL PROGRAM OF NICHD, AND THIS TALK IS BEING HOSTED BY THE CHEMISTRY SCIENTIFIC INTEREST GROUP. WHEN I WAS AN EARLY GRADUATE STUDENT, BEING CHRISTENED INTO THE CHEMICAL BIOLOGY WORLD FROM A PRETTY STRAITJACKETED CHEMISTRY CURRICULUM, I REMEMBER IN OUR CHEMICAL BIOLOGY LECTURE AT HARVARD, OUR PROFESSOR, YOUNG PROFESSOR DAVID LIU, TOLD US ABOUT THIS DISCOVERY WHERE SOMEONE HAD DESIGNED A NEW SUBSTRATE INSTEAD OF ATP, A SYNTHETIC ANALOG, AND GENERATED A MUTANT KINASE WITH THE IDEA", "id": "vof7x8r_ZUA_0", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  LIGAND SUBSTRATE PAIR, IF YOU WILL. AND THERE ARE A FEW THINGS THAT THE FIRST TIME YOU HEAR THEM, THEY KIND OF EXPLODE IN YOUR MIND, AND THEY STICK WITH YOU ALL YOUR LIFE. AND THIS WHOLE IDEA OF -- LIGAND PROTEIN OR ENZYME DISCOVERY IS ONE SUCH THING. AND IT'S A REAL PLEASURE AND HONOR FOR ME TO INTRODUCE OUR SPEAKER FROM WHOSE LAB THIS WORK ORIGINATED. AND THAT WAS 20-SOMETHING YEARS AGO, KEVAN WAS ALREADY A SUPERSTAR IN CHEMICAL BIOLOGY AND HE HAS SORT OF GONE ALONG THAT TRACK SO WE HAVE TO COIN A DIFFERENT TERM FOR FOLKS LIE KEVAN, MORE THAN SUPERSTARS IN CHEMICAL BIOLOGY BUT TO UNDERSTAND HIS IMPACT IN THE FIELD. I THINK IT'S IMPORTANT TO SAY TWO OTHER THINGS. ONE IS THAT KEVAN IS SOMEONE WHO'S BEEN AN EXTREMELY KIND AND", "id": "vof7x8r_ZUA_1", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  MENTORED AN AMAZING NUMBER OF TRAINEES AND THE IMPACT OF A SCIENTIST BEYOND THEIR SCIENCE IS, OF COURSE, THE TRAINEES THAT THEY SEND OUT INTO SCIENCE, AND KEVAN TRAINEES HAVE GONE INTO CHEMICAL, DEVELOPMENTAL BIOLOGY, YOU NAME IT. KEVAN'S RESEARCH NOW FOCUSES ON ESSENTIALLY NEW SMALL MOLECULE TOOLS AND DRUG CANDIDATES TARGETING PROTEIN AND LIPID KINASES, AND REALLY ENCOMPASSING ALL THE TOOLS OF SYNTHETIC ORGANIC CHEMISTRY, PROTEIN ENGINEERING, STRUCTURAL BIOLOGY, AND CELL BIOLOGY, AND IT INTERSECTS WITH NEURODEGENERATIVE DISEASES, ONCOLOGY, AND ONE ASPECT I THINK THAT'S PARTICULARLY SPECIAL ABOUT KEVAN IS THAT HE'S REALLY BEEN VERY INTIMATELY INVOLVED IN", "id": "vof7x8r_ZUA_2", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  ALL THE WAY TO MAKING A NEW DRUG. KEVAN RECEIVED HIS BACHELOR'S IN CHEMISTRY FROM READ COL REED CON 1986, ORGANIC CHEMISTRY AT UC BERKELEY WHERE I THINK HE WORKED ON -- ANTIBODIES, AND I THINK THAT PROBABLY SPURRED HIS POSTDOCTORAL CHOICE WHERE HE WAS A POSTDOC AT STANFORD WITH PROFESSOR CHRIS GOODENOW, AND HE STARTED HIS INDEPENDENT CAREER IN THE DEPARTMENT OF CHEMISTRY AT PRINCETON, BEFORE MOVING TO UCSF, WHERE HE'S NOW, I MOV HE D IN 1999, NOW AN INVESTIGATOR AT THE HOWARD HUGHES MEDICAL INSTITUTE AND PROFESSOR IN THE DEPARTMENT OF CELLULAR AND MOLECULAR PHARMACOLOGY AT UCSF, AND ALSO PROFESSOR AT THE DEPARTMENT OF CHEMISTRY AT UC BERKELEY. AND NOT SURPRISINGLY, KEVAN HAS", "id": "vof7x8r_ZUA_3", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  AWARDS. IN FACT, I THINK THE BIO HE HANDED US -- HE KNEW WE WOULDN'T BE ABLE TO MANAGE WITH ALL THE AWARDS HE HAS RECEIVED BUTLY STILL MENTION A FEW. SO HE WAS A PEW SCHOLAR, HE WAS A FOUNDATION FELLOW, RECEIVED THE EE LIVELILY AWARD AND THE PROTEIN SOCIETY'S YOUNG INVESTIGATOR AWARD, A MEMBER OF THE NATIONAL ACADEMY OF SCIENCES, NATIONAL ACADEMY OF MEDICINE AND ACADEMY OF ARTS AND SCIENCES. THANK YOU FOR COMING TO THE NIH,\nKEVAN. >> THANK YOU FOR THAT VERY\nNICE INTRODUCTION. IT'S ALWAYS FUN TO HEAR ABOUT SOME OF THE EARLY WORK. GREAT. THANK YOU VERY MUCH. IT'S REALLY GREAT TO BE HERE AND IT'S A REAL HONOR TO GIVE THIS LECTURE. AND IT'S VERY SPECIAL ALSO TO DO IT HERE AT THE NIH, AS YOU'LL", "id": "vof7x8r_ZUA_4", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  WAS DONE RIGHT HERE, AND SO I'LL TAKE YOU THROUGH A LITTLE BIT OF THAT BECAUSE IT'S SO FOUNDATIONAL AND I THINK WITHOUT THAT, NONE OF THE THINGS I COULD TELL YOU ABOUT WOULD HAVE BEEN POSSIBLE. SO FIRST OFF, I DO HAVE THE DISCLOSURES FROM SEVERAL COMPANIES THAT HAVE LICENSED COMPOUNDS FROM THE LAB. THE MOST RELEVANT IS FROM ARAXYX, THE LAST ONE ON THE LIST. I THINK WE'RE AWARE CANCER IS A GENETIC DISEASE, WE GET MUTATIONS IN OUR CHROMOSOMES THAT GIVE RISE TO SELECTION FOR GROWTH, AND I WANT TO BREAK IT DOWN INTO SORT OF CANCER INTO ONE SLIDE, IF THAT'S POSSIBLE. AND GIVE YOU A HIGH LEVEL PICTURE OF WHERE WE'RE GOING IN OUR LAB AND I THINK THE FIELD IS ALSO GOING. SO WE KNOW THAT PROTO ONCOGENES", "id": "vof7x8r_ZUA_5", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  FREQUENT ONCOGENIC MU TAKE THAT OCCURS OVER AND OVER IS IN THE KRAS GTPs, WHICH I'LL BE FOCUSING ON TODAY, AND THIS GIVES RISE TO CONSTITUTIVE ACTIVATION, BINDING TO GTP, TURNING ON MANY DOWNSTREAM EFFECTORS AND I JUST SHOW THE RAF MEK PATHWAY HERE AS ONE OF THE KEY DETERMINANTS, AND THAT GIVES RISE TO A SIGNAL THAT IS GROWTH FACTOR-INDEPENDENT, AND ANTI-APOPTOTIC, PRO-PROLIFERATIVE AND THINGS LIKE THAT. NOW, IF WE JUST HAVE CELLS THAT HAVE ONE OF THOSE MUTATIONS, WE DON'T HAVE A CANCER, AND THAT'S BECAUSE WE HAVE FANTASTIC PROTECTION MECHANISMS EXHIBITED BY THE TUMOR SUPPRESSORS LIKE P53 THAT SENSE ONCOGENIC STRESS WHEN P53 RECOGNIZES THAT STRESS, BECOMES A VERY ACTIVE TRANSCRIPTION FACTOR, TURNS ON HUNDREDS OF COORDINATE GENES", "id": "vof7x8r_ZUA_6", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  AND SEND THE CELLS INTO APOPTOSIS AND IN SOME CASES, AND THE CELL IS IN A SENESCENT STATE. SO AT THAT POINT, THE ONCOGENIC MUTATION IS NOT DRIVING PROLIFERATION. BUT THIS CELL, IN A SENESCENT STATE, IS ALSO SUBJECT TO A MUTATION AND SELECTION, AND OFTEN P53 IS MUTATED, LOSING ITS FUNCTION, LOSING ITS TUMOR PROTECTIVE EFFECT, AND THEN THE CELL STARTS TO DIVIDE. TO GIVE YOU AN IDEA OF THE NUMBERS OF PATIENTS, THIS IS -- RAS IS THE MOST FREQUENTLY MUTATED ONCOGENE. P53 IS THE MOST FREQUENTLY MUTATED TUMOR SUPPRESSOR. IF THERE ARE ABOUT 18 MILLION CANCER PATIENTS WORLDWIDE EVERY YEAR, ABOUT 9 MILLION WILL HAVE P53 MUTATIONS AND ABOUT 2 1/2 MILLION WILL HAVE KRAS MUTATIONS. SO IT'S A TREMENDOUSLY FREQUENT LESION THAT OCCURS TO DRIVE MANY, MANY CANCERS.", "id": "vof7x8r_ZUA_7", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  PROLIFERATING, THEONICS GEENL -- TUMOR SUPPRESSORS ARE LOST, THERE'S LOTS OF ERRORS DURING THE CELL CYCLE AND THE RAPID CELL DIVISION, AND SO YOU PICK UP EVEN MORE MUTATIONS BESIDES THE ONCOGENES AND THE TUMOR SUPPRESSORS. THIS SYMBOL JUST SORT OF IS MEANT TO EMPHASIZE THAT WE GET PATIENTS TUMORS GET PRIVATE MUTATIONS, MUTATIONS IN GENES THAT DON'T NECESSARILY HELP SELECTION BUT THEY JUST ACCUMULATE OVER TIME. AND THE NEXT CHANCE FOR THE BODY TO FIGHT THE CANCER IS THE IMMUNE SYSTEM. AND SO THESE MUTATIONS THAT OCCUR, EITHER THAT ARE PROMOTING GROWTH OR JUST BYSTANDING MERE TAITIMUTATIONS, GET PROCESSED IO PEPTIDES AND THESE PEPTIDES BECOME PRESENTED THROUGH ANTIGEN PRESENTATION ON CLASS I, AND BECAUSE THESE ARE MUTATIONS THAT", "id": "vof7x8r_ZUA_8", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  THEN THEY'RE POTENTIALLY RECOGNIZED AS NON-SELF, AND PATIENTS' T-CELLS CAN SEE THOSE IF THE T-CELL RECEPTORS HAVE THEIR REQUISITE SPECIFICITY, AND THEN THE T-CELLS CAN KILL THE TUMOR CELLS. NOW, AGAIN, THIS CASE, THE TUMOR CAN FIGHT THE IMMUNE SYSTEM, AND WE KNOW THAT FROM UPREGULATION OF THINGS LIKE PDL1, THAT THEN SIGNALS TO CHECKPOINT RECEPTORS ON T-CELLS LIKE PD-1, AND WE KNOW THAT THAT, THEN, SHORT CIRCUITS THE T-CELL FROM KILLING THE TUMOR CELL. BUT OF COURSE BECAUSE OF THE REMARKABLE ADVANCES FROM THE CHECKPOINT THERAPIES AND THE DISCOVERIES FROM JIM ALISON AND OTHERS, WE KNOW THAT INHIBITING PD-1 OR PDL1 CAN TAKE THE BRAKES OFF T-CELLS AND LET THEM KILL THE TUMOR CELL. AND THESE ARE FANTASTICALLY, YOU KNOW, POWERFUL THERAPIES. THE PROBLEM IS THAT THE", "id": "vof7x8r_ZUA_9", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  ONCOGENES AND THE TUMOR SUPPRESSORS OVER HERE ARE OFTEN POOR PEPTIDES TO BE PRESENTED OR ARE POORLY RECOGNIZED BY T-CELLS TO ALLOW KILLING OF THE TUMOR CELL. MANY OF THE ANTIGENS THAT PEOPLE HAVE IDENTIFIED THAT ARE SUITABLE FOR LETTING THE T-CELL KILL ARE REALLY FROM THESE BYSTANDING MUTATIONS OR THE PRIVATE MUTATIONS. SO IT'S DIFFICULT TO GET THE RIGHT RECOGNITION. NOW, HERE AT NIH, STEVE ROSENBERG'S GROUP AND GROUPS IN THAT VISION HAVE MADE FANTASTIC INROADS INTO IDENTIFYING PATIENTS' T-CELLS THAT DO SEE MUTANTS THAT ARE IN HOT SPOTS LIKE THE KRAS PEPTIDE AND THEY'VE SHOWN SOME FANTASTIC RESPONSES OF PATIENTS WHEN YOU INFUSE BACK THE RIGHT T-CELLS. SO THESE ARE REALLY FANTASTIC SIGNAL FINDING STUDIES. BUT WHAT WE REALLY HAVE BEEN", "id": "vof7x8r_ZUA_10", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  BASICALLY THERAPIES THAT INHIBIT THE ONCOGENES, REACTIVATE TUMOR SUPPRESSORS, AND SOMEHOW AVOID THE IMMUNE EVASION THAT OCCURS IN TUMORS. IF WE CAN FIND VERY SPECIFIC WAYS TO DO THAT, WE CAN COMBINE THESE THERAPIES AND WE MIGHT GET FARTHER TOWARDS DEEPER RESPONSES. SO THAT'S WHAT I'LL TELL YOU ABOUT TODAY. I'LL TALK MOSTLY ABOUT INHIBITING K-RAS I WON'T TALK ABOUT REACTIVATING TUMOR SUPPRESSORS LIKE P53.  P53 THAT WE'VE DONE. BUT I WILL TALK ABOUT HOW TO LEVERAGE SOME OF THE ANTIGEN PRESENTATION PATHWAY TO SORT OF GET DEEPER RESPONSES. OKAY. SO THE TITLE POINTED OUT THAT KRAS WAS CONSIDERED UNDRUGGABLE, AND I WANT TO GIVE YOU A LITTLE BIT OF A HISTORY OF KRAS FROM ITS DISCOVERY AND WHAT MADE IT A", "id": "vof7x8r_ZUA_11", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  FIRST DRUG. SO KRAS HERE, NOW YOU SEE, IS A GTPASE. ON ITS C TERMINAL TELL, IT'S LIP DATED WITH A GROUP THAT I'LL GO INTO. ONCE IT'S AT THE MEMBRANE AND IS CON STIT WECONSTITUENTLY AFTERTT TURNS ON THE SIGNALING PATHWAY THAT I SHOW HERE. SO IT WAS FIRST DISCOVERED AS A HUMAN ONCOGENE IN 2023 AND UNTIL 2021, THERE WERE NO DIRECT INHIBITORS APPROVED FOR PATIENTS. AND IF YOU WANT TO SORT OF COMPARE THAT TO WHAT A SUCCESSFUL DRUGGABLE PROTEIN LOOKS LIKE IN TERMS OF TIMELINE, THE BRAF MUTATIONS WERE DISCOVERED MANY, MANY YEARS LATER, IN 2002, BUT AS SOON AS THEY WERE DISCOVERED, NINE YEARS LATER, THERE WAS AN APPROVED BRAF INHIBITOR. SO FROM DISCOVERY OF THE ONCOGENE TO APPROVAL, IT WAS ONLY NINE YEARS, VERSUS DECADES IN THE KRAS CASE.", "id": "vof7x8r_ZUA_12", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  APPARENT WAS THAT THE WAYS WAY MAKE KINASE INHIBITORS IS BY INHIBITORS THAT COMPETE ATP. SO YOU WOULD THINK THAT BECAUSE KRAS IS A GTPASE, BINDS NUCLEOTIDE, MAKE A GTP-COMPETITIVE NUCLEOTIDE. HOWEVER, THE AFFINITY OF KRAS FOR GTP IS 20 PICOMOLAR COMPARED TO THE KM FOR ATP FOR DIE NAISKINASES,WHICH IS -- THAT MID DIFFERENCE MEANS THERE WILL PROBABLY NEVER BE A GTP COMPETITIVE KRAS INHIBITOR. THIS DISCOVERY WAS HERCULEAN EVEN TO MEASURE THIS NUMBER. THIS WAS DONE BY FRED AND ROGER AND DORTMAN AND THEY HAD TO ALMOST DENATURE THE PROTEIN AND ALLOW MEASURE KINETIC BINDING BECAUSE YOU COULDN'T DO EQUILIBRIUM SORT OF NUMBERS TO MEASURE THAT. SO THE FIRST CHALLENGE IS THE", "id": "vof7x8r_ZUA_13", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  OFF LIMITS. SO ONCE THAT WAS REALIZED, THEN OTHER STRATEGIES BEGAN TO BE ATTEMPTED. AND THE FIRST ONE TO MAKE TO THE CLINIC WERE ATTEMPTS TO BLOCK THE FARN SLAITION OF KRAS WHICH AS I'LL SHOW, THAT IS REQUIRED FOR KRAS ONCOGENESIS. SO GOING BACK TO THE DISCOVERY OF THE RAS GENES, PASSAGING IN ANIMALS AND HARVESTING THE VIRUSES AND SO H RAS WAS THE FIRST RAS GENE IDENTIFIED BY JENNIFER HARVEY, AND SOME OF THE VERY EARLY FOUNDATIONAL WORK WAS DONE BY MAR YA KNOW BARBASED WHEN HE WAS HERE AT THE NIH AND", "id": "vof7x8r_ZUA_14", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  GENE HOMOLOGOUS TO THE VIRUS, THE VIRAL GENE THAT WAS ISOLATED, AND I THINK A REALLY KEY PAPER FROM DOUG'S LAB WAS THIS ONE THAT SHOWED THAT THE SERINE -- THE CYSTINE, IF IT IS MUTATED TO -- BLOCKS ALL ONCOGENESIS DONE BY CLASSIC SOFT AUGER EXPERIMENTS. SO THIS ONE EXPERIMENT REALLY LIT UP EVERYBODY'S EXCITEMENT BECAUSE ALTHOUGH IT WAS DIFFICULT TO THINK OF A DRUG THAT COULD BIND TO THE PROTEIN ITSELF, WE KNEW THAT THE FARNSILLATION WAS ABSOLUTELY REQUIRED. SO SHORTLY AFTER THAT, A LOT OF WORK WAS DONE TO IDENTIFY WHAT THE LIPID WAS, IT TURNED OUT TO BE A FARNASIL GROUP, THEN THE TARGET THAT YOU WOULD INHIBIT", "id": "vof7x8r_ZUA_15", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  AFTER THAT INHIBITORS OF FARN SILL TRANSFERASE WERE DEVELOPED. SO ONCE THAT CYSTINE TO SERINE MUTANT WAS MADE AND THE PHENOTYPE WAS CLARIFIED, THEN IT WAS A MAD RUSH TO IDENTIFY INHIBITORS OF RAS FAR NIS LAITION. SO THAT WAS A REALLY AMAZING DECADE, I WOULD SAY, FROM THE UNDERSTANDING THAT IT WAS WHAT THE ONCOGENE WAS AND TO A PRE-CLINICAL CANDIDATE. SO WHAT I WANT TO TELL YOU IS ABOUT THE EXPERIENCE WITH THE FARNESYL -- ATRAS WAS REALLY THE FIRST HUMAN ONCOGENE DISCOVERED SO A LOT OF THE STUDIES WERE FROM THE BLADDER CANCER CELLS WHERE H RAS WAS TRANSFORMING AND THERE'S THE G DOMAIN THAT BINDS", "id": "vof7x8r_ZUA_16", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  A FLEXIBLE TAIL THAT HAS THE CYSTINE THAT BECOMES FARNESYLATED BY FARNESYL TRANS PHRASE. SO THIS BECOMES LABELED. THERE ARE A FEW OTHER STEPS OF PROCESSING, YOU'LL NOTICE THIS IS A TETRAPEPTIDE BUT THEN IN THE FINAL PROCESSED MEMBRANE LOCALIZED FORM, IT IS -- IT TERMINATES IN THE CYSTINE BUT IT'S CARBOXY METHYLATED. THAT WON'T BE SO IMPORTANT HERE, BUT IT'S A VERY REGULATED PROCESSING STEP. YOU CAN KIND OF TELL FROM MY DRAWING, IT'S GOT A VERY NICE POTENTIAL POTENTIAL -- INHIBITOS ABSOLUTELY PREVENT H RAS LOCALIZATION TO THE MEMBRANE AND KILL CELLS. ALL THE PRE-CLINICAL MODELS SAID THAT IT WORKED BEAUTIFULLY.", "id": "vof7x8r_ZUA_17", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  THE HUMAN CANCER PICTURE AND THERE ARE MUTANTS IN H RAS AND MUTANTS IN K-RAS THEY HAVE VERY SIMILAR G BINDING DOMAINS. BUT THEY HAVE DIFFERENT TAILS. SO H RAS HAS A DIFFERENT TAIL THAN K-RAS KRAS COMES IN TWO SPLICE VARIANTS, 4A AND 4B. AND THIS SEQUENCE HAS MULTIPLE SIGNALS ON IT FOR MULTIPLE POST TRANSLATIONAL MODIFICATIONS. SO IF I ADD THOSE SEQUENCES YOU SEE THAT H RAS AND KRAS 4A LOOK QUITE SIMILAR TO EACH OTHER, BUT THE KRAS 4B HAS A POLYLYSINE ADDED ON ITS C-TERMINUS. AND YOU'LL SEE THAT H RAS AND KRAS 4A HAVE ANOTHER CYSTINE UPSTREAM AND THAT ACTUALLY BECOMES PALMIDYLATED.", "id": "vof7x8r_ZUA_18", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  PERFECTLY. BUT NOW THERE'S AN ISSUE OF WHICH OF THESE ISOFORMS IS THE MOST IMPORTANT FOR HUMAN CANCER. AND WHAT YOU CAN TELL IS THAT THEIR FREQUENCY OF H RAS MUTANTS ARE JUST 1%, BUT FOR KRAS, IT'S 25%. SO NOW WE'RE INTERESTED IN WHETHER THESE CAN BE PREVENTED FROM MEMBRANE LOCALIZATION. THEN WE HAVE TO WONDER WHICH ALTERNATIVE SPLICE FORM IS MORE IMPORTANT FOR CANCER. AND THIS IS WHERE THE BIG PROBLEM CAME. THE KRAS 4B WITH THIS ALTERNATIVE POLYLYSINE SEQUENCE IS THE MOST IMPORTANT FOR ONCOGENIC SIGNALING. AND BECAUSE OF THAT, AND BECAUSE OF THIS DIFFERENT SEQUENCE, THE ENTIRE FARNASYL TRANS PHRASE APPROACH FOR KRAS MUTANTS FAILED, AND WHY WAS THAT?", "id": "vof7x8r_ZUA_19", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  FARNASYL TRANSFERASE. BUT THEN BIOLOGY HAD A BACKUP FOR KRAS 4B, AND THAT IS ONE ADDITIONAL ISOPRENE, DIFFERENT ENZYME, BUT THIS ENZYME CAN RECOGNIZE THE 4B TAIL. IT DOESN'T RECOGNIZE H RAS OR THE 4A TAIL, BUT IT RECOGNIZES THE 4B TAIL. SO ALL OF THE KRAS MUTANT CELLS THEN ALLOW LOCALIZATION, EVEN THOUGH THEY HAD A FANTASTIC DRUG. SO LATE IN THE 90s, THIS REPORT AND OTHERS REALLY POINTED OUT THAT THE DIFFERENCE IN THE TAIL COMPLETELY ALLOWED KRAS 4B MUTANT CELLS TO BYPASS FARNESYL TRANS PHRASE FACTORS. THOUSANDS OF PATIENTS ENROLLED, VERY FEW RESPONSES THIS IS AT", "id": "vof7x8r_ZUA_20", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  IS THE REASON FOR ALL THAT FAILURE. SO BETWEEN THE DIFFICULTY OF THE NUCLEOTIDE POCKET AND THIS INCREDIBLE RUSH TO FARNESYL TRAN FER ACE INHIBITORS WHICH ULTIMATELY FAILED, THAT REALLY LED TO THE PROBLEMS AND THE VIEW THAT KRAS WAS GOING TO BE UNDRUGGABLE. SO LET ME THEN GO BACK UP AND TELL YOU ABOUT THE GLYCINE 12 MUTATION AND WHY IT OCCURS SO FREQUENTLY AND WHAT DOES DO TO THE RAS CYCLE. SO RAS IS A NUCLEOTIDE THAT SWITCHES BETWEEN TWO CONFORMATIONS. YOU'LL HEAR ME TALK ABOUT THIS QUITE A LOT. IN THE GDP STATE, IT'S OFF. IN THE GTP STATE, IT'S ON. AND BY ON, I MEAN THAT THESE TWO FLEXIBLE LOOPS ON RAS, THESE TWO SWITCH LOOPS, FORM A CONFORMATION THAT ON THE SURFACE THAT IS COMPATIBLE WITH DOWNSTREAM -- SO WHEN RAS IS IN THE GTP STATE AT THE MEMO BRAIT,", "id": "vof7x8r_ZUA_21", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  FACTORS IN THE MEMBRANE AND THEN THEY TURN ON THE CASCADE. NOW, IN ORDER TO MAKE THE CYCLE GO FROM IN THE WILD TYPE CASE FROM OFF TO ON, AND I TOLD YOU THE AFFINITY IS PICOMOLAR, WELL, THERE NEEDS TO BE SOME WAY TO CATALYZE THE RELEASE AND THAT'S DONE BY THIS GUANINE EXCHANGE FACTOR THAT COMES UP TO THE PROTEIN AND ESSENTIALLY STRIPS OUT THE NUCLEOTIDE AND THEN ONCE FREE NUCLEOTIDE STATE IS PRESENT, THEN GTP COMBINED AND TURNS ON THE SIGH  CYCLE. THEN THERE'S A TIMER, THERE'S A FAST OFF AND SLOW OFF. THE FAST OFF IS THROUGH A DISTINGUISHED PROFESSOR ACE AGTPASE -- HELPS ACCELERATE GTB TO GDP HYDROLYSIS. THE KEY STRUCTURE THAT REALLY REVEALED ALL THIS WAS FROM FRED", "id": "vof7x8r_ZUA_22", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  WHEN GAP COMES UP TO RAS, IT PUSHES AN ARGININE TO FILL THE CATALYTIC SITE AND ACCELERATE HYDROLYSIS AND GUESS WHAT'S RIGHT AT THAT INTERFACE, IS GLYCINE 12. AND SO THAT SMALL AMINO ACID SIDE CHAIN IS SITTING RIGHT AT THE CATALYTIC RESIDUE, DONATED BY THE GAP PROTEIN. AND SO ANY RESIDUE OTHER THAN GLYCINE PUSHES THE AR JEAN -- STAYS IN THE GTP STATE, TURNS ON THE DOWNSTREAM SIGNAL. SO THIS IS REALLY THE MOST FREQUENT SINGLE AMINO ACID CHANGE IN ANY ONCOGENE, AND IT OCCURS OVER AND OVER AGAIN. BUT BECAUSE IT'S A STERIC PROBLEM, REALLY ANY AMINO ACID AT THAT GLYCINE POSITION CAN CAUSE ONCOGENESIS. AND WHAT'S FASCINATING IS THAT ACROSS THE DIFFERENT DISEASES, THESE ARE THE 12 MOST LETHAL CANCERS TYPICALLY IN A GIVEN YEAR IN THE UNITED STATES, STARTING WITH LUNG CANCER,", "id": "vof7x8r_ZUA_23", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  IS IN YELLOW. IN BLUE ARE THE FRACTION OF PATIENTS THAT HAVE A RAS MUTATION UPON PRESENTATION. SO THE KEY COMMON KRAS-DRIVEN DISEASES ARE LUNG, COLON, AND PANCREATIC CANCER. PANCREATIC CANCER IN PARTICULAR BECAUSE ALMOST 95% OF PATIENTS WILL HAVE A KRAS MUTATION. IT'S THE PROTOTYPICAL KRAS-DRIVEN DISEASE. THE GLYCINE 12 CAN BE MUTATED TO ANY NUMBER OF SINGLE BASE PAIR AMINO ACIDS THAT WILL SWITCH OVER AT THE CODON, AND IN PANCREAS, IT'S ASPARTATE, THEN VAI LEAN, THEN CYSTINE. IN COLON, THE SPECTRUM CHANGES A LITTLE BIT, AND YOU GET A GROWING OF CYSTINE AND DECREASE OF ASPARTATE, AND IN THE SMOKING-INDUCED CANCER, THE DOMINANT MUTATION IS CYSTINE. AND WHEN YOU LOOK AT ALL OF THE FREQUENCIES AND PUT THEM ALL TOGETHER, THE ORDER IS AS PART TATE, VALINE, CYSTINE, THEN --", "id": "vof7x8r_ZUA_24", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  THIS IS STILL A LITTLE BUILT OF A MYSTERY. WE DON'T QUITE KNOW WHY YOU GET THE SPECTRUM. IT HAS TO DO WITH MAYBE SLIGHT DIFFERENCES IN BIOCHEMICAL RATE OF HYDROLYSIS WHEN THESE DIFFERENT MUTATIONS ARE PRESENT, BUT WHATEVER THE CASE, AS A PERSON TRYING TO LOOK AT THE DRUGGABILITY OF COMPLEX, WE REALIZED, WELL, WE HAVE A NUMBER OF AMINO ACID WE COULD TARGET. WHENEVER WE SEE A CYSTINE IN A PROTEIN, WE IMMEDIATELY CHEMICALLY THINK OF A COVALENT BOND BECAUSE IT'S THE MOST NUCLEOPHILIC RESIDUE, SO IT HAS A VERY OPPORTUNE SORT OF CHANCE TO MAKE A COVALENT BOND TO IT. SO WE SET OUT TO MAKE A COVALENT BOND THAT WOULD BIND TO THE CYSTINE. NOW, ONE FEATURE THAT WE SORT OF GET FOR FREE IF WE MAKE A COVALENT BOND TO A SOMATIC CYSTINE MUTATION IS THAT THE", "id": "vof7x8r_ZUA_25", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  INHIBIT THE SIGNALING IN THE TUMOR CELL AND NOT THE REST OF THE BODY. THAT IS A WHOLE DIFFERENT THING THAN MOST PROTEIN KINASE, ALMOST ALL PROTEIN KINASE INHIBITORS WE HAVE. SO BY LEVERAGING THE CHEMICAL UNIQUENESS OF CYSTINE AND THE FACT THAT THE CYSTINE IS ONLY IN THE TUMOR, WE REALLY HAD A CHANCE TO MAKE NOT ONLY THE FIRST KRAS DRUG BUT ONE THAT WOULD BE HIGHLY SELECTIVE FOR THE TUMOR. SO IT CAME VERY, VERY ATTRACTIVE TO TAKE THIS APPROACH. SO WE SOLVED THE APOE STRUCTURE AND WE SEE THE CYSTINE RIGHT HERE AT THE 12 POSITION WITH GDP BOUND, HERE ARE THE TWO SWITCH REGIONS. ONE OF THE SWITCHES, SWITCH 2, HAS A DISORDERED ELEMENT THAT WAS NOT VISIBLE IN THE GDP STRUCTURE, THEN WE NEEDED AN APPROACH TO IDENTIFY A STARTING DRUG. AND WE TOOK AN APPROACH THAT WAS PIONEERED BY MY COLLEAGUE, JIM WELLS. THIS IS THE ADVANTAGE OF BEING", "id": "vof7x8r_ZUA_26", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  WHAT YOUR COLLEAGUES WORK ON, AND SORT OF HEARING ABOUT THEIR DISCOVERIES AND TRYING TO SEE WHEN THEY COULD ALL WORK TOGETHER. SO WITH JIM, HE HAD DEVELOPED THIS REALLY POWERFUL APPROACH FOR DISCOVERING COVALENT LIGANDS FOR CYSTINE CONTAINING PROTEINS. USING DISULFIDE CHEMISTRY. DISULFIDE CHEMISTRY HAS THE ADVANTAGE THAT IT DOES RAPID EXCHANGE REACTIONS, AND SO IT CAN DO BOTH FORWARD AND REVERSE REACTIONS, SO IT'S A THERMODYNAMIC KIND OF SEARCH, AND THEN THE LIGANDS HAVE SMALL DRUG FRAGMENTS ON THEM, AND IF ONE OF THEM IS COMPLIMENTARY TO A POCKET NEAR THE CYSTINE, THEN THE DISULFIDE BOND THAT'S MADE WILL BE QUITE STABLE. IT WILL BE SOMEWHAT RESISTANT TO A NONSPECIFIC -- SO THIS STRATEGY CAN BE READILY TUNED BY", "id": "vof7x8r_ZUA_27", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  CONCENTRATION, AND ANY MOLECULES THAT ARE BINDING ARE DETECTED BY WHOLE PROTEIN MASS SPEC, SO YOU KNOW EXACTLY THAT THE MOLECULE YOU EXPECTED IS THE ONE THAT'S BOUND. SO THERE'S LOTS OF FEATURES THAT ARE COMMON SORT OF CHALLENGES FOR FRAGMENT-BASED DRUG DISCOVERY. SO WE USED THAT APPROACH, IT WAS PIONEERED IN JIM'S LAB, AND WE IDENTIFIED THESE TWO HIT MOLECULES OUT OF 500 MOLECULES THAT BOUND TO THE PROTEIN, AND THEY BIND TO THE -- THEY DON'T BIND TO THE WILD TYPE AND THERE'S FIVE OTHER CYSTINES SO WE KNEW IT WAS SELECTIVE. AND THEN WE BEGAN TO DO ADDITIONAL STUDIES LIKE THE NUCLEOTIDE SPECIFICITY. THIS PROTEIN THAT WE USED WAS JUST HARVESTED FROM BACTERIA, SO IT HAD ALREADY CYCLED TO THE GDP STATE. SO WE PUT THE PROTEIN IN THE GTP STATE AND UNFORTUNATELY NOTHING BOUND TO THAT. NOW, REMEMBER I TOLD YOU THAT", "id": "vof7x8r_ZUA_28", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  PROTEIN IN THE GTP STATE. SO THE PROTEIN THAT'S IN THE CELL IS THE GTP STATE, AND THESE TWO DRUGS DON'T TOUCH THAT. YOU CAN SEE THE LABELING IS ALMOST NON-EXISTENT. SO WE IMMEDIATELY PUT THESE ASIDE, WE WENT BACK TO THE COLLECTION WITH THIS FORM OF THE PROTEIN AND ASKED, WHAT DO WE GET? ZERO. NOT A SINGLE MOLECULE TOUCHED THAT CYSTINE WHEN IT WAS IN THE NUCLEOTIDE, THE GTP STATE. SO WE BASICALLY DECIDED WE HAD NO OTHER LEADS SO LET'S GO WITH THESE AND TRY TO UNDERSTAND WHERE IT'S BINDING AND EVEN THOUGH THIS PROBABLY WOULD NEVER WORK EVEN IN A CELL, LET ALONE A PATIENT. SO WHEN WE SOLVED THE STRUCTURE, WE REALIZED IMMEDIATELY FIRST OF ALL THAT IT WAS BOUND TO THE -- THIS IS THE GLYCINE 12 POSITION, IT WAS THE DISULFIDE, SO YOU SEE BOTH SULFURS, AND THEN YOU SEE THE DRUG BINDING IN A BEAUTIFUL POCKET OF THE PROTEIN THAT WE", "id": "vof7x8r_ZUA_29", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  THE EXISTING APOE STRUCTURES, BECAUSE IT'S UNDERNEATH SWITCH 2, WHICH IS HALF CLOSED ON THE DRUG. AND THAT REALLY EXEMPLIFIES THAT YOU SORT OF NEEDED THE DRUG TO FIND THE POCKET. YOU COULDN'T LOOK IN THE PROTEIN AND FIND A POCKET. YOU HAD TO HAVE BOTH TOGETHER. IT ALSO EXPLAINED WHY THE DRUG DOESN'T BIND TO THE GTP STATE, BECAUSE IT'S BINDING IN THE POCKET THAT THE SWITCH 2 WOULD BE IN IF THERE WERE GTP BOUND. SO ALL OF THE THINGS ALL OF A SUDDEN WITH THAT ONE STRUCTURE REALLY BECAME VERY, VERY CLEAR. SO WE THEN REALIZED IN THE SLIDES THAT FOLLOW, I'LL PUT A RED DOT ON THE ATOM OF THE DRUG THAT IS ATTACKED BY THE GLYCINE 12 SUBSTITUTED RESIDUE, SO IN THIS CASE, THE CYSTINE ATTACKS THIS POSITION AND YOU'LL SEE THAT LATER.", "id": "vof7x8r_ZUA_30", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  MOLECULE. WE KNEW WE HAD THE FIRST POCKET ON THE PROTEIN. OTHER PEOPLE HAD FOUND A POCKET CALLED SWITCH 1 THAT WAS BEHIND THE SWITCH REGIONS, BUT THAT WAS EVEN MORE SHALLOW THAN THIS ONE. THAT WAS FROM THE FESSIC LAB AND THE GENENTECH LAB, USING SAR BY ENEMAR TO IDENTIFY LEADS. SO WE DECIDED TO CONTINUE TO PURSUE THIS POCKET. WE KNOW THAT THE DRUG HAD VERY MANY FULLY ROTATABLE BOBD BONDSD I SHOWED YOU THAT POCKET OF RAS IS VERY FLEXIBLE. THAT PART OF THE POCKET WAS NOT EVEN ORDERED IN THE GDP STATE, SO WE NEEDED TO SORT OF OPTIMIZE THE DRUG, AND THAT'S WHEN WE STARTED THE COMPANY ARAXYX THAT OPTIMIZED FOR BINDING UNTIL WE GOT TO THIS VERY KEY MOLECULE, WHICH IS MUCH MORE PREORGANIZED THAN ANY OF THE OTHERS, BUT STILL ORIENTS THE ACRYLAMIDE", "id": "vof7x8r_ZUA_31", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  THAT'S BACK HERE. AND ONCE THAT WAS RELEASED, A LOT OF COMPANIES BEGAN TO DEVELOP ADDITIONAL INHIBITORS AND THEY ALL SHARED THE SAME ACRYLAMIDE AND MANY OF THE SAME BINDING CONTRACT, ALTHOUGH EACH OF THEM HAVE SLIGHTLY DIFFERENT FLAVORS. THIS ONE IS FROM AM GENERAL, THIGEN, THIS ONEIS FROM MIROTTI. AND BEFORE I SHOW YOU THE CLINICAL DATA, JUST -- THIS IS JUST ILLUSTRATING WHAT SO MANY CHEMISTS ACROSS -- CAN REALLY DO TO ADD TO THE HIGH AFFINITY ELEMENTS THAT WE NEED FOR BETTER AND BETTER DRUGS. SO WE'VE BENEFITED A LOT OVER THE LAST FIVE YEARS FROM SEEING MANY OF THE ADVANCES THAT PEOPLE HAVE DONE ACROSS INDUSTRY. BUT THE GOOD THING WAS THAT EVEN THE FIRST DRUG SHOWED CLINICAL ACTIVITY. SO THIS IS THE AM MGEN SOTORASIB", "id": "vof7x8r_ZUA_32", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  PLOT OF PATIENTS THAT HAVE LUNG CANCER, RECEIVED THE APPROPRIATE DOSE OF THE DRUG, AND THIS WATERFALL PLOT SHOWS THE CHANGE IN TUMOR GROWTH. ON THE LEFT IS THE POOREST RESPONSE, ON THE RIGHT ARE THE BEST RESPONSES, AND YOU CAN SEE IN GREEN THERE ARE SOME COMPLETE RESPONSES BUT THE VAST MAJORITY, ABOUT 80% OF THE PATIENTS HAVE WHAT'S CALLED DISEASE CONTROL. WE DON'T HAVE CURES BUT WE HAVE TREATMENT BENEFIT. SO THIS REALLY OPENED THE DOOR TO SAY THAT A DIRECT KRAS INHIBITOR COULD WORK. ALTHOUGH I'M SKIPPING OVER A FEW SLIDES THAT EXPLAINED WHY THIS MOLECULE BOUND ONLY TO THE GDP STATE. IT STILL WORKED -- THE PICTURE OF THE PROTEIN BEING LOCKED IN THE GTP STATE WAS WHAT WE THOUGHT, IT DOES CYCLE.", "id": "vof7x8r_ZUA_33", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  THE PROTEIN TO CYCLE TO THE GDP STATE, THEN IT LACHES ON, THEN IT PULLS IT OUT OF THE CYCLE, SO IT'S NOT IDEAL, NOT NECESSARILY WHAT WE WOULD HAVE WANTED, BUT THE FACT THAT THAT EVEN WORKS IS EXCITING. SO SOME OF THE DRUGS THAT ARE IN THE CLINIC NOW, THE SO-CALLED SECOND GENERATION AND NEXT GENERATION DRUGS ARE ABLE TO BIND TO THE GDP STATE. THEY MAY BE BETTER THAN THESE. BUT THIS IS REALLY, I THINK, A GREAT START AND WE HAVE LOTS OF ROOM TO IMPROVE. SO WITH THE G12C TUMORS IN SOMEWHAT OF CONTROL WITH COMPOUNDS THAT ARE SHOWING EFFICACY IN THE CLINIC AND TWO DRUGS APPROVED, WE BEGAN TO THINK OF THE OTHER MUTATIONS. WE LIKED THIS COVALENT IDEA BECAUSE AGAIN LIKE I SAID, IT GIVES YOU VERY, VERY GOOD SPECIFICITY FOR THE TUMOR AND IT AVOIDS THE REST OF THE BODY.", "id": "vof7x8r_ZUA_34", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  AND '15, WE TRIED TO JUMP FROM CYSTINE TO ASPARTATE. YOU'LL KNOW THAT THAT'S A MUCH LESS KNEW CLEE FILLING AMINO ACID, MUCH MORE COMMON AMINO ACID, VERY BIG CHALLENGE, AND WE TOOK EVERYTHING WE KNEW FROM ABOUT SIX YEARS OF WORK AND APPLIED TO COVALENT AS PAR TIED -- THE BEST WE COULD DO WAS A 5% CONVERSION. THAT IS WHERE WE SORT OF MAXED OUT. SO WE BEGAN TO TAKE A STEP BACK AND SAID LET LEARN FROM THESE RESIDUES THAT HAVE NUCLEOPHILLIC POTENTIAL AND MAYBE WE CAN CARRY THAT FORWARD. I'LL TELL YOU ABOUT THE ONE IS THE SERINE. THE ARGININE, WE'RE STILL WORKING HEAVILY ON THE ASPARTATE. SO THE WARHEAD THAT WORKED THE BEST FOR SERINE IS THE STRAINED", "id": "vof7x8r_ZUA_35", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  THAT STRAIN, OF COURSE, WE HAVE THE CHANCE TO RELEASE A LOT OF ENERGY ONCE THE SERINE ATTACKS AND RELEASES THE ALCOHOL. THAT INSPIRATION CAME FROM THESE NATURAL PRODUCTS THAT END UP BUYING COVALENTLY OFTEN IN THE ACTIVE SITE OF PROTEINS. SO THE PROTEASOME OPENS UP THIS BETA LACKTONE, THIS SERINE OPENS UP THIS LACTONE, AND ASERINE -- SO THESE ALL SEEMED LIKE THEY HAD THE REQUISITE CHEMISTRY. THE PROBLEM WAS THAT THESE SERINES AND -- ARE THE ACTIVE SITE RESIDUE, WHICH HAVE VASTLY ACCELERATED POE POTENTIAL FOR NUCLEOPHILIC ATTACK SO WE DIDN'T KNOW WHETHER WE COULD TRANSFER THAT TO THE SERINE THAT CAME UP IN THIS ONCOGENE. SO WE PUT THE WARHEAD ON MANY,", "id": "vof7x8r_ZUA_36", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  THE FIRST ONE THAT SHOWED REALLY, REALLY ENCOURAGE BEING ACTIVITY WAS THIS FUSED BETA LACK TONE, AND THIS ELEMENT, I WON'T TALK TOO MUCH ABOUT, BUT I'LL SHOW YOU SOME ADVANCES ON THAT IN THE NEXT FEW SLIDES. THIS IS THE PART THAT'S BINDING REVERSIBLY IN THE SWITCH 2 POCKET AND PRESENTING THIS BETA LACK TONE. AND IT LABELS 100% TO PRETTY HIGH CONCENTRATIONS, BUT IT GIVES US GOOD ENCOURAGING ACTIVITY. BUT THE MOST SURPRISING THING ABOUT THESE WEAKER NUCLEOP HRK ILES AND THESE WARHEADS IS THAT THEY'RE INCREDIBLY SENSITIVE TO CHANGES. IF WE JUST SWAPPED THE CARBON, EVERYTHING ELSE WAS THE SAME, WE GET ALMOST NO -- OR IF WE ADDED A SINGLE METHYL GROUP TO THE RING, WE GET VERY LITTLE REACTIVITY. AGAIN METHYL GROUP ON THE DEAD", "id": "vof7x8r_ZUA_37", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  NOW WE DOES THIS SAR IS JUST TOF EXEMPLIFY HOW TIGHT EVERYTHING HAS TO BE TO BE POISED FOR THE ATTACK OF SOMETHING THAT IS NOT AS NUCLEOPHILIC AS CYSTINE. SO THAT TAUGHT US THAT WE COULD HAVE A LOT OF FRUSTRATING MOLECULES THAT DON'T DO ANYTHING BUT WE KEEP MAKING THE SMALL CHANGES BECAUSE IF YOU WERE RIGHT HERE, YOU WOULD THINK YOU DIDN'T HAVE ANYTHING, BUT AS SOON AS YOU MAKE THE OPPOSITE ONE, YOU'RE READY TO GO. AND SO WE SOLVED THE CRYSTAL STRUCTURE OF THIS BOUND IN THE PROTEIN, AND THIS IS ONLY THE SECOND COVALENT STRUCTURE TO A RAS ISOFORM, NOW THE SERINE, AND THIS ONE SHOWED THE SERINE MAKING AN ESTER BOND AND THEN IT OPENED UP THE ALCOHOL.  IT MAKES REHI BEAUTIFUL CONTACTS WITH THE REST OF THE POCKET, INCLUDING IMPORTANT ELEMENT LIKE THIS CONSERVED LYSINE THAT'S NOT MUTATED IN THE CANCER, BUT IT'S IMPORTANT POSSIBLY FOR THE CHEMISTRY.", "id": "vof7x8r_ZUA_38", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  SO WITH THAT, WE FOUND THE RIGHT WARHEAD, NOW THE QUESTION IS, I TOLD YOU THAT WE COULDN'T CHANGE THE WARHEAD VERY MUCH, RIGHT? EVERY TIME WE CHANGED IT, WE LOST ACTIVITY. SO HOW DO YOU IMPROVE SOMETHING IF YOU CAN'T TOUCH THE WARHEAD, AND THATLY COMES FROM REVERSIBLE AFFINITY IMPROVEMENTS. SO THIS WAS THE RATE OF REACTION OF THIS FIRST MOLECULE. YOU CAN SEE IT'S GOT ABOUT A 50 MINUTE TIME TO GET TO HALF OCC PAN SEAL. AND IF WE MAKE CHANGES THAT CAME FRP A COMPANY CALLED MAROT TRK I THAT HAD MADE MEARN, MANY OPTIMIZATIONS OF THIS FOR REVERSIBLE BINDING TO OTHER ALLELES, AS WE MAKE THIS CHANGES LIKE THIS FIVE MEMBERED RING -- YOU SEE IMPROVEMENT, BIG JUMP. AND THEN MAKING THIS UNSATURATED, WE GET AN EVEN BIGGER JUMP. SO THE GREAT PART IS, NOW WE CAN", "id": "vof7x8r_ZUA_39", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  FOR THESE RAS LIGANDS, BECAUSE SO MUCH KEM ST CHEMISTRY IS BEI, WE'RE GETTING LOTS OF IDEAL CONTACTS WITH THIS ONE POCKET. AND SO THAT IS WHAT WE WERE ABLE TO LEVERAGE, AND NOW WHEN WE GO INTO CELLS, WE CAN SEE THAT THOSE EARLY MOLECULES LIKE 3 AND 4 THAT DON'T LABEL VERY FAST, THEY DON'T BLOCK RAS GTP LEVELS, BUT THE RAPID ON MOLECULE DOES VERY WELL, AND YOU CAN SEE THAT IN A PANEL OF G12S -- THE FOSSIL IRC LEVEL ARE NOT ACCEPTSTIVE. SO AGAIN SH IT'S THIS MUTANT-SPECIFIC ONLY FOR THE SERINE. REALLY, REALLY ENCOURAGING FOR OUR ABILITY TO DIAL IN THESE MUTANTS. AND ONE THING I SHOULD HAVE SAID IS THERE'S A WHOLE OTHER REASON TO GO AFTER G12S, AND THAT IS THERE ARE GENETIC DEFECTS CALLED RASOPATHIES THAT OCCUR IN", "id": "vof7x8r_ZUA_40", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  SYNDROME, ONE OF THOSE RASOPATHIES, THE DOM NAPT DOMINT MUTATION THAT COMES IS IN H RAS, AND IT'S FROM THE G12 TO S ALLELE. SO ALMOST ALL THOSE PATIENTS HAVE THAT SINGLE RESIDUE CONVERSION. SO IF WE CAN CONVERT THIS MOLECULE, RIGHT NOW IT DOESN'T INHIBIT H RAS AND THE G12S STATE BECAUSE IT HAS SORT OF A PROBLEM WITH ONE OF THE RESIDUES THAT'S DIFFERENT BETWEEN H AND KRAS, BUT IF WE COULD DO THAT, THEN WE WOULD HAVE A THERAPY THAT WOULD INHIBIT JUST THE DISEASE-CAUSING MUTANT THAT IN THOSE PATIENTS IS IN EVERY ONE OF THEIR CELLS. SO THAT'S  THE DIRECTION THAT'S TAKING. ONE QUESTION YOU MIGHT BE THINKING IS, NOW THAT WE'VE GONE AFTER A LESS REACTIVE RESIDUE, WHEN WE PUT THIS MOLECULE INTO CELLS, DOES IT POTENTIALLY REACT", "id": "vof7x8r_ZUA_41", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  SO WE DID A PROTEOMICS EXPERIMENT, WHERE YOU CAN TAKE A QUICK VERSION OF THE MOLECULE AND COMPETE IT OFF WITH A NON-CLICK VERSION AND THEN LOOK AT WHICH PROTEINS LABELINGS ARE COMPETED, AND THIS IS IN A G12S HOMOZYGOUS CELL LINE. YOU SEE KRAS COMES UP THAT'S COMPETED BY THIS DRUG AND THERE'S ONLY THREE OTHER ENZYMES, ONE OF THEM IS A PHOSPHOLIPASE THAT HAS ONE OF THOSE ACTIVE SERINES, SO THAT'S KIND OF AN EXPECTED TARGET. BUT OVERALL THIS IS THE KIND OF SELECTIVITY WE SEE ON THE DRUGS. WE'RE ABLE TO GET SELECTIVE BINDING WITH ONLY LIMITED NUMBERS AFTER TARGETS. OKAY. WE ALSO TOOK WHAT WE LEARNED ON THE SERINE AND APPLIED TO ARGININE, AND G12R IS A VERY", "id": "vof7x8r_ZUA_42", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  IT OCCURS ACTUALLY ONLY IN -- DOMINANTLY IN PANCREATIC CANCER. AND BECAUSE THE ARGININE HAS THESE TWO, ZHANG DESIGNED A DOUBLE WARHEAD THAT WOULD REACT WITH EACH AMINE AND MAKE A FIVE MEMBER MORE UNUSUAL AMINO ACIDS IN A MUTANT-SPECIFIC WAY. THE BIG ONE WE'RE AFTER, OF COURSE, IS THE ASPARTATE AND WE'RE WORKING VERY HARD ON THAT RIGHT NOW AND THERE'S SOME ENCOURAGEMENT, I WOULD SAY, THAT'S LOOKING PROMISING. SO LET'S GO BACK A MOMENT, IN THE LAST FEW MINUTE, TO THE CURRENT CLINICAL PICTURE OF THE G12C PATIENT. AND THERE'S KIND OF TWO KINDS OF ISSUES GOING ON. ONE IS, THERE ARE A NUMBER OF PATIENTS THAT ARE REALLY", "id": "vof7x8r_ZUA_43", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  THEY HAVE THE MUTATION, THE DRUG HITS THE TARGET, BUT SOMETHING IN THE CELL SIGNALING IS NOT SENDING THOSE CELLS TO APOPTOSIS AND NOT SHRINKING THE TUMORS. THE OTHER PROBLEM IS PATIENTS THAT ARE OVER HERE, THEY RESPOND, BUT THEN THEY BECOME RESISTANT. CLASSIC TARGETED THERAPY PROBLEM WE KNEW ALL THE WAY FROM THE BEGINNING OF GLIVAC. AND SO WE'D LIKE TO TREAT BOTH OF THOSE SCENARIOS, AND THERE'S A LOT OF COMBINATION THERAPIES GOING ON RIGHT NOW TO TRY TO DEAL WITH THAT. BUT WE DECIDED WE WANTED TO TAKE A DIFFERENT APPROACH AND SEE IF WE COULD COME UP WITH A DIFFERENT STRATEGY. AND TO TELL YOU ABOUT THAT STRATEGY, I WANT TO TELL YOU A LITTLE BIT ABOUT THE RESISTANT SPECTRUM THAT IS COMING OUT OF THE PATIENT. AND SO THIS IS A PANEL THAT HAS FOUR PATIENTS LISTED. THE TOP PATIENT IS A -- ALL OF THEM RESPONDED AND THEN RELAPSED. THE FIRST PATIENT RESPONDED AND", "id": "vof7x8r_ZUA_44", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  KRAS G12C ALLELE, THIS Y96C, THAT DOESN'T LET THE DRUG BIND. SO THAT'S LIKE A BCRAABLE MUTATION THAT PREVENTS GLIV AC FROM BINDING. THAT'S KIND OF WHAT WE ANTICIPATED. BUT ACTUALLY THE MOST COMMON KIND OF CHANGE IS SHOWN HERE, ON THESE THREE PATIENTS, WHERE THE FIRST COLUMN IS THE CT DNA BEFORE THERAPY, AND THEN AFTER RELAPSE. AND WHAT YOU SEE IS A WHOLE SLEW OF MUTATIONS, AND IF YOU CAN READ THE LABELS, THEY'RE IN THE RAS PATHWAY, INCLUDING LIKE BRAF MUTATIONS. SO THEY'RE ADDICTED TO THE KRAS PATHWAY, AND YOU PICK UP NEW MUTATIONS IN THE RAS PATHWAY TO OVERCOME THE DRUG. BUT WHAT'S INTERESTING IS YOU STILL EXPRESS KRAS G12C. IT'S NOT GOTTEN RID. AND SOMETIMES, THERE ARE MULTICOPY AMPLIFICATIONS OF THE G12C ALLELE. SO WHAT THAT LOOKS LIKE IS, WE", "id": "vof7x8r_ZUA_45", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  GIVE THEM A G12C INHIBITOR. IT GOES INTO THE TUMOR. BINDS COVALENTLY. SHUTS OFF SIGNALING, AND THEN MUTANTS OCCUR DOWNSTREAM AND THE PATHWAY IS BACK ON. SO EVEN THOUGH THE PROTEIN IS THERE, THE DRUG IS STILL BINDING TO IT, IT DOESN'T MATTER BECAUSE THE SIGNALING IS STILL ON. SO WHAT WE BEGAN TO DO, AND I COLLABORATED WITH MY COLLEAGUE CHARLIE CRAIK ON THIS, IS TO TRY TO LEVERAGE THE COVALENT NATURE OF THE G12C DRUG -- TO BRING IN THE IMMUNE SYSTEM TO ATTACK THAT TUMOR. SO EVEN THOUGH THERE'S NO SIGNALING INHIBITION OR DIMINISHED SIGNALING INHIBITION, WITH KE CREATE A NEOANTIGEN OUT OF THE NEOCONGREGATE AND DOES IT GET PRESENTED AND INTO THE -- I KNOW THAT'S A COMPLICATED APPROACH BUT WHAT I'M GOING TO", "id": "vof7x8r_ZUA_46", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  CLASS 1 MHC, THE G12C PEPTIDE IS THERE, RED IS THE CYST 10, AND THE DRUG IS ATTACHED, A CAME FROM THE DRUG BINDING TO G12C AS I'VE SHOWN YOU AND NOW WE'RE GOING TO MAKE AN ANTIBODY THAT RECOGNIZED -- TO MAKE A BISPECIFIC T-CELL ENGAGER TO KILL THE CELL. SO THE FIRST QUESTION IS, DOES THIS PEPTIDE WITH THE DRUG STILL BIND TO MHC? AND WE CAN DO THAT BY A STABILIZATION ASSAY AND IF WE GIVE THE -- IF WE DON'T GIVE ANY PEPTIDE, THIS IS A TAP DEFICIENT CELL LINE, MHC COMES UP TO THE SURFACE AND IS NOT STABLE IF IT DOESN'T HAVE PEPTIDE, SO EXOGENOUS PEPTIDES CAN BE ADDED. WE HAVE A POSITIVE -- THE KRAS PEPTIDE, STABILIZED, DRUG CONGREGATE, SO WE KNOW IT BIND. THEN CHARLIE'S LAB MADE -- THAT", "id": "vof7x8r_ZUA_47", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  SUBSEQUENT ANTIBODIES WE'VE MADE RECOGNIZED DRUG, THE ACTUAL PEPTIDE SEQUENCE AND THE MHC. AND THEN WE CAN ASK, DOES THIS SYSTEM ALLOW KILLING OF A CELL THAT HAS A BYPASS RESISTANCE MECHANISM. SO HERE'S THE G12C SENSITIVE CELL LINE WITH THE DRUG AND THE BY SPECIFIC T -- NOW IF WE PUT IN GLYCINE 12 TO VAI LEAN, THE DRUG CAN'T TOUCH THAT, SO IT INHIBITS THE SIGNALING PRODUCED BY THE INHIBITOR, BUT NOW WE STILL GET EQUIVALENT KILLING. SO WE'VE FOUND A WAY TO KILL THE CELL INDEPENDENT OF ITS INHIBITION OF SIGNALING. AND THAT, WE THINK, COULD BE A GREAT WAY TO CROSS THE SORT OF BYPASS MECHANISMS THAT TUMORS MIGHT DEVELOP TO AVOID KILLING BY THESE DRUGS. AND BECAUSE THE IMMUNE SYSTEM IS SO POWERFUL AND T-CELLS CAN HAVE", "id": "vof7x8r_ZUA_48", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  DEAL WITH HETEROGENEITY AND A WHOLE NUMBER OF ISSUES THAT WE KNOW ARE EMERGING IN THE CLINIC. AND THEN LAST SLIDE JUST SHOWS THAT WHAT I'VE TOLD YOU IS WE'VE GOT OH WAY TO BRING IN A T-CELL TO KILL A TUMOR CELL, BUT THERE'S A LOT OF INTEREST IN RADIO LIGAND THERAPY AND MIKE EVANS, ONE OF MY COLLEAGUES WHO'S REALLY AN EXPERT IN THIS AREA, TO DO THAT KILLING AS WELL, AND THIS WON'T REQUIRE THAT A T-CELL IS ACCESSED IN THE TUMOR ENVIRONMENT. IT COULD BE IN THE DESERT AND THIS APPROACH SHOULD STILL WORK. SO WE'RE REALLY INTERESTED AS I POINTED OUT AT THE VERY BEGINNING TRYING TO LEVERAGE TARGETED THERAPIES AND IMMUNE THERAPIES TO TRY TO SORT OF SHORT-CIRCUIT A LOT OF THE BYPASS MECHANISMS THAT THE TUMORS SEEM TO ALWAYS HAVE READY TO AVOID NEW DRUGS. SO WITH THAT, I'LL JUST TELL YOU THAT TWO FANTASTIC PEOPLE IN THE LAB THAT DID THE FIRST G12C WORK", "id": "vof7x8r_ZUA_49", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  WE LEVERAGED BEAUTIFUL SCREENING APPROACH IN THE SMALL MOLECULE DISCOVERY CENTER. AND THE G12R, HE ALSO DEVELOPED THAT WARHEAD WITH THE CURRENT POSTDOC AND A ROTATION STUDENT ANDREW ECKER AND THEN THE COVALENT DRUG RECOGNITION WAS REALLY A COLLABORATION WITH CHARLIE CRAIK'S LAB AND THE RADIO LIGAND THERAPY IS WITH MIKE EVANS AND HIS POSTDOC, APURVA. SO THANK YOU VERY MUCH FOR YOUR ATTENTION. [APPLAUSE] WHAT IS THE MECHANISM FOR GLN61 AND ALA59 MUTATIONS?", "id": "vof7x8r_ZUA_50", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  NEOPLASMS. >> GIVE ME THE TWO NUMBERS\nAGAIN IN N RAS? >> WELL, IF THE FONTS ARE CORRECT, GLN61. >> YEAH, GLUTAMINE 61. >> AND ALA59. >> OH, WOW. THOSE ARE FASCINATING. ALA59, A TO T, WAS I THINK ACTUALLY DISCOVERED HERE AT THE NIH IN ONE OF THE VIRAL ISOLETTES. AND THAT A TO T MUTATION IS ONCOGENIC IN THE CONTEXT OF GLYCINE 12 TO SERINE MUTATION, AND EXACTLY WHY THE CAUSES MUTATION ONCOGENESIS IS NOT EXACTLY CLEAR, BUT IT WAS A FANTASTIC SORT OF POINT BECAUSE THE THREEANINE BECOMES PHOSPHORYLATED BY THE KRAS GTP,", "id": "vof7x8r_ZUA_51", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  LITTLE BIT OF A DETOUR IN THE RAS HISTORY, BECAUSE PEOPLE THOUGHT IT MIGHT HAVE BEEN THAT RAS WAS A KINASE, BECAUSE OF THAT THREEANINE. BUT GLUTAMINE 61 IS A -- IS LIKE THE MOST SEVERE MUTATION YOU CAN FIND BECAUSE IT LOSES ONE OF THE CATALYTIC BASES FOR INTRINSIC HYDROLYSIS AND GAP HYDROLYSIS. SO IT'S FULLY LOCKED IN THE GTP STATE, AND THAT'S THE COMMON MUTANT IN N RAS, WHICH I DIDN'T TALK ABOUT TOO MUCH. >> OKAY. WHAT ROLE DOES THE RATE OF KRAS TURNOVER PLAY IN THE SUCCESS OF THESE COVALENT INHIBITORS? >> GREAT QUESTION. I BELIEVE ITS TURNOVER IS VERY IMPORTANT BECAUSE WE KNOW THEY ONLY BIND TO THE GDP STATE. THE APPROACH THAT I -- THE EXPLANATION I GAVE YOU WAS THAT INTRINSIC HYDROLYSIS IS RESPONSIBLE FOR THE TURNOVER. BUT IN WORK FROM SLOAN", "id": "vof7x8r_ZUA_52", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  ALTERNATIVE GAP PROTEIN FROM THE ONE I SHOWED THAT SEEMS TO SUPPORT HYDROLYSIS. SO THERE MIGHT BE ALTERNATIVE GAPS. THEY'RE HARD TO FIND IN THE CELL. SO TURNOVER IS VERY IMPORTANT. I'D SAY WE DON'T FULLY UNDERSTAND ALL THE WAYS IT CAN GET TURNED OVER. >> OKAY. YOU GO AND I'LL HAVE ONE MORE FROM ONLINE. >> THEUP FOR THAT\nBEAUTIFUL TALK, AND THE LAST PART WITH THE T-CELL TARGETING BLEW MY MIND. BUT DOES THAT HELP WITH THE, LIKE, CLASSICAL VIEW OF EVASION OR RESISTANCE WHERE THE MUTATION IS IN KRAS AND BLOCKING THE BINDING AS YOU SAW SOME -- >> EXCELLENT POINT. THAT FIRST PATIENT IN THE PANEL THAT GOT THE MUTANT THAT DOESN'T ALLOW THE DRUG TO BIND, THAT APPROACH WOULDN'T WORK. TOTALLY RIGHT. THANK YOU FOR POINTING -- YEAH, THAT WOULD AVOID, YEAH. >> THANK YOU FOR THAT\nLECTURE.", "id": "vof7x8r_ZUA_53", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  COMPLICATED UNDERSTANDABLE. I REALLY, REALLY ENJOYED THAT VERY MUCH. WHAT I'M STILL NOT UNDERSTANDING IS, WHY IS THE KRAS 12, G12, A TARGET FOR MUTATION? IS IT SOMETHING ABOUT THE CHROMOSOME STRUCTURE, AND WHY IS IT DIFFERENT IN DIFFERENT TISSUES? >> GOSH. [INAUDIBLE] >> I'LL GET TO THAT. THIS IS LIKE THE MORE FUNDAMENTAL BIOLOGY QUESTION THAT HAS BEEN PERPLEXING FROM THE 80s. AND I JUST CAME FROM A RAS MEETING UP IN PHILADELPHIA, AND IT WAS THE TOPIC OF DISCUSSION. SO IT JUST TELLS YOU IT'S A GREAT, GREAT QUESTION. I THINK THE SUBTLE DIFFERENCES IN THE AMINO ACIDS AND HOW THE CYCLING IS AFFECTED CREATES A", "id": "vof7x8r_ZUA_54", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  AND BECAUSE FIRST TIME THE CELL IS EXPOSED TO THE ONCOGENE, IT HAS TO DEAL WITH THAT STRESS AND TURN ON THE P53 RESPONSE TO SURVIVE LONG ENOUGH TO GET THE NEXT HITS. I THINK WE THINK THAT A LOT OF THAT SPECIFICITY IS CONTROLLING THAT. SO OTHER BACKGROUND MUTATIONS ARE IMPORTANT. IT'S KIND OF WHY WE DON'T SEE Q MUTANTS IN K-RAS IT'S PROBABLY TOO STRONG OF A SIGNAL. AND THEN THE CELLS DIE. AND THEY CAN'T BE AROUND. IT'S STILL A FASCINATING QUESTION. >> LOTS TO BE DONE. >> YES, YES. >> THANK YOU SO MUCH. >> ONE OVER HERE. >> OH, YOU ASKED -- G12V,\nYES. THERE ARE -- >> IS THAT THE SAME THING? >> NO, IT'S A LITTLE\nDIFFERENT. IT'S CLOSE. BUT THE G12V CAN BE TARGETED AND ONE OF THE DRUGS I DIDN'T TALK ABOUT, I SHOULD HAVE A SLIDE ON ALL THE NEW DRUGS THAT ARE COMING OUT. THEY'RE COMING OUT VERY QUICKLY. IS A REVERSIBLE KRAS BINDER", "id": "vof7x8r_ZUA_55", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  AT LUNCH WITH SOME OF THE TRAINEES. AND THAT ALLELE CAN BE BOUND BY THAT MOLECULE. NOW, WHAT THE SELECTIVITY IS GOING TO BE AND ON ALL OF THAT, BUT WE DO HAVE A GRIP ON THAT NOW, I WOULD SAY. >> REALLY FANTASTIC TALK. THANKS FOR SHARING. I THOUGHT THE FORMATION OF THE SERINE ESTER WAS REALLY FASCINATING. I WAS WONDERING IF YOU HAD SEEN ANY EVIDENCE THAT THAT ESTER COULD BE RESOLVED BY EITHER DIFFERENT RESIDUE WITHIN THE SAME PROTEIN, LIKE ASYL TRANSFER OR WHETHER A DIFFERENT PROTEIN COULD TAKE UP THAT ESTER. >> YEAH, THAT'S A GREAT QUESTION. WE -- THANKS. WE THOUGHT THAT THIS LYSINE WOULD ATTACK THAT EXACTLY AND THEN TRANSFER, WE NEVER GOT EVIDENCE FOR THAT. SO IT IS FASCINATING. IT WOULD BE KIND OF COOL", "id": "vof7x8r_ZUA_56", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  IT, TO MAYBE MAKE A GAP PROTEIN AND TRY TO GET THE GAP TO KIND OF STICK ON TO IT, MAYBE THAT'S WHY -- I HADN'T THOUGHT ABOUT THAT. THAT'S A GOOD IDEA. >> THANK YOU. >> SO THAT WAS\nFASCINATING. I HAD A QUESTION ABOUT SORT OF YOUR -- THE GENERAL SORT OF STRATEGY OF COVALENT LABELING OF CYSTINES, BECAUSE FROM A VERY -- PERSPECTIVE, IF I HADN'T SEEN THIS WORK, I WOULD HAVE THOUGHT IT'S IMPOSSIBLE TO GET A CYSTINE-REACTIVE AGENT IN A CELL SELECTIVELY REACTING WITH A PROTEIN. BUT CLEARLY YOU CAN DEEM THAT REACTIVITY BY OTHER PARTS OF THE MOLECULE AND MULTIPLE ENGAGEMENT IS WHAT THEN MAKES THAT REACTION POSSIBLE. SO I GUESS WHAT I'M SAYING IS THAT DO YOU THINK THAT YOU OR THE FIELD HAS EXPLOITED THIS ALMOST SORT OF CAGED REACTIVITY AND CONTROLLING IT WITH", "id": "vof7x8r_ZUA_57", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  IMPOSSIBLE? >> THAT'S A GREAT\nQUESTION. I THINK THERE'S MULTIPLE LAYERS OF THAT. THE FIRST LAYER IS THAT THIS IS THE WARHEAD, BUT THIS PIECE IS PRETTY BIG, AND SO THERE MIGHT BE A LOT OF CYSTINES THAT WOULD REACT WITH THIS BUT BECAUSE IT'S ATTACHED TO SOMETHING STERIC, IT DOESN'T GET THERE. SO THAT'S ONE THING. THE OTHER THING IS, IT CAN BE TUNED, AND THE DIFFERENCE BETWEEN THIS MOLECULE AND THIS MOLECULE IS KIND OF INTERESTING. YOU SEE THAT THIS WARHEAD HAS ALPHA FLUORINE, WHICH DETUNES IT. THE REASON THEY NEEDED TO DO THAT IS BECAUSE THIS MOLECULE, THEY HAD THIS CYANOGROUP THAT WE ACTUALLY STILL DON'T QUITE UNDERSTAND, REACTIVATES THIS WARHEAD. SO IF YOU PUT THIS OVER HERE, THIS DRUG WOULD BE SOMEWHAT PROMISCUOUS. SO IN ORDER TO KEEP THAT, THEY NEEDED TO DETUNE. SO WE'RE STARTING TO GET REALLY", "id": "vof7x8r_ZUA_58", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  SELECTIVITIES GOING. AND I FORGOT THE THIRD THING I WAS GOING TO HIGHLIGHT. IT'S A LOT OF PRE-ORGANIZATION. ONE OF MY FAVORITE ASPECTS IS THIS MOLECULE. THIS BOND CAN BE RESOLVED. THEY'RE ATROP ISOMERS, DOUBLE DIORTHO SUBSTITUTION. THIS PARTICULAR ONE WHERE THE PHENOL IS OUT OF THE RING IS THE ACTIVE ONE. IF YOU MADE IT IN THE BACK OF THE RING, IT DOESN'T TOUCH RAS. UNBELIEVABLE. SO THESE THINGS ARE LIKE -- YEAH. >> HI, KEVAN, GREAT TALK. I WAS CURIOUS ABOUT LINKING THE TWO PARTS OF YOUR TALK. YOU TALKED ABOUT SOME OF THE REVERSIBLE INHIBITORS THAT ARE ALL COMING ONLINE. WHAT DO YOU THINK ABOUT THE PROSPECTS OF USING THOSE AS DEGRADERS TO PRIME UP THE IMMUNE SYSTEM FOR RECOGNITION OF G12B OR, YOU KNOW, SOME OF THESE ONES", "id": "vof7x8r_ZUA_59", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  KNOW, THEY'RE RARE BUT THERE ARE T-CELLS THAT WILL ENGAGE THOSE. >> YEAH, YEAH. MY FAVORITE -- YEAH, MY FAVORITE IDEA IS BASICALLY -- SO THE DEGRADER IDEA IS GREAT BECAUSE IF YOU INDUCE DEGRADATION AND YOU HAVE MORE PEPTIDES THAT -- THE COOL THING IS, WE WANT TO DO -- WE HAVEN'T DONE YET, BUT YOU'LL GET THIS, IF WE HAD A DEGRADER MOLECULE, THEN BASICALLY THIS PIECE WOULD LOOK LIKE THE PART THAT BOUND TO RAS BUT THEN THERE WOULD BE THE PRO T AC VERSION SO IT WOULD BIND TO SOME E3 RECRUITER. SO THEN WE COULD MAKE THIS THING NOT AN ANTIBODY OF THE DRUG BUT JUST TO THE -- JUST TO THE E3 RECRUITER. THEN YOU CAN USE IT FOR ANY COVALENT PRO TAC AND YOU WOULD HAVE THIS STRATEGY EVERY TIME. SO THAT'S GREAT.", "id": "vof7x8r_ZUA_60", "title": "1. What Makes Healthcare Unique?", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:38:19Z": "2020-10-22T19:38:19Z"}, {"text": "  MORE. BUT FIRST THE CME CODE, 44447? OKAY, GOOD. OKAY. SO GENERATING ANTI-DRUG MHC ANTIBODIES IS PROBABLY QUITE DIFFICULT. CAN YOU MAKE MHC RESTRICTED DISPLAY LIBRARIES THAT MAY BE BETTER FOR SCREENING DIFFERENT DRUG PEPTIDE ADD DUCTS? >> GREAT IDEA. WE HAVEN'T DONE THAT, BUT THIS CAME OUT OF A NAIVE LIBRARY, SO YEAH, IT WOULD BE HARD. THOSE ARE TYPICALLY VERY HARD. BUT BECAUSE THE DRUG IS SUCH A BIG EPITOPE, THAT KIND OF WAS THE SECRET SAUCE. >> ALL RIGHT. LET THANK KEVAN FOR A FANTASTIC TALK.", "id": "vof7x8r_ZUA_61"}, {"text": "  In 1964, the president of the American Cancer\u00a0\nSociety went on television to say this: \u201cAt last, the relationship\u00a0\nbetween heavy cigarette smoking\u00a0\u00a0 and the increased incidence of lung cancer in\u00a0\nman has been established as a fact. There can\u00a0\u00a0 no longer be any doubt that the heavy smoking\u00a0\nof cigarettes is a serious health hazard\u201d. Earlier that year, the US Surgeon\u00a0\nGeneral published this report, over\u00a0\u00a0 300 pages explaining the overwhelming evidence\u00a0\nshowing how smoking causes different diseases. Over the next few decades, the US kicked\u00a0\noff anti smoking public health campaigns,\u00a0\u00a0 made tobacco companies put\u00a0\nlabels on cigarette boxes,\u00a0\u00a0 and made 10 year olds across the country\u00a0\nlisten to cops tell us how bad drugs are. But getting to the point where the Surgeon\u00a0\nGeneral could say that smoking causes cancer\u00a0\u00a0 took a long time. Like by the 50s, we had lots\u00a0\nof studies pointing to a link between the two,\u00a0\u00a0 but not a causal relationship, but So how did we\u00a0\u00a0 get to the point where scientists could\u00a0\nsay that smoking for sure causes cancer?", "id": "DS97JV_o0Fs_0", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  The story starts in North America,\u00a0\u00a0 where Indigenous people have been using\u00a0\ntobacco medicinally for thousands of years. When Columbus came over, he and his crew saw the\u00a0\nTaino people in the Caribbean using tobacco smoke\u00a0\u00a0 like a kind of disinfecting air freshener,\u00a0\nand the snuff as a way to knock someone out\u00a0\u00a0 before surgery. As more Europeans came over,\u00a0\nthey noticed indigenous folks in modern day\u00a0\u00a0 Mexico using it as an antidiarrheal or as a wound\u00a0\nremedy. They also noticed that people in modern\u00a0\u00a0 day California would crush up tobacco leaves to\u00a0\nmake creams for s kin conditions like eczema. So the explorers took a bunch of tobacco seeds\u00a0\nand plants back to the Old World. And they went\u00a0\u00a0 wild with it. They used it for everything from\u00a0\ncoughs to worms to, and this is foreshadowing,\u00a0\u00a0 tumors. They were so enthusiastic about\u00a0\ntobacco because their list of drugs,\u00a0\u00a0 or pharmacopeia, was essentially useless,\u00a0\nso tobacco was hyped to high heavens.\u00a0\u00a0 Like finally, there was something that\u00a0\nmight actually do something helpful. One of the biggest converts was Jean Nicot,\u00a0\nthe French ambassador to Portugal and namesake\u00a0\u00a0 for tobacco\u2019s genus, Nicotiana. This dude was\u00a0\nobsessed with tobacco\u2019s medicinal properties\u00a0\u00a0", "id": "DS97JV_o0Fs_1", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of Nicot running his own ad hoc experiment\u00a0\nwhere he made a crushed tobacco paste,\u00a0\u00a0 rubbed it on some random man\u2019s tumor, and\u00a0\nwitnessed the tumor shrink...a little dubious. But that was about as experimental as doctors\u00a0\ngot back in the 1600s. Scholars could write\u00a0\u00a0 whatever they wanted about a plant, print it\u00a0\nin a book, and that just /became/ medicine. If\u00a0\u00a0 y\u2019all want to see an extreme example\u00a0\nof this by the way, Google Pliny the\u00a0\u00a0 Elder\u2019s Natural History. He was hundreds\u00a0\nof years before this, but the point stands. Now eventually someone made stuff\u00a0\nup about tobacco, and it wound up\u00a0\u00a0 in the 1612 printing of the Pharmacopeia\u00a0\nLondinensis. The textbook described tobacco\u00a0\u00a0 as a treatment for cold and lethargy.\u00a0\nAgain, with no evidence to back it up. And while It sounds unscientific to us today,\u00a0\nthat kind of pharmacology lined up with\u00a0\u00a0 popular concepts of medicine back then. European\u00a0\nhealers mostly subscribed to the humoral theory,\u00a0\u00a0 a system that said health came from the\u00a0\nbalance of four bodily fluids, or humors:\u00a0\u00a0 blood, phlegm, black bile, and yellow bile. You\u00a0\ngot sick when these humors were out of balance,\u00a0\u00a0", "id": "DS97JV_o0Fs_2", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  blood is making you sick, so let some out!\u00a0\nThey also believed in something called the\u00a0\u00a0 miasma model, which said infections and diseases\u00a0\nspread through invisible, bad smelling vapors\u00a0\u00a0 called miasmas. And since bad smells spread\u00a0\ndisease, good smells might prevent them.\u00a0\u00a0 This is how the plague doctor became a\u00a0\nthing \u2014 they\u2019d put nice smelling flowers,\u00a0\u00a0 potpourri, or tobacco in their beaks\u00a0\nand hoped it would keep them healthy. But if you didn\u2019t want to wear the whole get-up,\u00a0\u00a0 you could just smoke a pipe to\u00a0\nkeep yourself from catching plague. And this belief lasted for centuries.\u00a0\u00a0 You\u2019d find people in the 1800s smoking\u00a0\npipes in anatomy labs with the same logic.\u00a0\u00a0 They thought a smelly corpse carried\u00a0\ndisease so tobacco smoke would protect them. This combination of humoral theory\u00a0\nand miasma model also lent tobacco\u00a0\u00a0 to some of the dumbest treatments in medical\u00a0\nhistory. This textbook from 1747 recommended\u00a0\u00a0 blowing tobacco smoke in someone\u2019s ear to\u00a0\ntreat earache, or injecting tobacco for a\u00a0\u00a0 \u201ctwisting of the guts\u201d whatever that means (I\u00a0\ndon\u2019t know, it sounds like innuendo to me).\u00a0\u00a0", "id": "DS97JV_o0Fs_3", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  people in London regularly drowned in the\u00a0\nRiver Thames, so people needed to figure\u00a0\u00a0 out ways to revive them. The solution was tobacco\u00a0\nenemas. Literally it was a hose that let you blow\u00a0\u00a0 tobacco smoke up someone\u2019s butt. Supposedly, it\u00a0\nworked once and it became a thing after that. By the end of the 19th century, we\u2019d moved past\u00a0\nthe idea that tobacco was purely good for us.\u00a0\u00a0 The story starts getting more complicated\u00a0\nwhen we start talking about cancer. Superficial tumors can be easy to find,\u00a0\nso doctors have been diagnosing cancer for\u00a0\u00a0 thousands of years \u2014 way before modern\u00a0\nmammograms and biopsies. Hippocrates\u00a0\u00a0 was the first to name the disease\u00a0\nkarkinos which means crab in Greek. Nobody really knows why he called it that\u00a0\n\u2014 maybe because a lumpy tumor can be hard\u00a0\u00a0 like a crab shell. Regardless, 500 years\u00a0\nlater, a famous Roman doctor named Claudius\u00a0\u00a0 Galen dissected a breast tumor, saw all the\u00a0\nbranching blood vessels, and thought, \u201chuh,\u00a0\u00a0 I guess that is kinda like a crab\u201d.\u00a0\nAnd thus cancer got its modern name. But scientists didn\u2019t have any good evidence\u00a0\nfor the cause of cancer until the 18th century.", "id": "DS97JV_o0Fs_4", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  way more often than other men, so he proposed that\u00a0\nsomething about their job was giving them cancer. He didn\u2019t know what yet, but he kickstarted\u00a0\nthis idea that the cause of cancer\u00a0\u00a0 /could/ be found. And over the next century,\u00a0\nother doctors noticed more cases of mouth,\u00a0\u00a0 nose, and lip cancer among smokers. So at\u00a0\nthis point, some red flags are going up. But these were just anecdotes still. And lung\u00a0\ncancer wasn\u2019t even on the public health radar yet,\u00a0\u00a0 in part because smoking wasn\u2019t /that/ popular. Most American tobacco users preferred chewing,\u00a0\nsnuffing, or smoking a pipe, and they usually\u00a0\u00a0 didn\u2019t inhale the smoke. That\u2019s because the\u00a0\ntype of tobacco plant that\u2019s native to America,\u00a0\u00a0 Nicotiana rustica, is really strong \u2014 it\u2019s like\u00a0\n9% nicotine. But by the mid 1800s people started\u00a0\u00a0 asking for a milder tobacco. It\u2019s like if the\u00a0\nonly drink in America was a triple imperial IPA \u2014\u00a0\u00a0 Americans just wanted a Whiteclaw. Like, yeah,\u00a0\nit\u2019s less potent, but you can actually enjoy it.\u00a0 So farmers cultivated a few different species\u00a0\nand preparations that were less intense.", "id": "DS97JV_o0Fs_5", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  new tobacco was much easier to smoke, but\u00a0\ncigarettes weren\u2019t common yet because they\u00a0\u00a0 were expensive. It would take rollers a\u00a0\nfull minute to roll a cigarette by hand,\u00a0\u00a0 which made the final product more\u00a0\nof a luxury item than a commodity. Then in 1880, an American inventor\u00a0\nnamed James Bonsack filed a patent\u00a0\u00a0 for a cigarette machine that could roll\u00a0\n200 cigarettes a minute. Cigarettes became\u00a0\u00a0 affordable and available everywhere so\u00a0\nsmoking rates shot up in the late 1800s. But doctors didn\u2019t see lung cancer\u00a0\nshoot up right away for two reasons. The first: we still didn\u2019t know a ton about lung\u00a0\ncancer. We knew it existed before cigarettes got\u00a0\u00a0 popular, but diagnosing it was really difficult.\u00a0\nDoctors would\u2019ve confused it with tuberculosis or\u00a0\u00a0 other lung diseases, so your average physician\u00a0\nwasn\u2019t really familiar with lung cancer from\u00a0\u00a0 a clinical perspective. And scientific\u00a0\nresearch didn\u2019t have much to add \u2014 by 1890,\u00a0\u00a0 there were only 142 cases of lung cancer\u00a0\nin the entirety of medical literature. And even if you suspected one of\u00a0\nyour patients had lung cancer,\u00a0\u00a0 you couldn\u2019t confirm it until after\u00a0\nthey died and you did an autopsy.", "id": "DS97JV_o0Fs_6", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  was the only post-mortem lung cancer\u00a0\nstudy for a long time. The invention\u00a0\u00a0 of the X ray in the 1890s helped get more\u00a0\ndiagnoses while the person was still alive,\u00a0\u00a0 but until then, the diagnosis was\u00a0\ninconsistent and extremely rare. And this points to the second reason lung cancer\u00a0\nwasn\u2019t an emergency yet: statistics about death. Between the late 1890s and early 19-teens\u00a0\nwe were in this period where smoking was\u00a0\u00a0 getting way more popular, but not enough time had\u00a0\npassed for a ton of people to develop lung cancer. In 1878 the University of Dresden\u2019s\u00a0\nInstitute of Pathology estimated that,\u00a0\u00a0 of all of the cancers they\u2019d found\u00a0\non autopsy, only 1% were lung cancer. But the field of epidemiology\u00a0\nwas really young at that point,\u00a0\u00a0 and data collection wasn\u2019t standardized. So our\u00a0\nbaseline rates of lung cancer were really fuzzy. But a few years into the twentieth century and\u00a0\nthe data was overwhelming. Between 1922 and 1947,\u00a0\u00a0 deaths from lung cancer increased 15 times in the\u00a0\nUK. And between 1930 and 1950, deaths from lung\u00a0\u00a0", "id": "DS97JV_o0Fs_7", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  People blamed the increased cancer rates on\u00a0\npollution from cars, or maybe it was aftermath\u00a0\u00a0 of the 1918 influenza pandemic, or maybe men\u00a0\nwere inhaling dust from their industrial jobs. Either way, lung cancer\u00a0\nwas getting out of control,\u00a0\u00a0 and we had no good idea where\u00a0\nthe disease was coming from. And that\u2019s because cancer didn\u2019t behave\u00a0\nlike any disease we\u2019d beaten before.\u00a0 The humoral theory and miasma model of\u00a0\ndisease weren\u2019t getting us anywhere.\u00a0\u00a0 Medicine in mid nineteenth century Europe did\u00a0\nnothing at best, and could kill you at worst. The biggest change came in the late 1800s when\u00a0\nmicrobiologists like Louis Pasteur, Joseph Lister,\u00a0\u00a0 and Robert Koch showed that microscopic organisms\u00a0\ncause infectious disease. So tuberculosis is\u00a0\u00a0 caused by mycobacterium tuberculosis, and anthrax\u00a0\nis caused by Bacillus anthracis. Lil microbe guys. This idea was called the germ theory of disease,\u00a0\nand it was a huge deal because scientists could\u00a0\u00a0 finally identify the cause of a disease, which\u00a0\nmeant scientists could figure out treatments,\u00a0\u00a0", "id": "DS97JV_o0Fs_8", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  theory across allopathic medicine by the\u00a0\nend of the 19th century, and it became the\u00a0\u00a0 basis for the most effective public health\u00a0\ncampaigns in the early twentieth century. Medicine and public health based on germ\u00a0\ntheory worked so well that eventually,\u00a0\u00a0 people started living long enough to die from\u00a0\nchronic diseases like heart disease or cancer. And this shift in the leading cause of death\u00a0\nis part of something called the Epidemiological\u00a0\u00a0 Transition. It usually comes with an increase in\u00a0\nlife expectancy and decrease in birth rates too.\u00a0\u00a0 But importantly, it didn\u2019t happen everywhere\u00a0\nand it didn\u2019t happen all at once \u2014 just like\u00a0\u00a0 everything with public health, wealthy\u00a0\ncountries benefited more. To this day,\u00a0\u00a0 infectious diseases like tuberculosis are still\u00a0\nenormous health problems in African nations. So Germ theory was this powerful thing that worked\u00a0\ngreat against infectious diseases, but it couldn\u2019t\u00a0\u00a0 explain how chronic diseases worked. It wasn\u2019t\u00a0\nas simple as finding the single virus that caused\u00a0\u00a0 heart disease. Scientists would need a completely\u00a0\ndifferent approach to figure out cancer. So let\u2019s come back to the 1930s \u2014 doctors\u00a0\naround the world were seeing more cases\u00a0\u00a0 of lung cancer and they had to start with the\u00a0\nmost basic question: \u201care there any noticable\u00a0\u00a0", "id": "DS97JV_o0Fs_9", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  This kind of question lends itself to\u00a0\nsomething called a case control study,\u00a0\u00a0 which involves taking a group of people\u00a0\nwith a disease and a similar group of\u00a0\u00a0 people who don\u2019t have the disease and looking\u00a0\nfor any other differences between the groups. The researchers might look for\u00a0\ndifferences in medications they take,\u00a0\u00a0 exposure to other sick people,\u00a0\nor lifestyle habits like smoking. Scientists did a lot of these in the 20s\u00a0\nand 30s, so I\u2019ll give you just one example. In 1928, Lombard and Doering published a case\u00a0\ncontrol in the New England Journal of Medicine.\u00a0\u00a0 They sent questionnaires to hundreds\u00a0\nof patients with different cancers,\u00a0\u00a0 and sent the same questionnaire\u00a0\nto similar people without cancer. They asked some of the expected questions\u00a0\nlike how much tobacco do you smoke,\u00a0\u00a0 and how much salt do you eat? But they also\u00a0\nasked questions that seem completely out of\u00a0\u00a0 left field like how good are your teeth? Do you\u00a0\ntake laxatives? What\u2019s your \u201cconjugal state\u201d? Again, they had no idea where to\u00a0\nstart \u2014 they had to cast a wide net. Some of the questions showed\u00a0\nno differences between groups,\u00a0\u00a0 but the authors did notice that almost\u00a0\nall of their patients with lung, lip,\u00a0\u00a0 cheek or jaw cancers and 78.8% of all their\u00a0\ncancer patients were heavy smokers. Only 20%\u00a0\u00a0", "id": "DS97JV_o0Fs_10", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  And that seems like a huge difference, but case\u00a0\ncontrol studies still aren\u2019t the best evidence. They\u2019re academically interesting\u00a0\nand encourage more follow up study,\u00a0\u00a0 but they\u2019re retrospective \u2014\u00a0\nthey look back at patients who\u00a0\u00a0 have different lives full of confounding\u00a0\nvariables that could shape the results. Now while European scientists were\u00a0\nstarting to understand the epidemiology,\u00a0\u00a0 scientists in Latin America especially were\u00a0\ngathering pathological evidence \u2014 information\u00a0\u00a0 that helped them understand how the\u00a0\ndisease worked on a cellular level. In 1922, an Argentine scientist named Angel Roffo\u00a0\nopened the Institute of Experimental Medicine in\u00a0\u00a0 Buenos Aires and over the next few decades he\u00a0\ndedicated his career to studying lung cancer. Assuming that smoking did in fact cause\u00a0\ncancer, he focused on finding the chemicals\u00a0\u00a0 inside cigarettes that might cause cancer at\u00a0\nthe cellular level \u2014 mainly using rodents. And\u00a0\u00a0 as much as I love the visual of tiny mice smoking\u00a0\ntiny cigarettes, he used a different methodology. Roffo suspected that tar caused\u00a0\ncancer, not nicotine, so he came\u00a0\u00a0", "id": "DS97JV_o0Fs_11", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  then smeared them on rabbits\u2019 ears and\u00a0\nrecorded which formulas induced cancer. As expected, the tarry formulas induced\u00a0\ncancer while nicotine-only formulas didn\u2019t. It turned out it was these molecules\u00a0\nmade out of hydrogen and carbon that\u00a0\u00a0 were the primary carcinogens \u2014 the\u00a0\nchemicals that induce cancer in cells. Roffo\u2019s research inspired more\u00a0\nepidemiological studies in the 40s and 50s,\u00a0\u00a0 but he also got the attention of scientists\u00a0\nwho worked for the American tobacco industry. In an internal memo to the president\u00a0\nof the American Tobacco Company,\u00a0\u00a0 Roffo was described as the \u201cchief\u00a0\nprotagonist of the theory that there\u00a0\u00a0 is a causal relation between smoking\u00a0\nand cancer of the respiratory organs\u201d. He was Big Tobacco enemy number one. While they were worried about Roffo, he\u00a0\nwouldn\u2019t be a problem for too long. (not\u00a0\u00a0 because somebody assassinated him) but because he\u00a0\npublished his work mostly in German in the 1930s,\u00a0\u00a0 and scientific research published in German in\u00a0\nthat time has a complicated backstory thanks to\u2026. Nazi Germany hated tobacco \u2014 they hated anything\u00a0\nthat posed a risk to the future of the arian race.\u00a0\u00a0", "id": "DS97JV_o0Fs_12", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  so they implemented huge anti-smoking\u00a0\ncampaigns throughout the 30s and 40s. They banned smoking in public places like movie\u00a0\ntheaters, public transportation, and schools,\u00a0\u00a0 and banned the sale of cigarettes to\u00a0\npregnant women. They stopped short of\u00a0\u00a0 outright banning smoking though because\u00a0\nthey still needed those tax dollars. As part of the campaign, they also\u00a0\nfunded a ton of anti smoking research. Their first big study was published in 1939\u00a0\nby Franz Muller. You don\u2019t have to speak\u00a0\u00a0 German to recognize some of the familiar\u00a0\nfigures here \u2014 lung cancer is going up,\u00a0\u00a0 smoking rates are going up, let\u2019s see\u00a0\nif smoking plays a role in lung cancer. According to Google Translate, Muller\u00a0\nsays that the increase in smoking was\u00a0\u00a0 the single most important factor\u00a0\nin the increase of lung cancer.\u00a0\u00a0 He also pointed to a higher risk of heart\u00a0\ndisease, which we know today to be true. In 1941, the Nazis funded the creation\u00a0\nof a research center at the University\u00a0\u00a0 of Jena called the Institute for the Struggle\u00a0\nAgainst Tobacco Hazards, which is not subtle. Like they might as well have called\u00a0\nthe Institute for Confirmation Bias\u00a0\u00a0 if they already knew their research\u00a0\nwas going to say tobacco is bad.", "id": "DS97JV_o0Fs_13", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of lung cancer, only 3 were found in nonsmokers,\u00a0\nwhich was way fewer than in the control group. Other Nazi scientists would be the first to\u00a0\nmention the harmful effects of secondhand smoke,\u00a0\u00a0 or what they called \u201cpassive smoking\u201d. But all this German science\u00a0\ndidn\u2019t necessarily make a huge\u00a0\u00a0 impact on the overall state of smoking research. I read a paper from 2012 pointing out\u00a0\nthat Mueller\u2019s paper mostly repeated\u00a0\u00a0 claims made by another German paper\u00a0\nfrom 1931 \u2014 otherwise it was mostly a\u00a0\u00a0 review and case series. Not high quality research. Another historian pointed out that the 1943 paper\u00a0\nwas only cited a handful of times after the war.\u00a0\u00a0 Plus, the studies\u2019 methodologies received\u00a0\na good amount of criticism, since,\u00a0\u00a0 you know, the underlying motivation for\u00a0\nall these studies was explicitly racist. Hitler was quoted as saying tobacco was \u201cthe wrath\u00a0\u00a0 of the Red Man against the White Man\u00a0\nfor having been given hard liquor\u201d. Depending on which source you read,\u00a0\u00a0 the Nazi contribution to the science\u00a0\nbehind smoking was either \u201cthe single\u00a0\u00a0 defining moment in the scientific saga\u201d\u00a0\nor \u201cforgotten to the world of science\u201d.", "id": "DS97JV_o0Fs_14", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Imagine it\u2019s the late 1940s. Lung cancer\u00a0\nis now the most commonly diagnosed cancer\u00a0\u00a0 and second deadliest cancer after\u00a0\nstomach cancer. World War 2 was over,\u00a0\u00a0 so the governments of the UK and the US could\u00a0\nfinally put money into lung cancer research. This culminated in 1950 when 4\u00a0\nstudies came out that all claimed\u00a0\u00a0 smoking caused cancer. They\u00a0\nwere published in prestigious\u00a0\u00a0 journals and had the highest quality\u00a0\nepidemiological evidence seen to date. We\u2019ll go through them one\u00a0\nby one, starting in the US. In the May 27th issue of the Journal\u00a0\nof the American Medical Association,\u00a0\u00a0 Wynder and Graham published \u201cTobacco\u00a0\nSmoking as a Possible Etiological Factor\u00a0\u00a0 in Bronchogenic Carcinoma\u201d. That\u2019s jargon\u00a0\nfor smoking might contribute to lung cancer. They recruited over 600 patients\u00a0\nwith lung cancer into the study. First, they separated subjects by disease status:\u00a0\u00a0 they either had lung cancer or they didn\u2019t,\u00a0\nwhich meant they\u2019d be in the control group. Then the smokers in each group were classified\u00a0\nby their smoking habits \u2014 non smokers were those\u00a0\u00a0 who averaged less than one cigarette a day\u00a0\nwhile chain smokers smoked over 35 a day.", "id": "DS97JV_o0Fs_15", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  the researchers hypothesized that\u00a0\nlung cancer took years to develop,\u00a0\u00a0 so long term habits were more relevant\u00a0\nthan how much that person smoked last week. After running the numbers, they found that\u00a0\n96.5% of men with lung cancer were heavy\u00a0\u00a0 smokers or chain smokers while only 73.7%\u00a0\nof the general population smoked that much. They also saw something called a dose-response\u00a0\neffect, the more cigarettes a group smoked\u00a0\u00a0 corresponded to more cases of lung\u00a0\ncancer in that group. And this article\u00a0\u00a0 was so impactful because that dose response\u00a0\neffect was seen across a lot of participants\u00a0\u00a0 which made it a powerful result.\n \nBut this study alone still didn\u2019t\u00a0\u00a0 show that smoking causes cancer. As\u00a0\nWynder said himself, it was an \u201cimportant\u00a0\u00a0 factor in the induction of bronchogenic\u00a0\ncarcinoma\u201d, but he didn\u2019t say it was causal. In the same issue of JAMA, literally starting\u00a0\non the same page, Levin, Goldstein, and Gerhardt\u00a0\u00a0 published \u201cCancer and Tobacco Smoking: A\u00a0\nPreliminary Report\u201d, which used hospital\u00a0\u00a0 records to come to the same conclusion \u2014 it\u2019s a\u00a0\nfactor, but we can\u2019t show cause and effect yet. Also, they got \u201cwell actually\u201dd in a\u00a0\nletter to the editor a few months later.\u00a0\u00a0", "id": "DS97JV_o0Fs_16", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  date showing the link between cancer and\u00a0\nsmoking and haters still wrote mean comments. I bet they\u2019d fit in on YouTube. The third paper, \u201cTobacco Smoking\u00a0\nas an Etiologic Factor in Disease\u201d\u00a0\u00a0 was published in Cancer Research and used\u00a0\ndata from a veterans hospital in Illinois. For the last decade, the hospital made it\u00a0\nstandard practice to ask patients about\u00a0\u00a0 their smoking habits on intake. Then\u00a0\nthey\u2019d file the intake form with the\u00a0\u00a0 rest of the patient\u2019s medical record.. At\u00a0\nthe end of their data collection period,\u00a0\u00a0 the research group had 5,000\u00a0\npatient records to work with. That left them with 82 patients with lung\u00a0\ncancer and 73 with throat cancer which\u00a0\u00a0 they lumped together as respiratory cancers. They compared those patients\u2019 smoking habits to\u00a0\na control group of 522 patients with other tumors\u00a0\u00a0 and found that 85.4% of the throat\u00a0\nand lung cancer patients were smokers,\u00a0\u00a0 compared to 76.1% of the\u00a0\ncontrol group that smoked. The final study from 1950 came from these guys,\u00a0\u00a0 and they\u2019re my faves \u2014 Richard Doll and\u00a0\nAustin Bradford Hill who published \u201cSmoking\u00a0\u00a0 and Carcinoma of the Lung. Preliminary\u00a0\nReport\u201d in the British Medical Journal.", "id": "DS97JV_o0Fs_17", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  out why lung cancer was going up, and\u00a0\nneither of them knew where to start. Both of them were smokers and they weren\u2019t\u00a0\nautomatically convinced of the link between\u00a0\u00a0 smoking and cancer. They thought it could be cars,\u00a0\u00a0 coal, industry, or maybe smoking but these were\u00a0\nthe same ideas that people had 50 years earlier. In their study, they selected\u00a0\n20 hospitals in the UK,\u00a0\u00a0 and whenever anyone was admitted with\u00a0\nlung, stomach, colon, or rectal cancer,\u00a0\u00a0 an assigned person would come interview\u00a0\nthe patient with a questionnaire. Then that same person would\u00a0\nwalk through the same hospital,\u00a0\u00a0 looking for a patient of the same sex\u00a0\nand age, and ask them the same questions. Then after the patient was\u00a0\ndischarged from the hospital,\u00a0\u00a0 they confirmed that the patient\u00a0\nwas actually diagnosed with cancer\u00a0\u00a0 and asked how it was confirmed,\u00a0\nwhich was mostly by biopsy or X ray. Between April 1948 and October 1949 they got\u00a0\nalmost 2400 patients, but they decided to\u00a0\u00a0 exclude anyone over age 75, and patients who\u00a0\ngot a non-cancer diagnosis after admission. The group then shrank to 2,140 patients since\u00a0\nsome of the group were discharged before\u00a0\u00a0", "id": "DS97JV_o0Fs_18", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and one was a liar. That\u2019s\u00a0\nwhat it says, right there. Just like the Wynder and Graham\u00a0\nstudy, they categorized smokers\u00a0\u00a0 by how much they smoked so that they\u00a0\ncould check for a dose-response effect. At the end of the research, they\u00a0\nshowed the dose response effect,\u00a0\u00a0 but also showed something called specificity\u00a0\n\u2014 when the cause is associated with one\u00a0\u00a0 outcome and nothing else. They found a clear\u00a0\nassociation between lung cancer and smoking,\u00a0\u00a0 but no association between\u00a0\nany other cancers and smoking. In their own words \u201cit must be concluded\u00a0\nthat there is a real association between\u00a0\u00a0 carcinoma of the lung and smoking\u201d.\u00a0\nTheir results were so powerful that\u00a0\u00a0 Doll stopped smoking in the middle of the\u00a0\nstudy while Bradford Hill kept smoking. Side note, I don\u2019t get involved in the pissing\u00a0\nmatch of \u201cwho was first?\u201d in medical history,\u00a0\u00a0 but I personally think the British paper was the\u00a0\nmost convincing of the studies from 1950 \u2014 they\u00a0\u00a0 had all the best aspects of the 3 American\u00a0\nstudies including a dose response effect,\u00a0\u00a0 specificity, and a large sample size. But it was published in September while the\u00a0\nWynder and Graham paper was published in May,\u00a0\u00a0", "id": "DS97JV_o0Fs_19", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So it\u2019s 1950 and these four\u00a0\nmassive papers come out,\u00a0\u00a0 but smokers around the world\u00a0\ndidn\u2019t quit overnight. Why not? Well mostly because lay people don\u2019t\u00a0\nread technical scientific journals,\u00a0\u00a0 but they were also getting mixed\u00a0\nmessages from everyone else like doctors\u00a0\u00a0 and the media. I\u2019m gonna have my friend\u00a0\nKnowing Better teach you about that one. [Knowing Better] Look, everybody knows that\u00a0\u00a0 cigarettes are bad for you,\u00a0\nright? This isn't a secret. They figured that out as soon as\u00a0\npeople began smoking in the 20s,\u00a0\u00a0 when the main issue was throat\u00a0\nirritation and smokers\u2019 cough. The industry fixed that by adding menthol, which\u00a0\nprotected your throat with a cool fresh sensation. [Ad] Great tobacco and a light\u00a0\ntouch of cooling menthol. This\u00a0\u00a0 new spud gives you a fresh taste in your\u00a0\nlungs. Like the smoke was air conditioned. [KB] Since not everyone\u2019s a fan of that\u00a0\nflavor, they came out with King sized\u00a0\u00a0 cigarettes in the 30s, which traveled the\u00a0\nsmoke farther, making it smooth and mild. [Ad] Yes, Dunhill, thanks to its\u00a0\nking size, its finer tobaccos and\u00a0\u00a0 its natural filtering qualities, screens out\u00a0\nirritants to give you a far milder smoke.", "id": "DS97JV_o0Fs_20", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  studies were being published by fringe scientists. So most people weren\u2019t all\u00a0\nthat concerned. The industry\u00a0\u00a0 will find a solution just as they always have. And most doctors were regular\u00a0\nsmokers at this point,\u00a0\u00a0 so they began endorsing brands which\u00a0\nhad proven themselves to be healthier [Ad] What cigarette do you smoke doctor? The\u00a0\nbrand named most was Camel. Yes, surveys show more\u00a0\u00a0 doctors smoke Camels than any other cigarette.\u00a0\nSmoke Camels! The cigarette so many doctors enjoy. [KB] Their faith was rewarded. The industry\u00a0\nbegan adding filters to their cigarettes in\u00a0\u00a0 the early 50s to make them even safer. Millions\u00a0\nof consumers switched to filtered cigarettes,\u00a0\u00a0 secure in the knowledge that they\u00a0\nwere protected from lung cancer and\u00a0\u00a0 any other dangers the health\u00a0\nfascists were complaining about. [Ad] Yes, the perfect marriage\u00a0\nbetween a great new filter\u00a0\u00a0 and a great modern blend. Now\u00a0\nin a new Old Gold filter kings.", "id": "DS97JV_o0Fs_21", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Teryton\u2019s got a charcoal tip\u00a0\nand it's got a white one too. Yes it's the filter that counts, and L&M\u00a0\nhas the best. You get much more flavor,\u00a0\u00a0 much less nicotine. A light and mild smoke. This\u00a0\nis it. L&M filters \u2014 just what the doctor orders. [KB] See! Just what the doctor ordered!\u00a0\nI don\u2019t know why everyone is so worked up\u00a0\u00a0 over this cancer thing. They figured it out,\u00a0\nit\u2019s fine! Don\u2019t stop smoking, it\u2019s fine! People started taking the cancer risk\u00a0\nmore seriously after an article came\u00a0\u00a0 out in Reader\u2019s Digest, but you\u2019ll have to\u00a0\nwatch Knowing Better\u2019s video for that story. Back on the science side of things,\u00a0\nthere were still plenty of people who\u00a0\u00a0 smoked and didn\u2019t develop cancer, so how\u00a0\ncan smoking cause something if it doesn\u2019t\u00a0\u00a0 have a 100 percent hit rate? Was there some\u00a0\nother factor or confounder they didn\u2019t study? Just like the case controls before them, these\u00a0\nnew studies had all been retrospective \u2014 they\u00a0\u00a0 looked at a snapshot of data which was influenced\u00a0\nby all the messiness of life that you couldn\u2019t\u00a0\u00a0", "id": "DS97JV_o0Fs_22", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  Like you could recruit two sets of comparable\u00a0\nteens, and assign them to either a smoking group\u00a0\u00a0 or a non-smoking control group, and then see who\u00a0\ngot lung cancer at the end of their lives. But,\u00a0\u00a0 that kind of study isn\u2019t gonna\u00a0\nhappen because\u2026 you know, ethics. Bradford Hill and Doll knew that the next best\u00a0\nthing would be finding a group of adults who\u00a0\u00a0 were as homogenous as possible, but some\u00a0\nvoluntarily smoked while others didn\u2019t. Luckily, they knew a group of people\u00a0\nwho fit that description to a T. A group of people who all had\u00a0\nsimilar socioeconomic status,\u00a0\u00a0 were almost entirely white and almost entirely\u00a0\nmen \u2014 doctors. And even better for their study,\u00a0\u00a0 doctors in the UK had to keep their contact\u00a0\ninformation on file in order to practice medicine,\u00a0\u00a0 so it\u2019d be easy to follow up with them. Bradford Hill and Doll decided their next paper\u00a0\nwould use British doctors as their subjects. In 1951, Doll sent a questionnaire\u00a0\nto every available doctor in the UK\u00a0\u00a0 asking the following questions.\u00a0\nNumber one, do you smoke?\u00a0\u00a0 If you do, how much, and when did you\u00a0\nstart? If you quit, when did you quit?", "id": "DS97JV_o0Fs_23", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  how much they smoked, how they\u00a0\nsmoked, and whether they quit or not. After sending out almost 60,000 surveys they\u00a0\ngot 41,000 responses. Then, they excluded men\u00a0\u00a0 under 35 and women of all ages since lung\u00a0\ncancer wasn\u2019t as common in those groups. They were left with 24,389 participants that they\u00a0\nwould follow around for the next few decades. Then they waited. Over the next 29 months, they saw 789\u00a0\ndeaths, 36 of which were from lung cancer. They found that 0 of their nonsmokers\u00a0\ndeveloped lung cancer and saw the dose\u00a0\u00a0 response effect that they expected\u00a0\nfrom earlier studies. They also saw\u00a0\u00a0 a higher incidence of coronary thrombosis in\u00a0\nsmokers \u2014 what we\u2019d call heart attacks today. They published their initial results in June 1954\u00a0\u00a0 and it finally got some attention\u00a0\nfrom the scientific community. Not only did they present a ton of data,\u00a0\nbut they presented the first prospective\u00a0\u00a0 cohort trial about the topic, which made it\u00a0\nthe best statistical evidence to date that\u00a0\u00a0 smoking caused cancer. A few months later,\u00a0\nthe American Cancer Society funded a similar\u00a0\u00a0", "id": "DS97JV_o0Fs_24", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  That study\u2019s methodology was similar to\u00a0\nthe British Doctor Study, so I won\u2019t go\u00a0\u00a0 into depth \u2014 but they only included white\u00a0\nmen between the ages of 50 and 69 nice. Now, the 1954 publication in BMJ was a\u00a0\npreliminary result. Bradford Hill and Doll\u00a0\u00a0 brought on a younger researcher, Richard Peto,\u00a0\nand followed up on their subjects multiple times\u00a0\u00a0 from 1957 through 2001. At the ten year follow\u00a0\nup, they reported that lung cancer caused 0.07\u00a0\u00a0 deaths per thousand nonsmokers, but 3.15 deaths\u00a0\nper thousand in the heaviest smoking group. Then in the 80s, Doll led a similar study\u00a0\nshowing the same mortality effects of smoking\u00a0\u00a0 on female doctors, because remember, they were\u00a0\nexcluded from the original British Doctor Study. Now, scientific articles help inform individuals,\u00a0\nbut I argue that science is more impactful when\u00a0\u00a0 it informs policies. The British doctor\u00a0\nstudies have collectively been cited over\u00a0\u00a0 17,000 times by other scholars, which means\u00a0\nthese studies were extremely impactful,\u00a0\u00a0 both for follow up research and for science based\u00a0\npolicy. Which is what happened next in the story.", "id": "DS97JV_o0Fs_25", "title": "2. Overview of Clinical Care", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  mentioned at the beginning of the video. They\u00a0\nrecognized that smoking was a cause of cancer. It\u2019s 155,000 words long, which is longer than\u00a0\nthe Hobbit, and it\u2019s not nearly as fun to read. This is regarded as the landmark moment in\u00a0\nthe public health battle against smoking,\u00a0\u00a0 but as far as proving that smoking caused cancer,\u00a0\u00a0 it wasn\u2019t all that exciting since that science\u00a0\nwas already ten years old at that point. It did mark a new chapter in\u00a0\nAmerican public health though.\u00a0\u00a0 Now that we had solid evidence showing\u00a0\nsmoking causes cancer, what do we do? Hey before you go, I think you\u2019ll like this\u00a0\nvideo I made about snake oil salesmen or\u00a0\u00a0 this one about how generic drugs became a thing.\u00a0\nAnd if you want to keep learning about tobacco,\u00a0\u00a0 make sure to check out this video from Knowing\u00a0\nBetter about the history of the cigarette\u00a0\u00a0 industry and subscribe to him while you\u2019re at\u00a0\nit. And of course, thank you to my patrons on\u00a0\u00a0 Patreon who make these videos possible.\u00a0\nHave fun, be good, thanks for watching. KB Section\u00a0 [seated in my classroom at a big desk, 50s\u00a0\nnewscaster style with 4:3 aspect ratio]\u00a0 The Reader\u2019s Digest article spooked Americans\u00a0\nnationwide. The problem: \u201cthe increase in lung\u00a0\u00a0 cancer mortality shows a suspicious parallel to\u00a0\nthe enormous increase in cigarette consumption\u201d. [wider shot, showing projector\u00a0\nbehind me. I\u2019ll add some projector\u00a0\u00a0 reel foley work and stylize my slides]\nThe article quotes a recent study that showed\u00a0\u00a0 that the incidence of death by lung cancer had\u00a0\nincreased more than ten times from 1938 to 1948. And while it\u2019s true that the deaths correlated\u00a0\nwith increased smoking rates, that doesn\u2019t prove\u00a0\u00a0 that cigarettes cause lung cancer. So why were\u00a0\nscientists so confident in the causal link? [projector slide: case control]\nThroughout the early twentieth century,\u00a0\u00a0 epidemiologists conducted case control studies\u00a0\nto figure out why lung cancer rates were rising.\u00a0\u00a0 They\u2019d recruit people with cancer, the case,\u00a0\nthen recruit similar people without cancer,\u00a0\u00a0 the control. Then they\u2019d try to find\u00a0\nany differences between the two groups,\u00a0\u00a0 usually by giving them questionnaires about their\u00a0\nlifestyle and medical history. The idea is that\u00a0\u00a0 any difference on the questionnaire might show why\u00a0\none group developed cancer while the other didn\u2019t. By the 1920s and 30s, epidemiologists\u00a0\nwere already starting to notice that\u00a0\u00a0 their participants with cancer tended\u00a0\nto be smokers more often than their\u00a0\u00a0 controls. But these studies failed\u00a0\nat moving popular opinion or policy. [projector: 1950]\nBut in 1950, 4 studies\u00a0\u00a0 came out that provided the best evidence up\u00a0\nto that point that smoking causes lung cancer. Two were back to back articles in the May issue of\u00a0\nthe Journal of the American Medical Association.\u00a0\u00a0 One of them is actually referenced in the Digest\u00a0\npiece \u2014 the one by Wynder and Graham. Another\u00a0\u00a0 study came out in Cancer Research, and the fourth\u00a0\nin the British Medical Journal in September. [projector: unlabeled bar graphs. Time\u00a0\n\u201cdose response\u201d title with my spoken words]\u00a0 Not only did these studies each recruit hundreds\u00a0\nor thousands of people, they also showed a dose\u00a0\u00a0 response effect, which is when increased\u00a0\nexposure to the bad thing lines up with\u00a0\u00a0 increased or decreased rates of disease. So in\u00a0\nthis case, the groups that smoked more cigarettes\u00a0\u00a0 had the highest rates of lung cancer, while the\u00a0\ngroups that smoked less had less lung cancer. The dose response effect is easy to translate\u00a0\nto a lay audience too \u2014 the more you smoke,\u00a0\u00a0 the greater your risk of developing lung cancer. The Reader\u2019s Digest article marked\u00a0\na turning point in public education\u00a0\u00a0 about smoking and lung cancer, but as\u00a0\nfar as research that impacted /policy/,\u00a0\u00a0 we\u2019d have to wait until 1954, when a\u00a0\nmassive study of British doctors came out\u00a0\u00a0", "id": "DS97JV_o0Fs_26"}, {"text": "  so I stated in this video we're going to have a look at three major learning objectives the first of which is going to have a look at some of the chemical mediators of inflammation and then we're going to have a look at some of the drugs used to mitigate inflammation or reduce the inflammatory process and we've broken that up into two major objectives course objective 2 or the learning objective 2 for this is looking at the non-steroidal anti-inflammatory medications and the third is to have a look at the stoical anti-inflammatory medications so let's just first go back and just have a quick recap of what Graham was talking about in the inflammatory process remember that inflammation will occur any time damage has been inflicted or injury has been inflicted on vascularized tissue because you know that the inflammatory process first starts off with a vascular response and the inflammatory process is basically seen through the four cardinal signs okay the four cardinal signs of inflammation which is redness heat", "id": "0UFwGJe6ubg_0", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  get redness heat swelling and pain is because of the vascularized response so when that damage occurs what you'll find is that the blood vessels in that area will dilate increasing the amount of blood flow get into the injured site and the endothelial cells so remember what that means if you take a blood vessel the cells that line the inside of that blood vessel are called endothelial cells and that these inflammatory pro-inflammatory agents or chemical mediators of inflammation they can tell these endothelial cells to contract and get smaller which means that the space between one and athili or sell in the next gets larger and what that means is the blood flow that's coming through those vessels can be pushed out of the blood vessel into the surrounding tissue at the area where it's injured and this can be because it wants to dilute out", "id": "0UFwGJe6ubg_1", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  leukocytes or the white blood cells to that area it wants to wash away any potential foreign agent that is built up okay so the vascular response the inflammatory spots is vasodilation and endothelial constriction and the reason why these two things occur is because of these chemical mediators of inflammation they have been released from surrounding cells and some of these chemical mediators including Grable have spoken about these in the previous video prostaglandins looka trans nitric oxide Brady cannons histamine and cytokines so these are all chemical mediators that promote this process that we just spoke about so what that means is if we want to mitigate or we want to reduce the inflammatory process we need certain drugs or compounds that stop these chemicals from having that function okay in this video we're going to talk about two major chemical mediators of", "id": "0UFwGJe6ubg_2", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  leukotrienes and the reason why we're talking about those two is because they're part of the same cascading pathway so let's first start off with this if you take the cells in your body you know that those cells are surrounded by a phospholipid bilayer and this phospholipid bilayer is made up of phospholipids and if we were to draw a phospholipid you know that it has a phosphate head which loves water so it's hydrophilic and to fatty acid tails so lipid tails which does not like water so they're hydrophobic okay now the reason why I'm bringing this up is if you were to take a twenty carbon portion of this fatty acid tail right just take a small portion of this fatty acid tail that actually is what we call a ragged donek", "id": "0UFwGJe6ubg_3", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  arachidonic acid is important because it produces prostaglandins and leukotrienes okay so from phospholipids you have the fatty acid tails taking a twenty carbon portion of that fatty acid tail we can create a rack idiotic acid and a rack idiotic acid like I said will create prostaglandins and leukotrienes both of which are chemical mediators of inflammation what that means is they promote vasodilation and then promote endothelial constriction amongst others now let's first focus on the", "id": "0UFwGJe6ubg_4", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  prostaglandins exist within the body and yes a lot of them have a pro-inflammatory role and a role in producing pain and fever but also prostaglandins play many other roles within the body and these many different roles can be broken up into two major categories and that's what we're going to focus on today we're going to break up all the individualized prostaglandins into two major categories and focus on those so let's break prostaglandins up so both of these prostaglandins will promote like I stated inflammation both types of prostaglandins will promote pain both types of prostaglandins will", "id": "0UFwGJe6ubg_5", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  involved in that whole inflammatory process but prostaglandins aren't just involved in these things they're involved in other processes so for example the prostaglandins that will drawn up on this side of the whiteboard these prostaglandins are also involved in maintaining stomach or GI T integrity so what I can write up here is increased GI T integrity now what am i referring to here you know that in your stomach you have a mucous lining okay now this mucous lining has bicarbonate and mucus and it protects our stomach from the acid that it creates okay because we know that we create hydrochloric acid and there's a very low po2 around about 1.5 to 3 which is sufficient to break down the tissue of our stomach but this mucous lining", "id": "0UFwGJe6ubg_6", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  help us create this mucous lining and increase or maintain GI T integrity ok so that's a very important point put across these prostaglandins on this side also produce something called thromboxane so let's write thromboxane now from boxing you may see written as T X a to and from boxing is involved in the clotting pathway natural fact from boxing promotes platelet aggregation from boxing promotes platelet aggregation which means it promotes clotting okay so the prostaglandins on this side are involved in inflammation pain and fever maintaining stomach", "id": "0UFwGJe6ubg_7", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  pathway so increasing platelet aggregation if we ever look on the other side of these prostaglandins again they are involved in inflammation and pain and fever but some other processes for example you'll find that these prostaglandins are in high quantities in synovial fluid now why is this important to note because think about where we find the synovial fluid if either AB synovial joints okay so that includes the knee for example now if you increase the amount of these prostaglandins in the knee or the son of your fluid there at the knee it's going to promote inflammation of pain and can also lead through to rheumatoid arthritis so these prostaglandins do play a role in rheumatoid arthritis okay in addition", "id": "0UFwGJe6ubg_8", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  prostaglandins or this side of the pathway that involves thromboxane promotes platelet aggregation over on this side they reduce platelet aggregation so very different so what do these prostaglandins have in common both are involved in inflammation both involved in pain both you're involved in fever however the prostaglandins on this side maintain the mucous lining of our stomach and also promote clotting on this side yes inflammation pain fever but these prostaglandins are present high quantities in a synovial joints and therefore can play a role in rheumatoid arthritis and they reduce platelet aggregation now these are all important because we need or we have drugs that can reduce our inflammatory response okay now some of these drugs are called", "id": "0UFwGJe6ubg_9", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  drugs so let's write that up these drugs which we call NSAIDs so let's just write answers which like I said of the non-steroidal anti-inflammatory drugs they're non-steroidal okay so steroidal drugs often have their effect as signaling molecules okay in this manner because they're a steroid they can easily pass through the phospholipid bilayer of cells okay because they have a lipid base to them okay remember lipids love lipids what the lipids hate they hate water they hate anything that's polar okay polar means it's charged lipids hate charged things and waters charged but lipids like other lipids which are not charged like steroids now what that means is that steroid hormones if they", "id": "0UFwGJe6ubg_10", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  straight into the cell and go straight to the nucleus where the DNA is and that means that many steroids exert their effect by altering the transcription of our DNA meaning it tells what genes to be expressed and what genes not to be expressed okay so in this scenario we're talking about the non-steroidal anti-inflammatory drugs all that means is that these drugs do not exert their effect via this means okay now you've heard of these most oral anti-inflammatory drugs before the major ones include aspirin ibuprofen paracetamol and celecoxib they're the four major ones we're going to talk about today so what I'm going to do is this if I break these two sides of the prostaglandins up and discuss aspirin ibuprofen paracetamol and celecoxib what you'll find is that they either work by inhibiting these types of prostaglandins", "id": "0UFwGJe6ubg_11", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  do they do it well in order to produce the prostaglandins on this side we need a particular enzyme that turns them all on okay what is this enzyme this enzyme is called cox-1 cops once danced for cyclooxygenase one so this is an enzyme that comes along and promotes these prostaglandins and through bakso to have these functions in the body on this side cox-2 is the enzyme and cox-2 promotes these prostaglandins to have these particular functions and these nonsteroidal anti-inflammatory drugs how do they work well they'll inhibit these Cox enzymes okay so let's first start with aspirin what you'll find is aspirin inhibits both cox-1 and cox-2 so I can draw aspirin", "id": "0UFwGJe6ubg_12", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  also find is aspirin more specifically slightly more specifically will inhibit cox-1 now let's talk about what that means if aspirin inhibits the Cox enzymes specifically more so Cox 1 it's going to reduce inflammation is going to reduce pain and reduce fever which means that it has an anti-inflammatory role and analgesic role and an antipyretic role but what that also means it's going to reduce GI t in temporary so it's going to reduce our ability to maintain the mucous lining of our stomach and what that means is that the acid that's present in our stomach will more likely irritate our stomach and can cause damage to it so that's why you've probably heard of doctors or nurses stating take your aspirin on a full stomach to reduce this sort of stomach", "id": "0UFwGJe6ubg_13", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  you'll find is aspirin even though it's an NSAID and anti-inflammatory medication one of the other most common means or uses of aspirin I should say is for individuals who have cardiovascular disease or increased likelihood of stroke or heart attack why because taking a small amount of aspirin have a look down here will reduce platelet aggregation because it's inhibiting this side it's going to inhibit platelet aggregation so it's going to reduce the likelihood on their blood to clot and that's why aspirin is often handed out to patients who are likely to have strokes and heart attacks and so forth okay so the next one I want to talk about is ibuprofen and ibuprofen again inhibits both cops 1 and Cox 2 and therefore has a very similar effect anti-inflammatory effect as aspirin", "id": "0UFwGJe6ubg_14", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  acetaminophen okay so paracetamol is an acetaminophen is not a what would call a normal nonsteroidal anti-inflammatory drug it isn't it is a NSAID like drug and the reason why is this because paracetamol yes play around somewhere in this pathway it is a very efficient analgesic and antipyretic but paracetamol is actually not very good at reducing inflammation okay and so if it was to work upon blocking cox-1 and cox-2 it should be good as an anti-inflammatory drug but it's not which gives us an indication that it may not actually inhibit cox-1 and cox-2 but it may actually inhibit a third cox isoform called cox 3 however we do not have enough proof to demonstrate that this is the case but we just put paracetamol in as an NSAID okay", "id": "0UFwGJe6ubg_15", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  it's very good as an analgesic and antipyretic but it's quite a poor anti-inflammatory drug so let's write paracetamol and the last instead I want to discuss we call celecoxib you may hear of it by its name on the market which is celebrex and celecoxib you can see I've drawn just on the cox-2 side which means celecoxib is a Cox to specific inhibitor it's one of the few Cox to specific inhibitors available on the market in Australia there were some others but they've been taken off the market now let me tell you why celecoxib is good as an anti-inflammatory as an OG zyk as an antipyretic it is used in cases of rheumatoid arthritis to reduce", "id": "0UFwGJe6ubg_16", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  fluid but many of those cox to specific inhibitors if the normal prostaglandins in this pathway reduce platelet aggregation if you were to block it you're going to increase platelet aggregation which increases clotting which meant that a lot of the previous cox-2 inhibitors Cox to specific inhibitors promoted clotting and increase the patient's likelihood to clot which isn't good because it's going to increase their likelihood of myocardial infarction stroke and so forth okay but so far celecoxib has not shown to have this effect but has shown to be an efficient anti-inflammatory analgesic antipyretic and reduce rheumatoid arthritis so these are the non-steroidal anti-inflammatory drugs okay now if we move over to the Luca trends Luca trends yes are involved in inflammation so again I can write down inflammation for", "id": "0UFwGJe6ubg_17", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  we knew that but Luca trends also increase mucus secretion specifically in the Airways or I should say more so in the Airways and increases bronchoconstriction now I want you to have a look at these three major effects of Luca drinks Luca drinks promotes inflammation increases mucus production more so in the Airways and increases bronchoconstriction what do you think what process or what disease process do you think this plays a big role in asthma so you'll find that Luca trains are involved in the inflammatory process involved in a sport because asthma is a reversible inflammatory disease so inflammation is the crux of asthma and Luca trains are involved in this process", "id": "0UFwGJe6ubg_18", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  having their effect well we don't necessarily stop the synthesis of the Luca trends but we stop the Luca trains from binding to Luca tree receptors so we have drugs that come in which are called leukotriene receptor antagonists look a trained receptor antagonists so that will competitively bind at that receptor spot against Luca trains and therefore reduce inflammation mucus production of bronchial constriction okay now the very last point I want to talk about are the steroidal anti-inflammatory drugs so a lot of us have taken steroids at some point such as hydrocortisone cortisol called it just normal cortisone for many different reasons because we know that they have a very strong anti-inflammatory role but", "id": "0UFwGJe6ubg_19", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  but we're just going to talk about the anti-inflammatory role of these steroids now these steroids we call corticosteroids so let's write that up Clawd echo steroids now the reason why they called corticosteroids is this Qualicum refers to the cortex the cortex of the adrenal gland and steroids are referring to the fact that that steroids okay now remember you have your kidneys and on the kidneys you have your adrenal gland adrenal meaning near kidney or on kidney and that adrenal gland has an outer portion called the cortex and an inner portion called the medulla and different hormones are produced in different layers okay in the cortical layer we produce the corticosteroids okay and the major corticosteroids are", "id": "0UFwGJe6ubg_20", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  zone and cortisone now they're also known as glucocorticoids okay glucocorticoid so I'll get rid of corticosteroids and right glucocorticoids because you probably see them as either corticosteroids or glucocorticoids let's write glucocorticoids gluco corty coins because glucose cortex steroids now the gluco portion refers to the fact that these steroid hormones here what they do is they promote the release of glucose into our bloodstream okay this doesn't necessarily play a role in our", "id": "0UFwGJe6ubg_21", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  the cow again cortex and again refer to the steroid portion now how do glucocorticoids such as hydrocortisone ecause Oh play an anti-inflammatory role well first point is this they can actually come into this area here and then can stop phospholipid phospholipids from turning into arachidonic acid okay so glucocorticoids can stop phospholipids from turning into a record onic acid it means that they stop this entire cascading process right here they stop these prostaglandins from being produced they can stop these Luca trains from being produced so called glucocorticoids a very potent anti-inflammatory drugs but that's not the only role they play because again there's steroids which means that predominately out they have their function by moving into a cell and altering the transcription of genes and", "id": "0UFwGJe6ubg_22", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  transcription of a pro-inflammatory gene so there's many all these interleukins and cytokines glucocorticoids can reduce their expression or hibbett their expression therefore inhibiting inflammation okay so I hope that this makes sense talking about some of the pro-inflammatory agents okay such as prostaglandins Luca Trent's and we mentioned histamine and Brady cannons and cytokines and so forth we spoke about the non-steroidal anti-inflammatory drugs such as aspirin ibuprofen paracetamol and celecoxib and the fact that they work by inhibiting prostaglandins from being made either from cox-1 or cox-2 or both and we spoke about the non-steroid about the steroidal anti-inflammatory drugs the glucocorticoids and they play a role in inhibiting the production of a catana Cassatt but also inhibit the synthesis of pro-inflammatory genes so I hope that", "id": "0UFwGJe6ubg_23"}, {"text": "  transcription of a pro-inflammatory gene so there's many all these interleukins and cytokines glucocorticoids can reduce their expression or hibbett their expression therefore inhibiting inflammation okay so I hope that this makes sense talking about some of the pro-inflammatory agents okay such as prostaglandins Luca Trent's and we mentioned histamine and Brady cannons and cytokines and so forth we spoke about the non-steroidal anti-inflammatory drugs such as aspirin ibuprofen paracetamol and celecoxib and the fact that they work by inhibiting prostaglandins from being made either from cox-1 or cox-2 or both and we spoke about the non-steroid about the steroidal anti-inflammatory drugs the glucocorticoids and they play a role in inhibiting the production of a catana Cassatt but also inhibit the synthesis of pro-inflammatory genes so I hope that", "id": "0UFwGJe6ubg_23", "title": "3. Deep Dive Into Clinical Data", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:37:16Z": "2020-10-22T19:37:16Z"}, {"text": "  what's up everybody in this extremely high yield video i'm going to be teaching you everything that you need to know about the various drugs of abuse this is brought to you by dirty medicine this video will touch on everything that you see on this slide so we will go through these substances one at a time in the following order alcohol opioids benzos cocaine methamphetamine pcp mdma marijuana and then lsd in each of these substances we're going to be talking about what it looks like when somebody is intoxicated what it looks like when somebody is experiencing withdrawal and then we'll create some concept maps to help you associate these substances with other non-related comorbid medical conditions or complications that tend to occur as a result of utilization of one of these substances i think that this third point is the highest yield thing that you need to take out of this video because oftentimes on usmle or comlex", "id": "_shuV1tJbTU_0", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  starts with a substance like one of these drugs but the question will end with them challenging your ability to associate one of these substances with some complication or some seemingly unrelated pathophysiology that will be a third order question now officially before i get into this video please consider clicking the join button on my channel you can find that join button underneath any video as well as on my channel home page and then lastly probably the easiest place to find it is it's the first link in the description of any video on my channel when you click the join button or click that link you will sign up to be a dirty medicine member which means that in exchange for financial support of my channel for 4.99 a month you will get some cool perks those perks include the very sexy dirty medicine logo appearing after your name anytime you comment publicly on my channel and you'll also get access to", "id": "_shuV1tJbTU_1", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  where you can type in what you want the topic of the next video to be so if you support free medical education please consider clicking the join button i really appreciate your consideration now let's get into this video we're going to start with alcohol and first let's talk briefly about what it looks like when somebody's intoxicated from alcohol and when somebody's withdrawing from alcohol intoxication as most of you probably know already includes slurred speech ataxia emotional lability and disinhibition so i think that most of us have experienced somebody that had a little bit too much to drink and they showed these symptoms of alcohol intoxication now instead of just listing out alcohol withdrawal on this slide i want to talk about it on its own slide because it's very high yield discussion to know at what timeline certain symptoms are occurring and before we get into that i want to take this moment to pause and point something out", "id": "_shuV1tJbTU_2", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  symptoms you see in the intoxication state you generally see the opposite of that in the withdrawal state and vice versa so if in intoxication you have slurred speech ataxia disinhibition emotional lability you basically have a lot of sedation then you're going to see the opposite of that in withdrawal so just keep that in mind when you're taking usmle your complex if you're not exactly sure what the withdrawal symptom of a certain drug is but you know what the intoxication symptom is generally speaking they're going to be the opposite so if the drug is stimulating when it's you're intoxicated then in withdrawal it's going to be sedating and if the drug is a sedative when you're intoxicated then the withdrawal state will be excitation so just keep that in mind broadly but specific to alcohol it's very high yield to know the timeline for withdrawal so instead", "id": "_shuV1tJbTU_3", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  in one or two sentences i want to talk about the timeline because this comes up all the time so the first symptoms that you see between 0 and 36 hours of alcohol withdrawal are actually the mildest symptoms so you see gi upset tremors agitation and insomnia then what you see between 12 and 48 hours is what's known as alcoholic hallucinosis alcoholic hallucinosis is a very distinct syndrome of two things one visual hallucinations but two that occurs in the setting of intact orientation so even though the person is withdrawing from alcohol and even though the person is having visual hallucinations they still know their name they know the year they know where they are and generally speaking they know what's going on so alcoholic hallucinosis it's very high", "id": "_shuV1tJbTU_4", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  is distinct from something we'll talk about in just a moment delirium tremens because in alcoholic hallucinosis orientation is intact please keep that in mind so alcoholic hallucinosis occurs between 12 and 48 hours withdrawal seizures which are very severe occur between 6 and 48 hours and then lastly the most severe of all of these symptoms or syndromes delirium tremens commonly referred to as dts occurs at 48 hours and beyond so in dts you have delirium which the name implies so the orientation is no longer intact the person does not know their name does not know where they are does not know the situation etc and in the setting of delirium tremens in addition to that delirium or that confusion or that loss of orientation you get a whole host of other life-threatening symptoms such as", "id": "_shuV1tJbTU_5", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  this point in the alcohol withdrawal not only are they delirious but they are in such sympathetic overdrive that it can really compromise their cardiovascular system so let's just kind of conceptualize this to really hammer this home into your brain so say that you've got somebody who normally consumes a lot of alcohol under the normal consumption of alcohol alcohol will increase levels of gaba and gaba is a major inhibitory neurotransmitter so alcohol equals increased inhibition and likewise alcohol will de decrease glutamate now glutamate is a major excitatory neurotransmitter which means that when you consume alcohol you are increasing inhibition and decreasing excitation now anytime you change neurotransmitters especially at the site of receptors you have either up or down regulation of those receptors so with glutamate for example over time", "id": "_shuV1tJbTU_6", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  excitation therefore resulting in net inhibition those glutamate receptors will upregulate because the brain is thinking i need to counteract this in some way and this is really what leads to an excitation problem so over time you've got somebody who's used to constantly drinking alcohol and constantly putting their brain into this state of inhibition they do that by increasing the inhibitory neurotransmitter in gaba and decreasing the excitatory neurotransmitter in glutamate but over time if you suddenly cut off their ability to access alcohol what happens because of receptor upregulation especially with glutamate now you've got a massive increase in glutamate and a massive decrease in gaba when you decrease gaba you're decreasing inhibition which results in excitation", "id": "_shuV1tJbTU_7", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  especially in the setting of long-term receptor up regulation you get massive excitation so you've got a lot of receptors sitting there waiting to be excited so when you remove alcohol you get net excitation and when that happens the person goes into sympathetic overdrive so it should make sense to you when you think about the symptoms of alcohol withdrawal that you see agitation hypertension tremor gi upset and tachycardia and the reason that delirium tremens is so life-threatening is because when there's massive sympathetic overdrive confounded with the person being delirious and they're therefore disoriented you can get cardiovascular compromise resulting in death so that's alcohol withdrawal and it's very important to understand not only the timeline but also the pathophysiology in terms of how this manifests", "id": "_shuV1tJbTU_8", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  told you at the start of this video that i actually think that the highest yield piece of information to take out of this video are the associations because on tests you'll be given a question which will at first glance seem like they're going to ask you some question about a drug of abuse and then you're going to see them pivot and ask you some third order question about some complication that the drug might be associated with so when it comes to alcohol the different disease processes that you want to keep in mind are everything that you see on this slide cirrhosis wernicke korsakoff syndrome pancreatitis beriberi dilated cardiomyopathy peripheral neuropathy which is technically part of beriberi testicular atrophy cerebellar degeneration gastritis and marchia fava big nami disease and i'm definitely butchering the pronunciation there now just briefly just to really establish some awesome neural networks in your brain i want to", "id": "_shuV1tJbTU_9", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and just run through each of these one at a time pointing out the high yield things that you might look for on an exam if they started with a question on alcohol but then pivoted and wanted you to think about some other comorbid disease or complication so we're just going to briefly fill in these boxes with some of these diseases so recall that if they want you to think cirrhosis you might see portal hypertension bleeding jaundice asterixis spider angiomata palmer erythema the labs you'll see lfts you could see ggt they might show you decreased platelets or changes in clotting factors and obviously as you can tell by looking at this slide symptoms will be blue and labs or imaging will be green for wernicke korsakov syndrome you'll see encephalopathy ophthalmoplegia ataxia amnesia which can be either anterograde or retrograde and confabulation if they're going to give you anything lab related you might see thymine levels", "id": "_shuV1tJbTU_10", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  mammillary bodies for pancreatitis remember that you'll have a patient with epigastric abdominal pain which will raise to the back you might see nausea vomiting chills flank echimosis or perry umbilical echemosis if they give you labs obviously they'll give you amylase and light paste but actually what they could give you just by itself if they really want to be tricky is just to give you a calcium level and recall that hypocalcemia is what you want to look for for beriberi you have either the dry berry berry or the wet berry berry and all this means is that dry is symmetrical peripheral neuropathy and wet is high output heart failure they give you labs for beriberi they'll probably give you a thymine level or they'll describe somebody to you with other labs that will either paint the picture of somebody whose volume overloaded so that would be the high output heart failure or they'll give you somebody with non-inflammatory demyelination if they're going for the dry berry berry a couple more dilated", "id": "_shuV1tJbTU_11", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  somebody who has symptoms of systolic heart failure they'll have potentially a systolic murmur and the s3 heart sound and then if they give you some type of imaging or echo you could see dilated ventricles ballooning of the heart or a bundle branch block for cerebellar degeneration you'll have cerebellar symptoms as the name implies so you could see vertigo you could see ataxia you could see either dysarthria which is a speech abnormality or dysmetria which is uncoordinated movement and then if they give you some exam findings or labs you want to look for things like dysdiatycokinesia the finger to nose testing stuart holm sign which google this but it's basically known as rebound elbows or cerebellar drift so you're going to be looking mostly clinically in cerebellar degeneration for different exam findings but long-term alcohol use definitely results in cerebellar degeneration for gastritis you you will see", "id": "_shuV1tJbTU_12", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  upper gi pain indigestion and early satiety if they're going to go in the direction of imaging or labs they'll probably give you images from an endoscopy or they'll give you the results of some type of h pylori related testing usually that's either the stool antigen or the urease breath tests and then lastly for the disease whose name i really don't know how to pronounce marcia fava big nami disease you're going to see a whole host of neuropsychiatric symptoms could be personality change could be dementia could be sub-threshold psychotic symptoms but really what's going to steal the diagnosis here is going to be the imaging so you see corpus callosum degeneration plus or minus b vitamin levels and probably not b vitamin levels but the the prevailing theory is that this disease where you see corpus callosum degeneration is related to decreased vitamin b levels so again i just wanted to run through all of these and kind of create the network in your", "id": "_shuV1tJbTU_13", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  associations so if you get a test question and they tell you that the person drinks 10 you know whatever ounce beers a day this is what you want to start to think about so that when you then see the lab print out your brain is thinking all right they gave me a question about alcohol now they're showing me something with stool maybe it's gastritis or they gave me a patient who clearly has evidence of long-term alcohol use they gave me the presence of an s3 heart sound dilated cardiomyopathy so the sooner that you can make these associations the more free points that you're going to pick up on usmle and complex so that wraps up alcohol and as you will see moving forward in this video alcohol is the one drug with the most associations and the most nitty-gritty information that you need to know really honestly everything from here on forward is going to be extremely straightforward with classic signs of intoxication classic signs of withdrawal and very few relatively speaking fewer", "id": "_shuV1tJbTU_14", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  so let's talk about opioids so opioids when in the intoxicated state the person will feel euphoric they're going to feel really good they'll also have central nervous system and respiratory depression so this is really the cause of death when it comes to overdosing on opioids classically you'll see pinpoint pupils so pupillary constriction and a decreased gag reflex now remember back from the alcohol part of this video i told you that whatever the drug does in in its intoxication state it does the opposite in its withdrawal state so you can see here that the withdrawal state has dilated pupils because the intoxicated state has pinpoint pupils and the withdrawal state is like fluid and liquid coming out of everywhere so lacrimation sweating nausea vomiting and diarrhea and if you think about somebody classically who gets opioids for pain or related to surgery you might remember", "id": "_shuV1tJbTU_15", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  serious constipation so if when you take the opioid it causes constipation then when you withdraw from it and have the opposite effect you have liquid and fluid coming out of everywhere so sweating lacrimation nausea vomiting diarrhea should make sense the one particularly unique symptom in opioid withdrawal is yawning and because it's unique it shows up on usmle and comlex a lot and the way that i encourage you to remember this is the opioid whoosh the opioid os reminds me that opioids in withdrawal cause yawning the opioid oohs how do you like that big baryon now as far as associations go like i said opioids are much more straightforward than alcohol the big ones you want to think about are right-sided endocarditis and abscesses but you also want to link this in your brain", "id": "_shuV1tJbTU_16", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  overdose now when it comes to usmle and comlex the associations like i said you really want to remember right-sided endocarditis and abscesses so getting back to this idea of creating associations and concept maps to help you anticipate and dominate third order questions if the test writer wants you to pick endocarditis or wants to make that a connection when they start the question with opioids but end it with something cardiovascular they'll give you symptoms like roth spots janeway lesions splinter hemorrhages osler nodes fever shortness of breath and as far as imaging or labs you want to be on the lookout for valve vegetations anemia positive blood cultures etc recall that the reason that right-sided endocarditis is associated with opioids but even broadly intravenous drug abuse is because the blood will flow through the right side", "id": "_shuV1tJbTU_17", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and therefore a lot of these symptoms will cause right-sided endocarditis when the infectious agents or pathogens flow through the right side of the heart and similarly opioids are related to abscesses for you know this the same reasoning you got a lot of pathogen exposure when it comes to intravenous drug abuse so if they want you to think about abscesses they'll go after painful warm purulent or fluctuate masses and as far as labs go they could give you a leukocytosis crp procalcitonin and maybe even advanced imaging if they want you to suspect risk for osteomyelitis so when it comes to opioids just remember the things that are related to intravenous drug abuse right-sided endocarditis and abscesses so with that that wraps up high-yield opioids now let's talk about benzodiazepines so benzodiazepines are very similar in terms of intoxication", "id": "_shuV1tJbTU_18", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to alcohol because alcohol is a sedative it acts on gaba benzos and also we'll talk briefly barbiturates are also sedatives and they also work on gaba so unsurprisingly you're going to see similar intoxicated states and similar withdrawal states so for intoxication we are talking about things like ataxia mild respiratory depression and somnolence and if in the intoxication state you see all these inhibitory actions then the opposite of that is true in the withdrawal state so you see excitatory actions so if benzos make you sleep when you withdraw from them you get insomnia if benzos treat anxiety then when you withdraw from them you get anxiety and just like other sedatives you can get withdraw seizures for the same reasons we talked about back in the alcohol section now of note it is important for me to point out withdraw seizures from benzos can occur on a much more drawn out or delayed", "id": "_shuV1tJbTU_19", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  so you can have somebody that's withdrawing from benzos and then up to like one to three weeks later they can still have a withdrawal seizure so just being outside of that usual timeline that you think of when you think of alcohol withdrawal seizures does not necessarily mean that the withdrawal seizure can't be due to withdrawing from benzodiazepines now as far as associations it's actually not the related diseases or complications that you need to know when it comes to benzos for whatever reason test writers for usmle and comlex love love love to ask you the very subtle difference in the mechanism between benzodiazepines and barbiturates and the reason that they do this is because the mechanisms as you can see by looking on the slide are so similar so both benzos and barbs both affect chloride channel opening working through and modulating gaba the difference is that benzos increase the frequency of the chloride channel", "id": "_shuV1tJbTU_20", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  increase the duration of chloride channel opening and the way to remember this is that ben wants it to happen more often but barb wants it to last longer ah thank you all right so that wraps up benzodiazepines next we are going to talk about cocaine cocaine is a major stimulant so when it comes to being intoxicated with cocaine you see evidence of stimulation so you'll see pupil dilation agitation euphoria hallucinations which can be different types of hallucinations but for the purposes of us emily or comlex you want to think about tactile hallucinations so if the question gives you somebody who feels like bugs crawling on them think cocaine alertness and arousal so if all of that stuff if being like supercharged and stimulated is what it feels like to be intoxicated with cocaine then the opposite will be true in withdrawal so if cocaine wakes you up", "id": "_shuV1tJbTU_21", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  then when you withdraw from it people will feel sleepy and when you withdraw from it people will feel hungry because stimulants reduce appetite and if it makes you feel euphoric then when you withdraw from it people will experience depression now worth mentioning is that the mechanism by which cocaine acts in the brain is that it's basically acting acting as an antidepressant it's increasing dopamine norepinephrine and serotonin in the brain and that leads to these euphoric symptoms consequently this is why when you then have withdrawal from cocaine and have the opposite effect you experience depression so it shouldn't surprise you to hear that cocaine is highly associated with drug-induced depression and drug-induced suicidal ideation when people are coming down off of their cocaine trips the other associations that you want to keep in mind are what you see on this slide nasal septum perforation is a big one and this is due to the vasoconstrictive", "id": "_shuV1tJbTU_22", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  of the stimulant cocaine you can also get cocaine-induced cardiomyopathy so when that person gets really supercharged on the stimulant all of that supercharging puts them into sympathetic overdrive which does have effects on the heart people can also feel paranoid they get hallucinations and tactile hallucinations and a little bit of impaired judgment and impaired reality testing leading to paranoia we already talked about drug-induced depression and then two related topics which are extremely high yield for usmle and comlex are to remember that cocaine can lead to renal tubular necrosis and rhabdomyolysis so keep all these associations in mind because when you get that initial question on cocaine chances are they're not going after just what's it like to be drugged up on cocaine they're probably going after some complication or some association so that wraps up everything that we need to talk about with regards to cocaine next we're going to focus on a very similar stimulant", "id": "_shuV1tJbTU_23", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  now because methamphetamine is also a stimulant and works very similarly to how cocaine works it shouldn't surprise you that the intoxication and withdrawal states are pretty similar for intoxication you get some pathomemetic effects so we're seeing things like pupil dilation agitation euphoria hallucinations arousal alertness and wakefulness and consistent with the theme that we've talked about throughout this video thus far if you have one syndrome in the intoxication state then you have the opposite in the withdrawal state so if intoxication is sympathomimetic effects feeling supercharged etc then the withdrawal symptoms should be the opposite so you see that the opposite of agitation euphoria arousal alertness is sleepiness hunger and depression so the intoxication and withdrawal states of a lot of different stimulant drugs are very similar", "id": "_shuV1tJbTU_24", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  that you want to be on the lookout for on your examination usmle or comlex for methamphetamine that's referred to as meth mouth so meth mouth is the characteristic dental caries and just general poor dentition of people who suffer from stimulant use disorder particularly methamphetamine use disorder there are a couple different theories about why methmouth is an entity it seems to be a combination of a few things one when people are using methamphetamine they're using a substance that has vasoconstrictive properties and therefore it's possible that that vasoconstriction can lead to less blood flow or less salivary flow into the mouth with which generally predicts poor dental outcomes the second thing is that people who use methamphetamine in general are already more likely or less likely i should say to care", "id": "_shuV1tJbTU_25", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  their dental hygiene lastly there's an association between methamphetamine use and bruxism so the constant clenching or grinding of the teeth is obviously not good for dental hygiene but nonetheless you have all of these things working in combination with one another giving you this characteristic meth mouth so if you're taking usmle or comlex and you have a patient in your clinical vignette using an unidentified stimulant and they describe methmouth to you or they even show you a picture that looks like this the test writer is telling you hey they're using methamphetamine and lastly before i go any further i do want to just point out the mechanism or the pathophysiology of how methamphetamine works in the brain so it causes a very strong burst of release of monoamine neurotransmitters and when i use the terminology monoamine neurotransmitters i'm", "id": "_shuV1tJbTU_26", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  serotonin and norepinephrine so not only are methamphetamines sympathomimetic because it's causing the release of norepinephrine and dopamine but it also has profound mood changes because it releases the same monoamines that are affected by things like antidepressants and antipsychotic medications and when you have those monoamines being released in the brain in an uncontrolled abusive fashion you get things like alertness arousal euphoria psychosis etc so that's methamphetamine again bottom line here is be on the lookout for meth mouth be on the lookout for clinical descriptions of meth mouth if the test writer doesn't give you a picture that's methamphetamine now we're going to talk about pcp phencyclidine pcp intoxication is pretty characteristic on exams it's going to be someone who's violent so the classic test description", "id": "_shuV1tJbTU_27", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  with pcp is somebody that initially experiences a little bit of euphoria right they're feeling good and then all of a sudden they become violent and aggressive and in the clinical vignette it's going to say that like four to six people attempt to restrain the patient and it doesn't work so the classic description is this like herculean strength where you've got all these hospital security guards trying to hold the person down get them under control so that they don't hurt themselves or don't hurt somebody else but despite the fact that there's you know four to six security guards on the scene the person is just thrashing about and kind of like breaking free and causing all of this mayhem so if you get a test question on u.s emily or comlex and it's one person with seemingly super human strength the test writer is telling you hey it's pcp now as far as the pathophysiology of how pcp is working in the brain it's", "id": "_shuV1tJbTU_28", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  complex drug but on usmle or comlex what i believe the test writer will go after is its action at the nmda receptor specifically pcp is an nmda receptor antagonist and that action generally speaking causes psychosis and to lesser extents when combined with its sympathomimetic effects it causes analgesia so when you put together somebody who's a little psychotic a little euphoric and can't really feel pain and might have these like super human feelings that leads to this superhuman aggression violence and strength so bottom line here is that on a test somebody who's intoxicated with pcp is going to be going crazy and really hard to restrain and have this kind of like superhuman strength now when it comes to withdrawal withdrawal from pcp honestly on us emily and comlex is just not high yield at all i wouldn't worry about it too much but just for completeness sake typically you'll see some mood disturbances some insomnia", "id": "_shuV1tJbTU_29", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  high yield as what it looks like to be intoxicated on it the clinical association that you want to look for for pcp really the only thing is a unique type of nystagmus and specifically we're talking about rotatory also known as torsional nystagmus and i'm going to give you a mnemonic here to remember this but i also want to just show you an image of what this looks like because a lot of people are used to either vertical or horizontal nystagmus and if you look on the right part of this slide rotatory nystagmus has as the name implies this sort of rotatory or torsional movement where it moves about the axis in a in a somewhat diagonal direction and as far as my mnemonic is concerned i think that pairing up rotatory or torsional nystagmus with pcp is easy to do if you can remember the full name of pcp being phencyclidine so you see that i put the", "id": "_shuV1tJbTU_30", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  on the slide and that ring or that fen cyclidine reminds me that the rotatory nystagmus kind of moves in a ring like a pattern about an axis so the way that the eyes move in my head looks like the way that the rings are organized and i remember the rings because it's phencyclidine and cycla or cycle should make you think of a ring chemically speaking and i'm very sorry if i'm triggering anybody's post-traumatic stress disorder from your organic chemistry class back in the day but it nonetheless that's my mnemonic i think it's easy to remember bottom line when it comes to tests for pcp you want to know about that superhuman strength its mechanism being an nmda receptor antagonist and that the clinical finding or symptom that you'll see on exam if you look at somebody who's high on pcp is going to be that rotatory slash torsional nystagmus so that's pcp the next illicit drug that", "id": "_shuV1tJbTU_31", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  is mdma also known as ecstasy and as the name implies ecstasy makes people feel like they are in ecstasy so the symptoms of intoxication include hallucinations euphoria disinhibition bruxism and an altered sense of time and sensation now mdma works as basically like a supercharged antidepressant it works on serotonin receptors and dopamine receptors to block the reuptake of those neurotransmitters thereby increasing the amount of serotonin and dopamine in the synaptic cleft withdrawal symptoms of ecstasy just like the withdrawal symptoms of pcp are absolutely not high yield so tune me out for the next 10 seconds if you don't care but just for completeness sake the symptoms of withdrawal from ecstasy are anxiety concentration difficulties and depression since mdma kind of works like a supercharged ssri", "id": "_shuV1tJbTU_32", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  you think about what you might see in an ssri so serotonin syndrome hyponatremia slash thirst seizures should both make sense serotonin syndrome because this works on serotonin and you should know that your ssris can cause s-i-adh likewise mdma can cause the person to feel excessive thirst and when people are drinking water over and over and over again while intoxicated on mdma they can become hyponatremic and if their sodium level drops to a certain threshold they can experience seizures now bruxism is also a classic association so when you see bruxism you want to be thinking immediately about either mdma or methamphetamine and then kind of narrow it down further based on what else the question or clinical vignette gives you the unique clinical association to mdma is this thing called hallucinogen", "id": "_shuV1tJbTU_33", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  a fancy way of saying that when somebody does ecstasy some of that ecstasy can actually be stored in the body stored in some of the tissues in the body and then later on when the person is not actively choosing to get high on mdma those stores can actually slowly release and the person can re-experience the intoxicating effects of mdma so bottom line here as far as what you should take out of this slide for examination purposes know that mdma kind of works like a supercharged antidepressant and therefore when people are intoxicated on mdma they're going to feel really good and when they're coming off of it they're going to feel really depressed but as well the associations are going to be things like serotonin syndrome hyponatremia thirst seizures and mdma generally speaking is going to be described to you on an exam like the classic picture of what you think about when you think about your you know 70s era hippie getting high on some drug they're gonna feel good they're gonna have an altered sense of time and", "id": "_shuV1tJbTU_34", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  what's up man you want some that's what mdma is gonna look like on a test so pretty straightforward but that is mdma the last category that we need to talk about is marijuana so marijuana what uh basically works by interacting with the cb1 and cb2 receptor so we're talking about cannabinoid receptors now i think that most people are probably comfortable with the clinical picture of what it looks like to be intoxicated with marijuana it looks like somebody who is calm whose anxieties generally go away who may have altered judgment or slowed reaction time the munchies are characteristic so you're going to see increased appetite and conjunctival injection so redness in the eye pretty straightforward now we've talked about this ad nauseam today but if that's what intoxication looks like then withdrawal is the opposite so if when you smoke weed you are", "id": "_shuV1tJbTU_35", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  then when you're coming off of marijuana you have a decreased appetite you can't sleep and you're irritable so it should make a lot of sense that intoxication and withdrawal are opposites now there are two very characteristic associations with marijuana that you want to be on the lookout for on usmle and comlex one is that in a certain percentage of people that use marijuana they actually don't experience the calmness and the classic picture that you might have in your brain when you think about somebody smoking weed instead they have psychosis paranoia and it's really what's called quote a bad trip so for those people in particular marijuana can be quite psychotomatic meaning it makes you psychotic and paranoid the other high-yield clinical association to be on the lookout for is cannabinoid hyperemesis syndrome and the pathophysiology here is still somewhat poorly understood as more", "id": "_shuV1tJbTU_36", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  but what what this kind of looks like here is somebody uses marijuana consistently for a long time and that at some point they either change the amount of marijuana that they're using drastically so maybe they go from smoking four times a day to not at all or they have some type of paradoxical reaction where all of a sudden they just have this profuse vomiting so they're very nauseous they're vomiting non-stop and interestingly you want to look for this buzzword on u.s emily or comlex in in response to heat the symptoms get better so the classic patient with cannabinoid hyperemesis syndrome will have profuse vomiting and they'll learn that the only way to make that better or to make that stop is to take a hot shower i know this sounds ridiculous but this is a very poorly understood but increasingly common clint clinical", "id": "_shuV1tJbTU_37", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and for that reason you want to know this because it's my guess that within the next couple years maybe the pathophysiology will be very well understood and this will be a new high-yield buzzword thing that's going to show up on exams all the time and i would also couple that with the fact that if you look in the u.s what's happening with marijuana with all of these states legalizing it it's very important that you understand what the clinical sequelae might look like so be on the lookout for cannabinoid hyperemesis again somebody that's using marijuana that has profuse vomiting that gets better in response to heat that's marijuana all right so with that we've now gone through all of the high-yield drugs of abuse again just to summarize we talked about what it looks like to be intoxicated what it looks like when the person's going through withdrawal and i tried to touch on all of the high-yield clinical associations that are either associations with complications", "id": "_shuV1tJbTU_38", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  conditions so that you can train your brain to make those high yield neural networks and dominate those third order questions now what i want to do for the next like 20 seconds is just run through rapid review style some high yield symptoms to test you to see if you can pull out what you need to know when you're given certain buzzwords so let's just do this real quick i think this will be useful to you i'm going to show you a picture and that picture will either be a drug a certain type of drug intoxication or a certain type of drug withdrawal and based on that picture or that buzzword you have to tell me what drug intoxication or what drug withdrawal we're talking about so let's get started all right so you see pupillary constriction here and it's important to remember that this could actually be two things this is either opioid intoxication so that's pupil constriction or it's stimulant withdrawal and remember that this could be either the intoxication or", "id": "_shuV1tJbTU_39", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  opposite of a different type of intoxication so opioid intoxication causes pupillary constriction intoxication causes pupillary dilation and therefore the opposite of stimulant intoxication stimulant withdrawal would be pupillary constriction as well so this isn't either or and it would depend on other things in the question all right what do you see here you see a lot of redness of the eye this is conjunctival injection this is marijuana intoxication next what do you see here you see really poor dentition which should immediately make you think methmouth methamphetamine abuse remember vasoconstriction of blood supply and saliva to the mouth bruxism patients that are constantly abusing this substance and probably don't pay a lot of attention to their dental hygiene this is meth mouth what if you have somebody who's excessively thirsty what drug are we", "id": "_shuV1tJbTU_40", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  we're talking about mdma ecstasy intoxication remember this can lead to hyponatremia and seizures and you want to pair all that up with mdma because it's very serotonergic all right what if you see somebody who's just yawning a lot what should that make you think in terms of drugs of abuse that should make you think of the opioid oos remember the opioid o's opioid withdrawal causes yawning and then lastly somebody feeling like they have bugs crawling on their skin what should that make you think when it comes to drugs of abuse when it comes to those tactile hallucinations feeling like bugs are crawling on your skin you want to think cocaine intoxication all right so that wraps up everything in this video the last slide i want to give you is just for completeness sake and for your studying pleasure this is all the substances that we talked about with", "id": "_shuV1tJbTU_41", "title": "4. Risk Stratification, Part 1", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  mechanism or formal pathophysiology written out so if you want this come to this slide at the end of the video this again is just for your studying pleasure i went through this video this was a long one i went through it relatively quickly although i tried to spend a lot of time on the individual areas that i really want you to focus on because again i think the most important thing is making those high-yield associations a lot of questions on usmle and comlex start as if they're going to ask you about substance abuse and then it pivots into something else and they're either relying on a buzzword a clinical finding or an association or a complication this is a great way for test writers to challenge your brain to think clinically to think logically and to make high-yield connections so i hope this video was useful to you if it was please consider clicking the join button and contributing to my my channel financially wish you guys the best of luck", "id": "_shuV1tJbTU_42"}, {"text": "  \u266a Bob and Brad \u266a \u266a The two most famous\nphysical therapists \u266a \u266a On the internet \u266a - Hi folks, Brad Heineck,\nphysical therapist. - Bob, physical therapist,\nobviously not here. Chris is here, the pharmacist,\nto help us with this video. Bob is on vacation this week,\ntaking some well-deserved R&R. The title of today's video\nis ibuprofen versus Aleve versus turmeric versus Tylenol and we're updating this video, we did this video about three years ago and by popular demand,\nwe're gonna add aspirin. A lot of people were\nasking what about aspirin? So we're gonna include that this time and Chris has got some updated research and things that may come\nup, new that wasn't known back three years ago. Before we go any farther, I do need to get a little business out of the way. Here we go. How does this go again? Bob does this all the\ntime, I'm a little slow. Join us at BobandBrad.com, we do have", "id": "wqI_z1yumzY_0", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  It is the Knee Glide, which is a device great for range\nof motion of the knee before or after surgeries,\nknee surgeries that is. As well, if you have an arthritic knee, just sign up on the giveaway aspect there. Go to Facebook, it's pinned\nto the top of the page. Also join us at Tik Tak,\nInstagram or the Twitter account and you can see us,\nshorter versions there, if you don't wanna put up with our lengthy 10 minute version. Okay, Chris, again these medications, very popular. Probably everyone across\nthe nation, I would say and maybe worldwide, has\ntaken a form of these for their aches, their\npains, whatever it may be but so many people, do I take ibuprofen? Do I take Tylenol? Do I take acetaminophen? What's the difference? So that's what we're gonna talk about. So Chris, let's start with\nwhat we missed last time and so many people were\ninterested in, aspirin.", "id": "wqI_z1yumzY_1", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  - It's been around for thousands of years. Aspirin is the granddaddy of them all, so I mean, we've been using\nit for thousands of years, it actually was-- - We don't have aspirin here but we're just gonna talk about it. - We're just gonna talk about it but basically, aspirin, mostly\nextracted from willow bark. It was just a natural-- - What? - Willow bark. - Willow bark? - Yeah, just from the tree. So basically, they would,\nNative Americans would chew on it and it would\nreduce inflammation and fever, it was remarkable and then\nover the years we decided to try and make it a\nlittle bit more scientific and Dr Bayer in Germany,\nthe make Bayer Aspirin. - Sure. - He was the one that got\nthe patent in the 1890s and so it's been around as a patented drug for well over a century now. - Well, I know when I was\na child back in the 60s, that's all you heard about was aspirin. You need an aspirin, you can get aspirin and not so much anymore. - No, not so much anymore. I mean, aspirin does\nhave its share of risks and basically, most commonly\nfor most people it can be a bleed risk, which for\na lot of us would be the other type od stroke,\nnot the clotting stroke", "id": "wqI_z1yumzY_2", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  - The blood thinner-- - Blood thinner aspects of it and also for people, a lot of\nulceration or tummy problems. So we have to be kind of careful. Those are the two major ones\nthat most people will fall into and it's fallen out of favor\nas a fever reducer in children because of Reye's Syndrome,\nso it can be a rough-- - Reye's? - Reye's Syndrome. - R-Y-- - R-Y-E-S, I believe and\nso it's just something that we just don't give\nto kids with fever. - Okay. - So that's what Tylenol came around for. - And I know my mother\nhad baby aspirin for her, just a maintenance thing, for\na blood thinner I understand. - Yep as a blood thinner, yep. - And actually last year,\nthe doctor said no more, they're not doing that any more\nbecause of hazards or risks. - Bleed risks again, so. - Bleed? - Yep, so basically\nwhat happens is you have two types of strokes, you\nhave the clotting stroke where something clots up and\nblocks the flow of blood. - So in your brain? - In your brain. - The blood flow can give us that stroke. - And then the other type of\nstroke is the bleeding risk, which is actually more\nrare but more serious and that's where a capillary\nor something would burst", "id": "wqI_z1yumzY_3", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and then very dangerous\ndevastating things would occur. - Right, so that's the hemorrhagic stroke. - Hemorrhagic stroke, 100% correct. - Okay, so again, that is no longer, or the doctor, it's up to the doctor if it's-- - Yeah you wanna check with\nyour doctor if aspirin is appropriate for you for\nthat particular reason. They've changed a lot of the guidelines over the last couple of years. We used to have everybody taking aspirin for blood thinning purposes, even if you were healthy. Now we find that that's not the case, again because of the bleed risk. - So if you cut yourself\nand you're on aspirin, are you gonna bleed more? - Little more, little more. - It's not like you're gonna bleed out? - You're not gonna bleed\nout, you're not gonna get exsanguinated or anything like that. You'll clot up but it just\ntakes a little bit longer, a little more pressure,\nprobably a band-aid. - So it's still a good thing you can buy, you can buy it off the\nshelf, you can use it but again, it seems like these\nare more popular nowadays. - More popular, I mean\naspirin back in the day, it's a great anti-inflammatory,\nso it's good for arthritis. It's still good for arthritis and it's still good for\nfevers if you're sick, if you're an adult or over the age of 12.", "id": "wqI_z1yumzY_4", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  but we've moved on to\nother items like ibuprofen but again, basically it's just kind of a chemical modification on what aspirin's structure is and for a lot of us, I think it's probably a little bit\nbetter anti-inflammatory 'cause it's a little bit less GI upset and stomach irritation but it still can, so we always want to take\nthis with a little bit of food to protect the tummy. - And didn't you say, I\nthink you've told me milk, if nothing else, is a good base for it? - Yep, milk's a little bit of a buffer 'cause it's not like water\nwhere everything's transparent, milk has a little bit to it,\nso with all the other nutrients that are in milk, so it slows it down and protects it and buffers\nthe stomach a little bit. - So if you've got a sprained\nankle, something swelling up, that's that inflammatory\nprocess, that would be a good reason you might wanna take this? - Yeah it's a great reason to take it. - How 'bout a headache? - Headache is a great reason to take it. - And everyone kind of perform or responds differently\nto these, I understand? So which one do you wanna take? - Yeah usually my go-to, as a pharmacist,", "id": "wqI_z1yumzY_5", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  I'll ask a couple of\nquestions of a patient to see what's best for them\nbut really ibuprofen's my go-to for a lot of the patients\nthat I try and help but for a headache, I think it's great but there's reasons to go\nwith acetaminophen or Tylenol, there's reasons that\nsome people like Aleve because it lasts longer, so we have advantages on all products\nfor different reasons. - Sure, okay. How many do you take, if\ntwo's good, is six better? - No, no, no! We don't, more is good, some is good, more is\nbetter, definitely not. So we want to go to the regular directions that the manufacturer throws out there, which is one or two tablets, up to every four to six hours apart. So when you read the fine print-- - Is that written on here? I can't read it. - You can't read without your glasses, my contacts are in, so\nI can actually read it but it's one or two, every four to six hours apart as needed but no more than eight in a day. So eight is your hard line in the sand. - And isn't there something\nabout this, ibuprofen, being hard on your kidneys\nor your heart possibly? Or is that rare? - No it's not, it's not common but it's definitely\nsomething that doctors are", "id": "wqI_z1yumzY_6", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  So if you're on certain\nblood pressure medications, if you have certain kidney problems, we do have to be extra\ncautious with these guys and turmeric to a degree as well. So again, that's what the\nacetaminophen is in here for, so we'll kind of work our way down to that but the reality of it is,\nalways check with your doctor to make sure it's appropriate for you or check with your pharmacist\nto make sure it's appropriate. - Right, right, okay. Very good, anything else on ibuprofen? - No ibuprofen, again-- - I know it's like, we talk\nabout as you and I are active, I exercise a lot, we injure\nourself, over-exercise, those kinds of things. - Oh yeah. - Is this something that the\nathlete or the weekend athlete, the person who's running the marathons, is it gonna be good for\ntheir aches and pains to get to pulled muscles et cetera? - Yeah I think so, I think the advantages of anti-inflammatories and there's two schools of thought but fighting the\ninflammation fights the pain, so if we're uncomfortable, it's gonna allow us to train further, so it'll get you back\nout there, participating. There are some schools of\nthought, more modern science is saying some of that inflammation\nactually is a good thing, so we have to have a fine balance\nbut I guess in your world,", "id": "wqI_z1yumzY_7", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  usually rest is probably\nthe added measure of caution from that standpoint. Sometimes we don't listen\nvery well ourselves, as we've done on our\nSaturday morning expeditions. - I've got a five mile\nrun and if it hurts, I'll take some of these\nand I'll get through it but that's probably not the idea. Shall we go on to Aleve? - Yeah, Aleve or Naproxen sodium. - Aleve's not an anti-inflammatory? - It is an anti-inflammatory. - But it's not... - It's another cousin of\nibuprofen, so it behaves the same. I don't wanna say as similar\nas Coke and Pepsi per se and I don't wanna advocate sugar but that said, it's just a close analogy but this is another type\nof anti-inflammatory, if you were to actually\nbe really scientific and pop it under a microscope and look at all the, an\nelectron microscope to look at the structure, they're\nvery very similar skeletons. The advantage of Aleve over\nibuprofen in some cases is it lasts a lot longer and when you were bringing\nup that heart disease issue, some studies suggest that\nNaproxen sodium might be a little bit better for\nour blood pressure patients and people that have some kidney problems,", "id": "wqI_z1yumzY_8", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  but again, check with a\ndoctor to be on the safe side. - It sounds like if you've\ngot some co-morbidities, other problems and medications,\nthese over the counter ones, you really probably should check to make sure it's gonna fit your-- - It's always good to ask questions. The only question that's a bad question is the one that's never asked. So always check with your\npharmacist and your doctor, again, but the nice thing with\nAleve, with Naproxen sodium, specifically is it might\nwork a little bit better on soft tissue injuries, so\nif you've got a muscle ache, a tear, a pull, it might\ndo a little bit better job than even ibuprofen, at\nleast some of the older and some more modern studies have shown. - But as far as joint pain\nor headaches, it only? - No, it'll work for those too. Again, just as you and I\nare different, we respond to different medications across\nthe board differently too. So it kind of comes down to a\nlittle bit of trial and error and making sure it's appropriate for you. - Typically with this, I\nbelieve, I was dealing with this with my mother, she could\nonly take one in the morning, one at night and no more because\nthere's potential to have", "id": "wqI_z1yumzY_9", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  - Yep, again, stomach ulceration, probably a little bit worse\nwith this than the ibuprofen. So we have to be careful,\nso food or a glass of milk but food probably a little\nbit better with the Naproxen insofar as that also still\nhas the same bleed risk, so if you're on blood\nthinners, so again, ibuprofen, aspirin and Aleve and\nwhen we get to turmeric, we're gonna have to be\ncareful with that too. - Okay. All right, shall we go to? Now this is something, a lot of people, we did spell it wrong on the last video, apologize for that and the pronunciation, we need an English major on our staff but anyway, turmeric, it's a spice. It's used, I understand, for mustard and it's got some healing\nvalues, benefits to it. So you wanna talk about that? - Yep, turmeric basically, most of the research that's out there that's best and has\nmost scientific basis is for an anti-inflammatory, so\nit's gonna behave like aspirin, ibuprofen and Naproxen sodium.", "id": "wqI_z1yumzY_10", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and anywhere, you wanna make\nsure it's turmeric curcumin, it's the curcumins, that's the scientific, that's the thing that\nactually makes it yellow but it's also the business end, it's what makes this an anti-inflammatory. - Gives this medical benefits. - So that's where the\nbenefits are coming from and you wanna make sure that\nthe product that you choose, in a lot of cases you're\nnot gonna just take a spoonful of turmeric\nfrom the grocery store, you're probably gonna wanna buy something that's a standardized extract, you wanna make sure it's 95% pure. If you can find the symbol USP, it stands for United States Pharmacopeia, it means that it's been lab-reviewed and independently tested. So it's gonna ensure that\nwhat's in that capsule is in that capsule. - So using this, this is mine from home and I use it to spice my food. Is just putting it on your\nfood gonna be a benefit or is it? - It's gonna be a\nbenefit but it's gonna be really hard for you to really get the anti-inflammatory aspects of it. Turmeric, when it hits the\nstomach acid, dissolves quickly, so you really don't absorb a lot of it, so it makes everything taste great", "id": "wqI_z1yumzY_11", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  All right, so yeah, so\nPiperazine or it comes from black pepper extract and so with that, basically what we found is that seems to piggyback on to the turmeric and allow it to absorb better in the gut and there are some other\ncompanies that are getting really fancy and using enteric coating, so they make a special\ncoating on the capsule, so it doesn't dissolve in\nthe gut or in the stomach but it dissolves in the intestines and absorbs there a little bit better. - Sure. - And so they use kind of some\nfancy science to create that and then what happens\nis, as you're delivering more turmeric to the system and the dose that you're looking for is probably about 500 to 1000 milligrams, so it's a little bit different\nthen when I delivered-- - Per day? - Well, per dose, so you can-- - And this is from a study or something? - Multiple studies and\nusually 500 milligrams to 1000 milligrams up to per day or even a couple of times a day, depending upon what\nyou're going for for pain and inflammation management and most of the studies suggest\nusing it for osteoarthritis, which is that wear and tear,\naches and pains that we all get as we age.", "id": "wqI_z1yumzY_12", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  or even three months of regular use. - Okay. So is there really no risk\nwith this compared to this or is there some? - No there's still risk, it's still a natural anti-inflammatory, so it's gonna behave like your aspirin, behave like your ibuprofen and behave like your Naproxen sodium. So if you're on blood thinners,\nwe've gotta be careful. Talk to your doctor first, that's-- - But it's not gonna be a\nproblem just from your food? - Food amounts, probably\nnot unless you are really, I mean if you are really into the spice but and therapeutic\namounts, when you're taking a supplement, we have to be careful and also again, reflux,\nstomach issues kinda rear their ugly head, same with\naspirin, same with ibuprofen, same with Naproxen sodium. - So if people wanna do this\nand get the pill form online or at a box store or? - Yeah again, you can\ngo box store, online, all are good but again, you\nwanna make sure you've got that stamp of approval, look for USP or a highly reputable company because again, just with\nany supplement nature, you can have, it's the\nwild wild west out there.", "id": "wqI_z1yumzY_13", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and all of a sudden it's\ncrushed crab shells, so it's just something that\nyou wanna make sure that-- - Crushed crab shells? - Yep that's a different\nsubject all together, that's like glucosamine. - Is that a pharmaceutical-- - They use that for glucosamine but that's a whole different discussion, we'll do another video on that. - Okay. - [Cameraman] There's lots of\npotential with neuroscience. - Yeah there's lots of\nanecdotal studies out there that don't really-- - Now before we go on,\nour cameraman just asked a good question about\nthe neuroscience of this with relation to Parkinson's and dementia? - Yeah it can be cognition and dementia. Some people feel that using\nthe substance turmeric may improve symptoms like that,\nso more or less memory issues, dementia would be the ones\nthat I've seen most commonly but the studies have been small in scale, more anecdotal saying this worked. It's definitely safe enough, if you're not on any blood thinners, it's certainly something you could try but there needs to be a\nlot more research done", "id": "wqI_z1yumzY_14", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  - But if someone wants to try\nit and the doctor says fine, is it one of those things\nthat there's gonna be no harm to it, the worse\nthat'll happen is-- - Generally, well again, it's you know, I think that's one of the misconceptions with herbal type products is\nthat it's nature, so it's safe. I mean, yes for the most\npart it's going to be safe, always check with the doctor first, check with your pharmacist, make sure there's nothing\nin that profile of yours that would interact with\nit, like the blood thinners or other, all of a sudden\nthey're taking aspirin or ibuprofen for their sore joints and they're piling on\nturmeric because they want to try and help to improve\nmaybe some cognition it could cause some GI side\neffects at the very least and it could go even further than that, so like a bleed risk, so\nwe have to be safe with it and smart with it. - Again, there's always some people, if this is good, maybe I\nbetter throw this in there too and this and I'm gonna get better faster. - And we're gonna draw\nthe hard pass on that too because you don't wanna combine\nthem because you can have, you can combine them to get the benefits but you can combine them\nand it can blow up on you.", "id": "wqI_z1yumzY_15", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  - Exactly, so we do have\nto be careful with that. - Okay, now we've got the next one, which evidently is a different family but we have it on the shelf,\npeople use it all the time, which is acetaminophen. - Yep, acetaminophen. - Aceto, aceta? - Acetaminophen. - Yeah, that's a good one. - There you go, otherwise known as-- - What is that, that's a family show. - Yeah so basically from this, the big difference from these guys are it's a great pain reliever but it doesn't carry the\nbleed risks that these people plus the aspirin are going to have. So with this particular\nmedication, it's gonna allow you to lower fevers if you're\nsick, it's gonna get rid of that headache, it's\ngonna help those sore knees, it's gonna help those sore\nelbows and it's not gonna run the bleed risk or that stroke risk that could possibly be potentiated by the other anti-inflammatories. - It sounds like this would\nbe the safer of all of them. - I think in a lot of cases you can probably look at it as\nthe safest of the bunch. It's not quite as\neffective across the board because if, let's say,\nyou have a sprained ankle, to take our back example,\nit's not gonna reduce", "id": "wqI_z1yumzY_16", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  to at least get some pain relief. - And so you can take more\nof that without a problem? - No absolutely not, Tylenol\nactually, more than those in higher doses can be much more dangerous and a fatal dose of acetaminophen\nis much easier to achieve than say even doing something like this. It doesn't take long, the daily max that they recommend now is\nactually 3000 milligrams, which would be six of these. - All at once or? - No, you have to spread\nthem out throughout the day, so the directions, when\nyou read this, this is a 500 milligram capsule. There's also regular strength,\n325 milligram tablet. - Can we say who made that? - This is just a generic\nform of acetaminophen, so every store, every box store, online, they're all gonna be acetaminophen. These are the quick release\ngels, so they do absorb into the system a little faster. - I can't remember if we\ncan say the common name that everyone knows. - Tylenol. - Tylenol! - Tylenol is the name\nthat everybody knows it by and so from that standpoint, it doesn't matter where you\nget it or how you get it, it's certainly something that you can use but you want to use it in\nits controlled delivery,", "id": "wqI_z1yumzY_17", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  every four to six hours apart but if you're using the extra strength, which is what most people tend to purchase because stronger is better, you're gonna go to a\nmaximum of six capsules in divided doses throughout the day. - Sure and if you overdo\nit, what can happen to you? - Well you can have lots of\ntrouble with the kidneys, lots of trouble with the liver\nand a fatal dose is 15 grams and acetaminophen toxicity is something nobody wants to go through. - So that would, whatever that-- - It'd be a bottle. - Okay. (Brad laughs) Okay, all right, wow, very good. So there was one other thing\nabout this acetaminophen, is there anything that this will take care of that these won't? - Well no, generally speaking, no and a lot of times now, today\nbecause of the opioid crisis, which is something that we\nhaven't got into at all, we will see people, as pain management, like dental pain, extractions,\nthey're gonna have you combine ibuprofen and Tylenol. - Oh really? - Yep, so you actually\nget much better analgesia, which is pain relief. - Sure.", "id": "wqI_z1yumzY_18", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  and it's safe. - It is? - Yep, in small amounts. So if you do one or two ibuprofen\nand one or two of these, you can take them both together. So children, doctors have\nbeen recommending them for lowering fevers in children\nthat are sick for years, also works for your bad\nknee or tooth extraction or anything to that. - It sounds like maybe\nwe could have a video on that combination. - I could talk a long time on that. (Brad laughs) - 'Cause I'm, like you\nsaid, I'm assuming there are some proper or-- - There are proper guidelines\nand wrong ways too, so we do have to be very careful and again, something you want\nto check with a physician, just to be on the safe side. - All right, well I would say,\nany other questions, Tanner? Our cameraperson back there? But you know, I've been talking to Chris about these for years, I always ask 'cause I can never remember, so if you can't remember\nall this information, just go back and review the video. (Brad laughs) - [Cameraman] Generally speaking though, these all typically have, are there gonna be some dangers with them?", "id": "wqI_z1yumzY_19", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  - So all, they can't\nhear, so the question was can you take acetaminophen with Aleve? Is that gonna be a good combination? Because we just mentioned a\ndifferent combination is better. - You can still do it, so\nit's an excellent question. So again, Aleve, because it behaves like ibuprofen in controlled doses, you can take one of these twice a day, about every eight to 12 hours apart and you can take one or two of these, every four to six hours apart\nin conjunction with the Aleve. So whether it's Aleve,\nwhether it's ibuprofen or even conceivably aspirin,\nit's something that you could use to enhance pain relieving\naspects of it exclusively, or if you had a fever, same deal. It could help to lower that fever. So if you've got flu or maybe COVID and you're really uncomfortable\nwith a high fever, it certainly will help to reduce that. - Again, when you start\ncombining these things and maybe if you're already\non a couple other meds, definitely you need to consult. - Talk with your doctor,\ntalk with your pharmacist, can't stress that enough\nbecause a little bit's okay but a lot can be very bad and\nit can get bad in a hurry. - Wow, all right.", "id": "wqI_z1yumzY_20", "title": "5. Risk Stratification, Part 2", "MIT OpenCourseWare": "MIT OpenCourseWare", "2020-10-22T19:36:55Z": "2020-10-22T19:36:55Z"}, {"text": "  I've gotta digest it now. So once again Chris, appreciate you coming in, joining us. Bob will be back next week and again, we can fix just\nabout anything except for... - A broken heart. - And I don't think\nthese are gonna help that or am I wrong? - Well maybe the aspirin might help with the heart functioning. - Oh really? Well okay then, we got a start. We'll put that in a chapter. - There you go. - Enjoy the day. - Bye everyone.", "id": "wqI_z1yumzY_21"}]}