{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03d93f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting environment.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile environment.yml\n",
    "name: TorchEnv\n",
    "dependencies:\n",
    "  - numpy\n",
    "  - pandas\n",
    "  - openai\n",
    "  - pip:\n",
    "    - matplotlib\n",
    "    - pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6bee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Welcome.py\n",
    "import streamlit as st\n",
    "st.write('Welcome :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a138bf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pages/embeding_trans.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pages/embeding_trans.py\n",
    "import openai\n",
    "import pinecone\n",
    "import streamlit as st\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import datetime\n",
    "from time import sleep\n",
    "\n",
    "st.write('## ')\n",
    "@st.cache_resource\n",
    "def initialize():\n",
    "    openai.api_key = st.secrets['openai_key']\n",
    "    index_name = 'openai-youtube-transcriptions'\n",
    "\n",
    "    # initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "    pinecone.init(\n",
    "        api_key=st.secrets['pinecone_key'],\n",
    "        environment=st.secrets['pinecone_env']  # may be different, check at app.pinecone.io\n",
    "    )\n",
    "\n",
    "    # check if index already exists (it shouldn't if this is first time)\n",
    "    if index_name not in pinecone.list_indexes():\n",
    "        # if does not exist, create index\n",
    "        pinecone.create_index(\n",
    "            index_name,\n",
    "            dimension=1536,\n",
    "            metric='cosine',\n",
    "            metadata_config={'indexed': ['title']}\n",
    "        )\n",
    "    # connect to index\n",
    "    index = pinecone.Index(index_name)\n",
    "    return index\n",
    "index = initialize()\n",
    "\n",
    "# GUI\n",
    "_,col1,_ = st.columns([1,8,1])\n",
    "_,col2,_ = st.columns([1,8,1])\n",
    "with col1:     \n",
    "    uploaded_file = st.file_uploader(\"Upload a JSON file\", type=[\"json\"])\n",
    "    submit = st.button('Upload to Pinecone')\n",
    "with col2: \n",
    "    if uploaded_file is not None:\n",
    "        file_content = json.load(uploaded_file)\n",
    "        transcript_data = file_content['HC-ML']\n",
    "        for indice in transcript_data:\n",
    "            indice['text'] = indice['text'].replace(\"\\n\", \"\")\n",
    "        st.write('#### ç¡®è®¤ä¸€ä¸‹æ ¼å¼å¯¹ä¸å¯¹ï¼Œå¯¹çš„è¯å°±tmç‚¹ç»§ç»­')\n",
    "        st.write(transcript_data[0])\n",
    "        st.write('''example:\\n{'text': \"  [CLICK] DAVID SONTAG: So welcometo spring 2019 Machine Learning for Healthcare. My name is David Sontag. I'm a professor incomputer science. Also I'm in the Institutefor Medical Engineering and Science. My co-instructor todaywill be Pete Szolovits, who I'll introduce more towardsthe end of today's lecture, along with the restof the course staff. So the problem. The problem is that healthcarein the United States costs too much. Currently, we're spending$3 trillion a year, and we're not even necessarilydoing a very good job. Patients who havechronic disease often find that these chronicdiseases are diagnosed late. They're often not managed well.\",\n",
    "                 'id': 'vof7x8r_ZUA_0',\n",
    "                 'title': '1. What Makes Healthcare Unique?',\n",
    "                 'MIT OpenCourseWare': 'MIT OpenCourseWare',\n",
    "                 '2020-10-22T19:38:19Z': '2020-10-22T19:38:19Z'}''')\n",
    "    if submit:\n",
    "        embed_model = \"text-embedding-ada-002\"\n",
    "        batch_size = 100  # how many embeddings we create and insert at once\n",
    "        progress_text = \"Upload in progress. Please wait.\"\n",
    "        my_bar = st.progress(0, text=progress_text)\n",
    "        #for i in tqdm(range(0, len(transcript_data), batch_size)):\n",
    "        for i in tqdm(range(0, 2, batch_size)):\n",
    "            # find end of batch\n",
    "            i_end = min(len(transcript_data), i+batch_size)\n",
    "            meta_batch = transcript_data[i:i_end]\n",
    "            # get ids\n",
    "            ids_batch = [x['id'] for x in meta_batch]\n",
    "            # get texts to encode\n",
    "            texts = [x['text'] for x in meta_batch]\n",
    "            # create embeddings (try-except added to avoid RateLimitError)\n",
    "            try:\n",
    "                res = openai.Embedding.create(input=texts, engine=embed_model)\n",
    "            except:\n",
    "                done = False\n",
    "                while not done:\n",
    "                    sleep(5)\n",
    "                    try:\n",
    "                        res = openai.Embedding.create(input=texts, engine=embed_model)\n",
    "                        done = True\n",
    "                    except:\n",
    "                        pass\n",
    "            embeds = [record['embedding'] for record in res['data']]\n",
    "            # cleanup metadata\n",
    "            meta_batch = [{\n",
    "                'title': x.get('title','unkown'),\n",
    "                'text': x.get('text','NaN')\n",
    "            } for x in meta_batch]\n",
    "            to_upsert = list(zip(ids_batch, embeds, meta_batch))\n",
    "            # upsert to Pinecone\n",
    "            #index.upsert(vectors=to_upsert)\n",
    "            my_bar.progress((i+1)/len(transcript_data)*100, text=progress_text)\n",
    "        st.write('Complete! Do not upload same file')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d893b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pages/RAG-ChatGPT.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pages/RAG-ChatGPT.py\n",
    "import openai\n",
    "import pinecone\n",
    "import streamlit as st\n",
    "@st.cache_resource\n",
    "def initialize():\n",
    "    openai.api_key = st.secrets['openai_key']\n",
    "    index_name = 'openai-youtube-transcriptions'\n",
    "\n",
    "    # initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "    pinecone.init(\n",
    "        api_key=st.secrets['pinecone_key'],\n",
    "        environment=st.secrets['pinecone_env']  # may be different, check at app.pinecone.io\n",
    "    )\n",
    "\n",
    "    # check if index already exists (it shouldn't if this is first time)\n",
    "    if index_name not in pinecone.list_indexes():\n",
    "        # if does not exist, create index\n",
    "        pinecone.create_index(\n",
    "            index_name,\n",
    "            dimension=1536,\n",
    "            metric='cosine',\n",
    "            metadata_config={'indexed': ['title']}\n",
    "        )\n",
    "    # connect to index\n",
    "    index = pinecone.Index(index_name)\n",
    "    return index\n",
    "index = initialize()\n",
    "\n",
    "def complete(prompt):\n",
    "    # query text-davinci-003\n",
    "    res = openai.Completion.create(\n",
    "        engine='text-davinci-003',\n",
    "        prompt=prompt,\n",
    "        temperature=0,\n",
    "        max_tokens=400,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "    return res['choices'][0]['text'].strip()\n",
    "\n",
    "\n",
    "limit = 3750\n",
    "embed_model = \"text-embedding-ada-002\"\n",
    "def retrieve(query):\n",
    "    res = openai.Embedding.create(\n",
    "        input=[query],\n",
    "        engine=embed_model\n",
    "    )\n",
    "\n",
    "    # retrieve from Pinecone\n",
    "    xq = res['data'][0]['embedding']\n",
    "\n",
    "    # get relevant contexts\n",
    "    res = index.query(xq, top_k=3, include_metadata=True)\n",
    "    contexts = [\n",
    "        x['metadata']['text'] for x in res['matches']\n",
    "    ]\n",
    "\n",
    "    # build our prompt with the retrieved contexts included\n",
    "    prompt_start = (\n",
    "        \"Answer the question based on the context below.\\n\\n\"+\n",
    "        \"Context:\\n\"\n",
    "    )\n",
    "    prompt_end = (\n",
    "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    )\n",
    "    # append contexts until hitting limit\n",
    "    for i in range(1, len(contexts)):\n",
    "        if len(\"\\n\\n---\\n\\n\".join(contexts[:i])) >= limit:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts[:i-1]) +\n",
    "                prompt_end\n",
    "            )\n",
    "            break\n",
    "        elif i == len(contexts)-1:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts) +\n",
    "                prompt_end\n",
    "            )\n",
    "    return prompt\n",
    "\n",
    "_,col1,_ = st.columns([1,8,1])\n",
    "with col1: \n",
    "    form = st.form(key='myform')\n",
    "    query = form.text_input( \"Enter some text ðŸ‘‡\",\n",
    "        placeholder=\"Write your prompt here...\",\n",
    "    )\n",
    "    submit = form.form_submit_button('Submit')\n",
    "if submit:\n",
    "    # get context, additional info from pinecone\n",
    "    query_with_contexts = retrieve(query)\n",
    "    # call openai API\n",
    "    output = complete(query_with_contexts)\n",
    "\n",
    "    st.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160d4465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
